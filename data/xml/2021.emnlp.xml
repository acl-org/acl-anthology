<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.emnlp">
  <volume id="main" ingest-date="2021-11-03">
    <meta>
      <booktitle>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</booktitle>
      <editor><first>Marie-Francine</first><last>Moens</last></editor>
      <editor><first>Xuanjing</first><last>Huang</last></editor>
      <editor><first>Lucia</first><last>Specia</last></editor>
      <editor><first>Scott Wen-tau</first><last>Yih</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online and Punta Cana, Dominican Republic</address>
      <month>November</month>
      <year>2021</year>
      <url hash="94ae55c0">2021.emnlp-main</url>
      <venue>emnlp</venue>
    </meta>
    <frontmatter>
      <url hash="45ef6613">2021.emnlp-main.0</url>
      <bibkey>emnlp-2021</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>A</fixed-case>lig<fixed-case>NART</fixed-case>: Non-autoregressive Neural Machine Translation by Jointly Learning to Estimate Alignment and Translate</title>
      <author><first>Jongyoon</first><last>Song</last></author>
      <author><first>Sungwon</first><last>Kim</last></author>
      <author><first>Sungroh</first><last>Yoon</last></author>
      <pages>1–14</pages>
      <abstract>Non-autoregressive neural machine translation (NART) models suffer from the multi-modality problem which causes translation inconsistency such as token repetition. Most recent approaches have attempted to solve this problem by implicitly modeling dependencies between outputs. In this paper, we introduce AligNART, which leverages full alignment information to explicitly reduce the modality of the target distribution. AligNART divides the machine translation task into (i) alignment estimation and (ii) translation with aligned decoder inputs, guiding the decoder to focus on simplified one-to-one translation. To alleviate the alignment estimation problem, we further propose a novel alignment decomposition method. Our experiments show that AligNART outperforms previous non-iterative NART models that focus on explicit modality reduction on WMT14 En↔De and WMT16 Ro→En. Furthermore, AligNART achieves BLEU scores comparable to those of the state-of-the-art connectionist temporal classification based models on WMT14 En↔De. We also observe that AligNART effectively addresses the token repetition problem even without sequence-level knowledge distillation.</abstract>
      <url hash="0af5fdcc">2021.emnlp-main.1</url>
      <bibkey>song-etal-2021-alignart</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.1</doi>
      <video href="2021.emnlp-main.1.mp4"/>
    </paper>
    <paper id="2">
      <title>Zero-Shot Cross-Lingual Transfer of Neural Machine Translation with Multilingual Pretrained Encoders</title>
      <author><first>Guanhua</first><last>Chen</last></author>
      <author><first>Shuming</first><last>Ma</last></author>
      <author><first>Yun</first><last>Chen</last></author>
      <author><first>Li</first><last>Dong</last></author>
      <author><first>Dongdong</first><last>Zhang</last></author>
      <author><first>Jia</first><last>Pan</last></author>
      <author><first>Wenping</first><last>Wang</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <pages>15–26</pages>
      <abstract>Previous work mainly focuses on improving cross-lingual transfer for NLU tasks with a multilingual pretrained encoder (MPE), or improving the performance on supervised machine translation with BERT. However, it is under-explored that whether the MPE can help to facilitate the cross-lingual transferability of NMT model. In this paper, we focus on a zero-shot cross-lingual transfer task in NMT. In this task, the NMT model is trained with parallel dataset of only one language pair and an off-the-shelf MPE, then it is directly tested on zero-shot language pairs. We propose SixT, a simple yet effective model for this task. SixT leverages the MPE with a two-stage training schedule and gets further improvement with a position disentangled encoder and a capacity-enhanced decoder. Using this method, SixT significantly outperforms mBART, a pretrained multilingual encoder-decoder model explicitly designed for NMT, with an average improvement of 7.1 BLEU on zero-shot any-to-English test sets across 14 source languages. Furthermore, with much less training computation cost and training data, our model achieves better performance on 15 any-to-English test sets than CRISS and m2m-100, two strong multilingual NMT baselines.</abstract>
      <url hash="3327c149">2021.emnlp-main.2</url>
      <bibkey>chen-etal-2021-zero</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.2</doi>
      <video href="2021.emnlp-main.2.mp4"/>
      <pwccode url="https://github.com/ghchen18/emnlp2021-sixt" additional="false">ghchen18/emnlp2021-sixt</pwccode>
    </paper>
    <paper id="3">
      <title><fixed-case>ERNIE</fixed-case>-<fixed-case>M</fixed-case>: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora</title>
      <author><first>Xuan</first><last>Ouyang</last></author>
      <author><first>Shuohuan</first><last>Wang</last></author>
      <author><first>Chao</first><last>Pang</last></author>
      <author><first>Yu</first><last>Sun</last></author>
      <author><first>Hao</first><last>Tian</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <pages>27–38</pages>
      <abstract>Recent studies have demonstrated that pre-trained cross-lingual models achieve impressive performance in downstream cross-lingual tasks. This improvement benefits from learning a large amount of monolingual and parallel corpora. Although it is generally acknowledged that parallel corpora are critical for improving the model performance, existing methods are often constrained by the size of parallel corpora, especially for low-resource languages. In this paper, we propose Ernie-M, a new training method that encourages the model to align the representation of multiple languages with monolingual corpora, to overcome the constraint that the parallel corpus size places on the model performance. Our key insight is to integrate back-translation into the pre-training process. We generate pseudo-parallel sentence pairs on a monolingual corpus to enable the learning of semantic alignments between different languages, thereby enhancing the semantic modeling of cross-lingual models. Experimental results show that Ernie-M outperforms existing cross-lingual models and delivers new state-of-the-art results in various cross-lingual downstream tasks. The codes and pre-trained models will be made publicly available.</abstract>
      <url hash="0837dc82">2021.emnlp-main.3</url>
      <bibkey>ouyang-etal-2021-ernie</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.3</doi>
      <video href="2021.emnlp-main.3.mp4"/>
      <pwccode url="https://github.com/PaddlePaddle/ERNIE" additional="true">PaddlePaddle/ERNIE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xtreme">XTREME</pwcdataset>
    </paper>
    <paper id="4">
      <title>Cross Attention Augmented Transducer Networks for Simultaneous Translation</title>
      <author><first>Dan</first><last>Liu</last></author>
      <author><first>Mengge</first><last>Du</last></author>
      <author><first>Xiaoxi</first><last>Li</last></author>
      <author><first>Ya</first><last>Li</last></author>
      <author><first>Enhong</first><last>Chen</last></author>
      <pages>39–55</pages>
      <abstract>This paper proposes a novel architecture, Cross Attention Augmented Transducer (CAAT), for simultaneous translation. The framework aims to jointly optimize the policy and translation models. To effectively consider all possible READ-WRITE simultaneous translation action paths, we adapt the online automatic speech recognition (ASR) model, RNN-T, but remove the strong monotonic constraint, which is critical for the translation task to consider reordering. To make CAAT work, we introduce a novel latency loss whose expectation can be optimized by a forward-backward algorithm. We implement CAAT with Transformer while the general CAAT architecture can also be implemented with other attention-based encoder-decoder frameworks. Experiments on both speech-to-text (S2T) and text-to-text (T2T) simultaneous translation tasks show that CAAT achieves significantly better latency-quality trade-offs compared to the state-of-the-art simultaneous translation approaches.</abstract>
      <url hash="746f2138">2021.emnlp-main.4</url>
      <bibkey>liu-etal-2021-cross</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.4</doi>
      <video href="2021.emnlp-main.4.mp4"/>
      <pwccode url="https://github.com/danliu2/caat" additional="false">danliu2/caat</pwccode>
    </paper>
    <paper id="5">
      <title>Translating Headers of Tabular Data: A Pilot Study of Schema Translation</title>
      <author><first>Kunrui</first><last>Zhu</last></author>
      <author><first>Yan</first><last>Gao</last></author>
      <author><first>Jiaqi</first><last>Guo</last></author>
      <author><first>Jian-Guang</first><last>Lou</last></author>
      <pages>56–66</pages>
      <abstract>Schema translation is the task of automatically translating headers of tabular data from one language to another. High-quality schema translation plays an important role in cross-lingual table searching, understanding and analysis. Despite its importance, schema translation is not well studied in the community, and state-of-the-art neural machine translation models cannot work well on this task because of two intrinsic differences between plain text and tabular data: morphological difference and context difference. To facilitate the research study, we construct the first parallel dataset for schema translation, which consists of 3,158 tables with 11,979 headers written in 6 different languages, including English, Chinese, French, German, Spanish, and Japanese. Also, we propose the first schema translation model called CAST, which is a header-to-header neural machine translation model augmented with schema context. Specifically, we model a target header and its context as a directed graph to represent their entity types and relations. Then CAST encodes the graph with a relational-aware transformer and uses another transformer to decode the header in the target language. Experiments on our dataset demonstrate that CAST significantly outperforms state-of-the-art neural machine translation models. Our dataset will be released at https://github.com/microsoft/ContextualSP.</abstract>
      <url hash="401bcc32">2021.emnlp-main.5</url>
      <bibkey>zhu-etal-2021-translating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.5</doi>
      <video href="2021.emnlp-main.5.mp4"/>
      <pwccode url="https://github.com/microsoft/ContextualSP" additional="false">microsoft/ContextualSP</pwccode>
    </paper>
    <paper id="6">
      <title>Towards Making the Most of Dialogue Characteristics for Neural Chat Translation</title>
      <author><first>Yunlong</first><last>Liang</last></author>
      <author><first>Chulun</first><last>Zhou</last></author>
      <author><first>Fandong</first><last>Meng</last></author>
      <author><first>Jinan</first><last>Xu</last></author>
      <author><first>Yufeng</first><last>Chen</last></author>
      <author><first>Jinsong</first><last>Su</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>67–79</pages>
      <abstract>Neural Chat Translation (NCT) aims to translate conversational text between speakers of different languages. Despite the promising performance of sentence-level and context-aware neural machine translation models, there still remain limitations in current NCT models because the inherent dialogue characteristics of chat, such as dialogue coherence and speaker personality, are neglected. In this paper, we propose to promote the chat translation by introducing the modeling of dialogue characteristics into the NCT model. To this end, we design four auxiliary tasks including monolingual response generation, cross-lingual response generation, next utterance discrimination, and speaker identification. Together with the main chat translation task, we optimize the enhanced NCT model through the training objectives of all these tasks. By this means, the NCT model can be enhanced by capturing the inherent dialogue characteristics, thus generating more coherent and speaker-relevant translations. Comprehensive experiments on four language directions (English&lt;-&gt;German and English&lt;-&gt;Chinese) verify the effectiveness and superiority of the proposed approach.</abstract>
      <url hash="c9f3412a">2021.emnlp-main.6</url>
      <bibkey>liang-etal-2021-towards</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.6</doi>
      <video href="2021.emnlp-main.6.mp4"/>
      <pwccode url="https://github.com/xl2248/csa-nct" additional="false">xl2248/csa-nct</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bmeld">BMELD</pwcdataset>
    </paper>
    <paper id="7">
      <title>Low-Resource Dialogue Summarization with Domain-Agnostic Multi-Source Pretraining</title>
      <author><first>Yicheng</first><last>Zou</last></author>
      <author><first>Bolin</first><last>Zhu</last></author>
      <author><first>Xingwu</first><last>Hu</last></author>
      <author><first>Tao</first><last>Gui</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <pages>80–91</pages>
      <abstract>With the rapid increase in the volume of dialogue data from daily life, there is a growing demand for dialogue summarization. Unfortunately, training a large summarization model is generally infeasible due to the inadequacy of dialogue data with annotated summaries. Most existing works for low-resource dialogue summarization directly pretrain models in other domains, e.g., the news domain, but they generally neglect the huge difference between dialogues and conventional articles. To bridge the gap between out-of-domain pretraining and in-domain fine-tuning, in this work, we propose a multi-source pretraining paradigm to better leverage the external summary data. Specifically, we exploit large-scale in-domain non-summary data to separately pretrain the dialogue encoder and the summary decoder. The combined encoder-decoder model is then pretrained on the out-of-domain summary data using adversarial critics, aiming to facilitate domain-agnostic summarization. The experimental results on two public datasets show that with only limited training data, our approach achieves competitive performance and generalizes well in different dialogue scenarios.</abstract>
      <url hash="c734b477">2021.emnlp-main.7</url>
      <bibkey>zou-etal-2021-low</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.7</doi>
      <video href="2021.emnlp-main.7.mp4"/>
      <pwccode url="https://github.com/rowitzou/dams" additional="false">rowitzou/dams</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reddit-conversation-corpus">Reddit Conversation Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/samsum-corpus">SAMSum Corpus</pwcdataset>
    </paper>
    <paper id="8">
      <title>Controllable Neural Dialogue Summarization with Personal Named Entity Planning</title>
      <author><first>Zhengyuan</first><last>Liu</last></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <pages>92–106</pages>
      <abstract>In this paper, we propose a controllable neural generation framework that can flexibly guide dialogue summarization with personal named entity planning. The conditional sequences are modulated to decide what types of information or what perspective to focus on when forming summaries to tackle the under-constrained problem in summarization tasks. This framework supports two types of use cases: (1) Comprehensive Perspective, which is a general-purpose case with no user-preference specified, considering summary points from all conversational interlocutors and all mentioned persons; (2) Focus Perspective, positioning the summary based on a user-specified personal named entity, which could be one of the interlocutors or one of the persons mentioned in the conversation. During training, we exploit occurrence planning of personal named entities and coreference information to improve temporal coherence and to minimize hallucination in neural generation. Experimental results show that our proposed framework generates fluent and factually consistent summaries under various planning controls using both objective metrics and human evaluations.</abstract>
      <url hash="8adb9328">2021.emnlp-main.8</url>
      <bibkey>liu-chen-2021-controllable</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.8</doi>
      <video href="2021.emnlp-main.8.mp4"/>
      <pwccode url="https://github.com/seq-to-mind/planning_dial_summ" additional="false">seq-to-mind/planning_dial_summ</pwccode>
    </paper>
    <paper id="9">
      <title>Fine-grained Factual Consistency Assessment for Abstractive Summarization Models</title>
      <author><first>Sen</first><last>Zhang</last></author>
      <author><first>Jianwei</first><last>Niu</last></author>
      <author><first>Chuyuan</first><last>Wei</last></author>
      <pages>107–116</pages>
      <abstract>Factual inconsistencies existed in the output of abstractive summarization models with original documents are frequently presented. Fact consistency assessment requires the reasoning capability to find subtle clues to identify whether a model-generated summary is consistent with the original document. This paper proposes a fine-grained two-stage Fact Consistency assessment framework for Summarization models (SumFC). Given a document and a summary sentence, in the first stage, SumFC selects the top-K most relevant sentences with the summary sentence from the document. In the second stage, the model performs fine-grained consistency reasoning at the sentence level, and then aggregates all sentences’ consistency scores to obtain the final assessment result. We get the training data pairs by data synthesis and adopt contrastive loss of data pairs to help the model identify subtle cues. Experiment results show that SumFC has made a significant improvement over the previous state-of-the-art methods. Our experiments also indicate that SumFC distinguishes detailed differences better.</abstract>
      <url hash="95b1e684">2021.emnlp-main.9</url>
      <bibkey>zhang-etal-2021-fine-grained</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.9</doi>
      <video href="2021.emnlp-main.9.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="10">
      <title>Decision-Focused Summarization</title>
      <author><first>Chao-Chun</first><last>Hsu</last></author>
      <author><first>Chenhao</first><last>Tan</last></author>
      <pages>117–132</pages>
      <abstract>Relevance in summarization is typically de- fined based on textual information alone, without incorporating insights about a particular decision. As a result, to support risk analysis of pancreatic cancer, summaries of medical notes may include irrelevant information such as a knee injury. We propose a novel problem, decision-focused summarization, where the goal is to summarize relevant information for a decision. We leverage a predictive model that makes the decision based on the full text to provide valuable insights on how a decision can be inferred from text. To build a summary, we then select representative sentences that lead to similar model decisions as using the full text while accounting for textual non-redundancy. To evaluate our method (DecSum), we build a testbed where the task is to summarize the first ten reviews of a restaurant in support of predicting its future rating on Yelp. DecSum substantially outperforms text-only summarization methods and model-based explanation methods in decision faithfulness and representativeness. We further demonstrate that DecSum is the only method that enables humans to outperform random chance in predicting which restaurant will be better rated in the future.</abstract>
      <url hash="77f04c29">2021.emnlp-main.10</url>
      <bibkey>hsu-tan-2021-decision</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.10</doi>
      <video href="2021.emnlp-main.10.mp4"/>
      <pwccode url="https://github.com/chicagohai/decsum" additional="false">chicagohai/decsum</pwccode>
    </paper>
    <paper id="11">
      <title>Multiplex Graph Neural Network for Extractive Text Summarization</title>
      <author><first>Baoyu</first><last>Jing</last></author>
      <author><first>Zeyu</first><last>You</last></author>
      <author><first>Tao</first><last>Yang</last></author>
      <author><first>Wei</first><last>Fan</last></author>
      <author><first>Hanghang</first><last>Tong</last></author>
      <pages>133–139</pages>
      <abstract>Extractive text summarization aims at extracting the most representative sentences from a given document as its summary. To extract a good summary from a long text document, sentence embedding plays an important role. Recent studies have leveraged graph neural networks to capture the inter-sentential relationship (e.g., the discourse graph) within the documents to learn contextual sentence embedding. However, those approaches neither consider multiple types of inter-sentential relationships (e.g., semantic similarity and natural connection relationships), nor model intra-sentential relationships (e.g, semantic similarity and syntactic relationship among words). To address these problems, we propose a novel Multiplex Graph Convolutional Network (Multi-GCN) to jointly model different types of relationships among sentences and words. Based on Multi-GCN, we propose a Multiplex Graph Summarization (Multi-GraS) model for extractive text summarization. Finally, we evaluate the proposed models on the CNN/DailyMail benchmark dataset to demonstrate effectiveness of our method.</abstract>
      <url hash="3c8bcdb2">2021.emnlp-main.11</url>
      <bibkey>jing-etal-2021-multiplex</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.11</doi>
      <video href="2021.emnlp-main.11.mp4"/>
    </paper>
    <paper id="12">
      <title>A Thorough Evaluation of Task-Specific Pretraining for Summarization</title>
      <author><first>Sascha</first><last>Rothe</last></author>
      <author><first>Joshua</first><last>Maynez</last></author>
      <author><first>Shashi</first><last>Narayan</last></author>
      <pages>140–145</pages>
      <abstract>Task-agnostic pretraining objectives like masked language models or corrupted span prediction are applicable to a wide range of NLP downstream tasks (Raffel et al.,2019), but are outperformed by task-specific pretraining objectives like predicting extracted gap sentences on summarization (Zhang et al.,2020). We compare three summarization specific pretraining objectives with the task agnostic corrupted span prediction pretraining in controlled study. We also extend our study to a low resource and zero shot setup, to understand how many training examples are needed in order to ablate the task-specific pretraining without quality loss. Our results show that task-agnostic pretraining is sufficient for most cases which hopefully reduces the need for costly task-specific pretraining. We also report new state-of-the-art number for two summarization task using a T5 model with 11 billion parameters and an optimal beam search length penalty.</abstract>
      <url hash="5082584d">2021.emnlp-main.12</url>
      <bibkey>rothe-etal-2021-thorough</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.12</doi>
      <video href="2021.emnlp-main.12.mp4"/>
    </paper>
    <paper id="13">
      <title><fixed-case>HETFORMER</fixed-case>: Heterogeneous Transformer with Sparse Attention for Long-Text Extractive Summarization</title>
      <author><first>Ye</first><last>Liu</last></author>
      <author><first>Jianguo</first><last>Zhang</last></author>
      <author><first>Yao</first><last>Wan</last></author>
      <author><first>Congying</first><last>Xia</last></author>
      <author><first>Lifang</first><last>He</last></author>
      <author><first>Philip</first><last>Yu</last></author>
      <pages>146–154</pages>
      <abstract>To capture the semantic graph structure from raw text, most existing summarization approaches are built on GNNs with a pre-trained model. However, these methods suffer from cumbersome procedures and inefficient computations for long-text documents. To mitigate these issues, this paper proposes HetFormer, a Transformer-based pre-trained model with multi-granularity sparse attentions for long-text extractive summarization. Specifically, we model different types of semantic nodes in raw text as a potential heterogeneous graph and directly learn heterogeneous relationships (edges) among nodes by Transformer. Extensive experiments on both single- and multi-document summarization tasks show that HetFormer achieves state-of-the-art performance in Rouge F1 while using less memory and fewer parameters.</abstract>
      <url hash="c660f213">2021.emnlp-main.13</url>
      <attachment type="Software" hash="41b48d72">2021.emnlp-main.13.Software.zip</attachment>
      <bibkey>liu-etal-2021-hetformer</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.13</doi>
      <video href="2021.emnlp-main.13.mp4"/>
      <pwccode url="https://github.com/yeliu918/hetformer" additional="false">yeliu918/hetformer</pwccode>
    </paper>
    <paper id="14">
      <title>Unsupervised Keyphrase Extraction by Jointly Modeling Local and Global Context</title>
      <author><first>Xinnian</first><last>Liang</last></author>
      <author><first>Shuangzhi</first><last>Wu</last></author>
      <author><first>Mu</first><last>Li</last></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <pages>155–164</pages>
      <abstract>Embedding based methods are widely used for unsupervised keyphrase extraction (UKE) tasks. Generally, these methods simply calculate similarities between phrase embeddings and document embedding, which is insufficient to capture different context for a more effective UKE model. In this paper, we propose a novel method for UKE, where local and global contexts are jointly modeled. From a global view, we calculate the similarity between a certain phrase and the whole document in the vector space as transitional embedding based models do. In terms of the local view, we first build a graph structure based on the document where phrases are regarded as vertices and the edges are similarities between vertices. Then, we proposed a new centrality computation method to capture local salient information based on the graph structure. Finally, we further combine the modeling of global and local context for ranking. We evaluate our models on three public benchmarks (Inspec, DUC 2001, SemEval 2010) and compare with existing state-of-the-art models. The results show that our model outperforms most models while generalizing better on input documents with different domains and length. Additional ablation study shows that both the local and global information is crucial for unsupervised keyphrase extraction tasks.</abstract>
      <url hash="ebe6c57b">2021.emnlp-main.14</url>
      <bibkey>liang-etal-2021-unsupervised</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.14</doi>
      <video href="2021.emnlp-main.14.mp4"/>
      <pwccode url="https://github.com/xnliang98/uke_ccrank" additional="false">xnliang98/uke_ccrank</pwccode>
    </paper>
    <paper id="15">
      <title>Distantly Supervised Relation Extraction using Multi-Layer Revision Network and Confidence-based Multi-Instance Learning</title>
      <author><first>Xiangyu</first><last>Lin</last></author>
      <author><first>Tianyi</first><last>Liu</last></author>
      <author><first>Weijia</first><last>Jia</last></author>
      <author><first>Zhiguo</first><last>Gong</last></author>
      <pages>165–174</pages>
      <abstract>Distantly supervised relation extraction is widely used in the construction of knowledge bases due to its high efficiency. However, the automatically obtained instances are of low quality with numerous irrelevant words. In addition, the strong assumption of distant supervision leads to the existence of noisy sentences in the sentence bags. In this paper, we propose a novel Multi-Layer Revision Network (MLRN) which alleviates the effects of word-level noise by emphasizing inner-sentence correlations before extracting relevant information within sentences. Then, we devise a balanced and noise-resistant Confidence-based Multi-Instance Learning (CMIL) method to filter out noisy sentences as well as assign proper weights to relevant ones. Extensive experiments on two New York Times (NYT) datasets demonstrate that our approach achieves significant improvements over the baselines.</abstract>
      <url hash="38c85344">2021.emnlp-main.15</url>
      <bibkey>lin-etal-2021-distantly</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.15</doi>
      <video href="2021.emnlp-main.15.mp4"/>
    </paper>
    <paper id="16">
      <title>Logic-level Evidence Retrieval and Graph-based Verification Network for Table-based Fact Verification</title>
      <author><first>Qi</first><last>Shi</last></author>
      <author><first>Yu</first><last>Zhang</last></author>
      <author><first>Qingyu</first><last>Yin</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <pages>175–184</pages>
      <abstract>Table-based fact verification task aims to verify whether the given statement is supported by the given semi-structured table. Symbolic reasoning with logical operations plays a crucial role in this task. Existing methods leverage programs that contain rich logical information to enhance the verification process. However, due to the lack of fully supervised signals in the program generation process, spurious programs can be derived and employed, which leads to the inability of the model to catch helpful logical operations. To address the aforementioned problems, in this work, we formulate the table-based fact verification task as an evidence retrieval and reasoning framework, proposing the Logic-level Evidence Retrieval and Graph-based Verification network (LERGV). Specifically, we first retrieve logic-level program-like evidence from the given table and statement as supplementary evidence for the table. After that, we construct a logic-level graph to capture the logical relations between entities and functions in the retrieved evidence, and design a graph-based verification network to perform logic-level graph-based reasoning based on the constructed graph to classify the final entailment relation. Experimental results on the large-scale benchmark TABFACT show the effectiveness of the proposed approach.</abstract>
      <url hash="978f6b9a">2021.emnlp-main.16</url>
      <bibkey>shi-etal-2021-logic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.16</doi>
      <video href="2021.emnlp-main.16.mp4"/>
      <pwccode url="https://github.com/qshi95/lergv" additional="false">qshi95/lergv</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tabfact">TabFact</pwcdataset>
    </paper>
    <paper id="17">
      <title>A Partition Filter Network for Joint Entity and Relation Extraction</title>
      <author><first>Zhiheng</first><last>Yan</last></author>
      <author><first>Chong</first><last>Zhang</last></author>
      <author><first>Jinlan</first><last>Fu</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Zhongyu</first><last>Wei</last></author>
      <pages>185–197</pages>
      <abstract>In joint entity and relation extraction, existing work either sequentially encode task-specific features, leading to an imbalance in inter-task feature interaction where features extracted later have no direct contact with those that come first. Or they encode entity features and relation features in a parallel manner, meaning that feature representation learning for each task is largely independent of each other except for input sharing. We propose a partition filter network to model two-way interaction between tasks properly, where feature encoding is decomposed into two steps: partition and filter. In our encoder, we leverage two gates: entity and relation gate, to segment neurons into two task partitions and one shared partition. The shared partition represents inter-task information valuable to both tasks and is evenly shared across two tasks to ensure proper two-way interaction. The task partitions represent intra-task information and are formed through concerted efforts of both gates, making sure that encoding of task-specific features is dependent upon each other. Experiment results on six public datasets show that our model performs significantly better than previous approaches. In addition, contrary to what previous work has claimed, our auxiliary experiments suggest that relation prediction is contributory to named entity prediction in a non-negligible way. The source code can be found at https://github.com/Coopercoppers/PFN.</abstract>
      <url hash="5183a8fd">2021.emnlp-main.17</url>
      <bibkey>yan-etal-2021-partition</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.17</doi>
      <video href="2021.emnlp-main.17.mp4"/>
      <pwccode url="https://github.com/Coopercoppers/PFN" additional="false">Coopercoppers/PFN</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ace-2004">ACE 2004</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ace-2005">ACE 2005</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ade-corpus">Adverse Drug Events (ADE) Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scierc">SciERC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webnlg">WebNLG</pwcdataset>
    </paper>
    <paper id="18">
      <title><fixed-case>TEBNER</fixed-case>: Domain Specific Named Entity Recognition with Type Expanded Boundary-aware Network</title>
      <author><first>Zheng</first><last>Fang</last></author>
      <author><first>Yanan</first><last>Cao</last></author>
      <author><first>Tai</first><last>Li</last></author>
      <author><first>Ruipeng</first><last>Jia</last></author>
      <author><first>Fang</first><last>Fang</last></author>
      <author><first>Yanmin</first><last>Shang</last></author>
      <author><first>Yuhai</first><last>Lu</last></author>
      <pages>198–207</pages>
      <abstract>To alleviate label scarcity in Named Entity Recognition (NER) task, distantly supervised NER methods are widely applied to automatically label data and identify entities. Although the human effort is reduced, the generated incomplete and noisy annotations pose new challenges for learning effective neural models. In this paper, we propose a novel dictionary extension method which extracts new entities through the type expanded model. Moreover, we design a multi-granularity boundary-aware network which detects entity boundaries from both local and global perspectives. We conduct experiments on different types of datasets, the results show that our model outperforms previous state-of-the-art distantly supervised systems and even surpasses the supervised models.</abstract>
      <url hash="a51fa8fe">2021.emnlp-main.18</url>
      <bibkey>fang-etal-2021-tebner</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.18</doi>
      <video href="2021.emnlp-main.18.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/bc5cdr">BC5CDR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ncbi-disease-1">NCBI Disease</pwcdataset>
    </paper>
    <paper id="19">
      <title>Beta Distribution Guided Aspect-aware Graph for Aspect Category Sentiment Analysis with Affective Knowledge</title>
      <author><first>Bin</first><last>Liang</last></author>
      <author><first>Hang</first><last>Su</last></author>
      <author><first>Rongdi</first><last>Yin</last></author>
      <author><first>Lin</first><last>Gui</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <author><first>Qin</first><last>Zhao</last></author>
      <author><first>Xiaoqi</first><last>Yu</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <pages>208–218</pages>
      <abstract>In this paper, we investigate the Aspect Category Sentiment Analysis (ACSA) task from a novel perspective by exploring a Beta Distribution guided aspect-aware graph construction based on external knowledge. That is, we are no longer entangled about how to laboriously search the sentiment clues for coarse-grained aspects from the context, but how to preferably find the words highly related to the aspects in the context and determine their importance based on the public knowledge base. In this way, the contextual sentiment clues can be explicitly tracked in ACSA for the aspects in the light of these aspect-related words. To be specific, we first regard each aspect as a pivot to derive aspect-aware words that are highly related to the aspect from external affective commonsense knowledge. Then, we employ Beta Distribution to educe the aspect-aware weight, which reflects the importance to the aspect, for each aspect-aware word. Afterward, the aspect-aware words are served as the substitutes of the coarse-grained aspect to construct graphs for leveraging the aspect-related contextual sentiment dependencies in ACSA. Experiments on 6 benchmark datasets show that our approach significantly outperforms the state-of-the-art baseline methods.</abstract>
      <url hash="ae82f426">2021.emnlp-main.19</url>
      <attachment type="Software" hash="48239545">2021.emnlp-main.19.Software.zip</attachment>
      <bibkey>liang-etal-2021-beta</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.19</doi>
      <video href="2021.emnlp-main.19.mp4"/>
      <pwccode url="https://github.com/binliang-nlp/aagcn-acsa" additional="true">binliang-nlp/aagcn-acsa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="20">
      <title><fixed-case>DILBERT</fixed-case>: Customized Pre-Training for Domain Adaptation with Category Shift, with an Application to Aspect Extraction</title>
      <author><first>Entony</first><last>Lekhtman</last></author>
      <author><first>Yftah</first><last>Ziser</last></author>
      <author><first>Roi</first><last>Reichart</last></author>
      <pages>219–230</pages>
      <abstract>The rise of pre-trained language models has yielded substantial progress in the vast majority of Natural Language Processing (NLP) tasks. However, a generic approach towards the pre-training procedure can naturally be sub-optimal in some cases. Particularly, fine-tuning a pre-trained language model on a source domain and then applying it to a different target domain, results in a sharp performance decline of the eventual classifier for many source-target domain pairs. Moreover, in some NLP tasks, the output categories substantially differ between domains, making adaptation even more challenging. This, for example, happens in the task of aspect extraction, where the aspects of interest of reviews of, e.g., restaurants or electronic devices may be very different. This paper presents a new fine-tuning scheme for BERT, which aims to address the above challenges. We name this scheme DILBERT: Domain Invariant Learning with BERT, and customize it for aspect extraction in the unsupervised domain adaptation setting. DILBERT harnesses the categorical information of both the source and the target domains to guide the pre-training process towards a more domain and category invariant representation, thus closing the gap between the domains. We show that DILBERT yields substantial improvements over state-of-the-art baselines while using a fraction of the unlabeled data, particularly in more challenging domain adaptation setups.</abstract>
      <url hash="be594994">2021.emnlp-main.20</url>
      <bibkey>lekhtman-etal-2021-dilbert</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.20</doi>
      <video href="2021.emnlp-main.20.mp4"/>
      <pwccode url="https://github.com/tonylekhtman/dilbert" additional="false">tonylekhtman/dilbert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mams">MAMS</pwcdataset>
    </paper>
    <paper id="21">
      <title>Improving Multimodal fusion via Mutual Dependency Maximisation</title>
      <author><first>Pierre</first><last>Colombo</last></author>
      <author><first>Emile</first><last>Chapuis</last></author>
      <author><first>Matthieu</first><last>Labeau</last></author>
      <author><first>Chloé</first><last>Clavel</last></author>
      <pages>231–245</pages>
      <abstract>Multimodal sentiment analysis is a trending area of research, and multimodal fusion is one of its most active topic. Acknowledging humans communicate through a variety of channels (i.e visual, acoustic, linguistic), multimodal systems aim at integrating different unimodal representations into a synthetic one. So far, a consequent effort has been made on developing complex architectures allowing the fusion of these modalities. However, such systems are mainly trained by minimising simple losses such as <tex-math>L_1</tex-math> or cross-entropy. In this work, we investigate unexplored penalties and propose a set of new objectives that measure the dependency between modalities. We demonstrate that our new penalties lead to a consistent improvement (up to 4.3 on accuracy) across a large variety of state-of-the-art models on two well-known sentiment analysis datasets: CMU-MOSI and CMU-MOSEI. Our method not only achieves a new SOTA on both datasets but also produces representations that are more robust to modality drops. Finally, a by-product of our methods includes a statistical network which can be used to interpret the high dimensional representations learnt by the model.</abstract>
      <url hash="939c7b0b">2021.emnlp-main.21</url>
      <bibkey>colombo-etal-2021-improving</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.21</doi>
      <video href="2021.emnlp-main.21.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cmu-mosei">CMU-MOSEI</pwcdataset>
    </paper>
    <paper id="22">
      <title>Learning Implicit Sentiment in Aspect-based Sentiment Analysis with Supervised Contrastive Pre-Training</title>
      <author><first>Zhengyan</first><last>Li</last></author>
      <author><first>Yicheng</first><last>Zou</last></author>
      <author><first>Chong</first><last>Zhang</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Zhongyu</first><last>Wei</last></author>
      <pages>246–256</pages>
      <abstract>Aspect-based sentiment analysis aims to identify the sentiment polarity of a specific aspect in product reviews. We notice that about 30% of reviews do not contain obvious opinion words, but still convey clear human-aware sentiment orientation, which is known as implicit sentiment. However, recent neural network-based approaches paid little attention to implicit sentiment entailed in the reviews. To overcome this issue, we adopt Supervised Contrastive Pre-training on large-scale sentiment-annotated corpora retrieved from in-domain language resources. By aligning the representation of implicit sentiment expressions to those with the same sentiment label, the pre-training process leads to better capture of both implicit and explicit sentiment orientation towards aspects in reviews. Experimental results show that our method achieves state-of-the-art performance on SemEval2014 benchmarks, and comprehensive analysis validates its effectiveness on learning implicit sentiment.</abstract>
      <url hash="9f0f5e71">2021.emnlp-main.22</url>
      <bibkey>li-etal-2021-learning-implicit</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.22</doi>
      <video href="2021.emnlp-main.22.mp4"/>
      <pwccode url="https://github.com/tribleave/scapt-absa" additional="false">tribleave/scapt-absa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mams">MAMS</pwcdataset>
    </paper>
    <paper id="23">
      <title>Progressive Self-Training with Discriminator for Aspect Term Extraction</title>
      <author><first>Qianlong</first><last>Wang</last></author>
      <author><first>Zhiyuan</first><last>Wen</last></author>
      <author><first>Qin</first><last>Zhao</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <pages>257–268</pages>
      <abstract>Aspect term extraction aims to extract aspect terms from a review sentence that users have expressed opinions on. One of the remaining challenges for aspect term extraction resides in the lack of sufficient annotated data. While self-training is potentially an effective method to address this issue, the pseudo-labels it yields on unlabeled data could induce noise. In this paper, we use two means to alleviate the noise in the pseudo-labels. One is that inspired by the curriculum learning, we refine the conventional self-training to progressive self-training. Specifically, the base model infers pseudo-labels on a progressive subset at each iteration, where samples in the subset become harder and more numerous as the iteration proceeds. The other is that we use a discriminator to filter the noisy pseudo-labels. Experimental results on four SemEval datasets show that our model significantly outperforms the previous baselines and achieves state-of-the-art performance.</abstract>
      <url hash="9ceb2642">2021.emnlp-main.23</url>
      <bibkey>wang-etal-2021-progressive</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.23</doi>
      <video href="2021.emnlp-main.23.mp4"/>
    </paper>
    <paper id="24">
      <title>Reinforced Counterfactual Data Augmentation for Dual Sentiment Classification</title>
      <author><first>Hao</first><last>Chen</last></author>
      <author><first>Rui</first><last>Xia</last></author>
      <author><first>Jianfei</first><last>Yu</last></author>
      <pages>269–278</pages>
      <abstract>Data augmentation and adversarial perturbation approaches have recently achieved promising results in solving the over-fitting problem in many natural language processing (NLP) tasks including sentiment classification. However, existing studies aimed to improve the generalization ability by augmenting the training data with synonymous examples or adding random noises to word embeddings, which cannot address the spurious association problem. In this work, we propose an end-to-end reinforcement learning framework, which jointly performs counterfactual data generation and dual sentiment classification. Our approach has three characteristics:1) the generator automatically generates massive and diverse antonymous sentences; 2) the discriminator contains a original-side sentiment predictor and an antonymous-side sentiment predictor, which jointly evaluate the quality of the generated sample and help the generator iteratively generate higher-quality antonymous samples; 3) the discriminator is directly used as the final sentiment classifier without the need to build an extra one. Extensive experiments show that our approach outperforms strong data augmentation baselines on several benchmark sentiment classification datasets. Further analysis confirms our approach’s advantages in generating more diverse training samples and solving the spurious association problem in sentiment classification.</abstract>
      <url hash="83269a66">2021.emnlp-main.24</url>
      <bibkey>chen-etal-2021-reinforced</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.24</doi>
      <video href="2021.emnlp-main.24.mp4"/>
      <pwccode url="https://github.com/nustm/rcda" additional="false">nustm/rcda</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="25">
      <title>Idiosyncratic but not Arbitrary: Learning Idiolects in Online Registers Reveals Distinctive yet Consistent Individual Styles</title>
      <author><first>Jian</first><last>Zhu</last></author>
      <author><first>David</first><last>Jurgens</last></author>
      <pages>279–297</pages>
      <abstract>An individual’s variation in writing style is often a function of both social and personal attributes. While structured social variation has been extensively studied, e.g., gender based variation, far less is known about how to characterize individual styles due to their idiosyncratic nature. We introduce a new approach to studying idiolects through a massive cross-author comparison to identify and encode stylistic features. The neural model achieves strong performance at authorship identification on short texts and through an analogy-based probing task, showing that the learned representations exhibit surprising regularities that encode qualitative and quantitative shifts of idiolectal styles. Through text perturbation, we quantify the relative contributions of different linguistic elements to idiolectal variation. Furthermore, we provide a description of idiolects through measuring inter- and intra-author variation, showing that variation in idiolects is often distinctive yet consistent.</abstract>
      <url hash="5bfd7e81">2021.emnlp-main.25</url>
      <bibkey>zhu-jurgens-2021-idiosyncratic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.25</doi>
      <video href="2021.emnlp-main.25.mp4"/>
      <pwccode url="https://github.com/lingjzhu/idiolect" additional="false">lingjzhu/idiolect</pwccode>
    </paper>
    <paper id="26">
      <title>Narrative Theory for Computational Narrative Understanding</title>
      <author><first>Andrew</first><last>Piper</last></author>
      <author><first>Richard Jean</first><last>So</last></author>
      <author><first>David</first><last>Bamman</last></author>
      <pages>298–311</pages>
      <abstract>Over the past decade, the field of natural language processing has developed a wide array of computational methods for reasoning about narrative, including summarization, commonsense inference, and event detection. While this work has brought an important empirical lens for examining narrative, it is by and large divorced from the large body of theoretical work on narrative within the humanities, social and cognitive sciences. In this position paper, we introduce the dominant theoretical frameworks to the NLP community, situate current research in NLP within distinct narratological traditions, and argue that linking computational work in NLP to theory opens up a range of new empirical questions that would both help advance our understanding of narrative and open up new practical applications.</abstract>
      <url hash="05dacdfe">2021.emnlp-main.26</url>
      <bibkey>piper-etal-2021-narrative</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.26</doi>
      <video href="2021.emnlp-main.26.mp4"/>
    </paper>
    <paper id="27">
      <title>(Mis)alignment Between Stance Expressed in Social Media Data and Public Opinion Surveys</title>
      <author><first>Kenneth</first><last>Joseph</last></author>
      <author><first>Sarah</first><last>Shugars</last></author>
      <author><first>Ryan</first><last>Gallagher</last></author>
      <author><first>Jon</first><last>Green</last></author>
      <author><first>Alexi</first><last>Quintana Mathé</last></author>
      <author><first>Zijian</first><last>An</last></author>
      <author><first>David</first><last>Lazer</last></author>
      <pages>312–324</pages>
      <abstract>Stance detection, which aims to determine whether an individual is for or against a target concept, promises to uncover public opinion from large streams of social media data. Yet even human annotation of social media content does not always capture “stance” as measured by public opinion polls. We demonstrate this by directly comparing an individual’s self-reported stance to the stance inferred from their social media data. Leveraging a longitudinal public opinion survey with respondent Twitter handles, we conducted this comparison for 1,129 individuals across four salient targets. We find that recall is high for both “Pro’’ and “Anti’’ stance classifications but precision is variable in a number of cases. We identify three factors leading to the disconnect between text and author stance: temporal inconsistencies, differences in constructs, and measurement errors from both survey respondents and annotators. By presenting a framework for assessing the limitations of stance detection models, this work provides important insight into what stance detection truly measures.</abstract>
      <url hash="6f707f4e">2021.emnlp-main.27</url>
      <attachment type="Software" hash="048b3fc4">2021.emnlp-main.27.Software.zip</attachment>
      <bibkey>joseph-etal-2021-mis</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.27</doi>
      <video href="2021.emnlp-main.27.mp4"/>
    </paper>
    <paper id="28">
      <title>How Does Counterfactually Augmented Data Impact Models for Social Computing Constructs?</title>
      <author><first>Indira</first><last>Sen</last></author>
      <author><first>Mattia</first><last>Samory</last></author>
      <author><first>Fabian</first><last>Flöck</last></author>
      <author><first>Claudia</first><last>Wagner</last></author>
      <author><first>Isabelle</first><last>Augenstein</last></author>
      <pages>325–344</pages>
      <abstract>As NLP models are increasingly deployed in socially situated settings such as online abusive content detection, it is crucial to ensure that these models are robust. One way of improving model robustness is to generate counterfactually augmented data (CAD) for training models that can better learn to distinguish between core features and data artifacts. While models trained on this type of data have shown promising out-of-domain generalizability, it is still unclear what the sources of such improvements are. We investigate the benefits of CAD for social NLP models by focusing on three social computing constructs — sentiment, sexism, and hate speech. Assessing the performance of models trained with and without CAD across different types of datasets, we find that while models trained on CAD show lower in-domain performance, they generalize better out-of-domain. We unpack this apparent discrepancy using machine explanations and find that CAD reduces model reliance on spurious features. Leveraging a novel typology of CAD to analyze their relationship with model performance, we find that CAD which acts on the construct directly or a diverse set of CAD leads to higher performance.</abstract>
      <url hash="a81e529a">2021.emnlp-main.28</url>
      <bibkey>sen-etal-2021-counterfactually</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.28</doi>
      <video href="2021.emnlp-main.28.mp4"/>
      <pwccode url="https://github.com/gesiscss/socialcad" additional="false">gesiscss/socialcad</pwccode>
    </paper>
    <paper id="29">
      <title>Latent Hatred: A Benchmark for Understanding Implicit Hate Speech</title>
      <author><first>Mai</first><last>ElSherief</last></author>
      <author><first>Caleb</first><last>Ziems</last></author>
      <author><first>David</first><last>Muchlinski</last></author>
      <author><first>Vaishnavi</first><last>Anupindi</last></author>
      <author><first>Jordyn</first><last>Seybolt</last></author>
      <author><first>Munmun</first><last>De Choudhury</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>345–363</pages>
      <abstract>Hate speech has grown significantly on social media, causing serious consequences for victims of all demographics. Despite much attention being paid to characterize and detect discriminatory speech, most work has focused on explicit or overt hate speech, failing to address a more pervasive form based on coded or indirect language. To fill this gap, this work introduces a theoretically-justified taxonomy of implicit hate speech and a benchmark corpus with fine-grained labels for each message and its implication. We present systematic analyses of our dataset using contemporary baselines to detect and explain implicit hate speech, and we discuss key features that challenge existing models. This dataset will continue to serve as a useful benchmark for understanding this multifaceted issue.</abstract>
      <url hash="badbed00">2021.emnlp-main.29</url>
      <bibkey>elsherief-etal-2021-latent</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.29</doi>
      <video href="2021.emnlp-main.29.mp4"/>
      <pwccode url="https://github.com/gt-salt/implicit-hate" additional="false">gt-salt/implicit-hate</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/implicit-hate">Implicit Hate</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-speech">Hate Speech</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webtext">WebText</pwcdataset>
    </paper>
    <paper id="30">
      <title>Distilling Linguistic Context for Language Model Compression</title>
      <author><first>Geondo</first><last>Park</last></author>
      <author><first>Gyeongman</first><last>Kim</last></author>
      <author><first>Eunho</first><last>Yang</last></author>
      <pages>364–378</pages>
      <abstract>A computationally expensive and memory intensive neural network lies behind the recent success of language representation learning. Knowledge distillation, a major technique for deploying such a vast language model in resource-scarce environments, transfers the knowledge on individual word representations learned without restrictions. In this paper, inspired by the recent observations that language representations are relatively positioned and have more semantic knowledge as a whole, we present a new knowledge distillation objective for language representation learning that transfers the contextual knowledge via two types of relationships across representations: Word Relation and Layer Transforming Relation. Unlike other recent distillation techniques for the language models, our contextual distillation does not have any restrictions on architectural changes between teacher and student. We validate the effectiveness of our method on challenging benchmarks of language understanding tasks, not only in architectures of various sizes but also in combination with DynaBERT, the recently proposed adaptive size pruning method.</abstract>
      <url hash="af511c21">2021.emnlp-main.30</url>
      <bibkey>park-etal-2021-distilling</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.30</doi>
      <video href="2021.emnlp-main.30.mp4"/>
      <pwccode url="https://github.com/geondopark/ckd" additional="false">geondopark/ckd</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="31">
      <title>Dynamic Knowledge Distillation for Pre-trained Language Models</title>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Shuhuai</first><last>Ren</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <pages>379–389</pages>
      <abstract>Knowledge distillation (KD) has been proved effective for compressing large-scale pre-trained language models. However, existing methods conduct KD statically, e.g., the student model aligns its output distribution to that of a selected teacher model on the pre-defined training dataset. In this paper, we explore whether a dynamic knowledge distillation that empowers the student to adjust the learning procedure according to its competency, regarding the student performance and learning efficiency. We explore the dynamical adjustments on three aspects: teacher model adoption, data selection, and KD objective adaptation. Experimental results show that (1) proper selection of teacher model can boost the performance of student model; (2) conducting KD with 10% informative instances achieves comparable performance while greatly accelerates the training; (3) the student performance can be boosted by adjusting the supervision contribution of different alignment objective. We find dynamic knowledge distillation is promising and provide discussions on potential future directions towards more efficient KD methods.</abstract>
      <url hash="d3f2ade9">2021.emnlp-main.31</url>
      <bibkey>li-etal-2021-dynamic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.31</doi>
      <video href="2021.emnlp-main.31.mp4"/>
      <pwccode url="https://github.com/lancopku/dynamickd" additional="false">lancopku/dynamickd</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="32">
      <title>Few-Shot Text Generation with Natural Language Instructions</title>
      <author><first>Timo</first><last>Schick</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>390–402</pages>
      <abstract>Providing pretrained language models with simple task descriptions in natural language enables them to solve some tasks in a fully unsupervised fashion. Moreover, when combined with regular learning from examples, this idea yields impressive few-shot results for a wide range of text classification tasks. It is also a promising direction to improve data efficiency in generative settings, but there are several challenges to using a combination of task descriptions and example-based learning for text generation. In particular, it is crucial to find task descriptions that are easy to understand for the pretrained model and to ensure that it actually makes good use of them; furthermore, effective measures against overfitting have to be implemented. In this paper, we show how these challenges can be tackled: We introduce GenPET, a method for text generation that is based on pattern-exploiting training, a recent approach for combining textual instructions with supervised learning that only works for classification tasks. On several summarization and headline generation datasets, GenPET gives consistent improvements over strong baselines in few-shot settings.</abstract>
      <url hash="861ef6b4">2021.emnlp-main.32</url>
      <bibkey>schick-schutze-2021-shot</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.32</doi>
      <video href="2021.emnlp-main.32.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/aeslc">AESLC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reddit-tifu">Reddit TIFU</pwcdataset>
    </paper>
    <paper id="33">
      <title><fixed-case>SOM</fixed-case>-<fixed-case>NCSCM</fixed-case> : An Efficient Neural <fixed-case>C</fixed-case>hinese Sentence Compression Model Enhanced with Self-Organizing Map</title>
      <author><first>Kangli</first><last>Zi</last></author>
      <author><first>Shi</first><last>Wang</last></author>
      <author><first>Yu</first><last>Liu</last></author>
      <author><first>Jicun</first><last>Li</last></author>
      <author><first>Yanan</first><last>Cao</last></author>
      <author><first>Cungen</first><last>Cao</last></author>
      <pages>403–415</pages>
      <abstract>Sentence Compression (SC), which aims to shorten sentences while retaining important words that express the essential meanings, has been studied for many years in many languages, especially in English. However, improvements on Chinese SC task are still quite few due to several difficulties: scarce of parallel corpora, different segmentation granularity of Chinese sentences, and imperfect performance of syntactic analyses. Furthermore, entire neural Chinese SC models have been under-investigated so far. In this work, we construct an SC dataset of Chinese colloquial sentences from a real-life question answering system in the telecommunication domain, and then, we propose a neural Chinese SC model enhanced with a Self-Organizing Map (SOM-NCSCM), to gain a valuable insight from the data and improve the performance of the whole neural Chinese SC model in a valid manner. Experimental results show that our SOM-NCSCM can significantly benefit from the deep investigation of similarity among data, and achieve a promising F1 score of 89.655 and BLEU4 score of 70.116, which also provides a baseline for further research on Chinese SC task.</abstract>
      <url hash="91eb6552">2021.emnlp-main.33</url>
      <bibkey>zi-etal-2021-som</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.33</doi>
      <video href="2021.emnlp-main.33.mp4"/>
    </paper>
    <paper id="34">
      <title>Efficient Multi-Task Auxiliary Learning: Selecting Auxiliary Data by Feature Similarity</title>
      <author><first>Po-Nien</first><last>Kung</last></author>
      <author><first>Sheng-Siang</first><last>Yin</last></author>
      <author><first>Yi-Cheng</first><last>Chen</last></author>
      <author><first>Tse-Hsuan</first><last>Yang</last></author>
      <author><first>Yun-Nung</first><last>Chen</last></author>
      <pages>416–428</pages>
      <abstract>Multi-task auxiliary learning utilizes a set of relevant auxiliary tasks to improve the performance of a primary task. A common usage is to manually select multiple auxiliary tasks for multi-task learning on all data, which raises two issues: (1) selecting beneficial auxiliary tasks for a primary task is nontrivial; (2) when the auxiliary datasets are large, training on all data becomes time-expensive and impractical. Therefore, this paper focuses on addressing these problems and proposes a time-efficient sampling method to select the data that is most relevant to the primary task. The proposed method allows us to only train on the most beneficial sub-datasets from the auxiliary tasks, achieving efficient multi-task auxiliary learning. The experiments on three benchmark datasets (RTE, MRPC, STS-B) show that our method significantly outperforms random sampling and ST-DNN. Also, by applying our method, the model can surpass fully-trained MT-DNN on RTE, MRPC, STS-B, using only 50%, 66%, and 1% of data, respectively.</abstract>
      <url hash="e8ace5a1">2021.emnlp-main.34</url>
      <bibkey>kung-etal-2021-efficient</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.34</doi>
      <video href="2021.emnlp-main.34.mp4"/>
      <pwccode url="https://github.com/miulab/fastmtl" additional="false">miulab/fastmtl</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="35">
      <title><fixed-case>GOLD</fixed-case>: Improving Out-of-Scope Detection in Dialogues using Data Augmentation</title>
      <author><first>Derek</first><last>Chen</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <pages>429–442</pages>
      <abstract>Practical dialogue systems require robust methods of detecting out-of-scope (OOS) utterances to avoid conversational breakdowns and related failure modes. Directly training a model with labeled OOS examples yields reasonable performance, but obtaining such data is a resource-intensive process. To tackle this limited-data problem, previous methods focus on better modeling the distribution of in-scope (INS) examples. We introduce GOLD as an orthogonal technique that augments existing data to train better OOS detectors operating in low-data regimes. GOLD generates pseudo-labeled candidates using samples from an auxiliary dataset and keeps only the most beneficial candidates for training through a novel filtering mechanism. In experiments across three target benchmarks, the top GOLD model outperforms all existing methods on all key metrics, achieving relative gains of 52.4%, 48.9% and 50.3% against median baseline performance. We also analyze the unique properties of OOS data to identify key factors for optimally applying our proposed method.</abstract>
      <url hash="94658ebb">2021.emnlp-main.35</url>
      <bibkey>chen-yu-2021-gold</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.35</doi>
      <video href="2021.emnlp-main.35.mp4"/>
      <pwccode url="https://github.com/asappresearch/gold" additional="false">asappresearch/gold</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/rostd">ROSTD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/star">STAR</pwcdataset>
    </paper>
    <paper id="36">
      <title>Graph Based Network with Contextualized Representations of Turns in Dialogue</title>
      <author><first>Bongseok</first><last>Lee</last></author>
      <author><first>Yong Suk</first><last>Choi</last></author>
      <pages>443–455</pages>
      <abstract>Dialogue-based relation extraction (RE) aims to extract relation(s) between two arguments that appear in a dialogue. Because dialogues have the characteristics of high personal pronoun occurrences and low information density, and since most relational facts in dialogues are not supported by any single sentence, dialogue-based relation extraction requires a comprehensive understanding of dialogue. In this paper, we propose the TUrn COntext awaRE Graph Convolutional Network (TUCORE-GCN) modeled by paying attention to the way people understand dialogues. In addition, we propose a novel approach which treats the task of emotion recognition in conversations (ERC) as a dialogue-based RE. Experiments on a dialogue-based RE dataset and three ERC datasets demonstrate that our model is very effective in various dialogue-based natural language understanding tasks. In these experiments, TUCORE-GCN outperforms the state-of-the-art models on most of the benchmark datasets. Our code is available at https://github.com/BlackNoodle/TUCORE-GCN.</abstract>
      <url hash="c037cdbb">2021.emnlp-main.36</url>
      <bibkey>lee-choi-2021-graph</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.36</doi>
      <video href="2021.emnlp-main.36.mp4"/>
      <pwccode url="https://github.com/blacknoodle/tucore-gcn" additional="false">blacknoodle/tucore-gcn</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/dialogre">DialogRE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/emorynlp">EmoryNLP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
    </paper>
    <paper id="37">
      <title>Automatically Exposing Problems with Neural Dialog Models</title>
      <author><first>Dian</first><last>Yu</last></author>
      <author><first>Kenji</first><last>Sagae</last></author>
      <pages>456–470</pages>
      <abstract>Neural dialog models are known to suffer from problems such as generating unsafe and inconsistent responses. Even though these problems are crucial and prevalent, they are mostly manually identified by model designers through interactions. Recently, some research instructs crowdworkers to goad the bots into triggering such problems. However, humans leverage superficial clues such as hate speech, while leaving systematic problems undercover. In this paper, we propose two methods including reinforcement learning to automatically trigger a dialog model into generating problematic responses. We show the effect of our methods in exposing safety and contradiction issues with state-of-the-art dialog models.</abstract>
      <url hash="fb2b6f5c">2021.emnlp-main.37</url>
      <bibkey>yu-sagae-2021-automatically</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.37</doi>
      <video href="2021.emnlp-main.37.mp4"/>
      <pwccode url="https://github.com/diandyu/trigger" additional="false">diandyu/trigger</pwccode>
    </paper>
    <paper id="38">
      <title><fixed-case>E</fixed-case>vent Coreference Data (Almost) for Free: <fixed-case>M</fixed-case>ining Hyperlinks from Online News</title>
      <author><first>Michael</first><last>Bugert</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>471–491</pages>
      <abstract>Cross-document event coreference resolution (CDCR) is the task of identifying which event mentions refer to the same events throughout a collection of documents. Annotating CDCR data is an arduous and expensive process, explaining why existing corpora are small and lack domain coverage. To overcome this bottleneck, we automatically extract event coreference data from hyperlinks in online news: When referring to a significant real-world event, writers often add a hyperlink to another article covering this event. We demonstrate that collecting hyperlinks which point to the same article(s) produces extensive and high-quality CDCR data and create a corpus of 2M documents and 2.7M silver-standard event mentions called HyperCoref. We evaluate a state-of-the-art system on three CDCR corpora and find that models trained on small subsets of HyperCoref are highly competitive, with performance similar to models trained on gold-standard data. With our work, we free CDCR research from depending on costly human-annotated training data and open up possibilities for research beyond English CDCR, as our data extraction approach can be easily adapted to other languages.</abstract>
      <url hash="347b18cc">2021.emnlp-main.38</url>
      <bibkey>bugert-gurevych-2021-event</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.38</doi>
      <video href="2021.emnlp-main.38.mp4"/>
      <pwccode url="https://github.com/ukplab/emnlp2021-hypercoref-cdcr" additional="false">ukplab/emnlp2021-hypercoref-cdcr</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ecb">ECB+</pwcdataset>
    </paper>
    <paper id="39">
      <title>Inducing Stereotypical Character Roles from Plot Structure</title>
      <author><first>Labiba</first><last>Jahan</last></author>
      <author><first>Rahul</first><last>Mittal</last></author>
      <author><first>Mark</first><last>Finlayson</last></author>
      <pages>492–497</pages>
      <abstract>Stereotypical character roles-also known as archetypes or dramatis personae-play an important function in narratives: they facilitate efficient communication with bundles of default characteristics and associations and ease understanding of those characters’ roles in the overall narrative. We present a fully unsupervised k-means clustering approach for learning stereotypical roles given only structural plot information. We demonstrate the technique on Vladimir Propp’s structural theory of Russian folktales (captured in the extended ProppLearner corpus, with 46 tales), showing that our approach can induce six out of seven of Propp’s dramatis personae with F1 measures of up to 0.70 (0.58 average), with an additional category for minor characters. We have explored various feature sets and variations of a cluster evaluation method. The best-performing feature set comprises plot functions, unigrams, tf-idf weights, and embeddings over coreference chain heads. Roles that are mentioned more often (Hero, Villain), or have clearly distinct plot patterns (Princess) are more strongly differentiated than less frequent or distinct roles (Dispatcher, Helper, Donor). Detailed error analysis suggests that the quality of the coreference chain and plot functions annotations are critical for this task. We provide all our data and code for reproducibility.</abstract>
      <url hash="6d207d87">2021.emnlp-main.39</url>
      <bibkey>jahan-etal-2021-inducing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.39</doi>
      <video href="2021.emnlp-main.39.mp4"/>
    </paper>
    <paper id="40">
      <title>Multitask Semi-Supervised Learning for Class-Imbalanced Discourse Classification</title>
      <author><first>Alexander</first><last>Spangher</last></author>
      <author><first>Jonathan</first><last>May</last></author>
      <author><first>Sz-Rung</first><last>Shiang</last></author>
      <author><first>Lingjia</first><last>Deng</last></author>
      <pages>498–517</pages>
      <abstract>As labeling schemas evolve over time, small differences can render datasets following older schemas unusable. This prevents researchers from building on top of previous annotation work and results in the existence, in discourse learning in particular, of many small class-imbalanced datasets. In this work, we show that a multitask learning approach can combine discourse datasets from similar and diverse domains to improve discourse classification. We show an improvement of 4.9% Micro F1-score over current state-of-the-art benchmarks on the <i>NewsDiscourse</i> dataset, one of the largest discourse datasets recently published, due in part to label correlations across tasks, which improve performance for underrepresented classes. We also offer an extensive review of additional techniques proposed to address resource-poor problems in NLP, and show that none of these approaches can improve classification accuracy in our setting.</abstract>
      <url hash="7f80349e">2021.emnlp-main.40</url>
      <bibkey>spangher-etal-2021-multitask</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.40</doi>
      <video href="2021.emnlp-main.40.mp4"/>
    </paper>
    <paper id="41">
      <title>Low Frequency Names Exhibit Bias and Overfitting in Contextualizing Language Models</title>
      <author><first>Robert</first><last>Wolfe</last></author>
      <author><first>Aylin</first><last>Caliskan</last></author>
      <pages>518–532</pages>
      <abstract>We use a dataset of U.S. first names with labels based on predominant gender and racial group to examine the effect of training corpus frequency on tokenization, contextualization, similarity to initial representation, and bias in BERT, GPT-2, T5, and XLNet. We show that predominantly female and non-white names are less frequent in the training corpora of these four language models. We find that infrequent names are more self-similar across contexts, with Spearman’s rho between frequency and self-similarity as low as -.763. Infrequent names are also less similar to initial representation, with Spearman’s rho between frequency and linear centered kernel alignment (CKA) similarity to initial representation as high as .702. Moreover, we find Spearman’s rho between racial bias and name frequency in BERT of .492, indicating that lower-frequency minority group names are more associated with unpleasantness. Representations of infrequent names undergo more processing, but are more self-similar, indicating that models rely on less context-informed representations of uncommon and minority names which are overfit to a lower number of observed contexts.</abstract>
      <url hash="df05fda7">2021.emnlp-main.41</url>
      <attachment type="Software" hash="b79e0e4c">2021.emnlp-main.41.Software.zip</attachment>
      <bibkey>wolfe-caliskan-2021-low</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.41</doi>
      <video href="2021.emnlp-main.41.mp4"/>
      <erratum id="1" hash="a79a9df3" date="2022-11-30">2021.emnlp-main.41e1</erratum>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/openwebtext">OpenWebText</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webtext">WebText</pwcdataset>
    </paper>
    <paper id="42">
      <title>Mitigating Language-Dependent Ethnic Bias in <fixed-case>BERT</fixed-case></title>
      <author><first>Jaimeen</first><last>Ahn</last></author>
      <author><first>Alice</first><last>Oh</last></author>
      <pages>533–549</pages>
      <abstract>In this paper, we study ethnic bias and how it varies across languages by analyzing and mitigating ethnic bias in monolingual BERT for English, German, Spanish, Korean, Turkish, and Chinese. To observe and quantify ethnic bias, we develop a novel metric called Categorical Bias score. Then we propose two methods for mitigation; first using a multilingual model, and second using contextual word alignment of two monolingual models. We compare our proposed methods with monolingual BERT and show that these methods effectively alleviate the ethnic bias. Which of the two methods works better depends on the amount of NLP resources available for that language. We additionally experiment with Arabic and Greek to verify that our proposed methods work for a wider variety of languages.</abstract>
      <url hash="dca81d03">2021.emnlp-main.42</url>
      <bibkey>ahn-oh-2021-mitigating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.42</doi>
      <video href="2021.emnlp-main.42.mp4"/>
      <pwccode url="https://github.com/jaimeenahn/ethnic_bias" additional="false">jaimeenahn/ethnic_bias</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="43">
      <title>Adversarial Scrubbing of Demographic Information for Text Classification</title>
      <author><first>Somnath</first><last>Basu Roy Chowdhury</last></author>
      <author><first>Sayan</first><last>Ghosh</last></author>
      <author><first>Yiyuan</first><last>Li</last></author>
      <author><first>Junier</first><last>Oliva</last></author>
      <author><first>Shashank</first><last>Srivastava</last></author>
      <author><first>Snigdha</first><last>Chaturvedi</last></author>
      <pages>550–562</pages>
      <abstract>Contextual representations learned by language models can often encode undesirable attributes, like demographic associations of the users, while being trained for an unrelated target task. We aim to scrub such undesirable attributes and learn fair representations while maintaining performance on the target task. In this paper, we present an adversarial learning framework “Adversarial Scrubber” (AdS), to debias contextual representations. We perform theoretical analysis to show that our framework converges without leaking demographic information under certain conditions. We extend previous evaluation techniques by evaluating debiasing performance using Minimum Description Length (MDL) probing. Experimental evaluations on 8 datasets show that AdS generates representations with minimal information about demographic attributes while being maximally informative about the target task.</abstract>
      <url hash="86f63645">2021.emnlp-main.43</url>
      <attachment type="Software" hash="c3c357e6">2021.emnlp-main.43.Software.zip</attachment>
      <bibkey>basu-roy-chowdhury-etal-2021-adversarial</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.43</doi>
      <video href="2021.emnlp-main.43.mp4"/>
      <pwccode url="https://github.com/brcsomnath/adversarial-scrubber" additional="false">brcsomnath/adversarial-scrubber</pwccode>
    </paper>
    <paper id="44">
      <title>Open-domain clarification question generation without question examples</title>
      <author><first>Julia</first><last>White</last></author>
      <author><first>Gabriel</first><last>Poesia</last></author>
      <author><first>Robert</first><last>Hawkins</last></author>
      <author><first>Dorsa</first><last>Sadigh</last></author>
      <author><first>Noah</first><last>Goodman</last></author>
      <pages>563–570</pages>
      <abstract>An overarching goal of natural language processing is to enable machines to communicate seamlessly with humans. However, natural language can be ambiguous or unclear. In cases of uncertainty, humans engage in an interactive process known as repair: asking questions and seeking clarification until their uncertainty is resolved. We propose a framework for building a visually grounded question-asking model capable of producing polar (yes-no) clarification questions to resolve misunderstandings in dialogue. Our model uses an expected information gain objective to derive informative questions from an off-the-shelf image captioner without requiring any supervised question-answer data. We demonstrate our model’s ability to pose questions that improve communicative success in a goal-oriented 20 questions game with synthetic and human answerers.</abstract>
      <url hash="3eac608c">2021.emnlp-main.44</url>
      <bibkey>white-etal-2021-open</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.44</doi>
      <video href="2021.emnlp-main.44.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/shapeworld">ShapeWorld</pwcdataset>
    </paper>
    <paper id="45">
      <title>Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting</title>
      <author><first>Wangchunshu</first><last>Zhou</last></author>
      <author><first>Tao</first><last>Ge</last></author>
      <author><first>Canwen</first><last>Xu</last></author>
      <author><first>Ke</first><last>Xu</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <pages>571–582</pages>
      <abstract>In this paper, we propose <b>S</b>equence <b>S</b>pan <b>R</b>ewriting (SSR), a self-supervised task for sequence-to-sequence (Seq2Seq) pre-training. SSR learns to refine the machine-generated imperfect text spans into ground truth text. SSR provides more fine-grained and informative supervision in addition to the original text-infilling objective. Compared to the prevalent text infilling objectives for Seq2Seq pre-training, SSR is naturally more consistent with many downstream generation tasks that require sentence rewriting (e.g., text summarization, question generation, grammatical error correction, and paraphrase generation). We conduct extensive experiments by using SSR to improve the typical Seq2Seq pre-trained model T5 in a continual pre-training setting and show substantial improvements over T5 on various natural language generation tasks.</abstract>
      <url hash="9e568e70">2021.emnlp-main.45</url>
      <bibkey>zhou-etal-2021-improving-sequence</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.45</doi>
      <video href="2021.emnlp-main.45.mp4"/>
      <pwccode url="https://github.com/michaelzhouwang/sequence_span_rewriting" additional="false">michaelzhouwang/sequence_span_rewriting</pwccode>
    </paper>
    <paper id="46">
      <title><fixed-case>C</fixed-case>oarse2<fixed-case>F</fixed-case>ine: Fine-grained Text Classification on Coarsely-grained Annotated Data</title>
      <author><first>Dheeraj</first><last>Mekala</last></author>
      <author><first>Varun</first><last>Gangal</last></author>
      <author><first>Jingbo</first><last>Shang</last></author>
      <pages>583–594</pages>
      <abstract>Existing text classification methods mainly focus on a fixed label set, whereas many real-world applications require extending to new fine-grained classes as the number of samples per label increases. To accommodate such requirements, we introduce a new problem called coarse-to-fine grained classification, which aims to perform fine-grained classification on coarsely annotated data. Instead of asking for new fine-grained human annotations, we opt to leverage label surface names as the only human guidance and weave in rich pre-trained generative language models into the iterative weak supervision strategy. Specifically, we first propose a label-conditioned fine-tuning formulation to attune these generators for our task. Furthermore, we devise a regularization objective based on the coarse-fine label constraints derived from our problem setting, giving us even further improvements over the prior formulation. Our framework uses the fine-tuned generative models to sample pseudo-training data for training the classifier, and bootstraps on real unlabeled data for model refinement. Extensive experiments and case studies on two real-world datasets demonstrate superior performance over SOTA zero-shot classification baselines.</abstract>
      <url hash="2294e7ba">2021.emnlp-main.46</url>
      <bibkey>mekala-etal-2021-coarse2fine</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.46</doi>
      <video href="2021.emnlp-main.46.mp4"/>
    </paper>
    <paper id="47">
      <title><fixed-case>T</fixed-case>ext2<fixed-case>M</fixed-case>ol: Cross-Modal Molecule Retrieval with Natural Language Queries</title>
      <author><first>Carl</first><last>Edwards</last></author>
      <author><first>ChengXiang</first><last>Zhai</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <pages>595–607</pages>
      <abstract>We propose a new task, Text2Mol, to retrieve molecules using natural language descriptions as queries. Natural language and molecules encode information in very different ways, which leads to the exciting but challenging problem of integrating these two very different modalities. Although some work has been done on text-based retrieval and structure-based retrieval, this new task requires integrating molecules and natural language more directly. Moreover, this can be viewed as an especially challenging cross-lingual retrieval problem by considering the molecules as a language with a very unique grammar. We construct a paired dataset of molecules and their corresponding text descriptions, which we use to learn an aligned common semantic embedding space for retrieval. We extend this to create a cross-modal attention-based model for explainability and reranking by interpreting the attentions as association rules. We also employ an ensemble approach to integrate our different architectures, which significantly improves results from 0.372 to 0.499 MRR. This new multimodal approach opens a new perspective on solving problems in chemistry literature understanding and molecular machine learning.</abstract>
      <url hash="973b90dd">2021.emnlp-main.47</url>
      <bibkey>edwards-etal-2021-text2mol</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.47</doi>
      <video href="2021.emnlp-main.47.mp4"/>
      <pwccode url="https://github.com/cnedwards/text2mol" additional="false">cnedwards/text2mol</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/chebi-20">ChEBI-20</pwcdataset>
    </paper>
    <paper id="48">
      <title>Classification of hierarchical text using geometric deep learning: the case of clinical trials corpus</title>
      <author><first>Sohrab</first><last>Ferdowsi</last></author>
      <author><first>Nikolay</first><last>Borissov</last></author>
      <author><first>Julien</first><last>Knafou</last></author>
      <author><first>Poorya</first><last>Amini</last></author>
      <author><first>Douglas</first><last>Teodoro</last></author>
      <pages>608–618</pages>
      <abstract>We consider the hierarchical representation of documents as graphs and use geometric deep learning to classify them into different categories. While graph neural networks can efficiently handle the variable structure of hierarchical documents using the permutation invariant message passing operations, we show that we can gain extra performance improvements using our proposed selective graph pooling operation that arises from the fact that some parts of the hierarchy are invariable across different documents. We applied our model to classify clinical trial (CT) protocols into completed and terminated categories. We use bag-of-words based, as well as pre-trained transformer-based embeddings to featurize the graph nodes, achieving f1-scoresaround 0.85 on a publicly available large scale CT registry of around 360K protocols. We further demonstrate how the selective pooling can add insights into the CT termination status prediction. We make the source code and dataset splits accessible.</abstract>
      <url hash="fd109006">2021.emnlp-main.48</url>
      <bibkey>ferdowsi-etal-2021-classification</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.48</doi>
      <video href="2021.emnlp-main.48.mp4"/>
      <pwccode url="https://github.com/sssohrab/ct-classification-graphs" additional="false">sssohrab/ct-classification-graphs</pwccode>
    </paper>
    <paper id="49">
      <title>The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers</title>
      <author><first>Róbert</first><last>Csordás</last></author>
      <author><first>Kazuki</first><last>Irie</last></author>
      <author><first>Juergen</first><last>Schmidhuber</last></author>
      <pages>619–634</pages>
      <abstract>Recently, many datasets have been proposed to test the systematic generalization ability of neural networks. The companion baseline Transformers, typically trained with default hyper-parameters from standard tasks, are shown to fail dramatically. Here we demonstrate that by revisiting model configurations as basic as scaling of embeddings, early stopping, relative positional embedding, and Universal Transformer variants, we can drastically improve the performance of Transformers on systematic generalization. We report improvements on five popular datasets: SCAN, CFQ, PCFG, COGS, and Mathematics dataset. Our models improve accuracy from 50% to 85% on the PCFG productivity split, and from 35% to 81% on COGS. On SCAN, relative positional embedding largely mitigates the EOS decision problem (Newman et al., 2020), yielding 100% accuracy on the length split with a cutoff at 26. Importantly, performance differences between these models are typically invisible on the IID data split. This calls for proper generalization validation sets for developing neural networks that generalize systematically. We publicly release the code to reproduce our results.</abstract>
      <url hash="c1847c23">2021.emnlp-main.49</url>
      <bibkey>csordas-etal-2021-devil</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.49</doi>
      <video href="2021.emnlp-main.49.mp4"/>
      <pwccode url="https://github.com/robertcsordas/transformer_generalization" additional="false">robertcsordas/transformer_generalization</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cfq">CFQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mathematics">Mathematics Dataset</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scan">SCAN</pwcdataset>
    </paper>
    <paper id="50">
      <title>Artificial Text Detection via Examining the Topology of Attention Maps</title>
      <author><first>Laida</first><last>Kushnareva</last></author>
      <author><first>Daniil</first><last>Cherniavskii</last></author>
      <author><first>Vladislav</first><last>Mikhailov</last></author>
      <author><first>Ekaterina</first><last>Artemova</last></author>
      <author><first>Serguei</first><last>Barannikov</last></author>
      <author><first>Alexander</first><last>Bernstein</last></author>
      <author><first>Irina</first><last>Piontkovskaya</last></author>
      <author><first>Dmitri</first><last>Piontkovski</last></author>
      <author><first>Evgeny</first><last>Burnaev</last></author>
      <pages>635–649</pages>
      <abstract>The impressive capabilities of recent generative models to create texts that are challenging to distinguish from the human-written ones can be misused for generating fake news, product reviews, and even abusive content. Despite the prominent performance of existing methods for artificial text detection, they still lack interpretability and robustness towards unseen models. To this end, we propose three novel types of interpretable topological features for this task based on Topological Data Analysis (TDA) which is currently understudied in the field of NLP. We empirically show that the features derived from the BERT model outperform count- and neural-based baselines up to 10% on three common datasets, and tend to be the most robust towards unseen GPT-style generation models as opposed to existing methods. The probing analysis of the features reveals their sensitivity to the surface and syntactic properties. The results demonstrate that TDA is a promising line with respect to NLP tasks, specifically the ones that incorporate surface and structural information.</abstract>
      <url hash="11e4cf06">2021.emnlp-main.50</url>
      <bibkey>kushnareva-etal-2021-artificial</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.50</doi>
      <video href="2021.emnlp-main.50.mp4"/>
      <revision id="1" href="2021.emnlp-main.50v1" hash="60727d92"/>
      <revision id="2" href="2021.emnlp-main.50v2" hash="11e4cf06" date="2022-09-11">The revision includes stating equal contributions among the first three authors.</revision>
      <pwccode url="https://github.com/danchern97/tda4atd" additional="true">danchern97/tda4atd</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/realnews">RealNews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webtext">WebText</pwcdataset>
    </paper>
    <paper id="51">
      <title>Active Learning by Acquiring Contrastive Examples</title>
      <author><first>Katerina</first><last>Margatina</last></author>
      <author><first>Giorgos</first><last>Vernikos</last></author>
      <author><first>Loïc</first><last>Barrault</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <pages>650–663</pages>
      <abstract>Common acquisition functions for active learning use either uncertainty or diversity sampling, aiming to select difficult and diverse data points from the pool of unlabeled data, respectively. In this work, leveraging the best of both worlds, we propose an acquisition function that opts for selecting contrastive examples, i.e. data points that are similar in the model feature space and yet the model outputs maximally different predictive likelihoods. We compare our approach, CAL (Contrastive Active Learning), with a diverse set of acquisition functions in four natural language understanding tasks and seven datasets. Our experiments show that CAL performs consistently better or equal than the best performing baseline across all tasks, on both in-domain and out-of-domain data. We also conduct an extensive ablation study of our method and we further analyze all actively acquired datasets showing that CAL achieves a better trade-off between uncertainty and diversity compared to other strategies.</abstract>
      <url hash="593c8cf3">2021.emnlp-main.51</url>
      <bibkey>margatina-etal-2021-active</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.51</doi>
      <video href="2021.emnlp-main.51.mp4"/>
      <pwccode url="https://github.com/mourga/contrastive-active-learning" additional="false">mourga/contrastive-active-learning</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="52">
      <title>Conditional <fixed-case>P</fixed-case>oisson Stochastic Beams</title>
      <author><first>Clara</first><last>Meister</last></author>
      <author><first>Afra</first><last>Amini</last></author>
      <author><first>Tim</first><last>Vieira</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <pages>664–681</pages>
      <abstract>Beam search is the default decoding strategy for many sequence generation tasks in NLP. The set of approximate K-best items returned by the algorithm is a useful summary of the distribution for many applications; however, the candidates typically exhibit high overlap and may give a highly biased estimate for expectations under our model. These problems can be addressed by instead using stochastic decoding strategies. In this work, we propose a new method for turning beam search into a stochastic process: Conditional Poisson stochastic beam search. Rather than taking the maximizing set at each iteration, we sample K candidates without replacement according to the conditional Poisson sampling design. We view this as a more natural alternative to Kool et al. (2019)’s stochastic beam search (SBS). Furthermore, we show how samples generated under the CPSBS design can be used to build consistent estimators and sample diverse sets from sequence models. In our experiments, we observe CPSBS produces lower variance and more efficient estimators than SBS, even showing improvements in high entropy settings.</abstract>
      <url hash="8ed3e952">2021.emnlp-main.52</url>
      <bibkey>meister-etal-2021-conditional</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.52</doi>
      <video href="2021.emnlp-main.52.mp4"/>
    </paper>
    <paper id="53">
      <title>Building Adaptive Acceptability Classifiers for Neural <fixed-case>NLG</fixed-case></title>
      <author><first>Soumya</first><last>Batra</last></author>
      <author><first>Shashank</first><last>Jain</last></author>
      <author><first>Peyman</first><last>Heidari</last></author>
      <author><first>Ankit</first><last>Arun</last></author>
      <author><first>Catharine</first><last>Youngs</last></author>
      <author><first>Xintong</first><last>Li</last></author>
      <author><first>Pinar</first><last>Donmez</last></author>
      <author><first>Shawn</first><last>Mei</last></author>
      <author><first>Shiunzu</first><last>Kuo</last></author>
      <author><first>Vikas</first><last>Bhardwaj</last></author>
      <author><first>Anuj</first><last>Kumar</last></author>
      <author><first>Michael</first><last>White</last></author>
      <pages>682–697</pages>
      <abstract>We propose a novel framework to train models to classify acceptability of responses generated by natural language generation (NLG) models, improving upon existing sentence transformation and model-based approaches. An NLG response is considered acceptable if it is both semantically correct and grammatical. We don’t make use of any human references making the classifiers suitable for runtime deployment. Training data for the classifiers is obtained using a 2-stage approach of first generating synthetic data using a combination of existing and new model-based approaches followed by a novel validation framework to filter and sort the synthetic data into acceptable and unacceptable classes. Our 2-stage approach adapts to a wide range of data representations and does not require additional data beyond what the NLG models are trained on. It is also independent of the underlying NLG model architecture, and is able to generate more realistic samples close to the distribution of the NLG model-generated responses. We present results on 5 datasets (WebNLG, Cleaned E2E, ViGGO, Alarm, and Weather) with varying data representations. We compare our framework with existing techniques that involve synthetic data generation using simple sentence transformations and/or model-based techniques, and show that building acceptability classifiers using data that resembles the generation model outputs followed by a validation framework outperforms the existing techniques, achieving state-of-the-art results. We also show that our techniques can be used in few-shot settings using self-training.</abstract>
      <url hash="110a97d3">2021.emnlp-main.53</url>
      <bibkey>batra-etal-2021-building</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.53</doi>
      <video href="2021.emnlp-main.53.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/viggo">ViGGO</pwcdataset>
    </paper>
    <paper id="54">
      <title>Moral Stories: Situated Reasoning about Norms, Intents, Actions, and their Consequences</title>
      <author><first>Denis</first><last>Emelin</last></author>
      <author><first>Ronan</first><last>Le Bras</last></author>
      <author><first>Jena D.</first><last>Hwang</last></author>
      <author><first>Maxwell</first><last>Forbes</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>698–718</pages>
      <abstract>In social settings, much of human behavior is governed by unspoken rules of conduct rooted in societal norms. For artificial systems to be fully integrated into social environments, adherence to such norms is a central prerequisite. To investigate whether language generation models can serve as behavioral priors for systems deployed in social settings, we evaluate their ability to generate action descriptions that achieve predefined goals under normative constraints. Moreover, we examine if models can anticipate likely consequences of actions that either observe or violate known norms, or explain why certain actions are preferable by generating relevant norm hypotheses. For this purpose, we introduce Moral Stories, a crowd-sourced dataset of structured, branching narratives for the study of grounded, goal-oriented social reasoning. Finally, we propose decoding strategies that combine multiple expert models to significantly improve the quality of generated actions, consequences, and norms compared to strong baselines.</abstract>
      <url hash="87967e0e">2021.emnlp-main.54</url>
      <attachment type="Software" hash="75043371">2021.emnlp-main.54.Software.zip</attachment>
      <bibkey>emelin-etal-2021-moral</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.54</doi>
      <video href="2021.emnlp-main.54.mp4"/>
      <pwccode url="https://github.com/demelin/moral_stories" additional="false">demelin/moral_stories</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/moral-stories">Moral Stories</pwcdataset>
    </paper>
    <paper id="55">
      <title>Truth-Conditional Captions for Time Series Data</title>
      <author><first>Harsh</first><last>Jhamtani</last></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last></author>
      <pages>719–733</pages>
      <abstract>In this paper, we explore the task of automatically generating natural language descriptions of salient patterns in a time series, such as stock prices of a company over a week. A model for this task should be able to extract high-level patterns such as presence of a peak or a dip. While typical contemporary neural models with attention mechanisms can generate fluent output descriptions for this task, they often generate factually incorrect descriptions. We propose a computational model with a truth-conditional architecture which first runs small learned programs on the input time series, then identifies the programs/patterns which hold true for the given input, and finally conditions on *only* the chosen valid program (rather than the input time series) to generate the output text description. A program in our model is constructed from modules, which are small neural networks that are designed to capture numerical patterns and temporal information. The modules are shared across multiple programs, enabling compositionality as well as efficient learning of module parameters. The modules, as well as the composition of the modules, are unobserved in data, and we learn them in an end-to-end fashion with the only training signal coming from the accompanying natural language text descriptions. We find that the proposed model is able to generate high-precision captions even though we consider a small and simple space of module types.</abstract>
      <url hash="0fe2cacc">2021.emnlp-main.55</url>
      <bibkey>jhamtani-berg-kirkpatrick-2021-truth</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.55</doi>
      <video href="2021.emnlp-main.55.mp4"/>
      <pwccode url="https://github.com/harsh19/truce" additional="false">harsh19/truce</pwccode>
    </paper>
    <paper id="56">
      <title>Injecting Entity Types into Entity-Guided Text Generation</title>
      <author><first>Xiangyu</first><last>Dong</last></author>
      <author><first>Wenhao</first><last>Yu</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Meng</first><last>Jiang</last></author>
      <pages>734–741</pages>
      <abstract>Recent successes in deep generative modeling have led to significant advances in natural language generation (NLG). Incorporating entities into neural generation models has demonstrated great improvements by assisting to infer the summary topic and to generate coherent content. To enhance the role of entity in NLG, in this paper, we aim to model the entity type in the decoding phase to generate contextual words accurately. We develop a novel NLG model to produce a target sequence based on a given list of entities. Our model has a multi-step decoder that injects the entity types into the process of entity mention generation. Experiments on two public news datasets demonstrate type injection performs better than existing type embedding concatenation baselines.</abstract>
      <url hash="1c5ea289">2021.emnlp-main.56</url>
      <bibkey>dong-etal-2021-injecting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.56</doi>
      <video href="2021.emnlp-main.56.mp4"/>
      <pwccode url="https://github.com/wyu97/InjType" additional="true">wyu97/InjType</pwccode>
    </paper>
    <paper id="57">
      <title>Smelting Gold and Silver for Improved Multilingual <fixed-case>AMR</fixed-case>-to-<fixed-case>T</fixed-case>ext Generation</title>
      <author><first>Leonardo F. R.</first><last>Ribeiro</last></author>
      <author><first>Jonas</first><last>Pfeiffer</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>742–750</pages>
      <abstract>Recent work on multilingual AMR-to-text generation has exclusively focused on data augmentation strategies that utilize silver AMR. However, this assumes a high quality of generated AMRs, potentially limiting the transferability to the target task. In this paper, we investigate different techniques for automatically generating AMR annotations, where we aim to study which source of information yields better multilingual results. Our models trained on gold AMR with silver (machine translated) sentences outperform approaches which leverage generated silver AMR. We find that combining both complementary sources of information further improves multilingual AMR-to-text generation. Our models surpass the previous state of the art for German, Italian, Spanish, and Chinese by a large margin.</abstract>
      <url hash="3ae8143b">2021.emnlp-main.57</url>
      <bibkey>ribeiro-etal-2021-smelting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.57</doi>
      <video href="2021.emnlp-main.57.mp4"/>
      <pwccode url="https://github.com/ukplab/m-amr2text" additional="false">ukplab/m-amr2text</pwccode>
    </paper>
    <paper id="58">
      <title>Learning Compact Metrics for <fixed-case>MT</fixed-case></title>
      <author><first>Amy</first><last>Pu</last></author>
      <author><first>Hyung Won</first><last>Chung</last></author>
      <author><first>Ankur</first><last>Parikh</last></author>
      <author><first>Sebastian</first><last>Gehrmann</last></author>
      <author><first>Thibault</first><last>Sellam</last></author>
      <pages>751–762</pages>
      <abstract>Recent developments in machine translation and multilingual text generation have led researchers to adopt trained metrics such as COMET or BLEURT, which treat evaluation as a regression problem and use representations from multilingual pre-trained models such as XLM-RoBERTa or mBERT. Yet studies on related tasks suggest that these models are most efficient when they are large, which is costly and impractical for evaluation. We investigate the trade-off between multilinguality and model capacity with RemBERT, a state-of-the-art multilingual language model, using data from the WMT Metrics Shared Task. We present a series of experiments which show that model size is indeed a bottleneck for cross-lingual transfer, then demonstrate how distillation can help addressing this bottleneck, by leveraging synthetic data generation and transferring knowledge from one teacher to multiple students trained on related languages. Our method yields up to 10.5% improvement over vanilla fine-tuning and reaches 92.6% of RemBERT’s performance using only a third of its parameters.</abstract>
      <url hash="01c0de6f">2021.emnlp-main.58</url>
      <bibkey>pu-etal-2021-learning</bibkey>
      <revision id="1" href="2021.emnlp-main.58v1" hash="def5905f"/>
      <revision id="2" href="2021.emnlp-main.58v2" hash="01c0de6f" date="2021-11-09">updated Figure 5 and Section 3</revision>
      <doi>10.18653/v1/2021.emnlp-main.58</doi>
      <video href="2021.emnlp-main.58.mp4"/>
      <pwccode url="https://github.com/google-research/bleurt" additional="false">google-research/bleurt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/c4">C4</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mc4">mC4</pwcdataset>
    </paper>
    <paper id="59">
      <title>The Impact of Positional Encodings on Multilingual Compression</title>
      <author><first>Vinit</first><last>Ravishankar</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>763–777</pages>
      <abstract>In order to preserve word-order information in a non-autoregressive setting, transformer architectures tend to include positional knowledge, by (for instance) adding positional encodings to token embeddings. Several modifications have been proposed over the sinusoidal positional encodings used in the original transformer architecture; these include, for instance, separating position encodings and token embeddings, or directly modifying attention weights based on the distance between word pairs. We first show that surprisingly, while these modifications tend to improve monolingual language models, none of them result in better multilingual language models. We then answer why that is: sinusoidal encodings were explicitly designed to facilitate compositionality by allowing linear projections over arbitrary time steps. Higher variances in multilingual training distributions requires higher compression, in which case, compositionality becomes indispensable. Learned absolute positional encodings (e.g., in mBERT) tend to approximate sinusoidal embeddings in multilingual settings, but more complex positional encoding architectures lack the inductive bias to effectively learn cross-lingual alignment. In other words, while sinusoidal positional encodings were designed for monolingual applications, they are particularly useful in multilingual language models.</abstract>
      <url hash="fb003bd3">2021.emnlp-main.59</url>
      <bibkey>ravishankar-sogaard-2021-impact</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.59</doi>
      <video href="2021.emnlp-main.59.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="60">
      <title>Disentangling Representations of Text by Masking Transformers</title>
      <author><first>Xiongyi</first><last>Zhang</last></author>
      <author><first>Jan-Willem</first><last>van de Meent</last></author>
      <author><first>Byron</first><last>Wallace</last></author>
      <pages>778–791</pages>
      <abstract>Representations from large pretrained models such as BERT encode a range of features into monolithic vectors, affording strong predictive accuracy across a range of downstream tasks. In this paper we explore whether it is possible to learn disentangled representations by identifying existing subnetworks within pretrained models that encode distinct, complementary aspects. Concretely, we learn binary masks over transformer weights or hidden units to uncover subsets of features that correlate with a specific factor of variation; this eliminates the need to train a disentangled model from scratch for a particular task. We evaluate this method with respect to its ability to disentangle representations of sentiment from genre in movie reviews, toxicity from dialect in Tweets, and syntax from semantics. By combining masking with magnitude pruning we find that we can identify sparse subnetworks within BERT that strongly encode particular aspects (e.g., semantics) while only weakly encoding others (e.g., syntax). Moreover, despite only learning masks, disentanglement-via-masking performs as well as — and often better than —previously proposed methods based on variational autoencoders and adversarial training.</abstract>
      <url hash="87689b47">2021.emnlp-main.60</url>
      <bibkey>zhang-etal-2021-disentangling</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.60</doi>
      <video href="2021.emnlp-main.60.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="61">
      <title>Exploring the Role of <fixed-case>BERT</fixed-case> Token Representations to Explain Sentence Probing Results</title>
      <author><first>Hosein</first><last>Mohebbi</last></author>
      <author><first>Ali</first><last>Modarressi</last></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last></author>
      <pages>792–806</pages>
      <abstract>Several studies have been carried out on revealing linguistic features captured by BERT. This is usually achieved by training a diagnostic classifier on the representations obtained from different layers of BERT. The subsequent classification accuracy is then interpreted as the ability of the model in encoding the corresponding linguistic property. Despite providing insights, these studies have left out the potential role of token representations. In this paper, we provide a more in-depth analysis on the representation space of BERT in search for distinct and meaningful subspaces that can explain the reasons behind these probing results. Based on a set of probing tasks and with the help of attribution methods we show that BERT tends to encode meaningful knowledge in specific token representations (which are often ignored in standard classification setups), allowing the model to detect syntactic and semantic abnormalities, and to distinctively separate grammatical number and tense subspaces.</abstract>
      <url hash="9b528f3a">2021.emnlp-main.61</url>
      <bibkey>mohebbi-etal-2021-exploring</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.61</doi>
      <video href="2021.emnlp-main.61.mp4"/>
      <pwccode url="https://github.com/hmohebbi/explain-probing-results" additional="false">hmohebbi/explain-probing-results</pwccode>
    </paper>
    <paper id="62">
      <title>Do Long-Range Language Models Actually Use Long-Range Context?</title>
      <author><first>Simeng</first><last>Sun</last></author>
      <author><first>Kalpesh</first><last>Krishna</last></author>
      <author><first>Andrew</first><last>Mattarella-Micke</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <pages>807–822</pages>
      <abstract>Language models are generally trained on short, truncated input sequences, which limits their ability to use discourse-level information present in long-range context to improve their predictions. Recent efforts to improve the efficiency of self-attention have led to a proliferation of long-range Transformer language models, which can process much longer sequences than models of the past. However, the ways in which such models take advantage of the long-range context remain unclear. In this paper, we perform a fine-grained analysis of two long-range Transformer language models (including the Routing Transformer, which achieves state-of-the-art perplexity on the PG-19 long-sequence LM benchmark dataset) that accept input sequences of up to 8K tokens. Our results reveal that providing long-range context (i.e., beyond the previous 2K tokens) to these models only improves their predictions on a small set of tokens (e.g., those that can be copied from the distant context) and does not help at all for sentence-level prediction tasks. Finally, we discover that PG-19 contains a variety of different document types and domains, and that long-range context helps most for literary novels (as opposed to textbooks or magazines).</abstract>
      <url hash="dde33f63">2021.emnlp-main.62</url>
      <bibkey>sun-etal-2021-long</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.62</doi>
      <video href="2021.emnlp-main.62.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/pg-19">PG-19</pwcdataset>
    </paper>
    <paper id="63">
      <title><fixed-case>T</fixed-case>he <fixed-case>W</fixed-case>orld of an <fixed-case>O</fixed-case>ctopus: <fixed-case>H</fixed-case>ow <fixed-case>R</fixed-case>eporting <fixed-case>B</fixed-case>ias <fixed-case>I</fixed-case>nfluences a <fixed-case>L</fixed-case>anguage <fixed-case>M</fixed-case>odel’s <fixed-case>P</fixed-case>erception of <fixed-case>C</fixed-case>olor</title>
      <author><first>Cory</first><last>Paik</last></author>
      <author><first>Stéphane</first><last>Aroca-Ouellette</last></author>
      <author><first>Alessandro</first><last>Roncone</last></author>
      <author><first>Katharina</first><last>Kann</last></author>
      <pages>823–835</pages>
      <abstract>Recent work has raised concerns about the inherent limitations of text-only pretraining. In this paper, we first demonstrate that reporting bias, the tendency of people to not state the obvious, is one of the causes of this limitation, and then investigate to what extent multimodal training can mitigate this issue. To accomplish this, we 1) generate the Color Dataset (CoDa), a dataset of human-perceived color distributions for 521 common objects; 2) use CoDa to analyze and compare the color distribution found in text, the distribution captured by language models, and a human’s perception of color; and 3) investigate the performance differences between text-only and multimodal models on CoDa. Our results show that the distribution of colors that a language model recovers correlates more strongly with the inaccurate distribution found in text than with the ground-truth, supporting the claim that reporting bias negatively impacts and inherently limits text-only training. We then demonstrate that multimodal models can leverage their visual training to mitigate these effects, providing a promising avenue for future research.</abstract>
      <url hash="d5503eb4">2021.emnlp-main.63</url>
      <bibkey>paik-etal-2021-world</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.63</doi>
      <video href="2021.emnlp-main.63.mp4"/>
      <pwccode url="https://github.com/nala-cub/coda" additional="false">nala-cub/coda</pwccode>
    </paper>
    <paper id="64">
      <title><fixed-case>SELFEXPLAIN</fixed-case>: A Self-Explaining Architecture for Neural Text Classifiers</title>
      <author><first>Dheeraj</first><last>Rajagopal</last></author>
      <author><first>Vidhisha</first><last>Balachandran</last></author>
      <author><first>Eduard H</first><last>Hovy</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <pages>836–850</pages>
      <abstract>We introduce SelfExplain, a novel self-explaining model that explains a text classifier’s predictions using phrase-based concepts. SelfExplain augments existing neural classifiers by adding (1) a globally interpretable layer that identifies the most influential concepts in the training set for a given sample and (2) a locally interpretable layer that quantifies the contribution of each local input concept by computing a relevance score relative to the predicted label. Experiments across five text-classification datasets show that SelfExplain facilitates interpretability without sacrificing performance. Most importantly, explanations from SelfExplain show sufficiency for model predictions and are perceived as adequate, trustworthy and understandable by human judges compared to existing widely-used baselines.</abstract>
      <url hash="edbdbccc">2021.emnlp-main.64</url>
      <bibkey>rajagopal-etal-2021-selfexplain</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.64</doi>
      <video href="2021.emnlp-main.64.mp4"/>
      <pwccode url="https://github.com/dheerajrajagopal/SelfExplain" additional="true">dheerajrajagopal/SelfExplain</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="65">
      <title>Memory and Knowledge Augmented Language Models for Inferring Salience in Long-Form Stories</title>
      <author><first>David</first><last>Wilmot</last></author>
      <author><first>Frank</first><last>Keller</last></author>
      <pages>851–865</pages>
      <abstract>Measuring event salience is essential in the understanding of stories. This paper takes a recent unsupervised method for salience detection derived from Barthes Cardinal Functions and theories of surprise and applies it to longer narrative forms. We improve the standard transformer language model by incorporating an external knowledgebase (derived from Retrieval Augmented Generation) and adding a memory mechanism to enhance performance on longer works. We use a novel approach to derive salience annotation using chapter-aligned summaries from the Shmoop corpus for classic literary works. Our evaluation against this data demonstrates that our salience detection model improves performance over and above a non-knowledgebase and memory augmented language model, both of which are crucial to this improvement.</abstract>
      <url hash="1518c71b">2021.emnlp-main.65</url>
      <bibkey>wilmot-keller-2021-memory</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.65</doi>
      <video href="2021.emnlp-main.65.mp4"/>
      <pwccode url="https://github.com/dwlmt/story-fragments" additional="false">dwlmt/story-fragments</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/shmoop-corpus">Shmoop Corpus</pwcdataset>
    </paper>
    <paper id="66">
      <title>Semantic Novelty Detection in Natural Language Descriptions</title>
      <author><first>Nianzu</first><last>Ma</last></author>
      <author><first>Alexander</first><last>Politowicz</last></author>
      <author><first>Sahisnu</first><last>Mazumder</last></author>
      <author><first>Jiahua</first><last>Chen</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Eric</first><last>Robertson</last></author>
      <author><first>Scott</first><last>Grigsby</last></author>
      <pages>866–882</pages>
      <abstract>This paper proposes to study a fine-grained semantic novelty detection task, which can be illustrated with the following example. It is normal that a person walks a dog in the park, but if someone says “A man is walking a chicken in the park”, it is novel. Given a set of natural language descriptions of normal scenes, we want to identify descriptions of novel scenes. We are not aware of any existing work that solves the problem. Although existing novelty or anomaly detection algorithms are applicable, since they are usually topic-based, they perform poorly on our fine-grained semantic novelty detection task. This paper proposes an effective model (called GAT-MA) to solve the problem and also contributes a new dataset. Experimental evaluation shows that GAT-MA outperforms 11 baselines by large margins.</abstract>
      <url hash="62ac87aa">2021.emnlp-main.66</url>
      <bibkey>ma-etal-2021-semantic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.66</doi>
      <video href="2021.emnlp-main.66.mp4"/>
      <pwccode url="https://github.com/nianzuma/semantic-novelty-detection-in-natural-language-descriptions" additional="false">nianzuma/semantic-novelty-detection-in-natural-language-descriptions</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
    <paper id="67">
      <title>Jump-Starting Item Parameters for Adaptive Language Tests</title>
      <author><first>Arya D.</first><last>McCarthy</last></author>
      <author><first>Kevin P.</first><last>Yancey</last></author>
      <author><first>Geoffrey T.</first><last>LaFlair</last></author>
      <author><first>Jesse</first><last>Egbert</last></author>
      <author><first>Manqian</first><last>Liao</last></author>
      <author><first>Burr</first><last>Settles</last></author>
      <pages>883–899</pages>
      <abstract>A challenge in designing high-stakes language assessments is calibrating the test item difficulties, either a priori or from limited pilot test data. While prior work has addressed ‘cold start’ estimation of item difficulties without piloting, we devise a multi-task generalized linear model with BERT features to jump-start these estimates, rapidly improving their quality with as few as 500 test-takers and a small sample of item exposures (≈6 each) from a large item bank (≈4,000 items). Our joint model provides a principled way to compare test-taker proficiency, item difficulty, and language proficiency frameworks like the Common European Framework of Reference (CEFR). This also enables new item difficulty estimates without piloting them first, which in turn limits item exposure and thus enhances test item security. Finally, using operational data from the Duolingo English Test, a high-stakes English proficiency test, we find that the difficulty estimates derived using this method correlate strongly with lexico-grammatical features that correlate with reading complexity.</abstract>
      <url hash="9f922ac6">2021.emnlp-main.67</url>
      <bibkey>mccarthy-etal-2021-jump</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.67</doi>
      <video href="2021.emnlp-main.67.mp4"/>
    </paper>
    <paper id="68">
      <title>Voice Query Auto Completion</title>
      <author><first>Raphael</first><last>Tang</last></author>
      <author><first>Karun</first><last>Kumar</last></author>
      <author><first>Kendra</first><last>Chalkley</last></author>
      <author><first>Ji</first><last>Xin</last></author>
      <author><first>Liming</first><last>Zhang</last></author>
      <author><first>Wenyan</first><last>Li</last></author>
      <author><first>Gefei</first><last>Yang</last></author>
      <author><first>Yajie</first><last>Mao</last></author>
      <author><first>Junho</first><last>Shin</last></author>
      <author><first>Geoffrey Craig</first><last>Murray</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>900–906</pages>
      <abstract>Query auto completion (QAC) is the task of predicting a search engine user’s final query from their intermediate, incomplete query. In this paper, we extend QAC to the streaming voice search setting, where automatic speech recognition systems produce intermediate transcriptions as users speak. Naively applying existing methods fails because the intermediate transcriptions often don’t form prefixes or even substrings of the final transcription. To address this issue, we propose to condition QAC approaches on intermediate transcriptions to complete voice queries. We evaluate our models on a speech-enabled smart television with real-life voice search traffic, finding that this ASR-aware conditioning improves the completion quality. Our best method obtains an 18% relative improvement in mean reciprocal rank over previous methods.</abstract>
      <url hash="65dc89f2">2021.emnlp-main.68</url>
      <bibkey>tang-etal-2021-voice</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.68</doi>
      <video href="2021.emnlp-main.68.mp4"/>
    </paper>
    <paper id="69">
      <title><fixed-case>C</fixed-case>o<fixed-case>PHE</fixed-case>: A Count-Preserving Hierarchical Evaluation Metric in Large-Scale Multi-Label Text Classification</title>
      <author><first>Matúš</first><last>Falis</last></author>
      <author><first>Hang</first><last>Dong</last></author>
      <author><first>Alexandra</first><last>Birch</last></author>
      <author><first>Beatrice</first><last>Alex</last></author>
      <pages>907–912</pages>
      <abstract>Large-Scale Multi-Label Text Classification (LMTC) includes tasks with hierarchical label spaces, such as automatic assignment of ICD-9 codes to discharge summaries. Performance of models in prior art is evaluated with standard precision, recall, and F1 measures without regard for the rich hierarchical structure. In this work we argue for hierarchical evaluation of the predictions of neural LMTC models. With the example of the ICD-9 ontology we describe a structural issue in the representation of the structured label space in prior art, and propose an alternative representation based on the depth of the ontology. We propose a set of metrics for hierarchical evaluation using the depth-based representation. We compare the evaluation scores from the proposed metrics with previously used metrics on prior art LMTC models for ICD-9 coding in MIMIC-III. We also propose further avenues of research involving the proposed ontological representation.</abstract>
      <url hash="0fedef02">2021.emnlp-main.69</url>
      <bibkey>falis-etal-2021-cophe</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.69</doi>
      <video href="2021.emnlp-main.69.mp4"/>
      <pwccode url="https://github.com/modr00cka/cophe" additional="false">modr00cka/cophe</pwccode>
    </paper>
    <paper id="70">
      <title>Learning Universal Authorship Representations</title>
      <author><first>Rafael A.</first><last>Rivera-Soto</last></author>
      <author><first>Olivia Elizabeth</first><last>Miano</last></author>
      <author><first>Juanita</first><last>Ordonez</last></author>
      <author><first>Barry Y.</first><last>Chen</last></author>
      <author><first>Aleem</first><last>Khan</last></author>
      <author><first>Marcus</first><last>Bishop</last></author>
      <author><first>Nicholas</first><last>Andrews</last></author>
      <pages>913–919</pages>
      <abstract>Determining whether two documents were composed by the same author, also known as authorship verification, has traditionally been tackled using statistical methods. Recently, authorship representations learned using neural networks have been found to outperform alternatives, particularly in large-scale settings involving hundreds of thousands of authors. But do such representations learned in a particular domain transfer to other domains? Or are these representations inherently entangled with domain-specific features? To study these questions, we conduct the first large-scale study of cross-domain transfer for authorship verification considering zero-shot transfers involving three disparate domains: Amazon reviews, fanfiction short stories, and Reddit comments. We find that although a surprising degree of transfer is possible between certain domains, it is not so successful between others. We examine properties of these domains that influence generalization and propose simple but effective methods to improve transfer.</abstract>
      <url hash="cba82a32">2021.emnlp-main.70</url>
      <bibkey>rivera-soto-etal-2021-learning</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.70</doi>
      <video href="2021.emnlp-main.70.mp4"/>
    </paper>
    <paper id="71">
      <title>Predicting emergent linguistic compositions through time: Syntactic frame extension via multimodal chaining</title>
      <author><first>Lei</first><last>Yu</last></author>
      <author><first>Yang</first><last>Xu</last></author>
      <pages>920–931</pages>
      <abstract>Natural language relies on a finite lexicon to express an unbounded set of emerging ideas. One result of this tension is the formation of new compositions, such that existing linguistic units can be combined with emerging items into novel expressions. We develop a framework that exploits the cognitive mechanisms of chaining and multimodal knowledge to predict emergent compositional expressions through time. We present the syntactic frame extension model (SFEM) that draws on the theory of chaining and knowledge from “percept”, “concept”, and “language” to infer how verbs extend their frames to form new compositions with existing and novel nouns. We evaluate SFEM rigorously on the 1) modalities of knowledge and 2) categorization models of chaining, in a syntactically parsed English corpus over the past 150 years. We show that multimodal SFEM predicts newly emerged verb syntax and arguments substantially better than competing models using purely linguistic or unimodal knowledge. We find support for an exemplar view of chaining as opposed to a prototype view and reveal how the joint approach of multimodal chaining may be fundamental to the creation of literal and figurative language uses including metaphor and metonymy.</abstract>
      <url hash="4d22e181">2021.emnlp-main.71</url>
      <bibkey>yu-xu-2021-predicting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.71</doi>
      <video href="2021.emnlp-main.71.mp4"/>
      <pwccode url="https://github.com/jadeleiyu/frame_extension" additional="false">jadeleiyu/frame_extension</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imagenet">ImageNet</pwcdataset>
    </paper>
    <paper id="72">
      <title>Frequency Effects on Syntactic Rule Learning in Transformers</title>
      <author><first>Jason</first><last>Wei</last></author>
      <author><first>Dan</first><last>Garrette</last></author>
      <author><first>Tal</first><last>Linzen</last></author>
      <author><first>Ellie</first><last>Pavlick</last></author>
      <pages>932–948</pages>
      <abstract>Pre-trained language models perform well on a variety of linguistic tasks that require symbolic reasoning, raising the question of whether such models implicitly represent abstract symbols and rules. We investigate this question using the case study of BERT’s performance on English subject–verb agreement. Unlike prior work, we train multiple instances of BERT from scratch, allowing us to perform a series of controlled interventions at pre-training time. We show that BERT often generalizes well to subject–verb pairs that never occurred in training, suggesting a degree of rule-governed behavior. We also find, however, that performance is heavily influenced by word frequency, with experiments showing that both the absolute frequency of a verb form, as well as the frequency relative to the alternate inflection, are causally implicated in the predictions BERT makes at inference time. Closer analysis of these frequency effects reveals that BERT’s behavior is consistent with a system that correctly applies the SVA rule in general but struggles to overcome strong training priors and to estimate agreement features (singular vs. plural) on infrequent lexical items.</abstract>
      <url hash="eadf4986">2021.emnlp-main.72</url>
      <bibkey>wei-etal-2021-frequency</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.72</doi>
      <video href="2021.emnlp-main.72.mp4"/>
      <pwccode url="https://github.com/google-research/language" additional="false">google-research/language</pwccode>
    </paper>
    <paper id="73">
      <title>A surprisal–duration trade-off across and within the world’s languages</title>
      <author><first>Tiago</first><last>Pimentel</last></author>
      <author><first>Clara</first><last>Meister</last></author>
      <author><first>Elizabeth</first><last>Salesky</last></author>
      <author><first>Simone</first><last>Teufel</last></author>
      <author><first>Damián</first><last>Blasi</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <pages>949–962</pages>
      <abstract>While there exist scores of natural languages, each with its unique features and idiosyncrasies, they all share a unifying theme: enabling human communication. We may thus reasonably predict that human cognition shapes how these languages evolve and are used. Assuming that the capacity to process information is roughly constant across human populations, we expect a surprisal–duration trade-off to arise both across and within languages. We analyse this trade-off using a corpus of 600 languages and, after controlling for several potential confounds, we find strong supporting evidence in both settings. Specifically, we find that, on average, phones are produced faster in languages where they are less surprising, and vice versa. Further, we confirm that more surprising phones are longer, on average, in 319 languages out of the 600. We thus conclude that there is strong evidence of a surprisal–duration trade-off in operation, both across and within the world’s languages.</abstract>
      <url hash="1216b3fe">2021.emnlp-main.73</url>
      <bibkey>pimentel-etal-2021-surprisal</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.73</doi>
      <video href="2021.emnlp-main.73.mp4"/>
      <pwccode url="https://github.com/rycolab/surprisal-duration-tradeoff" additional="false">rycolab/surprisal-duration-tradeoff</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/voxclamantis">VoxClamantis</pwcdataset>
    </paper>
    <paper id="74">
      <title>Revisiting the <fixed-case>U</fixed-case>niform <fixed-case>I</fixed-case>nformation <fixed-case>D</fixed-case>ensity Hypothesis</title>
      <author><first>Clara</first><last>Meister</last></author>
      <author><first>Tiago</first><last>Pimentel</last></author>
      <author><first>Patrick</first><last>Haller</last></author>
      <author><first>Lena</first><last>Jäger</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Roger</first><last>Levy</last></author>
      <pages>963–980</pages>
      <abstract>The uniform information density (UID) hypothesis posits a preference among language users for utterances structured such that information is distributed uniformly across a signal. While its implications on language production have been well explored, the hypothesis potentially makes predictions about language comprehension and linguistic acceptability as well. Further, it is unclear how uniformity in a linguistic signal—or lack thereof—should be measured, and over which linguistic unit, e.g., the sentence or language level, this uniformity should hold. Here we investigate these facets of the UID hypothesis using reading time and acceptability data. While our reading time results are generally consistent with previous work, they are also consistent with a weakly super-linear effect of surprisal, which would be compatible with UID’s predictions. For acceptability judgments, we find clearer evidence that non-uniformity in information density is predictive of lower acceptability. We then explore multiple operationalizations of UID, motivated by different interpretations of the original hypothesis, and analyze the scope over which the pressure towards uniformity is exerted. The explanatory power of a subset of the proposed operationalizations suggests that the strongest trend may be a regression towards a mean surprisal across the language, rather than the phrase, sentence, or document—a finding that supports a typical interpretation of UID, namely that it is the byproduct of language users maximizing the use of a (hypothetical) communication channel.</abstract>
      <url hash="75220e7a">2021.emnlp-main.74</url>
      <bibkey>meister-etal-2021-revisiting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.74</doi>
      <video href="2021.emnlp-main.74.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-stories">Natural Stories</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="75">
      <title>Condenser: a Pre-training Architecture for Dense Retrieval</title>
      <author><first>Luyu</first><last>Gao</last></author>
      <author><first>Jamie</first><last>Callan</last></author>
      <pages>981–993</pages>
      <abstract>Pre-trained Transformer language models (LM) have become go-to text representation encoders. Prior research fine-tunes deep LMs to encode text sequences such as sentences and passages into single dense vector representations for efficient text comparison and retrieval. However, dense encoders require a lot of data and sophisticated techniques to effectively train and suffer in low data situations. This paper finds a key reason is that standard LMs’ internal attention structure is not ready-to-use for dense encoders, which needs to aggregate text information into the dense representation. We propose to pre-train towards dense encoder with a novel Transformer architecture, Condenser, where LM prediction CONditions on DENSE Representation. Our experiments show Condenser improves over standard LM by large margins on various text retrieval and similarity tasks.</abstract>
      <url hash="6dae7ee9">2021.emnlp-main.75</url>
      <bibkey>gao-callan-2021-condenser</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.75</doi>
      <video href="2021.emnlp-main.75.mp4"/>
      <pwccode url="https://github.com/luyug/Condenser" additional="false">luyug/Condenser</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="76">
      <title>Monitoring geometrical properties of word embeddings for detecting the emergence of new topics.</title>
      <author><first>Clément</first><last>Christophe</last></author>
      <author><first>Julien</first><last>Velcin</last></author>
      <author><first>Jairo</first><last>Cugliari</last></author>
      <author><first>Manel</first><last>Boumghar</last></author>
      <author><first>Philippe</first><last>Suignard</last></author>
      <pages>994–1003</pages>
      <abstract>Slow emerging topic detection is a task between event detection, where we aggregate behaviors of different words on short period of time, and language evolution, where we monitor their long term evolution. In this work, we tackle the problem of early detection of slowly emerging new topics. To this end, we gather evidence of weak signals at the word level. We propose to monitor the behavior of words representation in an embedding space and use one of its geometrical properties to characterize the emergence of topics. As evaluation is typically hard for this kind of task, we present a framework for quantitative evaluation and show positive results that outperform state-of-the-art methods. Our method is evaluated on two public datasets of press and scientific articles.</abstract>
      <url hash="fc15f8ce">2021.emnlp-main.76</url>
      <bibkey>christophe-etal-2021-monitoring</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.76</doi>
      <video href="2021.emnlp-main.76.mp4"/>
    </paper>
    <paper id="77">
      <title>Contextualized Query Embeddings for Conversational Search</title>
      <author><first>Sheng-Chieh</first><last>Lin</last></author>
      <author><first>Jheng-Hong</first><last>Yang</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>1004–1015</pages>
      <abstract>This paper describes a compact and effective model for low-latency passage retrieval in conversational search based on learned dense representations. Prior to our work, the state-of-the-art approach uses a multi-stage pipeline comprising conversational query reformulation and information retrieval modules. Despite its effectiveness, such a pipeline often includes multiple neural models that require long inference times. In addition, independently optimizing each module ignores dependencies among them. To address these shortcomings, we propose to integrate conversational query reformulation directly into a dense retrieval model. To aid in this goal, we create a dataset with pseudo-relevance labels for conversational search to overcome the lack of training data and to explore different training strategies. We demonstrate that our model effectively rewrites conversational queries as dense representations in conversational search and open-domain question answering datasets. Finally, after observing that our model learns to adjust the L2 norm of query token embeddings, we leverage this property for hybrid retrieval and to support error analysis.</abstract>
      <url hash="8aa06882">2021.emnlp-main.77</url>
      <bibkey>lin-etal-2021-contextualized</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.77</doi>
      <video href="2021.emnlp-main.77.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/canard">CANARD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/orconvqa">ORConvQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quac">QuAC</pwcdataset>
    </paper>
    <paper id="78">
      <title>Ultra-High Dimensional Sparse Representations with Binarization for Efficient Text Retrieval</title>
      <author><first>Kyoung-Rok</first><last>Jang</last></author>
      <author><first>Junmo</first><last>Kang</last></author>
      <author><first>Giwon</first><last>Hong</last></author>
      <author><first>Sung-Hyon</first><last>Myaeng</last></author>
      <author><first>Joohee</first><last>Park</last></author>
      <author><first>Taewon</first><last>Yoon</last></author>
      <author><first>Heecheol</first><last>Seo</last></author>
      <pages>1016–1029</pages>
      <abstract>The semantic matching capabilities of neural information retrieval can ameliorate synonymy and polysemy problems of symbolic approaches. However, neural models’ dense representations are more suitable for re-ranking, due to their inefficiency. Sparse representations, either in symbolic or latent form, are more efficient with an inverted index. Taking the merits of the sparse and dense representations, we propose an ultra-high dimensional (UHD) representation scheme equipped with directly controllable sparsity. UHD’s large capacity and minimal noise and interference among the dimensions allow for binarized representations, which are highly efficient for storage and search. Also proposed is a bucketing method, where the embeddings from multiple layers of BERT are selected/merged to represent diverse linguistic aspects. We test our models with MS MARCO and TREC CAR, showing that our models outperforms other sparse models.</abstract>
      <url hash="47288f03">2021.emnlp-main.78</url>
      <attachment type="Software" hash="8230fe2a">2021.emnlp-main.78.Software.zip</attachment>
      <bibkey>jang-etal-2021-ultra</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.78</doi>
      <video href="2021.emnlp-main.78.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
    </paper>
    <paper id="79">
      <title><fixed-case>IR</fixed-case> like a <fixed-case>SIR</fixed-case>: <fixed-case>S</fixed-case>ense-enhanced <fixed-case>I</fixed-case>nformation <fixed-case>R</fixed-case>etrieval for <fixed-case>M</fixed-case>ultiple <fixed-case>L</fixed-case>anguages</title>
      <author><first>Rexhina</first><last>Blloshmi</last></author>
      <author><first>Tommaso</first><last>Pasini</last></author>
      <author><first>Niccolò</first><last>Campolungo</last></author>
      <author><first>Somnath</first><last>Banerjee</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <author><first>Gabriella</first><last>Pasi</last></author>
      <pages>1030–1041</pages>
      <abstract>With the advent of contextualized embeddings, attention towards neural ranking approaches for Information Retrieval increased considerably. However, two aspects have remained largely neglected: i) queries usually consist of few keywords only, which increases ambiguity and makes their contextualization harder, and ii) performing neural ranking on non-English documents is still cumbersome due to shortage of labeled datasets. In this paper we present SIR (Sense-enhanced Information Retrieval) to mitigate both problems by leveraging word sense information. At the core of our approach lies a novel multilingual query expansion mechanism based on Word Sense Disambiguation that provides sense definitions as additional semantic information for the query. Importantly, we use senses as a bridge across languages, thus allowing our model to perform considerably better than its supervised and unsupervised alternatives across French, German, Italian and Spanish languages on several CLEF benchmarks, while being trained on English Robust04 data only. We release SIR at https://github.com/SapienzaNLP/sir.</abstract>
      <url hash="d8809a09">2021.emnlp-main.79</url>
      <bibkey>blloshmi-etal-2021-ir</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.79</doi>
      <video href="2021.emnlp-main.79.mp4"/>
      <pwccode url="https://github.com/sapienzanlp/sir" additional="false">sapienzanlp/sir</pwccode>
    </paper>
    <paper id="80">
      <title>Neural Attention-Aware Hierarchical Topic Model</title>
      <author><first>Yuan</first><last>Jin</last></author>
      <author><first>He</first><last>Zhao</last></author>
      <author><first>Ming</first><last>Liu</last></author>
      <author><first>Lan</first><last>Du</last></author>
      <author><first>Wray</first><last>Buntine</last></author>
      <pages>1042–1052</pages>
      <abstract>Neural topic models (NTMs) apply deep neural networks to topic modelling. Despite their success, NTMs generally ignore two important aspects: (1) only document-level word count information is utilized for the training, while more fine-grained sentence-level information is ignored, and (2) external semantic knowledge regarding documents, sentences and words are not exploited for the training. To address these issues, we propose a variational autoencoder (VAE) NTM model that jointly reconstructs the sentence and document word counts using combinations of bag-of-words (BoW) topical embeddings and pre-trained semantic embeddings. The pre-trained embeddings are first transformed into a common latent topical space to align their semantics with the BoW embeddings. Our model also features hierarchical KL divergence to leverage embeddings of each document to regularize those of their sentences, paying more attention to semantically relevant sentences. Both quantitative and qualitative experiments have shown the efficacy of our model in 1) lowering the reconstruction errors at both the sentence and document levels, and 2) discovering more coherent topics from real-world datasets.</abstract>
      <url hash="07933aa6">2021.emnlp-main.80</url>
      <bibkey>jin-etal-2021-neural</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.80</doi>
      <video href="2021.emnlp-main.80.mp4"/>
    </paper>
    <paper id="81">
      <title><fixed-case>R</fixed-case>elational <fixed-case>W</fixed-case>orld <fixed-case>K</fixed-case>nowledge <fixed-case>R</fixed-case>epresentation in <fixed-case>C</fixed-case>ontextual <fixed-case>L</fixed-case>anguage <fixed-case>M</fixed-case>odels: <fixed-case>A</fixed-case> <fixed-case>R</fixed-case>eview</title>
      <author><first>Tara</first><last>Safavi</last></author>
      <author><first>Danai</first><last>Koutra</last></author>
      <pages>1053–1067</pages>
      <abstract>Relational knowledge bases (KBs) are commonly used to represent world knowledge in machines. However, while advantageous for their high degree of precision and interpretability, KBs are usually organized according to manually-defined schemas, which limit their expressiveness and require significant human efforts to engineer and maintain. In this review, we take a natural language processing perspective to these limitations, examining how they may be addressed in part by training deep contextual language models (LMs) to internalize and express relational knowledge in more flexible forms. We propose to organize knowledge representation strategies in LMs by the level of KB supervision provided, from no KB supervision at all to entity- and relation-level supervision. Our contributions are threefold: (1) We provide a high-level, extensible taxonomy for knowledge representation in LMs; (2) Within our taxonomy, we highlight notable models, evaluation tasks, and findings, in order to provide an up-to-date review of current knowledge representation capabilities in LMs; and (3) We suggest future research directions that build upon the complementary aspects of LMs and KBs as knowledge representations.</abstract>
      <url hash="e8384181">2021.emnlp-main.81</url>
      <bibkey>safavi-koutra-2021-relational</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.81</doi>
      <video href="2021.emnlp-main.81.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
    </paper>
    <paper id="82">
      <title>Certified Robustness to Programmable Transformations in <fixed-case>LSTM</fixed-case>s</title>
      <author><first>Yuhao</first><last>Zhang</last></author>
      <author><first>Aws</first><last>Albarghouthi</last></author>
      <author><first>Loris</first><last>D’Antoni</last></author>
      <pages>1068–1083</pages>
      <abstract>Deep neural networks for natural language processing are fragile in the face of adversarial examples—small input perturbations, like synonym substitution or word duplication, which cause a neural network to change its prediction. We present an approach to certifying the robustness of LSTMs (and extensions of LSTMs) and training models that can be efficiently certified. Our approach can certify robustness to intractably large perturbation spaces defined programmatically in a language of string transformations. Our evaluation shows that (1) our approach can train models that are more robust to combinations of string transformations than those produced using existing techniques; (2) our approach can show high certification accuracy of the resulting models.</abstract>
      <url hash="01523321">2021.emnlp-main.82</url>
      <attachment type="Software" hash="1872f891">2021.emnlp-main.82.Software.zip</attachment>
      <bibkey>zhang-etal-2021-certified</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.82</doi>
      <pwccode url="https://github.com/foreverzyh/certified_lstms" additional="false">foreverzyh/certified_lstms</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="83">
      <title><fixed-case>R</fixed-case>e<fixed-case>G</fixed-case>en: <fixed-case>R</fixed-case>einforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models</title>
      <author><first>Pierre</first><last>Dognin</last></author>
      <author><first>Inkit</first><last>Padhi</last></author>
      <author><first>Igor</first><last>Melnyk</last></author>
      <author><first>Payel</first><last>Das</last></author>
      <pages>1084–1099</pages>
      <abstract>Automatic construction of relevant Knowledge Bases (KBs) from text, and generation of semantically meaningful text from KBs are both long-standing goals in Machine Learning. In this paper, we present ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning to improve performance. Graph linearization enables us to re-frame both tasks as a sequence to sequence generation problem regardless of the generative direction, which in turn allows the use of Reinforcement Learning for sequence training where the model itself is employed as its own critic leading to Self-Critical Sequence Training (SCST). We present an extensive investigation demonstrating that the use of RL via SCST benefits graph and text generation on WebNLG+ 2020 and TekGen datasets. Our system provides state-of-the-art results on WebNLG+ 2020 by significantly improving upon published results from the WebNLG 2020+ Challenge for both text-to-graph and graph-to-text generation tasks. More details at https://github.com/IBM/regen.</abstract>
      <url hash="326c9605">2021.emnlp-main.83</url>
      <bibkey>dognin-etal-2021-regen</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.83</doi>
      <video href="2021.emnlp-main.83.mp4"/>
      <pwccode url="https://github.com/IBM/regen" additional="false">IBM/regen</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/kelm">KELM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tekgen">TekGen</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webnlg">WebNLG</pwcdataset>
    </paper>
    <paper id="84">
      <title>Contrastive Out-of-Distribution Detection for Pretrained Transformers</title>
      <author><first>Wenxuan</first><last>Zhou</last></author>
      <author><first>Fangyu</first><last>Liu</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <pages>1100–1111</pages>
      <abstract>Pretrained Transformers achieve remarkable performance when training and test data are from the same distribution. However, in real-world scenarios, the model often faces out-of-distribution (OOD) instances that can cause severe semantic shift problems at inference time. Therefore, in practice, a reliable model should identify such instances, and then either reject them during inference or pass them over to models that handle another distribution. In this paper, we develop an unsupervised OOD detection method, in which only the in-distribution (ID) data are used in training. We propose to fine-tune the Transformers with a contrastive loss, which improves the compactness of representations, such that OOD instances can be better differentiated from ID ones. These OOD instances can then be accurately detected using the Mahalanobis distance in the model’s penultimate layer. We experiment with comprehensive settings and achieve near-perfect OOD detection performance, outperforming baselines drastically. We further investigate the rationales behind the improvement, finding that more compact representations through margin-based contrastive learning bring the improvement. We release our code to the community for future research.</abstract>
      <url hash="5ebfdcb0">2021.emnlp-main.84</url>
      <bibkey>zhou-etal-2021-contrastive</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.84</doi>
      <video href="2021.emnlp-main.84.mp4"/>
      <pwccode url="https://github.com/wzhouad/Contra-OOD" additional="false">wzhouad/Contra-OOD</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
    </paper>
    <paper id="85">
      <title><fixed-case>M</fixed-case>ind<fixed-case>C</fixed-case>raft: Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks</title>
      <author><first>Cristian-Paul</first><last>Bara</last></author>
      <author><first>Sky</first><last>CH-Wang</last></author>
      <author><first>Joyce</first><last>Chai</last></author>
      <pages>1112–1125</pages>
      <abstract>An ideal integration of autonomous agents in a human world implies that they are able to collaborate on human terms. In particular, theory of mind plays an important role in maintaining common ground during human collaboration and communication. To enable theory of mind modeling in situated interactions, we introduce a fine-grained dataset of collaborative tasks performed by pairs of human subjects in the 3D virtual blocks world of Minecraft. It provides information that captures partners’ beliefs of the world and of each other as an interaction unfolds, bringing abundant opportunities to study human collaborative behaviors in situated language communication. As a first step towards our goal of developing embodied AI agents able to infer belief states of collaborative partners in situ, we build and present results on computational models for several theory of mind tasks.</abstract>
      <url hash="f79e0dbd">2021.emnlp-main.85</url>
      <bibkey>bara-etal-2021-mindcraft</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.85</doi>
      <video href="2021.emnlp-main.85.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mindcraft">MindCraft</pwcdataset>
    </paper>
    <paper id="86">
      <title>Detecting Speaker Personas from Conversational Texts</title>
      <author><first>Jia-Chen</first><last>Gu</last></author>
      <author><first>Zhenhua</first><last>Ling</last></author>
      <author><first>Yu</first><last>Wu</last></author>
      <author><first>Quan</first><last>Liu</last></author>
      <author><first>Zhigang</first><last>Chen</last></author>
      <author><first>Xiaodan</first><last>Zhu</last></author>
      <pages>1126–1136</pages>
      <abstract>Personas are useful for dialogue response prediction. However, the personas used in current studies are pre-defined and hard to obtain before a conversation. To tackle this issue, we study a new task, named Speaker Persona Detection (SPD), which aims to detect speaker personas based on the plain conversational text. In this task, a best-matched persona is searched out from candidates given the conversational text. This is a many-to-many semantic matching task because both contexts and personas in SPD are composed of multiple sentences. The long-term dependency and the dynamic redundancy among these sentences increase the difficulty of this task. We build a dataset for SPD, dubbed as Persona Match on Persona-Chat (PMPC). Furthermore, we evaluate several baseline models and propose utterance-to-profile (U2P) matching networks for this task. The U2P models operate at a fine granularity which treat both contexts and personas as sets of multiple sequences. Then, each sequence pair is scored and an interpretable overall score is obtained for a context-persona pair through aggregation. Evaluation results show that the U2P models outperform their baseline counterparts significantly.</abstract>
      <url hash="d5a7e646">2021.emnlp-main.86</url>
      <bibkey>gu-etal-2021-detecting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.86</doi>
      <video href="2021.emnlp-main.86.mp4"/>
      <pwccode url="https://github.com/jasonforjoy/spd" additional="false">jasonforjoy/spd</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/pmpc">PMPC</pwcdataset>
    </paper>
    <paper id="87">
      <title>Cross-lingual Intermediate Fine-tuning improves Dialogue State Tracking</title>
      <author><first>Nikita</first><last>Moghe</last></author>
      <author><first>Mark</first><last>Steedman</last></author>
      <author><first>Alexandra</first><last>Birch</last></author>
      <pages>1137–1150</pages>
      <abstract>Recent progress in task-oriented neural dialogue systems is largely focused on a handful of languages, as annotation of training data is tedious and expensive. Machine translation has been used to make systems multilingual, but this can introduce a pipeline of errors. Another promising solution is using cross-lingual transfer learning through pretrained multilingual models. Existing methods train multilingual models with additional code-mixed task data or refine the cross-lingual representations through parallel ontologies. In this work, we enhance the transfer learning process by intermediate fine-tuning of pretrained multilingual models, where the multilingual models are fine-tuned with different but related data and/or tasks. Specifically, we use parallel and conversational movie subtitles datasets to design cross-lingual intermediate tasks suitable for downstream dialogue tasks. We use only 200K lines of parallel data for intermediate fine-tuning which is already available for 1782 language pairs. We test our approach on the cross-lingual dialogue state tracking task for the parallel MultiWoZ (English -&gt; Chinese, Chinese -&gt; English) and Multilingual WoZ (English -&gt; German, English -&gt; Italian) datasets. We achieve impressive improvements (&gt; 20% on joint goal accuracy) on the parallel MultiWoZ dataset and the Multilingual WoZ dataset over the vanilla baseline with only 10% of the target language task data and zero-shot setup respectively.</abstract>
      <url hash="481ac43c">2021.emnlp-main.87</url>
      <bibkey>moghe-etal-2021-cross</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.87</doi>
      <video href="2021.emnlp-main.87.mp4"/>
      <pwccode url="https://github.com/nikitacs16/xlift_dst" additional="false">nikitacs16/xlift_dst</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="88">
      <title><fixed-case>ConvFiT:</fixed-case> <fixed-case>C</fixed-case>onversational Fine-Tuning of Pretrained Language Models</title>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Pei-Hao</first><last>Su</last></author>
      <author><first>Samuel</first><last>Coope</last></author>
      <author><first>Daniela</first><last>Gerz</last></author>
      <author><first>Paweł</first><last>Budzianowski</last></author>
      <author><first>Iñigo</first><last>Casanueva</last></author>
      <author><first>Nikola</first><last>Mrkšić</last></author>
      <author><first>Tsung-Hsien</first><last>Wen</last></author>
      <pages>1151–1168</pages>
      <abstract>Transformer-based language models (LMs) pretrained on large text collections are proven to store a wealth of semantic knowledge. However, 1) they are not effective as sentence encoders when used off-the-shelf, and 2) thus typically lag behind conversationally pretrained (e.g., via response selection) encoders on conversational tasks such as intent detection (ID). In this work, we propose ConvFiT, a simple and efficient two-stage procedure which turns any pretrained LM into a universal conversational encoder (after Stage 1 ConvFiT-ing) and task-specialised sentence encoder (after Stage 2). We demonstrate that 1) full-blown conversational pretraining is not required, and that LMs can be quickly transformed into effective conversational encoders with much smaller amounts of unannotated data; 2) pretrained LMs can be fine-tuned into task-specialised sentence encoders, optimised for the fine-grained semantics of a particular task. Consequently, such specialised sentence encoders allow for treating ID as a simple semantic similarity task based on interpretable nearest neighbours retrieval. We validate the robustness and versatility of the ConvFiT framework with such similarity-based inference on the standard ID evaluation sets: ConvFiT-ed LMs achieve state-of-the-art ID performance across the board, with particular gains in the most challenging, few-shot setups.</abstract>
      <url hash="7d2efc00">2021.emnlp-main.88</url>
      <bibkey>vulic-etal-2021-convfit</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.88</doi>
      <video href="2021.emnlp-main.88.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/banking77">BANKING77</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/clinc150">CLINC150</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hwu64">HWU64</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reddit-corpus">Reddit Corpus</pwcdataset>
    </paper>
    <paper id="89">
      <title>We’ve had this conversation before: A Novel Approach to Measuring Dialog Similarity</title>
      <author><first>Ofer</first><last>Lavi</last></author>
      <author><first>Ella</first><last>Rabinovich</last></author>
      <author><first>Segev</first><last>Shlomov</last></author>
      <author><first>David</first><last>Boaz</last></author>
      <author><first>Inbal</first><last>Ronen</last></author>
      <author><first>Ateret</first><last>Anaby Tavor</last></author>
      <pages>1169–1177</pages>
      <abstract>Dialog is a core building block of human natural language interactions. It contains multi-party utterances used to convey information from one party to another in a dynamic and evolving manner. The ability to compare dialogs is beneficial in many real world use cases, such as conversation analytics for contact center calls and virtual agent design. We propose a novel adaptation of the edit distance metric to the scenario of dialog similarity. Our approach takes into account various conversation aspects such as utterance semantics, conversation flow, and the participants. We evaluate this new approach and compare it to existing document similarity measures on two publicly available datasets. The results demonstrate that our method outperforms the other approaches in capturing dialog flow, and is better aligned with the human perception of conversation similarity.</abstract>
      <url hash="f744db65">2021.emnlp-main.89</url>
      <bibkey>lavi-etal-2021-weve</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.89</doi>
      <video href="2021.emnlp-main.89.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
    </paper>
    <paper id="90">
      <title>Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental <fixed-case>NLU</fixed-case></title>
      <author><first>Patrick</first><last>Kahardipraja</last></author>
      <author><first>Brielen</first><last>Madureira</last></author>
      <author><first>David</first><last>Schlangen</last></author>
      <pages>1178–1189</pages>
      <abstract>Incremental processing allows interactive systems to respond based on partial inputs, which is a desirable property e.g. in dialogue agents. The currently popular Transformer architecture inherently processes sequences as a whole, abstracting away the notion of time. Recent work attempts to apply Transformers incrementally via restart-incrementality by repeatedly feeding, to an unchanged model, increasingly longer input prefixes to produce partial outputs. However, this approach is computationally costly and does not scale efficiently for long sequences. In parallel, we witness efforts to make Transformers more efficient, e.g. the Linear Transformer (LT) with a recurrence mechanism. In this work, we examine the feasibility of LT for incremental NLU in English. Our results show that the recurrent LT model has better incremental performance and faster inference speed compared to the standard Transformer and LT with restart-incrementality, at the cost of part of the non-incremental (full sequence) quality. We show that the performance drop can be mitigated by training the model to wait for right context before committing to an output and that training with input prefixes is beneficial for delivering correct partial outputs.</abstract>
      <url hash="9479ca2b">2021.emnlp-main.90</url>
      <bibkey>kahardipraja-etal-2021-towards</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.90</doi>
      <video href="2021.emnlp-main.90.mp4"/>
      <pwccode url="https://github.com/pkhdipraja/towards-incremental-transformers" additional="false">pkhdipraja/towards-incremental-transformers</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="91">
      <title>Feedback Attribution for Counterfactual Bandit Learning in Multi-Domain Spoken Language Understanding</title>
      <author><first>Tobias</first><last>Falke</last></author>
      <author><first>Patrick</first><last>Lehnen</last></author>
      <pages>1190–1198</pages>
      <abstract>With counterfactual bandit learning, models can be trained based on positive and negative feedback received for historical predictions, with no labeled data needed. Such feedback is often available in real-world dialog systems, however, the modularized architecture commonly used in large-scale systems prevents the direct application of such algorithms. In this paper, we study the feedback attribution problem that arises when using counterfactual bandit learning for multi-domain spoken language understanding. We introduce an experimental setup to simulate the problem on small-scale public datasets, propose attribution methods inspired by multi-agent reinforcement learning and evaluate them against multiple baselines. We find that while directly using overall feedback leads to disastrous performance, our proposed attribution methods can allow training competitive models from user feedback.</abstract>
      <url hash="db9b6c9c">2021.emnlp-main.91</url>
      <bibkey>falke-lehnen-2021-feedback</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.91</doi>
      <video href="2021.emnlp-main.91.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="92">
      <title>Label Verbalization and Entailment for Effective Zero and Few-Shot Relation Extraction</title>
      <author><first>Oscar</first><last>Sainz</last></author>
      <author><first>Oier</first><last>Lopez de Lacalle</last></author>
      <author><first>Gorka</first><last>Labaka</last></author>
      <author><first>Ander</first><last>Barrena</last></author>
      <author><first>Eneko</first><last>Agirre</last></author>
      <pages>1199–1212</pages>
      <abstract>Relation extraction systems require large amounts of labeled examples which are costly to annotate. In this work we reformulate relation extraction as an entailment task, with simple, hand-made, verbalizations of relations produced in less than 15 min per relation. The system relies on a pretrained textual entailment engine which is run as-is (no training examples, zero-shot) or further fine-tuned on labeled examples (few-shot or fully trained). In our experiments on TACRED we attain 63% F1 zero-shot, 69% with 16 examples per relation (17% points better than the best supervised system on the same conditions), and only 4 points short to the state-of-the-art (which uses 20 times more training data). We also show that the performance can be improved significantly with larger entailment models, up to 12 points in zero-shot, allowing to report the best results to date on TACRED when fully trained. The analysis shows that our few-shot systems are specially effective when discriminating between relations, and that the performance difference in low data regimes comes mainly from identifying no-relation cases.</abstract>
      <url hash="26a706c7">2021.emnlp-main.92</url>
      <bibkey>sainz-etal-2021-label</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.92</doi>
      <video href="2021.emnlp-main.92.mp4"/>
      <pwccode url="https://github.com/osainz59/Ask2Transformers" additional="false">osainz59/Ask2Transformers</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="93">
      <title>Extend, don’t rebuild: Phrasing conditional graph modification as autoregressive sequence labelling</title>
      <author><first>Leon</first><last>Weber</last></author>
      <author><first>Jannes</first><last>Münchmeyer</last></author>
      <author><first>Samuele</first><last>Garda</last></author>
      <author><first>Ulf</first><last>Leser</last></author>
      <pages>1213–1224</pages>
      <abstract>Deriving and modifying graphs from natural language text has become a versatile basis technology for information extraction with applications in many subfields, such as semantic parsing or knowledge graph construction. A recent work used this technique for modifying scene graphs (He et al. 2020), by first encoding the original graph and then generating the modified one based on this encoding. In this work, we show that we can considerably increase performance on this problem by phrasing it as graph extension instead of graph generation. We propose the first model for the resulting graph extension problem based on autoregressive sequence labelling. On three scene graph modification data sets, this formulation leads to improvements in accuracy over the state-of-the-art between 13 and 24 percentage points. Furthermore, we introduce a novel data set from the biomedical domain which has much larger linguistic variability and more complex graphs than the scene graph modification data sets. For this data set, the state-of-the art fails to generalize, while our model can produce meaningful predictions.</abstract>
      <url hash="3065b40e">2021.emnlp-main.93</url>
      <bibkey>weber-etal-2021-extend</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.93</doi>
      <pwccode url="https://github.com/leonweber/extend" additional="false">leonweber/extend</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="94">
      <title>Zero-Shot Information Extraction as a Unified Text-to-Triple Translation</title>
      <author><first>Chenguang</first><last>Wang</last></author>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Zui</first><last>Chen</last></author>
      <author><first>Haoyun</first><last>Hong</last></author>
      <author><first>Jie</first><last>Tang</last></author>
      <author><first>Dawn</first><last>Song</last></author>
      <pages>1225–1238</pages>
      <abstract>We cast a suite of information extraction tasks into a text-to-triple translation framework. Instead of solving each task relying on task-specific datasets and models, we formalize the task as a translation between task-specific input text and output triples. By taking the task-specific input, we enable a task-agnostic translation by leveraging the latent knowledge that a pre-trained language model has about the task. We further demonstrate that a simple pre-training task of predicting which relational information corresponds to which input text is an effective way to produce task-specific outputs. This enables the zero-shot transfer of our framework to downstream tasks. We study the zero-shot performance of this framework on open information extraction (OIE2016, NYT, WEB, PENN), relation classification (FewRel and TACRED), and factual probe (Google-RE and T-REx). The model transfers non-trivially to most tasks and is often competitive with a fully supervised method without the need for any task-specific training. For instance, we significantly outperform the F1 score of the supervised open information extraction without needing to use its training set.</abstract>
      <url hash="f1896a99">2021.emnlp-main.94</url>
      <bibkey>wang-etal-2021-zero</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.94</doi>
      <video href="2021.emnlp-main.94.mp4"/>
      <pwccode url="https://github.com/cgraywang/deepex" additional="false">cgraywang/deepex</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/oie2016">OIE2016</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tacred">TACRED</pwcdataset>
    </paper>
    <paper id="95">
      <title>Learning Logic Rules for Document-Level Relation Extraction</title>
      <author><first>Dongyu</first><last>Ru</last></author>
      <author><first>Changzhi</first><last>Sun</last></author>
      <author><first>Jiangtao</first><last>Feng</last></author>
      <author><first>Lin</first><last>Qiu</last></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <author><first>Weinan</first><last>Zhang</last></author>
      <author><first>Yong</first><last>Yu</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>1239–1250</pages>
      <abstract>Document-level relation extraction aims to identify relations between entities in a whole document. Prior efforts to capture long-range dependencies have relied heavily on implicitly powerful representations learned through (graph) neural networks, which makes the model less transparent. To tackle this challenge, in this paper, we propose LogiRE, a novel probabilistic model for document-level relation extraction by learning logic rules. LogiRE treats logic rules as latent variables and consists of two modules: a rule generator and a relation extractor. The rule generator is to generate logic rules potentially contributing to final predictions, and the relation extractor outputs final predictions based on the generated logic rules. Those two modules can be efficiently optimized with the expectation-maximization (EM) algorithm. By introducing logic rules into neural networks, LogiRE can explicitly capture long-range dependencies as well as enjoy better interpretation. Empirical results show that significantly outperforms several strong baselines in terms of relation performance and logical consistency. Our code is available at https://github.com/rudongyu/LogiRE.</abstract>
      <url hash="e34e0708">2021.emnlp-main.95</url>
      <bibkey>ru-etal-2021-learning</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.95</doi>
      <video href="2021.emnlp-main.95.mp4"/>
      <pwccode url="https://github.com/rudongyu/logire" additional="false">rudongyu/logire</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dwie">DWIE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
    </paper>
    <paper id="96">
      <title>A Large-Scale Dataset for Empathetic Response Generation</title>
      <author><first>Anuradha</first><last>Welivita</last></author>
      <author><first>Yubo</first><last>Xie</last></author>
      <author><first>Pearl</first><last>Pu</last></author>
      <pages>1251–1264</pages>
      <abstract>Recent development in NLP shows a strong trend towards refining pre-trained models with a domain-specific dataset. This is especially the case for response generation where emotion plays an important role. However, existing empathetic datasets remain small, delaying research efforts in this area, for example, the development of emotion-aware chatbots. One main technical challenge has been the cost of manually annotating dialogues with the right emotion labels. In this paper, we describe a large-scale silver dataset consisting of 1M dialogues annotated with 32 fine-grained emotions, eight empathetic response intents, and the Neutral category. To achieve this goal, we have developed a novel data curation pipeline starting with a small seed of manually annotated data and eventually scaling it to a satisfactory size. We compare its quality against a state-of-the-art gold dataset using both offline experiments and visual validation methods. The resultant procedure can be used to create similar datasets in the same domain as well as in other domains.</abstract>
      <url hash="478e2132">2021.emnlp-main.96</url>
      <attachment type="Software" hash="aa44c511">2021.emnlp-main.96.Software.zip</attachment>
      <bibkey>welivita-etal-2021-large</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.96</doi>
      <video href="2021.emnlp-main.96.mp4"/>
      <pwccode url="https://github.com/anuradha1992/edos" additional="false">anuradha1992/edos</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/emotionlines">EmotionLines</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
    </paper>
    <paper id="97">
      <title>The Perils of Using <fixed-case>M</fixed-case>echanical <fixed-case>T</fixed-case>urk to Evaluate Open-Ended Text Generation</title>
      <author><first>Marzena</first><last>Karpinska</last></author>
      <author><first>Nader</first><last>Akoury</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <pages>1265–1285</pages>
      <abstract>Recent text generation research has increasingly focused on open-ended domains such as story and poetry generation. Because models built for such tasks are difficult to evaluate automatically, most researchers in the space justify their modeling choices by collecting crowdsourced human judgments of text quality (e.g., Likert scores of coherence or grammaticality) from Amazon Mechanical Turk (AMT). In this paper, we first conduct a survey of 45 open-ended text generation papers and find that the vast majority of them fail to report crucial details about their AMT tasks, hindering reproducibility. We then run a series of story evaluation experiments with both AMT workers and English teachers and discover that even with strict qualification filters, AMT workers (unlike teachers) fail to distinguish between model-generated text and human-generated references. We show that AMT worker judgments improve when they are shown model-generated output alongside human-generated references, which enables the workers to better calibrate their ratings. Finally, interviews with the English teachers provide deeper insights into the challenges of the evaluation process, particularly when rating model-generated text.</abstract>
      <url hash="2e2934c8">2021.emnlp-main.97</url>
      <bibkey>karpinska-etal-2021-perils</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.97</doi>
      <video href="2021.emnlp-main.97.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/writingprompts">WritingPrompts</pwcdataset>
    </paper>
    <paper id="98">
      <title>Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus</title>
      <author><first>Jesse</first><last>Dodge</last></author>
      <author><first>Maarten</first><last>Sap</last></author>
      <author><first>Ana</first><last>Marasović</last></author>
      <author><first>William</first><last>Agnew</last></author>
      <author><first>Gabriel</first><last>Ilharco</last></author>
      <author><first>Dirk</first><last>Groeneveld</last></author>
      <author><first>Margaret</first><last>Mitchell</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <pages>1286–1305</pages>
      <abstract>Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet.</abstract>
      <url hash="5812bef4">2021.emnlp-main.98</url>
      <bibkey>dodge-etal-2021-documenting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.98</doi>
      <video href="2021.emnlp-main.98.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/c4">C4</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webtext">WebText</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikibio">WikiBio</pwcdataset>
    </paper>
    <paper id="99">
      <title><fixed-case>A</fixed-case>fro<fixed-case>MT</fixed-case>: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 <fixed-case>A</fixed-case>frican Languages</title>
      <author><first>Machel</first><last>Reid</last></author>
      <author><first>Junjie</first><last>Hu</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Yutaka</first><last>Matsuo</last></author>
      <pages>1306–1320</pages>
      <abstract>Reproducible benchmarks are crucial in driving progress of machine translation research. However, existing machine translation benchmarks have been mostly limited to high-resource or well-represented languages. Despite an increasing interest in low-resource machine translation, there are no standardized reproducible benchmarks for many African languages, many of which are used by millions of speakers but have less digitized textual data. To tackle these challenges, we propose AfroMT, a standardized, clean, and reproducible machine translation benchmark for eight widely spoken African languages. We also develop a suite of analysis tools for system diagnosis taking into account the unique properties of these languages. Furthermore, we explore the newly considered case of low-resource focused pretraining and develop two novel data augmentation-based strategies, leveraging word-level alignment information and pseudo-monolingual data for pretraining multilingual sequence-to-sequence models. We demonstrate significant improvements when pretraining on 11 languages, with gains of up to 2 BLEU points over strong baselines. We also show gains of up to 12 BLEU points over cross-lingual transfer baselines in data-constrained scenarios. All code and pretrained models will be released as further steps towards larger reproducible benchmarks for African languages.</abstract>
      <url hash="71fac500">2021.emnlp-main.99</url>
      <bibkey>reid-etal-2021-afromt</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.99</doi>
      <video href="2021.emnlp-main.99.mp4"/>
      <pwccode url="https://github.com/machelreid/afromt" additional="false">machelreid/afromt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="100">
      <title>Evaluating the Evaluation Metrics for Style Transfer: A Case Study in Multilingual Formality Transfer</title>
      <author><first>Eleftheria</first><last>Briakou</last></author>
      <author><first>Sweta</first><last>Agrawal</last></author>
      <author><first>Joel</first><last>Tetreault</last></author>
      <author><first>Marine</first><last>Carpuat</last></author>
      <pages>1321–1336</pages>
      <abstract>While the field of style transfer (ST) has been growing rapidly, it has been hampered by a lack of standardized practices for automatic evaluation. In this paper, we evaluate leading automatic metrics on the oft-researched task of formality style transfer. Unlike previous evaluations, which focus solely on English, we expand our focus to Brazilian-Portuguese, French, and Italian, making this work the first multilingual evaluation of metrics in ST. We outline best practices for automatic evaluation in (formality) style transfer and identify several models that correlate well with human judgments and are robust across languages. We hope that this work will help accelerate development in ST, where human evaluation is often challenging to collect.</abstract>
      <url hash="4b2d8333">2021.emnlp-main.100</url>
      <bibkey>briakou-etal-2021-evaluating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.100</doi>
      <video href="2021.emnlp-main.100.mp4"/>
      <pwccode url="https://github.com/fuzhenxin/Style-Transfer-in-Text" additional="true">fuzhenxin/Style-Transfer-in-Text</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="101">
      <title><fixed-case>MS</fixed-case>-Mentions: Consistently Annotating Entity Mentions in Materials Science Procedural Text</title>
      <author><first>Tim</first><last>O’Gorman</last></author>
      <author><first>Zach</first><last>Jensen</last></author>
      <author><first>Sheshera</first><last>Mysore</last></author>
      <author><first>Kevin</first><last>Huang</last></author>
      <author><first>Rubayyat</first><last>Mahbub</last></author>
      <author><first>Elsa</first><last>Olivetti</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>1337–1352</pages>
      <abstract>Material science synthesis procedures are a promising domain for scientific NLP, as proper modeling of these recipes could provide insight into new ways of creating materials. However, a fundamental challenge in building information extraction models for material science synthesis procedures is getting accurate labels for the materials, operations, and other entities of those procedures. We present a new corpus of entity mention annotations over 595 Material Science synthesis procedural texts (157,488 tokens), which greatly expands the training data available for the Named Entity Recognition task. We outline a new label inventory designed to provide consistent annotations and a new annotation approach intended to maximize the consistency and annotation speed of domain experts. Inter-annotator agreement studies and baseline models trained upon the data suggest that the corpus provides high-quality annotations of these mention types. This corpus helps lay a foundation for future high-quality modeling of synthesis procedures.</abstract>
      <url hash="d979c2f3">2021.emnlp-main.101</url>
      <bibkey>ogorman-etal-2021-ms</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.101</doi>
      <revision id="1" href="2021.emnlp-main.101v1" hash="b1672106"/>
      <revision id="2" href="2021.emnlp-main.101v2" hash="d979c2f3" date="2022-02-27">Added missing acknowledgements section.</revision>
      <video href="2021.emnlp-main.101.mp4"/>
    </paper>
    <paper id="102">
      <title>Understanding Politics via Contextualized Discourse Processing</title>
      <author><first>Rajkumar</first><last>Pujari</last></author>
      <author><first>Dan</first><last>Goldwasser</last></author>
      <pages>1353–1367</pages>
      <abstract>Politicians often have underlying agendas when reacting to events. Arguments in contexts of various events reflect a fairly consistent set of agendas for a given entity. In spite of recent advances in Pretrained Language Models, those text representations are not designed to capture such nuanced patterns. In this paper, we propose a Compositional Reader model consisting of encoder and composer modules, that captures and leverages such information to generate more effective representations for entities, issues, and events. These representations are contextualized by tweets, press releases, issues, news articles, and participating entities. Our model processes several documents at once and generates composed representations for multiple entities over several issues or events. Via qualitative and quantitative empirical analysis, we show that these representations are meaningful and effective.</abstract>
      <url hash="8de76779">2021.emnlp-main.102</url>
      <attachment type="Software" hash="d114b293">2021.emnlp-main.102.Software.zip</attachment>
      <bibkey>pujari-goldwasser-2021-understanding</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.102</doi>
      <video href="2021.emnlp-main.102.mp4"/>
      <pwccode url="https://github.com/pujari-rajkumar/compositional_learner" additional="false">pujari-rajkumar/compositional_learner</pwccode>
    </paper>
    <paper id="103">
      <title>Conundrums in Event Coreference Resolution: Making Sense of the State of the Art</title>
      <author><first>Jing</first><last>Lu</last></author>
      <author><first>Vincent</first><last>Ng</last></author>
      <pages>1368–1380</pages>
      <abstract>Despite recent promising results on the application of span-based models for event reference interpretation, there is a lack of understanding of what has been improved. We present an empirical analysis of a state-of-the-art span-based event reference systems with the goal of providing the general NLP audience with a better understanding of the state of the art and reference researchers with directions for future research.</abstract>
      <url hash="9ed1e538">2021.emnlp-main.103</url>
      <bibkey>lu-ng-2021-conundrums</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.103</doi>
      <video href="2021.emnlp-main.103.mp4"/>
    </paper>
    <paper id="104">
      <title>Weakly supervised discourse segmentation for multiparty oral conversations</title>
      <author><first>Lila</first><last>Gravellier</last></author>
      <author><first>Julie</first><last>Hunter</last></author>
      <author><first>Philippe</first><last>Muller</last></author>
      <author><first>Thomas</first><last>Pellegrini</last></author>
      <author><first>Isabelle</first><last>Ferrané</last></author>
      <pages>1381–1392</pages>
      <abstract>Discourse segmentation, the first step of discourse analysis, has been shown to improve results for text summarization, translation and other NLP tasks. While segmentation models for written text tend to perform well, they are not directly applicable to spontaneous, oral conversation, which has linguistic features foreign to written text. Segmentation is less studied for this type of language, where annotated data is scarce, and existing corpora more heterogeneous. We develop a weak supervision approach to adapt, using minimal annotation, a state of the art discourse segmenter trained on written text to French conversation transcripts. Supervision is given by a latent model bootstrapped by manually defined heuristic rules that use linguistic and acoustic information. The resulting model improves the original segmenter, especially in contexts where information on speaker turns is lacking or noisy, gaining up to 13% in F-score. Evaluation is performed on data like those used to define our heuristic rules, but also on transcripts from two other corpora.</abstract>
      <url hash="88cbc0fb">2021.emnlp-main.104</url>
      <bibkey>gravellier-etal-2021-weakly</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.104</doi>
      <video href="2021.emnlp-main.104.mp4"/>
      <pwccode url="https://github.com/linto-project/linto-dialogue-act-segmentation" additional="false">linto-project/linto-dialogue-act-segmentation</pwccode>
    </paper>
    <paper id="105">
      <title>Narrative Embedding: <fixed-case>R</fixed-case>e-<fixed-case>C</fixed-case>ontextualization Through Attention</title>
      <author><first>Sean</first><last>Wilner</last></author>
      <author><first>Daniel</first><last>Woolridge</last></author>
      <author><first>Madeleine</first><last>Glick</last></author>
      <pages>1393–1405</pages>
      <abstract>Narrative analysis is becoming increasingly important for a number of linguistic tasks including summarization, knowledge extraction, and question answering. We present a novel approach for narrative event representation using attention to re-contextualize events across the whole story. Comparing to previous analysis we find an unexpected attachment of event semantics to predicate tokens within a popular transformer model. We test the utility of our approach on narrative completion prediction, achieving state of the art performance on Multiple Choice Narrative Cloze and scoring competitively on the Story Cloze Task.</abstract>
      <url hash="676ed77f">2021.emnlp-main.105</url>
      <bibkey>wilner-etal-2021-narrative</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.105</doi>
      <video href="2021.emnlp-main.105.mp4"/>
    </paper>
    <paper id="106">
      <title>Focus on what matters: Applying Discourse Coherence Theory to Cross Document Coreference</title>
      <author><first>William</first><last>Held</last></author>
      <author><first>Dan</first><last>Iter</last></author>
      <author><first>Dan</first><last>Jurafsky</last></author>
      <pages>1406–1417</pages>
      <abstract>Performing event and entity coreference resolution across documents vastly increases the number of candidate mentions, making it intractable to do the full <tex-math>n^2</tex-math> pairwise comparisons. Existing approaches simplify by considering coreference only within document clusters, but this fails to handle inter-cluster coreference, common in many applications. As a result cross-document coreference algorithms are rarely applied to downstream tasks. We draw on an insight from discourse coherence theory: potential coreferences are constrained by the reader’s discourse focus. We model the entities/events in a reader’s focus as a neighborhood within a learned latent embedding space which minimizes the distance between mentions and the centroids of their gold coreference clusters. We then use these neighborhoods to sample only hard negatives to train a fine-grained classifier on mention pairs and their local discourse features. Our approach achieves state-of-the-art results for both events and entities on the ECB+, Gun Violence, Football Coreference, and Cross-Domain Cross-Document Coreference corpora. Furthermore, training on multiple corpora improves average performance across all datasets by 17.2 F1 points, leading to a robust coreference resolution model that is now feasible to apply to downstream tasks.</abstract>
      <url hash="289c46b6">2021.emnlp-main.106</url>
      <bibkey>held-etal-2021-focus</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.106</doi>
      <video href="2021.emnlp-main.106.mp4"/>
      <pwccode url="https://github.com/helw150/event_entity_coref_ecb_plus" additional="false">helw150/event_entity_coref_ecb_plus</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ecb">ECB+</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gun-violence-corpus">Gun Violence Corpus</pwcdataset>
    </paper>
    <paper id="107">
      <title>Salience-Aware Event Chain Modeling for Narrative Understanding</title>
      <author><first>Xiyang</first><last>Zhang</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <author><first>Jonathan</first><last>May</last></author>
      <pages>1418–1428</pages>
      <abstract>Storytelling, whether via fables, news reports, documentaries, or memoirs, can be thought of as the communication of interesting and related events that, taken together, form a concrete process. It is desirable to extract the event chains that represent such processes. However, this extraction remains a challenging problem. We posit that this is due to the nature of the texts from which chains are discovered. Natural language text interleaves a narrative of concrete, salient events with background information, contextualization, opinion, and other elements that are important for a variety of necessary discourse and pragmatics acts but are not part of the principal chain of events being communicated. We introduce methods for extracting this principal chain from natural language text, by filtering away non-salient events and supportive sentences. We demonstrate the effectiveness of our methods at isolating critical event chains by comparing their effect on downstream tasks. We show that by pre-training large language models on our extracted chains, we obtain improvements in two tasks that benefit from a clear understanding of event chains: narrative prediction and event-based temporal question answering. The demonstrated improvements and ablative studies confirm that our extraction method isolates critical event chains.</abstract>
      <url hash="fede07d4">2021.emnlp-main.107</url>
      <bibkey>zhang-etal-2021-salience</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.107</doi>
      <video href="2021.emnlp-main.107.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/rocstories">ROCStories</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/torque">Torque</pwcdataset>
    </paper>
    <paper id="108">
      <title>Asking It All: Generating Contextualized Questions for any Semantic Role</title>
      <author><first>Valentina</first><last>Pyatkin</last></author>
      <author><first>Paul</first><last>Roit</last></author>
      <author><first>Julian</first><last>Michael</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <author><first>Reut</first><last>Tsarfaty</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <pages>1429–1441</pages>
      <abstract>Asking questions about a situation is an inherent step towards understanding it. To this end, we introduce the task of role question generation, which, given a predicate mention and a passage, requires producing a set of questions asking about all possible semantic roles of the predicate. We develop a two-stage model for this task, which first produces a context-independent question prototype for each role and then revises it to be contextually appropriate for the passage. Unlike most existing approaches to question generation, our approach does not require conditioning on existing answers in the text. Instead, we condition on the type of information to inquire about, regardless of whether the answer appears explicitly in the text, could be inferred from it, or should be sought elsewhere. Our evaluation demonstrates that we generate diverse and well-formed questions for a large, broad-coverage ontology of predicates and roles.</abstract>
      <url hash="9d93bf70">2021.emnlp-main.108</url>
      <bibkey>pyatkin-etal-2021-asking</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.108</doi>
      <video href="2021.emnlp-main.108.mp4"/>
      <pwccode url="https://github.com/valentinapy/roleqgeneration" additional="false">valentinapy/roleqgeneration</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/nombank">NomBank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qa-srl">QA-SRL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qa-srl-bank-2-0">QA-SRL Bank 2.0</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="109">
      <title>Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders</title>
      <author><first>Fangyu</first><last>Liu</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <author><first>Nigel</first><last>Collier</last></author>
      <pages>1442–1459</pages>
      <abstract>Previous work has indicated that pretrained Masked Language Models (MLMs) are not effective as universal lexical and sentence encoders off-the-shelf, i.e., without further task-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks using annotated task data. In this work, we demonstrate that it is possible to turn MLMs into effective lexical and sentence encoders even without any additional data, relying simply on self-supervision. We propose an extremely simple, fast, and effective contrastive learning technique, termed Mirror-BERT, which converts MLMs (e.g., BERT and RoBERTa) into such encoders in 20-30 seconds with no access to additional external knowledge. Mirror-BERT relies on identical and slightly modified string pairs as positive (i.e., synonymous) fine-tuning examples, and aims to maximise their similarity during “identity fine-tuning”. We report huge gains over off-the-shelf MLMs with Mirror-BERT both in lexical-level and in sentence-level tasks, across different domains and different languages. Notably, in sentence similarity (STS) and question-answer entailment (QNLI) tasks, our self-supervised Mirror-BERT model even matches the performance of the Sentence-BERT models from prior work which rely on annotated task data. Finally, we delve deeper into the inner workings of MLMs, and suggest some evidence on why this simple Mirror-BERT fine-tuning approach can yield effective universal lexical and sentence encoders.</abstract>
      <url hash="d1602175">2021.emnlp-main.109</url>
      <bibkey>liu-etal-2021-fast</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.109</doi>
      <video href="2021.emnlp-main.109.mp4"/>
      <pwccode url="https://github.com/cambridgeltl/mirror-bert" additional="false">cambridgeltl/mirror-bert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cometa">COMETA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sts-benchmark">STS Benchmark</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/semantic-textual-similarity-2012-2016">Semantic Textual Similarity (2012 - 2016)</pwcdataset>
    </paper>
    <paper id="110">
      <title><fixed-case>R</fixed-case>ule<fixed-case>BERT</fixed-case>: Teaching Soft Rules to Pre-Trained Language Models</title>
      <author><first>Mohammed</first><last>Saeed</last></author>
      <author><first>Naser</first><last>Ahmadi</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <author><first>Paolo</first><last>Papotti</last></author>
      <pages>1460–1476</pages>
      <abstract>While pre-trained language models (PLMs) are the go-to solution to tackle many natural language processing problems, they are still very limited in their ability to capture and to use common-sense knowledge. In fact, even if information is available in the form of approximate (soft) logical rules, it is not clear how to transfer it to a PLM in order to improve its performance for deductive reasoning tasks. Here, we aim to bridge this gap by teaching PLMs how to reason with soft Horn rules. We introduce a classification task where, given facts and soft rules, the PLM should return a prediction with a probability for a given hypothesis. We release the first dataset for this task, and we propose a revised loss function that enables the PLM to learn how to predict precise probabilities for the task. Our evaluation results show that the resulting fine-tuned models achieve very high performance, even on logical rules that were unseen at training. Moreover, we demonstrate that logical notions expressed by the rules are transferred to the fine-tuned model, yielding state-of-the-art results on external datasets.</abstract>
      <url hash="5569ca54">2021.emnlp-main.110</url>
      <bibkey>saeed-etal-2021-rulebert</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.110</doi>
      <video href="2021.emnlp-main.110.mp4"/>
      <pwccode url="https://github.com/mhmdsaiid/rulebert" additional="false">mhmdsaiid/rulebert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
    </paper>
    <paper id="111">
      <title>Stepmothers are mean and academics are pretentious: What do pretrained language models learn about you?</title>
      <author><first>Rochelle</first><last>Choenni</last></author>
      <author><first>Ekaterina</first><last>Shutova</last></author>
      <author><first>Robert</first><last>van Rooij</last></author>
      <pages>1477–1491</pages>
      <abstract>In this paper, we investigate what types of stereotypical information are captured by pretrained language models. We present the first dataset comprising stereotypical attributes of a range of social groups and propose a method to elicit stereotypes encoded by pretrained language models in an unsupervised fashion. Moreover, we link the emergent stereotypes to their manifestation as basic emotions as a means to study their emotional effects in a more generalized manner. To demonstrate how our methods can be used to analyze emotion and stereotype shifts due to linguistic experience, we use fine-tuning on news sources as a case study. Our experiments expose how attitudes towards different social groups vary across models and how quickly emotions and stereotypes can shift at the fine-tuning stage.</abstract>
      <url hash="7d796f71">2021.emnlp-main.111</url>
      <bibkey>choenni-etal-2021-stepmothers</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.111</doi>
      <video href="2021.emnlp-main.111.mp4"/>
    </paper>
    <paper id="112">
      <title><fixed-case>C</fixed-case>on<fixed-case>S</fixed-case>e<fixed-case>C</fixed-case>: Word Sense Disambiguation as Continuous Sense Comprehension</title>
      <author><first>Edoardo</first><last>Barba</last></author>
      <author><first>Luigi</first><last>Procopio</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>1492–1503</pages>
      <abstract>Supervised systems have nowadays become the standard recipe for Word Sense Disambiguation (WSD), with Transformer-based language models as their primary ingredient. However, while these systems have certainly attained unprecedented performances, virtually all of them operate under the constraining assumption that, given a context, each word can be disambiguated individually with no account of the other sense choices. To address this limitation and drop this assumption, we propose CONtinuous SEnse Comprehension (ConSeC), a novel approach to WSD: leveraging a recent re-framing of this task as a text extraction problem, we adapt it to our formulation and introduce a feedback loop strategy that allows the disambiguation of a target word to be conditioned not only on its context but also on the explicit senses assigned to nearby words. We evaluate ConSeC and examine how its components lead it to surpass all its competitors and set a new state of the art on English WSD. We also explore how ConSeC fares in the cross-lingual setting, focusing on 8 languages with various degrees of resource availability, and report significant improvements over prior systems. We release our code at https://github.com/SapienzaNLP/consec.</abstract>
      <url hash="24c093e6">2021.emnlp-main.112</url>
      <bibkey>barba-etal-2021-consec</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.112</doi>
      <video href="2021.emnlp-main.112.mp4"/>
      <pwccode url="https://github.com/sapienzanlp/consec" additional="false">sapienzanlp/consec</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="113">
      <title>Shortcutted Commonsense: Data Spuriousness in Deep Learning of Commonsense Reasoning</title>
      <author><first>Ruben</first><last>Branco</last></author>
      <author><first>António</first><last>Branco</last></author>
      <author><first>João</first><last>António Rodrigues</last></author>
      <author><first>João Ricardo</first><last>Silva</last></author>
      <pages>1504–1521</pages>
      <abstract>Commonsense is a quintessential human capacity that has been a core challenge to Artificial Intelligence since its inception. Impressive results in Natural Language Processing tasks, including in commonsense reasoning, have consistently been achieved with Transformer neural language models, even matching or surpassing human performance in some benchmarks. Recently, some of these advances have been called into question: so called data artifacts in the training data have been made evident as spurious correlations and shallow shortcuts that in some cases are leveraging these outstanding results. In this paper we seek to further pursue this analysis into the realm of commonsense related language processing tasks. We undertake a study on different prominent benchmarks that involve commonsense reasoning, along a number of key stress experiments, thus seeking to gain insight on whether the models are learning transferable generalizations intrinsic to the problem at stake or just taking advantage of incidental shortcuts in the data items. The results obtained indicate that most datasets experimented with are problematic, with models resorting to non-robust features and appearing not to be learning and generalizing towards the overall tasks intended to be conveyed or exemplified by the datasets.</abstract>
      <url hash="383365d9">2021.emnlp-main.113</url>
      <attachment type="Software" hash="822c363a">2021.emnlp-main.113.Software.zip</attachment>
      <bibkey>branco-etal-2021-shortcutted</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.113</doi>
      <video href="2021.emnlp-main.113.mp4"/>
      <pwccode url="https://github.com/nlx-group/shortcutted-commonsense-reasoning" additional="false">nlx-group/shortcutted-commonsense-reasoning</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/arc">ARC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/piqa">PIQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webtext">WebText</pwcdataset>
    </paper>
    <paper id="114">
      <title>When differential privacy meets <fixed-case>NLP</fixed-case>: The devil is in the detail</title>
      <author><first>Ivan</first><last>Habernal</last></author>
      <pages>1522–1528</pages>
      <abstract>Differential privacy provides a formal approach to privacy of individuals. Applications of differential privacy in various scenarios, such as protecting users’ original utterances, must satisfy certain mathematical properties. Our contribution is a formal analysis of ADePT, a differentially private auto-encoder for text rewriting (Krishna et al, 2021). ADePT achieves promising results on downstream tasks while providing tight privacy guarantees. Our proof reveals that ADePT is not differentially private, thus rendering the experimental results unsubstantiated. We also quantify the impact of the error in its private mechanism, showing that the true sensitivity is higher by at least factor 6 in an optimistic case of a very small encoder’s dimension and that the amount of utterances that are not privatized could easily reach 100% of the entire dataset. Our intention is neither to criticize the authors, nor the peer-reviewing process, but rather point out that if differential privacy applications in NLP rely on formal guarantees, these should be outlined in full and put under detailed scrutiny.</abstract>
      <url hash="cdc5bc96">2021.emnlp-main.114</url>
      <bibkey>habernal-2021-differential</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.114</doi>
      <video href="2021.emnlp-main.114.mp4"/>
      <pwccode url="https://github.com/habernal/emnlp2021-differential-privacy-nlp" additional="true">habernal/emnlp2021-differential-privacy-nlp</pwccode>
    </paper>
    <paper id="115">
      <title>Achieving Model Robustness through Discrete Adversarial Training</title>
      <author><first>Maor</first><last>Ivgi</last></author>
      <author><first>Jonathan</first><last>Berant</last></author>
      <pages>1529–1544</pages>
      <abstract>Discrete adversarial attacks are symbolic perturbations to a language input that preserve the output label but lead to a prediction error. While such attacks have been extensively explored for the purpose of evaluating model robustness, their utility for improving robustness has been limited to offline augmentation only. Concretely, given a trained model, attacks are used to generate perturbed (adversarial) examples, and the model is re-trained exactly once. In this work, we address this gap and leverage discrete attacks for online augmentation, where adversarial examples are generated at every training step, adapting to the changing nature of the model. We propose (i) a new discrete attack, based on best-first search, and (ii) random sampling attacks that unlike prior work are not based on expensive search-based procedures. Surprisingly, we find that random sampling leads to impressive gains in robustness, outperforming the commonly-used offline augmentation, while leading to a speedup at training time of ~10x. Furthermore, online augmentation with search-based attacks justifies the higher training cost, significantly improving robustness on three datasets. Last, we show that our new attack substantially improves robustness compared to prior methods.</abstract>
      <url hash="48e24533">2021.emnlp-main.115</url>
      <bibkey>ivgi-berant-2021-achieving</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.115</doi>
      <video href="2021.emnlp-main.115.mp4"/>
      <pwccode url="https://github.com/Mivg/robust_transformers" additional="false">Mivg/robust_transformers</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="116">
      <title>Debiasing Methods in Natural Language Understanding Make Bias More Accessible</title>
      <author><first>Michael</first><last>Mendelson</last></author>
      <author><first>Yonatan</first><last>Belinkov</last></author>
      <pages>1545–1557</pages>
      <abstract>Model robustness to bias is often determined by the generalization on carefully designed out-of-distribution datasets. Recent debiasing methods in natural language understanding (NLU) improve performance on such datasets by pressuring models into making unbiased predictions. An underlying assumption behind such methods is that this also leads to the discovery of more robust features in the model’s inner representations. We propose a general probing-based framework that allows for post-hoc interpretation of biases in language models, and use an information-theoretic approach to measure the extractability of certain biases from the model’s representations. We experiment with several NLU datasets and known biases, and show that, counter-intuitively, the more a language model is pushed towards a debiased regime, the more bias is actually encoded in its inner representations.</abstract>
      <url hash="52c518c3">2021.emnlp-main.116</url>
      <bibkey>mendelson-belinkov-2021-debiasing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.116</doi>
      <video href="2021.emnlp-main.116.mp4"/>
      <pwccode url="https://github.com/technion-cs-nlp/bias-probing" additional="false">technion-cs-nlp/bias-probing</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="117">
      <title>Evaluating the Robustness of Neural Language Models to Input Perturbations</title>
      <author><first>Milad</first><last>Moradi</last></author>
      <author><first>Matthias</first><last>Samwald</last></author>
      <pages>1558–1570</pages>
      <abstract>High-performance neural language models have obtained state-of-the-art results on a wide range of Natural Language Processing (NLP) tasks. However, results for common benchmark datasets often do not reflect model reliability and robustness when applied to noisy, real-world data. In this study, we design and implement various types of character-level and word-level perturbation methods to simulate realistic scenarios in which input texts may be slightly noisy or different from the data distribution on which NLP systems were trained. Conducting comprehensive experiments on different NLP tasks, we investigate the ability of high-performance language models such as BERT, XLNet, RoBERTa, and ELMo in handling different types of input perturbations. The results suggest that language models are sensitive to input perturbations and their performance can decrease even when small changes are introduced. We highlight that models need to be further improved and that current benchmarks are not reflecting model robustness well. We argue that evaluations on perturbed inputs should routinely complement widely-used benchmarks in order to yield a more realistic understanding of NLP systems’ robustness.</abstract>
      <url hash="8789c234">2021.emnlp-main.117</url>
      <bibkey>moradi-samwald-2021-evaluating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.117</doi>
      <video href="2021.emnlp-main.117.mp4"/>
      <pwccode url="https://github.com/mmoradi-iut/nlp-perturbation" additional="false">mmoradi-iut/nlp-perturbation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="118">
      <title>How much pretraining data do language models need to learn syntax?</title>
      <author><first>Laura</first><last>Pérez-Mayos</last></author>
      <author><first>Miguel</first><last>Ballesteros</last></author>
      <author><first>Leo</first><last>Wanner</last></author>
      <pages>1571–1582</pages>
      <abstract>Transformers-based pretrained language models achieve outstanding results in many well-known NLU benchmarks. However, while pretraining methods are very convenient, they are expensive in terms of time and resources. This calls for a study of the impact of pretraining data size on the knowledge of the models. We explore this impact on the syntactic capabilities of RoBERTa, using models trained on incremental sizes of raw text data. First, we use syntactic structural probes to determine whether models pretrained on more data encode a higher amount of syntactic information. Second, we perform a targeted syntactic evaluation to analyze the impact of pretraining data size on the syntactic generalization performance of the models. Third, we compare the performance of the different models on three downstream applications: part-of-speech tagging, dependency parsing and paraphrase identification. We complement our study with an analysis of the cost-benefit trade-off of training such models. Our experiments show that while models pretrained on more data encode more syntactic knowledge and perform better on downstream applications, they do not always offer a better performance across the different syntactic phenomena and come at a higher financial and environmental cost.</abstract>
      <url hash="d7c4b99f">2021.emnlp-main.118</url>
      <bibkey>perez-mayos-etal-2021-much</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.118</doi>
      <video href="2021.emnlp-main.118.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="119">
      <title>Sorting through the noise: Testing robustness of information processing in pre-trained language models</title>
      <author><first>Lalchand</first><last>Pandia</last></author>
      <author><first>Allyson</first><last>Ettinger</last></author>
      <pages>1583–1596</pages>
      <abstract>Pre-trained LMs have shown impressive performance on downstream NLP tasks, but we have yet to establish a clear understanding of their sophistication when it comes to processing, retaining, and applying information presented in their input. In this paper we tackle a component of this question by examining robustness of models’ ability to deploy relevant context information in the face of distracting content. We present models with cloze tasks requiring use of critical context information, and introduce distracting content to test how robustly the models retain and use that critical information for prediction. We also systematically manipulate the nature of these distractors, to shed light on dynamics of models’ use of contextual cues. We find that although models appear in simple contexts to make predictions based on understanding and applying relevant facts from prior context, the presence of distracting but irrelevant content has clear impact in confusing model predictions. In particular, models appear particularly susceptible to factors of semantic similarity and word position. The findings are consistent with the conclusion that LM predictions are driven in large part by superficial contextual cues, rather than by robust representations of context meaning.</abstract>
      <url hash="ca031c31">2021.emnlp-main.119</url>
      <bibkey>pandia-ettinger-2021-sorting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.119</doi>
      <video href="2021.emnlp-main.119.mp4"/>
    </paper>
    <paper id="120">
      <title>Contrastive Explanations for Model Interpretability</title>
      <author><first>Alon</first><last>Jacovi</last></author>
      <author><first>Swabha</first><last>Swayamdipta</last></author>
      <author><first>Shauli</first><last>Ravfogel</last></author>
      <author><first>Yanai</first><last>Elazar</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <pages>1597–1611</pages>
      <abstract>Contrastive explanations clarify why an event occurred in contrast to another. They are inherently intuitive to humans to both produce and comprehend. We propose a method to produce contrastive explanations in the latent space, via a projection of the input representation, such that only the features that differentiate two potential decisions are captured. Our modification allows model behavior to consider only contrastive reasoning, and uncover which aspects of the input are useful for and against particular decisions. Our contrastive explanations can additionally answer for which label, and against which alternative label, is a given input feature useful. We produce contrastive explanations via both high-level abstract concept attribution and low-level input token/span attribution for two NLP classification benchmarks. Our findings demonstrate the ability of label-contrastive explanations to provide fine-grained interpretability of model decisions.</abstract>
      <url hash="6c1403c5">2021.emnlp-main.120</url>
      <bibkey>jacovi-etal-2021-contrastive</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.120</doi>
      <video href="2021.emnlp-main.120.mp4"/>
      <pwccode url="https://github.com/allenai/contrastive-explanations" additional="false">allenai/contrastive-explanations</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="121">
      <title>On the Transferability of Adversarial Attacks against Neural Text Classifier</title>
      <author><first>Liping</first><last>Yuan</last></author>
      <author><first>Xiaoqing</first><last>Zheng</last></author>
      <author><first>Yi</first><last>Zhou</last></author>
      <author><first>Cho-Jui</first><last>Hsieh</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>1612–1625</pages>
      <abstract>Deep neural networks are vulnerable to adversarial attacks, where a small perturbation to an input alters the model prediction. In many cases, malicious inputs intentionally crafted for one model can fool another model. In this paper, we present the first study to systematically investigate the transferability of adversarial examples for text classification models and explore how various factors, including network architecture, tokenization scheme, word embedding, and model capacity, affect the transferability of adversarial examples. Based on these studies, we propose a genetic algorithm to find an ensemble of models that can be used to induce adversarial examples to fool almost all existing models. Such adversarial examples reflect the defects of the learning process and the data bias in the training set. Finally, we derive word replacement rules that can be used for model diagnostics from these adversarial examples.</abstract>
      <url hash="c0faf0f7">2021.emnlp-main.121</url>
      <bibkey>yuan-etal-2021-transferability</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.121</doi>
      <video href="2021.emnlp-main.121.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="122">
      <title>Conditional probing: measuring usable information beyond a baseline</title>
      <author><first>John</first><last>Hewitt</last></author>
      <author><first>Kawin</first><last>Ethayarajh</last></author>
      <author><first>Percy</first><last>Liang</last></author>
      <author><first>Christopher</first><last>Manning</last></author>
      <pages>1626–1639</pages>
      <abstract>Probing experiments investigate the extent to which neural representations make properties—like part-of-speech—predictable. One suggests that a representation encodes a property if probing that representation produces higher accuracy than probing a baseline representation like non-contextual word embeddings. Instead of using baselines as a point of comparison, we’re interested in measuring information that is contained in the representation but not in the baseline. For example, current methods can detect when a representation is more useful than the word identity (a baseline) for predicting part-of-speech; however, they cannot detect when the representation is predictive of just the aspects of part-of-speech not explainable by the word identity. In this work, we extend a theory of usable information called V-information and propose conditional probing, which explicitly conditions on the information in the baseline. In a case study, we find that after conditioning on non-contextual word embeddings, properties like part-of-speech are accessible at deeper layers of a network than previously thought.</abstract>
      <url hash="f129b85e">2021.emnlp-main.122</url>
      <bibkey>hewitt-etal-2021-conditional</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.122</doi>
      <video href="2021.emnlp-main.122.mp4"/>
      <pwccode url="https://github.com/john-hewitt/conditional-probing" additional="false">john-hewitt/conditional-probing</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="123">
      <title><fixed-case>GFST</fixed-case>: <fixed-case>G</fixed-case>ender-Filtered Self-Training for More Accurate Gender in Translation</title>
      <author><first>Prafulla Kumar</first><last>Choubey</last></author>
      <author><first>Anna</first><last>Currey</last></author>
      <author><first>Prashant</first><last>Mathur</last></author>
      <author><first>Georgiana</first><last>Dinu</last></author>
      <pages>1640–1654</pages>
      <abstract>Targeted evaluations have found that machine translation systems often output incorrect gender in translations, even when the gender is clear from context. Furthermore, these incorrectly gendered translations have the potential to reflect or amplify social biases. We propose gender-filtered self-training (GFST) to improve gender translation accuracy on unambiguously gendered inputs. Our GFST approach uses a source monolingual corpus and an initial model to generate gender-specific pseudo-parallel corpora which are then filtered and added to the training data. We evaluate GFST on translation from English into five languages, finding that it improves gender accuracy without damaging generic quality. We also show the viability of GFST on several experimental settings, including re-training from scratch, fine-tuning, controlling the gender balance of the data, forward translation, and back-translation.</abstract>
      <url hash="22313a5b">2021.emnlp-main.123</url>
      <bibkey>choubey-etal-2021-gfst</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.123</doi>
      <video href="2021.emnlp-main.123.mp4"/>
      <pwccode url="https://github.com/amazon-research/gfst-nmt" additional="false">amazon-research/gfst-nmt</pwccode>
    </paper>
    <paper id="124">
      <title>“Wikily” Supervised Neural Translation Tailored to Cross-Lingual Tasks</title>
      <author><first>Mohammad Sadegh</first><last>Rasooli</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <author><first>Derry Tanti</first><last>Wijaya</last></author>
      <pages>1655–1670</pages>
      <abstract>We present a simple but effective approach for leveraging Wikipedia for neural machine translation as well as cross-lingual tasks of image captioning and dependency parsing without using any direct supervision from external parallel data or supervised models in the target language. We show that first sentences and titles of linked Wikipedia pages, as well as cross-lingual image captions, are strong signals for a seed parallel data to extract bilingual dictionaries and cross-lingual word embeddings for mining parallel text from Wikipedia. Our final model achieves high BLEU scores that are close to or sometimes higher than strong <i>supervised</i> baselines in low-resource languages; e.g. supervised BLEU of 4.0 versus 12.1 from our model in English-to-Kazakh. Moreover, we tailor our <i>wikily</i> translation models to unsupervised image captioning, and cross-lingual dependency parser transfer. In image captioning, we train a multi-tasking machine translation and image captioning pipeline for Arabic and English from which the Arabic training data is a <i>wikily</i> translation of the English captioning data. Our captioning results on Arabic are slightly <i>better</i> than that of its supervised model. In dependency parsing, we translate a large amount of monolingual text, and use it as an artificial training data in an <i>annotation projection</i> framework. We show that our model outperforms recent work on cross-lingual transfer of dependency parsers.</abstract>
      <url hash="6485cc00">2021.emnlp-main.124</url>
      <bibkey>rasooli-etal-2021-wikily</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.124</doi>
      <video href="2021.emnlp-main.124.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptual-captions">Conceptual Captions</pwcdataset>
    </paper>
    <paper id="125">
      <title>m<fixed-case>T</fixed-case>6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs</title>
      <author><first>Zewen</first><last>Chi</last></author>
      <author><first>Li</first><last>Dong</last></author>
      <author><first>Shuming</first><last>Ma</last></author>
      <author><first>Shaohan</first><last>Huang</last></author>
      <author><first>Saksham</first><last>Singhal</last></author>
      <author><first>Xian-Ling</first><last>Mao</last></author>
      <author><first>Heyan</first><last>Huang</last></author>
      <author><first>Xia</first><last>Song</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <pages>1671–1683</pages>
      <abstract>Multilingual T5 pretrains a sequence-to-sequence model on massive monolingual texts, which has shown promising results on many cross-lingual tasks. In this paper, we improve multilingual text-to-text transfer Transformer with translation pairs (mT6). Specifically, we explore three cross-lingual text-to-text pre-training tasks, namely, machine translation, translation pair span corruption, and translation span corruption. In addition, we propose a partially non-autoregressive objective for text-to-text pre-training. We evaluate the methods on seven multilingual benchmark datasets, including sentence classification, named entity recognition, question answering, and abstractive summarization. Experimental results show that the proposed mT6 improves cross-lingual transferability over mT5.</abstract>
      <url hash="080a730a">2021.emnlp-main.125</url>
      <bibkey>chi-etal-2021-mt6</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.125</doi>
      <video href="2021.emnlp-main.125.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws-x">PAWS-X</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDi QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikilingua">WikiLingua</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="126">
      <title>Improving Zero-Shot Cross-Lingual Transfer Learning via Robust Training</title>
      <author><first>Kuan-Hao</first><last>Huang</last></author>
      <author><first>Wasi</first><last>Ahmad</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>1684–1697</pages>
      <abstract>Pre-trained multilingual language encoders, such as multilingual BERT and XLM-R, show great potential for zero-shot cross-lingual transfer. However, these multilingual encoders do not precisely align words and phrases across languages. Especially, learning alignments in the multilingual embedding space usually requires sentence-level or word-level parallel corpora, which are expensive to be obtained for low-resource languages. An alternative is to make the multilingual encoders more robust; when fine-tuning the encoder using downstream task, we train the encoder to tolerate noise in the contextual embedding spaces such that even if the representations of different languages are not aligned well, the model can still achieve good performance on zero-shot cross-lingual transfer. In this work, we propose a learning strategy for training robust models by drawing connections between adversarial examples and the failure cases of zero-shot cross-lingual transfer. We adopt two widely used robust training methods, adversarial training and randomized smoothing, to train the desired robust model. The experimental results demonstrate that robust training improves zero-shot cross-lingual transfer on text classification tasks. The improvement is more significant in the generalized cross-lingual transfer setting, where the pair of input sentences belong to two different languages.</abstract>
      <url hash="0eff3601">2021.emnlp-main.126</url>
      <bibkey>huang-etal-2021-improving-zero</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.126</doi>
      <video href="2021.emnlp-main.126.mp4"/>
      <pwccode url="https://github.com/uclanlp/robust-xlt" additional="false">uclanlp/robust-xlt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/paws-x">PAWS-X</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="127">
      <title>Speechformer: Reducing Information Loss in Direct Speech Translation</title>
      <author><first>Sara</first><last>Papi</last></author>
      <author><first>Marco</first><last>Gaido</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <pages>1698–1706</pages>
      <abstract>Transformer-based models have gained increasing popularity achieving state-of-the-art performance in many research fields including speech translation. However, Transformer’s quadratic complexity with respect to the input sequence length prevents its adoption as is with audio signals, which are typically represented by long sequences. Current solutions resort to an initial sub-optimal compression based on a fixed sampling of raw audio features. Therefore, potentially useful linguistic information is not accessible to higher-level layers in the architecture. To solve this issue, we propose Speechformer, an architecture that, thanks to reduced memory usage in the attention layers, avoids the initial lossy compression and aggregates information only at a higher level according to more informed linguistic criteria. Experiments on three language pairs (en→de/es/nl) show the efficacy of our solution, with gains of up to 0.8 BLEU on the standard MuST-C corpus and of up to 4.0 BLEU in a low resource scenario.</abstract>
      <url hash="a52d3ec5">2021.emnlp-main.127</url>
      <bibkey>papi-etal-2021-speechformer</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.127</doi>
      <video href="2021.emnlp-main.127.mp4"/>
      <pwccode url="https://github.com/sarapapi/fbk-fairseq" additional="false">sarapapi/fbk-fairseq</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="128">
      <title>Is “moby dick” a Whale or a Bird? Named Entities and Terminology in Speech Translation</title>
      <author><first>Marco</first><last>Gaido</last></author>
      <author><first>Susana</first><last>Rodríguez</last></author>
      <author><first>Matteo</first><last>Negri</last></author>
      <author><first>Luisa</first><last>Bentivogli</last></author>
      <author><first>Marco</first><last>Turchi</last></author>
      <pages>1707–1716</pages>
      <abstract>Automatic translation systems are known to struggle with rare words. Among these, named entities (NEs) and domain-specific terms are crucial, since errors in their translation can lead to severe meaning distortions. Despite their importance, previous speech translation (ST) studies have neglected them, also due to the dearth of publicly available resources tailored to their specific evaluation. To fill this gap, we i) present the first systematic analysis of the behavior of state-of-the-art ST systems in translating NEs and terminology, and ii) release NEuRoparl-ST, a novel benchmark built from European Parliament speeches annotated with NEs and terminology. Our experiments on the three language directions covered by our benchmark (en→es/fr/it) show that ST systems correctly translate 75–80% of terms and 65–70% of NEs, with very low performance (37–40%) on person names.</abstract>
      <url hash="3fe143fd">2021.emnlp-main.128</url>
      <bibkey>gaido-etal-2021-moby</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.128</doi>
      <video href="2021.emnlp-main.128.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/europarl-st">Europarl-ST</pwcdataset>
    </paper>
    <paper id="129">
      <title><fixed-case>H</fixed-case>inted<fixed-case>BT</fixed-case>: <fixed-case>A</fixed-case>ugmenting <fixed-case>B</fixed-case>ack-<fixed-case>T</fixed-case>ranslation with Quality and Transliteration Hints</title>
      <author><first>Sahana</first><last>Ramnath</last></author>
      <author><first>Melvin</first><last>Johnson</last></author>
      <author><first>Abhirut</first><last>Gupta</last></author>
      <author><first>Aravindan</first><last>Raghuveer</last></author>
      <pages>1717–1733</pages>
      <abstract>Back-translation (BT) of target monolingual corpora is a widely used data augmentation strategy for neural machine translation (NMT), especially for low-resource language pairs. To improve effectiveness of the available BT data, we introduce HintedBT—a family of techniques which provides hints (through tags) to the encoder and decoder. First, we propose a novel method of using both high and low quality BT data by providing hints (as source tags on the encoder) to the model about the quality of each source-target pair. We don’t filter out low quality data but instead show that these hints enable the model to learn effectively from noisy data. Second, we address the problem of predicting whether a source token needs to be translated or transliterated to the target language, which is common in cross-script translation tasks (i.e., where source and target do not share the written script). For such cases, we propose training the model with additional hints (as target tags on the decoder) that provide information about the operation required on the source (translation or both translation and transliteration). We conduct experiments and detailed analyses on standard WMT benchmarks for three cross-script low/medium-resource language pairs: Hindi,Gujarati,Tamil-to-English. Our methods compare favorably with five strong and well established baselines. We show that using these hints, both separately and together, significantly improves translation quality and leads to state-of-the-art performance in all three language pairs in corresponding bilingual settings.</abstract>
      <url hash="a0266bca">2021.emnlp-main.129</url>
      <bibkey>ramnath-etal-2021-hintedbt</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.129</doi>
      <video href="2021.emnlp-main.129.mp4"/>
    </paper>
    <paper id="130">
      <title>Translation-based Supervision for Policy Generation in Simultaneous Neural Machine Translation</title>
      <author><first>Ashkan</first><last>Alinejad</last></author>
      <author><first>Hassan S.</first><last>Shavarani</last></author>
      <author><first>Anoop</first><last>Sarkar</last></author>
      <pages>1734–1744</pages>
      <abstract>In simultaneous machine translation, finding an agent with the optimal action sequence of reads and writes that maintain a high level of translation quality while minimizing the average lag in producing target tokens remains an extremely challenging problem. We propose a novel supervised learning approach for training an agent that can detect the minimum number of reads required for generating each target token by comparing simultaneous translations against full-sentence translations during training to generate oracle action sequences. These oracle sequences can then be used to train a supervised model for action generation at inference time. Our approach provides an alternative to current heuristic methods in simultaneous translation by introducing a new training objective, which is easier to train than previous attempts at training the agent using reinforcement learning techniques for this task. Our experimental results show that our novel training method for action generation produces much higher quality translations while minimizing the average lag in simultaneous translation.</abstract>
      <url hash="283b04f8">2021.emnlp-main.130</url>
      <bibkey>alinejad-etal-2021-translation</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.130</doi>
      <video href="2021.emnlp-main.130.mp4"/>
      <pwccode url="https://github.com/sfu-natlang/supervised-simultaneous-mt" additional="false">sfu-natlang/supervised-simultaneous-mt</pwccode>
    </paper>
    <paper id="131">
      <title>Nearest Neighbour Few-Shot Learning for Cross-lingual Classification</title>
      <author><first>M Saiful</first><last>Bari</last></author>
      <author><first>Batool</first><last>Haider</last></author>
      <author><first>Saab</first><last>Mansour</last></author>
      <pages>1745–1753</pages>
      <abstract>Even though large pre-trained multilingual models (e.g. mBERT, XLM-R) have led to significant performance gains on a wide range of cross-lingual NLP tasks, success on many downstream tasks still relies on the availability of sufficient annotated data. Traditional fine-tuning of pre-trained models using only a few target samples can cause over-fitting. This can be quite limiting as most languages in the world are under-resourced. In this work, we investigate cross-lingual adaptation using a simple nearest-neighbor few-shot (<tex-math>&lt;15</tex-math> samples) inference technique for classification tasks. We experiment using a total of 16 distinct languages across two NLP tasks- XNLI and PAWS-X. Our approach consistently improves traditional fine-tuning using only a handful of labeled samples in target locales. We also demonstrate its generalization capability across tasks.</abstract>
      <url hash="a06e5a09">2021.emnlp-main.131</url>
      <bibkey>bari-etal-2021-nearest</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.131</doi>
      <video href="2021.emnlp-main.131.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="132">
      <title>Cross-Attention is All You Need: <fixed-case>A</fixed-case>dapting Pretrained <fixed-case>T</fixed-case>ransformers for Machine Translation</title>
      <author><first>Mozhdeh</first><last>Gheini</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <author><first>Jonathan</first><last>May</last></author>
      <pages>1754–1765</pages>
      <abstract>We study the power of cross-attention in the Transformer architecture within the context of transfer learning for machine translation, and extend the findings of studies into cross-attention when training from scratch. We conduct a series of experiments through fine-tuning a translation model on data where either the source or target language has changed. These experiments reveal that fine-tuning only the cross-attention parameters is nearly as effective as fine-tuning all parameters (i.e., the entire translation model). We provide insights into why this is the case and observe that limiting fine-tuning in this manner yields cross-lingually aligned embeddings. The implications of this finding for researchers and practitioners include a mitigation of catastrophic forgetting, the potential for zero-shot translation, and the ability to extend machine translation models to several new language pairs with reduced parameter storage overhead.</abstract>
      <url hash="d256e97e">2021.emnlp-main.132</url>
      <bibkey>gheini-etal-2021-cross</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.132</doi>
      <video href="2021.emnlp-main.132.mp4"/>
      <pwccode url="https://github.com/mgheini/xattn-transfer-for-mt" additional="false">mgheini/xattn-transfer-for-mt</pwccode>
    </paper>
    <paper id="133">
      <title>Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent</title>
      <author><first>William</first><last>Merrill</last></author>
      <author><first>Vivek</first><last>Ramanujan</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <author><first>Roy</first><last>Schwartz</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>1766–1781</pages>
      <abstract>The capacity of neural networks like the widely adopted transformer is known to be very high. Evidence is emerging that they learn successfully due to inductive bias in the training routine, typically a variant of gradient descent (GD). To better understand this bias, we study the tendency for transformer parameters to grow in magnitude (<tex-math>\ell_2</tex-math> norm) during training, and its implications for the emergent representations within self attention layers. Empirically, we document norm growth in the training of transformer language models, including T5 during its pretraining. As the parameters grow in magnitude, we prove that the network approximates a discretized network with saturated activation functions. Such “saturated” networks are known to have a reduced capacity compared to the full network family that can be described in terms of formal languages and automata. Our results suggest saturation is a new characterization of an inductive bias implicit in GD of particular interest for NLP. We leverage the emergent discrete structure in a saturated transformer to analyze the role of different attention heads, finding that some focus locally on a small number of positions, while other heads compute global averages, allowing counting. We believe understanding the interplay between these two capabilities may shed further light on the structure of computation within large transformers.</abstract>
      <url hash="bdb4b222">2021.emnlp-main.133</url>
      <bibkey>merrill-etal-2021-effects</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.133</doi>
      <video href="2021.emnlp-main.133.mp4"/>
      <pwccode url="https://github.com/viking-sudo-rm/norm-growth" additional="false">viking-sudo-rm/norm-growth</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="134">
      <title>Foreseeing the Benefits of Incidental Supervision</title>
      <author><first>Hangfeng</first><last>He</last></author>
      <author><first>Mingyuan</first><last>Zhang</last></author>
      <author><first>Qiang</first><last>Ning</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>1782–1800</pages>
      <abstract>Real-world applications often require improved models by leveraging *a range of cheap incidental supervision signals*. These could include partial labels, noisy labels, knowledge-based constraints, and cross-domain or cross-task annotations – all having statistical associations with gold annotations but not exactly the same. However, we currently lack a principled way to measure the benefits of these signals to a given target task, and the common practice of evaluating these benefits is through exhaustive experiments with various models and hyperparameters. This paper studies whether we can, *in a single framework, quantify the benefits of various types of incidental signals for a given target task without going through combinatorial experiments*. We propose a unified PAC-Bayesian motivated informativeness measure, PABI, that characterizes the uncertainty reduction provided by incidental supervision signals. We demonstrate PABI’s effectiveness by quantifying the value added by various types of incidental signals to sequence tagging tasks. Experiments on named entity recognition (NER) and question answering (QA) show that PABI’s predictions correlate well with learning performance, providing a promising way to determine, ahead of learning, which supervision signals would be beneficial.</abstract>
      <url hash="b0c3ac2b">2021.emnlp-main.134</url>
      <bibkey>he-etal-2021-foreseeing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.134</doi>
      <video href="2021.emnlp-main.134.mp4"/>
      <pwccode url="https://github.com/CogComp/PABI" additional="true">CogComp/PABI</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/qa-srl-bank-2-0">QA-SRL Bank 2.0</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="135">
      <title>Competency Problems: On Finding and Removing Artifacts in Language Data</title>
      <author><first>Matt</first><last>Gardner</last></author>
      <author><first>William</first><last>Merrill</last></author>
      <author><first>Jesse</first><last>Dodge</last></author>
      <author><first>Matthew</first><last>Peters</last></author>
      <author><first>Alexis</first><last>Ross</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>1801–1813</pages>
      <abstract>Much recent work in NLP has documented dataset artifacts, bias, and spurious correlations between input features and output labels. However, how to tell which features have “spurious” instead of legitimate correlations is typically left unspecified. In this work we argue that for complex language understanding tasks, all simple feature correlations are spurious, and we formalize this notion into a class of problems which we call competency problems. For example, the word “amazing” on its own should not give information about a sentiment label independent of the context in which it appears, which could include negation, metaphor, sarcasm, etc. We theoretically analyze the difficulty of creating data for competency problems when human bias is taken into account, showing that realistic datasets will increasingly deviate from competency problems as dataset size increases. This analysis gives us a simple statistical test for dataset artifacts, which we use to show more subtle biases than were described in prior work, including demonstrating that models are inappropriately affected by these less extreme biases. Our theoretical treatment of this problem also allows us to analyze proposed solutions, such as making local edits to dataset instances, and to give recommendations for future data collection and model design efforts that target competency problems.</abstract>
      <url hash="76991bde">2021.emnlp-main.135</url>
      <attachment type="Software" hash="089b5fb0">2021.emnlp-main.135.Software.tgz</attachment>
      <bibkey>gardner-etal-2021-competency</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.135</doi>
      <video href="2021.emnlp-main.135.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/english-web-treebank">English Web Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="136">
      <title>Knowledge-Aware Meta-learning for Low-Resource Text Classification</title>
      <author><first>Huaxiu</first><last>Yao</last></author>
      <author><first>Ying-xin</first><last>Wu</last></author>
      <author><first>Maruan</first><last>Al-Shedivat</last></author>
      <author><first>Eric</first><last>Xing</last></author>
      <pages>1814–1821</pages>
      <abstract>Meta-learning has achieved great success in leveraging the historical learned knowledge to facilitate the learning process of the new task. However, merely learning the knowledge from the historical tasks, adopted by current meta-learning algorithms, may not generalize well to testing tasks when they are not well-supported by training tasks. This paper studies a low-resource text classification problem and bridges the gap between meta-training and meta-testing tasks by leveraging the external knowledge bases. Specifically, we propose KGML to introduce additional representation for each sentence learned from the extracted sentence-specific knowledge graph. The extensive experiments on three datasets demonstrate the effectiveness of KGML under both supervised adaptation and unsupervised adaptation settings.</abstract>
      <url hash="5e07393f">2021.emnlp-main.136</url>
      <bibkey>yao-etal-2021-knowledge</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.136</doi>
      <video href="2021.emnlp-main.136.mp4"/>
      <pwccode url="https://github.com/huaxiuyao/KGML" additional="false">huaxiuyao/KGML</pwccode>
    </paper>
    <paper id="137">
      <title>Sentence Bottleneck Autoencoders from Transformer Language Models</title>
      <author><first>Ivan</first><last>Montero</last></author>
      <author><first>Nikolaos</first><last>Pappas</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>1822–1831</pages>
      <abstract>Representation learning for text via pretraining a language model on a large corpus has become a standard starting point for building NLP systems. This approach stands in contrast to autoencoders, also trained on raw text, but with the objective of learning to encode each input as a vector that allows full reconstruction. Autoencoders are attractive because of their latent space structure and generative properties. We therefore explore the construction of a sentence-level autoencoder from a pretrained, frozen transformer language model. We adapt the masked language modeling objective as a generative, denoising one, while only training a sentence bottleneck and a single-layer modified transformer decoder. We demonstrate that the sentence representations discovered by our model achieve better quality than previous methods that extract representations from pretrained transformers on text similarity tasks, style transfer (an example of controlled generation), and single-sentence classification tasks in the GLUE benchmark, while using fewer parameters than large pretrained models.</abstract>
      <url hash="8bde0262">2021.emnlp-main.137</url>
      <bibkey>montero-etal-2021-sentence</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.137</doi>
      <video href="2021.emnlp-main.137.mp4"/>
      <pwccode url="https://github.com/ivanmontero/autobot" additional="false">ivanmontero/autobot</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="138">
      <title>Efficient Contrastive Learning via Novel Data Augmentation and Curriculum Learning</title>
      <author><first>Seonghyeon</first><last>Ye</last></author>
      <author><first>Jiseon</first><last>Kim</last></author>
      <author><first>Alice</first><last>Oh</last></author>
      <pages>1832–1838</pages>
      <abstract>We introduce EfficientCL, a memory-efficient continual pretraining method that applies contrastive learning with novel data augmentation and curriculum learning. For data augmentation, we stack two types of operation sequentially: cutoff and PCA jittering. While pretraining steps proceed, we apply curriculum learning by incrementing the augmentation degree for each difficulty step. After data augmentation is finished, contrastive learning is applied on projected embeddings of original and augmented examples. When finetuned on GLUE benchmark, our model outperforms baseline models, especially for sentence-level tasks. Additionally, this improvement is capable with only 70% of computational memory compared to the baseline model.</abstract>
      <url hash="fbd8e08f">2021.emnlp-main.138</url>
      <attachment type="Software" hash="cb2d62a0">2021.emnlp-main.138.Software.zip</attachment>
      <bibkey>ye-etal-2021-efficient</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.138</doi>
      <video href="2021.emnlp-main.138.mp4"/>
      <pwccode url="https://github.com/vano1205/efficientcl" additional="false">vano1205/efficientcl</pwccode>
    </paper>
    <paper id="139">
      <title><fixed-case>CR</fixed-case>-Walker: Tree-Structured Graph Reasoning and Dialog Acts for Conversational Recommendation</title>
      <author><first>Wenchang</first><last>Ma</last></author>
      <author><first>Ryuichi</first><last>Takanobu</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>1839–1851</pages>
      <abstract>Growing interests have been attracted in Conversational Recommender Systems (CRS), which explore user preference through conversational interactions in order to make appropriate recommendation. However, there is still a lack of ability in existing CRS to (1) traverse multiple reasoning paths over background knowledge to introduce relevant items and attributes, and (2) arrange selected entities appropriately under current system intents to control response generation. To address these issues, we propose CR-Walker in this paper, a model that performs tree-structured reasoning on a knowledge graph, and generates informative dialog acts to guide language generation. The unique scheme of tree-structured reasoning views the traversed entity at each hop as part of dialog acts to facilitate language generation, which links how entities are selected and expressed. Automatic and human evaluations show that CR-Walker can arrive at more accurate recommendation, and generate more informative and engaging responses.</abstract>
      <url hash="f5bcd32e">2021.emnlp-main.139</url>
      <bibkey>ma-etal-2021-cr</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.139</doi>
      <video href="2021.emnlp-main.139.mp4"/>
      <pwccode url="https://github.com/truthless11/CR-Walker" additional="false">truthless11/CR-Walker</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/redial">ReDial</pwcdataset>
    </paper>
    <paper id="140">
      <title><fixed-case>DIALKI</fixed-case>: Knowledge Identification in Conversational Systems through Dialogue-Document Contextualization</title>
      <author><first>Zeqiu</first><last>Wu</last></author>
      <author><first>Bo-Ru</first><last>Lu</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <author><first>Mari</first><last>Ostendorf</last></author>
      <pages>1852–1863</pages>
      <abstract>Identifying relevant knowledge to be used in conversational systems that are grounded in long documents is critical to effective response generation. We introduce a knowledge identification model that leverages the document structure to provide dialogue-contextualized passage encodings and better locate knowledge relevant to the conversation. An auxiliary loss captures the history of dialogue-document connections. We demonstrate the effectiveness of our model on two document-grounded conversational datasets and provide analyses showing generalization to unseen documents and long dialogue contexts.</abstract>
      <url hash="e3d5606e">2021.emnlp-main.140</url>
      <bibkey>wu-etal-2021-dialki</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.140</doi>
      <video href="2021.emnlp-main.140.mp4"/>
      <pwccode url="https://github.com/ellenmellon/dialki" additional="false">ellenmellon/dialki</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/doc2dial-1">Doc2Dial</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/holl-e">Holl-E</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/doc2dial">doc2dial</pwcdataset>
    </paper>
    <paper id="141">
      <title>Iconary: A Pictionary-Based Game for Testing Multimodal Communication with Drawings and Text</title>
      <author><first>Christopher</first><last>Clark</last></author>
      <author><first>Jordi</first><last>Salvador</last></author>
      <author><first>Dustin</first><last>Schwenk</last></author>
      <author><first>Derrick</first><last>Bonafilia</last></author>
      <author><first>Mark</first><last>Yatskar</last></author>
      <author><first>Eric</first><last>Kolve</last></author>
      <author><first>Alvaro</first><last>Herrasti</last></author>
      <author><first>Jonghyun</first><last>Choi</last></author>
      <author><first>Sachin</first><last>Mehta</last></author>
      <author><first>Sam</first><last>Skjonsberg</last></author>
      <author><first>Carissa</first><last>Schoenick</last></author>
      <author><first>Aaron</first><last>Sarnat</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <author><first>Aniruddha</first><last>Kembhavi</last></author>
      <author><first>Oren</first><last>Etzioni</last></author>
      <author><first>Ali</first><last>Farhadi</last></author>
      <pages>1864–1886</pages>
      <abstract>Communicating with humans is challenging for AIs because it requires a shared understanding of the world, complex semantics (e.g., metaphors or analogies), and at times multi-modal gestures (e.g., pointing with a finger, or an arrow in a diagram). We investigate these challenges in the context of Iconary, a collaborative game of drawing and guessing based on Pictionary, that poses a novel challenge for the research community. In Iconary, a Guesser tries to identify a phrase that a Drawer is drawing by composing icons, and the Drawer iteratively revises the drawing to help the Guesser in response. This back-and-forth often uses canonical scenes, visual metaphor, or icon compositions to express challenging words, making it an ideal test for mixing language and visual/symbolic communication in AI. We propose models to play Iconary and train them on over 55,000 games between human players. Our models are skillful players and are able to employ world knowledge in language models to play with words unseen during training.</abstract>
      <url hash="97743190">2021.emnlp-main.141</url>
      <bibkey>clark-etal-2021-iconary</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.141</doi>
      <video href="2021.emnlp-main.141.mp4"/>
      <pwccode url="https://github.com/allenai/iconary" additional="false">allenai/iconary</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/iconary">Iconary</pwcdataset>
    </paper>
    <paper id="142">
      <title>Self-training Improves Pre-training for Few-shot Learning in Task-oriented Dialog Systems</title>
      <author><first>Fei</first><last>Mi</last></author>
      <author><first>Wanhao</first><last>Zhou</last></author>
      <author><first>Lingjing</first><last>Kong</last></author>
      <author><first>Fengyu</first><last>Cai</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <author><first>Boi</first><last>Faltings</last></author>
      <pages>1887–1898</pages>
      <abstract>As the labeling cost for different modules in task-oriented dialog (ToD) systems is expensive, a major challenge is to train different modules with the least amount of labeled data. Recently, large-scale pre-trained language models, have shown promising results for few-shot learning in ToD. In this paper, we devise a self-training approach to utilize the abundant unlabeled dialog data to further improve state-of-the-art pre-trained models in few-shot learning scenarios for ToD systems. Specifically, we propose a self-training approach that iteratively labels the most confident unlabeled data to train a stronger Student model. Moreover, a new text augmentation technique (GradAug) is proposed to better train the Student by replacing non-crucial tokens using a masked language model. We conduct extensive experiments and present analyses on four downstream tasks in ToD, including intent classification, dialog state tracking, dialog act prediction, and response selection. Empirical results demonstrate that the proposed self-training approach consistently improves state-of-the-art pre-trained models (BERT, ToD-BERT) when only a small number of labeled data are available.</abstract>
      <url hash="13353330">2021.emnlp-main.142</url>
      <bibkey>mi-etal-2021-self</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.142</doi>
      <video href="2021.emnlp-main.142.mp4"/>
      <pwccode url="https://github.com/mifei/st-tod" additional="false">mifei/st-tod</pwccode>
    </paper>
    <paper id="143">
      <title>Contextual Rephrase Detection for Reducing Friction in Dialogue Systems</title>
      <author><first>Zhuoyi</first><last>Wang</last></author>
      <author><first>Saurabh</first><last>Gupta</last></author>
      <author><first>Jie</first><last>Hao</last></author>
      <author><first>Xing</first><last>Fan</last></author>
      <author><first>Dingcheng</first><last>Li</last></author>
      <author><first>Alexander Hanbo</first><last>Li</last></author>
      <author><first>Chenlei</first><last>Guo</last></author>
      <pages>1899–1905</pages>
      <abstract>For voice assistants like Alexa, Google Assistant, and Siri, correctly interpreting users’ intentions is of utmost importance. However, users sometimes experience friction with these assistants, caused by errors from different system components or user errors such as slips of the tongue. Users tend to rephrase their queries until they get a satisfactory response. Rephrase detection is used to identify the rephrases and has long been treated as a task with pairwise input, which does not fully utilize the contextual information (e.g. users’ implicit feedback). To this end, we propose a contextual rephrase detection model ContReph to automatically identify rephrases from multi-turn dialogues. We showcase how to leverage the dialogue context and user-agent interaction signals, including the user’s implicit feedback and the time gap between different turns, which can help significantly outperform the pairwise rephrase detection models.</abstract>
      <url hash="e658da22">2021.emnlp-main.143</url>
      <bibkey>wang-etal-2021-contextual</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.143</doi>
      <video href="2021.emnlp-main.143.mp4"/>
    </paper>
    <paper id="144">
      <title>Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning</title>
      <author><first>Jianguo</first><last>Zhang</last></author>
      <author><first>Trung</first><last>Bui</last></author>
      <author><first>Seunghyun</first><last>Yoon</last></author>
      <author><first>Xiang</first><last>Chen</last></author>
      <author><first>Zhiwei</first><last>Liu</last></author>
      <author><first>Congying</first><last>Xia</last></author>
      <author><first>Quan Hung</first><last>Tran</last></author>
      <author><first>Walter</first><last>Chang</last></author>
      <author><first>Philip</first><last>Yu</last></author>
      <pages>1906–1912</pages>
      <abstract>In this work, we focus on a more challenging few-shot intent detection scenario where many intents are fine-grained and semantically similar. We present a simple yet effective few-shot intent detection schema via contrastive pre-training and fine-tuning. Specifically, we first conduct self-supervised contrastive pre-training on collected intent datasets, which implicitly learns to discriminate semantically similar utterances without using any labels. We then perform few-shot intent detection together with supervised contrastive learning, which explicitly pulls utterances from the same intent closer and pushes utterances across different intents farther. Experimental results show that our proposed method achieves state-of-the-art performance on three challenging intent detection datasets under 5-shot and 10-shot settings.</abstract>
      <url hash="35bcd944">2021.emnlp-main.144</url>
      <bibkey>zhang-etal-2021-shot</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.144</doi>
      <video href="2021.emnlp-main.144.mp4"/>
      <pwccode url="https://github.com/jianguoz/Few-Shot-Intent-Detection" additional="true">jianguoz/Few-Shot-Intent-Detection</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/banking77">BANKING77</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hwu64">HWU64</pwcdataset>
    </paper>
    <paper id="145">
      <title>“It doesn’t look good for a date”: Transforming Critiques into Preferences for Conversational Recommendation Systems</title>
      <author><first>Victor</first><last>Bursztyn</last></author>
      <author><first>Jennifer</first><last>Healey</last></author>
      <author><first>Nedim</first><last>Lipka</last></author>
      <author><first>Eunyee</first><last>Koh</last></author>
      <author><first>Doug</first><last>Downey</last></author>
      <author><first>Larry</first><last>Birnbaum</last></author>
      <pages>1913–1918</pages>
      <abstract>Conversations aimed at determining good recommendations are iterative in nature. People often express their preferences in terms of a critique of the current recommendation (e.g., “It doesn’t look good for a date”), requiring some degree of common sense for a preference to be inferred. In this work, we present a method for transforming a user critique into a positive preference (e.g., “I prefer more romantic”) in order to retrieve reviews pertaining to potentially better recommendations (e.g., “Perfect for a romantic dinner”). We leverage a large neural language model (LM) in a few-shot setting to perform critique-to-preference transformation, and we test two methods for retrieving recommendations: one that matches embeddings, and another that fine-tunes an LM for the task. We instantiate this approach in the restaurant domain and evaluate it using a new dataset of restaurant critiques. In an ablation study, we show that utilizing critique-to-preference transformation improves recommendations, and that there are at least three general cases that explain this improved performance.</abstract>
      <url hash="ced9dd7e">2021.emnlp-main.145</url>
      <attachment type="Software" hash="058883c1">2021.emnlp-main.145.Software.zip</attachment>
      <bibkey>bursztyn-etal-2021-doesnt</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.145</doi>
      <video href="2021.emnlp-main.145.mp4"/>
      <pwccode url="https://github.com/vbursztyn/critique-to-preference-emnlp2021" additional="false">vbursztyn/critique-to-preference-emnlp2021</pwccode>
    </paper>
    <paper id="146">
      <title><fixed-case>A</fixed-case>ttention<fixed-case>R</fixed-case>ank: Unsupervised Keyphrase Extraction using Self and Cross Attentions</title>
      <author><first>Haoran</first><last>Ding</last></author>
      <author><first>Xiao</first><last>Luo</last></author>
      <pages>1919–1928</pages>
      <abstract>Keyword or keyphrase extraction is to identify words or phrases presenting the main topics of a document. This paper proposes the AttentionRank, a hybrid attention model, to identify keyphrases from a document in an unsupervised manner. AttentionRank calculates self-attention and cross-attention using a pre-trained language model. The self-attention is designed to determine the importance of a candidate within the context of a sentence. The cross-attention is calculated to identify the semantic relevance between a candidate and sentences within a document. We evaluate the AttentionRank on three publicly available datasets against seven baselines. The results show that the AttentionRank is an effective and robust unsupervised keyphrase extraction model on both long and short documents. Source code is available on Github.</abstract>
      <url hash="b8c7b71d">2021.emnlp-main.146</url>
      <bibkey>ding-luo-2021-attentionrank</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.146</doi>
      <video href="2021.emnlp-main.146.mp4"/>
      <pwccode url="https://github.com/hd10-iupui/attentionrank" additional="false">hd10-iupui/attentionrank</pwccode>
    </paper>
    <paper id="147">
      <title>Unsupervised Relation Extraction: A Variational Autoencoder Approach</title>
      <author><first>Chenhan</first><last>Yuan</last></author>
      <author><first>Hoda</first><last>Eldardiry</last></author>
      <pages>1929–1938</pages>
      <abstract>Unsupervised relation extraction works by clustering entity pairs that have the same relations in the text. Some existing variational autoencoder (VAE)-based approaches train the relation extraction model as an encoder that generates relation classifications. A decoder is trained along with the encoder to reconstruct the encoder input based on the encoder-generated relation classifications. These classifications are a latent variable so they are required to follow a pre-defined prior distribution which results in unstable training. We propose a VAE-based unsupervised relation extraction technique that overcomes this limitation by using the classifications as an intermediate variable instead of a latent variable. Specifically, classifications are conditioned on sentence input, while the latent variable is conditioned on both the classifications and the sentence input. This allows our model to connect the decoder with the encoder without putting restrictions on the classification distribution; which improves training stability. Our approach is evaluated on the NYT dataset and outperforms state-of-the-art methods.</abstract>
      <url hash="113693c2">2021.emnlp-main.147</url>
      <bibkey>yuan-eldardiry-2021-unsupervised</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.147</doi>
      <video href="2021.emnlp-main.147.mp4"/>
    </paper>
    <paper id="148">
      <title>Robust Retrieval Augmented Generation for Zero-shot Slot Filling</title>
      <author><first>Michael</first><last>Glass</last></author>
      <author><first>Gaetano</first><last>Rossiello</last></author>
      <author><first>Md Faisal Mahbub</first><last>Chowdhury</last></author>
      <author><first>Alfio</first><last>Gliozzo</last></author>
      <pages>1939–1949</pages>
      <abstract>Automatically inducing high quality knowledge graphs from a given collection of documents still remains a challenging problem in AI. One way to make headway for this problem is through advancements in a related task known as slot filling. In this task, given an entity query in form of [Entity, Slot, ?], a system is asked to ‘fill’ the slot by generating or extracting the missing value exploiting evidence extracted from relevant passage(s) in the given document collection. The recent works in the field try to solve this task in an end-to-end fashion using retrieval-based language models. In this paper, we present a novel approach to zero-shot slot filling that extends dense passage retrieval with hard negatives and robust training procedures for retrieval augmented generation models. Our model reports large improvements on both T-REx and zsRE slot filling datasets, improving both passage retrieval and slot value generation, and ranking at the top-1 position in the KILT leaderboard. Moreover, we demonstrate the robustness of our system showing its domain adaptation capability on a new variant of the TACRED dataset for slot filling, through a combination of zero/few-shot learning. We release the source code and pre-trained models.</abstract>
      <url hash="9c7ab772">2021.emnlp-main.148</url>
      <bibkey>glass-etal-2021-robust</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.148</doi>
      <video href="2021.emnlp-main.148.mp4"/>
      <pwccode url="https://github.com/ibm/kgi-slot-filling" additional="true">ibm/kgi-slot-filling</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/kilt">KILT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/t-rex">T-REx</pwcdataset>
    </paper>
    <paper id="149">
      <title>Everything Is All It Takes: A Multipronged Strategy for Zero-Shot Cross-Lingual Information Extraction</title>
      <author><first>Mahsa</first><last>Yarmohammadi</last></author>
      <author><first>Shijie</first><last>Wu</last></author>
      <author><first>Marc</first><last>Marone</last></author>
      <author><first>Haoran</first><last>Xu</last></author>
      <author><first>Seth</first><last>Ebner</last></author>
      <author><first>Guanghui</first><last>Qin</last></author>
      <author><first>Yunmo</first><last>Chen</last></author>
      <author><first>Jialiang</first><last>Guo</last></author>
      <author><first>Craig</first><last>Harman</last></author>
      <author><first>Kenton</first><last>Murray</last></author>
      <author><first>Aaron Steven</first><last>White</last></author>
      <author><first>Mark</first><last>Dredze</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>1950–1967</pages>
      <abstract>Zero-shot cross-lingual information extraction (IE) describes the construction of an IE model for some target language, given existing annotations exclusively in some other language, typically English. While the advance of pretrained multilingual encoders suggests an easy optimism of “train on English, run on any language”, we find through a thorough exploration and extension of techniques that a combination of approaches, both new and old, leads to better performance than any one cross-lingual strategy in particular. We explore techniques including data projection and self-training, and how different pretrained encoders impact them. We use English-to-Arabic IE as our initial example, demonstrating strong performance in this setting for event extraction, named entity recognition, part-of-speech tagging, and dependency parsing. We then apply data projection and self-training to three tasks across eight target languages. Because no single set of techniques performs the best across all tasks, we encourage practitioners to explore various configurations of the techniques described in this work when seeking to improve on zero-shot training.</abstract>
      <url hash="3d756100">2021.emnlp-main.149</url>
      <bibkey>yarmohammadi-etal-2021-everything</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.149</doi>
      <video href="2021.emnlp-main.149.mp4"/>
      <pwccode url="https://github.com/shijie-wu/crosslingual-nlp" additional="true">shijie-wu/crosslingual-nlp</pwccode>
    </paper>
    <paper id="150">
      <title>Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies</title>
      <author><first>Sunipa</first><last>Dev</last></author>
      <author><first>Masoud</first><last>Monajatipoor</last></author>
      <author><first>Anaelia</first><last>Ovalle</last></author>
      <author><first>Arjun</first><last>Subramonian</last></author>
      <author><first>Jeff</first><last>Phillips</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>1968–1994</pages>
      <abstract>Gender is widely discussed in the context of language tasks and when examining the stereotypes propagated by language models. However, current discussions primarily treat gender as binary, which can perpetuate harms such as the cyclical erasure of non-binary gender identities. These harms are driven by model and dataset biases, which are consequences of the non-recognition and lack of understanding of non-binary genders in society. In this paper, we explain the complexity of gender and language around it, and survey non-binary persons to understand harms associated with the treatment of gender as binary in English language technologies. We also detail how current language representations (e.g., GloVe, BERT) capture and perpetuate these harms and related challenges that need to be acknowledged and addressed for representations to equitably encode gender information.</abstract>
      <url hash="aabd4ddb">2021.emnlp-main.150</url>
      <bibkey>dev-etal-2021-harms</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.150</doi>
      <video href="2021.emnlp-main.150.mp4"/>
    </paper>
    <paper id="151">
      <title>Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search</title>
      <author><first>Jialu</first><last>Wang</last></author>
      <author id="yang-liu-umich"><first>Yang</first><last>Liu</last></author>
      <author><first>Xin</first><last>Wang</last></author>
      <pages>1995–2008</pages>
      <abstract>Internet search affects people’s cognition of the world, so mitigating biases in search results and learning fair models is imperative for social good. We study a unique gender bias in image search in this work: the search images are often gender-imbalanced for gender-neutral natural language queries. We diagnose two typical image search models, the specialized model trained on in-domain datasets and the generalized representation model pre-trained on massive image and text data across the internet. Both models suffer from severe gender bias. Therefore, we introduce two novel debiasing approaches: an in-processing fair sampling method to address the gender imbalance issue for training models, and a post-processing feature clipping method base on mutual information to debias multimodal representations of pre-trained models. Extensive experiments on MS-COCO and Flickr30K benchmarks show that our methods significantly reduce the gender bias in image search models.</abstract>
      <url hash="6880d28d">2021.emnlp-main.151</url>
      <attachment type="Software" hash="ceabaa30">2021.emnlp-main.151.Software.zip</attachment>
      <bibkey>wang-etal-2021-gender</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.151</doi>
      <video href="2021.emnlp-main.151.mp4"/>
      <pwccode url="https://github.com/kuanghuei/SCAN" additional="false">kuanghuei/SCAN</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
    </paper>
    <paper id="152">
      <title>Style Pooling: Automatic Text Style Obfuscation for Improved Classification Fairness</title>
      <author><first>Fatemehsadat</first><last>Mireshghallah</last></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last></author>
      <pages>2009–2022</pages>
      <abstract>Text style can reveal sensitive attributes of the author (e.g. age and race) to the reader, which can, in turn, lead to privacy violations and bias in both human and algorithmic decisions based on text. For example, the style of writing in job applications might reveal protected attributes of the candidate which could lead to bias in hiring decisions, regardless of whether hiring decisions are made algorithmically or by humans. We propose a VAE-based framework that obfuscates stylistic features of human-generated text through style transfer, by automatically re-writing the text itself. Critically, our framework operationalizes the notion of obfuscated style in a flexible way that enables two distinct notions of obfuscated style: (1) a minimal notion that effectively intersects the various styles seen in training, and (2) a maximal notion that seeks to obfuscate by adding stylistic features of all sensitive attributes to text, in effect, computing a union of styles. Our style-obfuscation framework can be used for multiple purposes, however, we demonstrate its effectiveness in improving the fairness of downstream classifiers. We also conduct a comprehensive study on style-pooling’s effect on fluency, semantic consistency, and attribute removal from text, in two and three domain style transfer.</abstract>
      <url hash="e3860074">2021.emnlp-main.152</url>
      <bibkey>mireshghallah-berg-kirkpatrick-2021-style</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.152</doi>
      <video href="2021.emnlp-main.152.mp4"/>
      <pwccode url="https://github.com/mireshghallah/style-pooling" additional="false">mireshghallah/style-pooling</pwccode>
    </paper>
    <paper id="153">
      <title>Modeling Disclosive Transparency in <fixed-case>NLP</fixed-case> Application Descriptions</title>
      <author><first>Michael</first><last>Saxon</last></author>
      <author><first>Sharon</first><last>Levy</last></author>
      <author><first>Xinyi</first><last>Wang</last></author>
      <author><first>Alon</first><last>Albalak</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>2023–2037</pages>
      <abstract>Broader disclosive transparency—truth and clarity in communication regarding the function of AI systems—is widely considered desirable. Unfortunately, it is a nebulous concept, difficult to both define and quantify. This is problematic, as previous work has demonstrated possible trade-offs and negative consequences to disclosive transparency, such as a confusion effect, where “too much information” clouds a reader’s understanding of what a system description means. Disclosive transparency’s subjective nature has rendered deep study into these problems and their remedies difficult. To improve this state of affairs, We introduce neural language model-based probabilistic metrics to directly model disclosive transparency, and demonstrate that they correlate with user and expert opinions of system transparency, making them a valid objective proxy. Finally, we demonstrate the use of these metrics in a pilot study quantifying the relationships between transparency, confusion, and user perceptions in a corpus of real NLP system descriptions.</abstract>
      <url hash="d4cf9b97">2021.emnlp-main.153</url>
      <bibkey>saxon-etal-2021-modeling</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.153</doi>
      <video href="2021.emnlp-main.153.mp4"/>
      <pwccode url="https://github.com/michaelsaxon/disclosive-transparency" additional="false">michaelsaxon/disclosive-transparency</pwccode>
    </paper>
    <paper id="154">
      <title>Reconstruction Attack on Instance Encoding for Language Understanding</title>
      <author><first>Shangyu</first><last>Xie</last></author>
      <author><first>Yuan</first><last>Hong</last></author>
      <pages>2038–2044</pages>
      <abstract>A private learning scheme TextHide was recently proposed to protect the private text data during the training phase via so-called instance encoding. We propose a novel reconstruction attack to break TextHide by recovering the private training data, and thus unveil the privacy risks of instance encoding. We have experimentally validated the effectiveness of the reconstruction attack with two commonly-used datasets for sentence classification. Our attack would advance the development of privacy preserving machine learning in the context of natural language processing.</abstract>
      <url hash="9e4baf94">2021.emnlp-main.154</url>
      <bibkey>xie-hong-2021-reconstruction</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.154</doi>
      <video href="2021.emnlp-main.154.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="155">
      <title>Fairness-aware Class Imbalanced Learning</title>
      <author><first>Shivashankar</first><last>Subramanian</last></author>
      <author><first>Afshin</first><last>Rahimi</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <author><first>Lea</first><last>Frermann</last></author>
      <pages>2045–2051</pages>
      <abstract>Class imbalance is a common challenge in many NLP tasks, and has clear connections to bias, in that bias in training data often leads to higher accuracy for majority groups at the expense of minority groups. However there has traditionally been a disconnect between research on class-imbalanced learning and mitigating bias, and only recently have the two been looked at through a common lens. In this work we evaluate long-tail learning methods for tweet sentiment and occupation classification, and extend a margin-loss based approach with methods to enforce fairness. We empirically show through controlled experiments that the proposed approaches help mitigate both class imbalance and demographic biases.</abstract>
      <url hash="6acc4529">2021.emnlp-main.155</url>
      <bibkey>subramanian-etal-2021-fairness</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.155</doi>
      <video href="2021.emnlp-main.155.mp4"/>
    </paper>
    <paper id="156">
      <title><fixed-case>CRYPTOGRU</fixed-case>: Low Latency Privacy-Preserving Text Analysis With <fixed-case>GRU</fixed-case></title>
      <author><first>Bo</first><last>Feng</last></author>
      <author><first>Qian</first><last>Lou</last></author>
      <author><first>Lei</first><last>Jiang</last></author>
      <author><first>Geoffrey</first><last>Fox</last></author>
      <pages>2052–2057</pages>
      <abstract>Homomorphic encryption (HE) and garbled circuit (GC) provide the protection for users’ privacy. However, simply mixing the HE and GC in RNN models suffer from long inference latency due to slow activation functions. In this paper, we present a novel hybrid structure of HE and GC gated recurrent unit (GRU) network, , for low-latency secure inferences. replaces computationally expensive GC-based <tex-math>tanh</tex-math> with fast GC-based <tex-math>ReLU</tex-math>, and then quantizes <tex-math>sigmoid</tex-math> and <tex-math>ReLU</tex-math> to smaller bit-length to accelerate activations in a GRU. We evaluate with multiple GRU models trained on 4 public datasets. Experimental results show achieves top-notch accuracy and improves the secure inference latency by up to <tex-math>138\times</tex-math> over one of the state-of-the-art secure networks on the Penn Treebank dataset.</abstract>
      <url hash="c9d13de1">2021.emnlp-main.156</url>
      <bibkey>feng-etal-2021-cryptogru</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.156</doi>
      <video href="2021.emnlp-main.156.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="157">
      <title>Local Word Discovery for Interactive Transcription</title>
      <author><first>William</first><last>Lane</last></author>
      <author><first>Steven</first><last>Bird</last></author>
      <pages>2058–2067</pages>
      <abstract>Human expertise and the participation of speech communities are essential factors in the success of technologies for low-resource languages. Accordingly, we propose a new computational task which is tuned to the available knowledge and interests in an Indigenous community, and which supports the construction of high quality texts and lexicons. The task is illustrated for Kunwinjku, a morphologically-complex Australian language. We combine a finite state implementation of a published grammar with a partial lexicon, and apply this to a noisy phone representation of the signal. We locate known lexemes in the signal and use the morphological transducer to build these out into hypothetical, morphologically-complex words for human validation. We show that applying a single iteration of this method results in a relative transcription density gain of 17%. Further, we find that 75% of breath groups in the test set receive at least one correct partial or full-word suggestion.</abstract>
      <url hash="3c358880">2021.emnlp-main.157</url>
      <bibkey>lane-bird-2021-local</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.157</doi>
      <video href="2021.emnlp-main.157.mp4"/>
    </paper>
    <paper id="158">
      <title>Segment, Mask, and Predict: Augmenting <fixed-case>C</fixed-case>hinese Word Segmentation with Self-Supervision</title>
      <author><first>Mieradilijiang</first><last>Maimaiti</last></author>
      <author id="yang-liu-ict"><first>Yang</first><last>Liu</last></author>
      <author><first>Yuanhang</first><last>Zheng</last></author>
      <author><first>Gang</first><last>Chen</last></author>
      <author><first>Kaiyu</first><last>Huang</last></author>
      <author><first>Ji</first><last>Zhang</last></author>
      <author><first>Huanbo</first><last>Luan</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>2068–2077</pages>
      <abstract>Recent state-of-the-art (SOTA) effective neural network methods and fine-tuning methods based on pre-trained models (PTM) have been used in Chinese word segmentation (CWS), and they achieve great results. However, previous works focus on training the models with the fixed corpus at every iteration. The intermediate generated information is also valuable. Besides, the robustness of the previous neural methods is limited by the large-scale annotated data. There are a few noises in the annotated corpus. Limited efforts have been made by previous studies to deal with such problems. In this work, we propose a self-supervised CWS approach with a straightforward and effective architecture. First, we train a word segmentation model and use it to generate the segmentation results. Then, we use a revised masked language model (MLM) to evaluate the quality of the segmentation results based on the predictions of the MLM. Finally, we leverage the evaluations to aid the training of the segmenter by improved minimum risk training. Experimental results show that our approach outperforms previous methods on 9 different CWS datasets with single criterion training and multiple criteria training and achieves better robustness.</abstract>
      <url hash="bde8d9c8">2021.emnlp-main.158</url>
      <bibkey>maimaiti-etal-2021-segment</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.158</doi>
      <video href="2021.emnlp-main.158.mp4"/>
    </paper>
    <paper id="159">
      <title>Minimal Supervision for Morphological Inflection</title>
      <author><first>Omer</first><last>Goldman</last></author>
      <author><first>Reut</first><last>Tsarfaty</last></author>
      <pages>2078–2088</pages>
      <abstract>Neural models for the various flavours of morphological reinflection tasks have proven to be extremely accurate given ample labeled data, yet labeled data may be slow and costly to obtain. In this work we aim to overcome this annotation bottleneck by bootstrapping labeled data from a seed as small as <i>five</i> labeled inflection tables, accompanied by a large bulk of unlabeled text. Our bootstrapping method exploits the orthographic and semantic regularities in morphological systems in a two-phased setup, where word tagging based on <i>analogies</i> is followed by word pairing based on <i>distances</i>. Our experiments with the Paradigm Cell Filling Problem over eight typologically different languages show that in languages with relatively simple morphology, orthographic regularities on their own allow inflection models to achieve respectable accuracy. Combined orthographic and semantic regularities alleviate difficulties with particularly complex morpho-phonological systems. We further show that our bootstrapping methods substantially outperform hallucination-based methods commonly used for overcoming the annotation bottleneck in morphological reinflection tasks.</abstract>
      <url hash="e13448e7">2021.emnlp-main.159</url>
      <bibkey>goldman-tsarfaty-2021-minimal</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.159</doi>
      <video href="2021.emnlp-main.159.mp4"/>
      <pwccode url="https://github.com/onlplab/morphodetection" additional="false">onlplab/morphodetection</pwccode>
    </paper>
    <paper id="160">
      <title>Fast <fixed-case>W</fixed-case>ord<fixed-case>P</fixed-case>iece Tokenization</title>
      <author><first>Xinying</first><last>Song</last></author>
      <author><first>Alex</first><last>Salcianu</last></author>
      <author><first>Yang</first><last>Song</last></author>
      <author><first>Dave</first><last>Dopson</last></author>
      <author><first>Denny</first><last>Zhou</last></author>
      <pages>2089–2103</pages>
      <abstract>Tokenization is a fundamental preprocessing step for almost all NLP tasks. In this paper, we propose efficient algorithms for the WordPiece tokenization used in BERT, from single-word tokenization to general text (e.g., sentence) tokenization. When tokenizing a single word, WordPiece uses a longest-match-first strategy, known as maximum matching. The best known algorithms so far are O(nˆ2) (where n is the input length) or O(nm) (where m is the maximum vocabulary token length). We propose a novel algorithm whose tokenization complexity is strictly O(n). Our method is inspired by the Aho-Corasick algorithm. We introduce additional linkages on top of the trie built from the vocabulary, allowing smart transitions when the trie matching cannot continue. For general text, we further propose an algorithm that combines pre-tokenization (splitting the text into words) and our linear-time WordPiece method into a single pass. Experimental results show that our method is 8.2x faster than HuggingFace Tokenizers and 5.1x faster than TensorFlow Text on average for general text tokenization.</abstract>
      <url hash="549b9ddf">2021.emnlp-main.160</url>
      <bibkey>song-etal-2021-fast</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.160</doi>
      <video href="2021.emnlp-main.160.mp4"/>
      <pwccode url="https://github.com/tensorflow/text/blob/master/docs/api_docs/python/text/FastWordpieceTokenizer.md" additional="false">tensorflow/text</pwccode>
    </paper>
    <paper id="161">
      <title>You should evaluate your language model on marginal likelihood over tokenisations</title>
      <author><first>Kris</first><last>Cao</last></author>
      <author><first>Laura</first><last>Rimell</last></author>
      <pages>2104–2114</pages>
      <abstract>Neural language models typically tokenise input text into sub-word units to achieve an open vocabulary. The standard approach is to use a single canonical tokenisation at both train and test time. We suggest that this approach is unsatisfactory and may bottleneck our evaluation of language model performance. Using only the one-best tokenisation ignores tokeniser uncertainty over alternative tokenisations, which may hurt model out-of-domain performance. In this paper, we argue that instead, language models should be evaluated on their marginal likelihood over tokenisations. We compare different estimators for the marginal likelihood based on sampling, and show that it is feasible to estimate the marginal likelihood with a manageable number of samples. We then evaluate a pretrained language model on both the one-best-tokenisation and marginal perplexities, and show that the marginal perplexity can be significantly better than the one best, especially on out-of-domain data. We link this difference in perplexity to the tokeniser uncertainty as measured by tokeniser entropy. We discuss some implications of our results for language model training and evaluation, particularly with regard to tokenisation robustness.</abstract>
      <url hash="2fbc818c">2021.emnlp-main.161</url>
      <bibkey>cao-rimell-2021-evaluate</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.161</doi>
      <video href="2021.emnlp-main.161.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mc4">mC4</pwcdataset>
    </paper>
    <paper id="162">
      <title>Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning</title>
      <author><first>Da</first><last>Yin</last></author>
      <author><first>Liunian Harold</first><last>Li</last></author>
      <author><first>Ziniu</first><last>Hu</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>2115–2129</pages>
      <abstract>Commonsense is defined as the knowledge on which everyone agrees. However, certain types of commonsense knowledge are correlated with culture and geographic locations and they are only shared locally. For example, the scenes of wedding ceremonies vary across regions due to different customs influenced by historical and religious factors. Such regional characteristics, however, are generally omitted in prior work. In this paper, we construct a Geo-Diverse Visual Commonsense Reasoning dataset (GD-VCR) to test vision-and-language models’ ability to understand cultural and geo-location-specific commonsense. In particular, we study two state-of-the-art Vision-and-Language models, VisualBERT and ViLBERT trained on VCR, a standard benchmark with images primarily from Western regions. We then evaluate how well the trained models can generalize to answering the questions in GD-VCR. We find that the performance of both models for non-Western regions including East Asia, South Asia, and Africa is significantly lower than that for Western region. We analyze the reasons behind the performance disparity and find that the performance gap is larger on QA pairs that: 1) are concerned with culture-related scenarios, e.g., weddings, religious activities, and festivals; 2) require high-level geo-diverse commonsense reasoning rather than low-order perception and recognition. Dataset and code are released at https://github.com/WadeYin9712/GD-VCR.</abstract>
      <url hash="af1525c9">2021.emnlp-main.162</url>
      <bibkey>yin-etal-2021-broaden</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.162</doi>
      <video href="2021.emnlp-main.162.mp4"/>
      <pwccode url="https://github.com/wadeyin9712/gd-vcr" additional="false">wadeyin9712/gd-vcr</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gd-vcr">GD-VCR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vcr">VCR</pwcdataset>
    </paper>
    <paper id="163">
      <title>Reference-Centric Models for Grounded Collaborative Dialogue</title>
      <author><first>Daniel</first><last>Fried</last></author>
      <author><first>Justin</first><last>Chiu</last></author>
      <author><first>Dan</first><last>Klein</last></author>
      <pages>2130–2147</pages>
      <abstract>We present a grounded neural dialogue model that successfully collaborates with people in a partially-observable reference game. We focus on a setting where two agents each observe an overlapping part of a world context and need to identify and agree on some object they share. Therefore, the agents should pool their information and communicate pragmatically to solve the task. Our dialogue agent accurately grounds referents from the partner’s utterances using a structured reference resolver, conditions on these referents using a recurrent memory, and uses a pragmatic generation procedure to ensure the partner can resolve the references the agent produces. We evaluate on the OneCommon spatial grounding dialogue task (Udagawa and Aizawa 2019), involving a number of dots arranged on a board with continuously varying positions, sizes, and shades. Our agent substantially outperforms the previous state of the art for the task, obtaining a 20% relative improvement in successful task completion in self-play evaluations and a 50% relative improvement in success in human evaluations.</abstract>
      <url hash="6d204d3c">2021.emnlp-main.163</url>
      <bibkey>fried-etal-2021-reference</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.163</doi>
      <video href="2021.emnlp-main.163.mp4"/>
      <pwccode url="https://github.com/dpfried/onecommon" additional="false">dpfried/onecommon</pwccode>
    </paper>
    <paper id="164">
      <title><fixed-case>C</fixed-case>ross<fixed-case>VQA</fixed-case>: Scalably Generating Benchmarks for Systematically Testing <fixed-case>VQA</fixed-case> Generalization</title>
      <author><first>Arjun</first><last>Akula</last></author>
      <author><first>Soravit</first><last>Changpinyo</last></author>
      <author><first>Boqing</first><last>Gong</last></author>
      <author><first>Piyush</first><last>Sharma</last></author>
      <author><first>Song-Chun</first><last>Zhu</last></author>
      <author><first>Radu</first><last>Soricut</last></author>
      <pages>2148–2166</pages>
      <abstract>One challenge in evaluating visual question answering (VQA) models in the cross-dataset adaptation setting is that the distribution shifts are multi-modal, making it difficult to identify if it is the shifts in visual or language features that play a key role. In this paper, we propose a semi-automatic framework for generating disentangled shifts by introducing a controllable visual question-answer generation (VQAG) module that is capable of generating highly-relevant and diverse question-answer pairs with the desired dataset style. We use it to create CrossVQA, a collection of test splits for assessing VQA generalization based on the VQA2, VizWiz, and Open Images datasets. We provide an analysis of our generated datasets and demonstrate its utility by using them to evaluate several state-of-the-art VQA systems. One important finding is that the visual shifts in cross-dataset VQA matter more than the language shifts. More broadly, we present a scalable framework for systematically evaluating the machine with little human intervention.</abstract>
      <url hash="cd78b493">2021.emnlp-main.164</url>
      <bibkey>akula-etal-2021-crossvqa</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.164</doi>
      <video href="2021.emnlp-main.164.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gqa">GQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vcr">VCR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vqg">VQG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering-v2-0">Visual Question Answering v2.0</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vizwiz">VizWiz</pwcdataset>
    </paper>
    <paper id="165">
      <title>Visual Goal-Step Inference using wiki<fixed-case>H</fixed-case>ow</title>
      <author><first>Yue</first><last>Yang</last></author>
      <author><first>Artemis</first><last>Panagopoulou</last></author>
      <author><first>Qing</first><last>Lyu</last></author>
      <author><first>Li</first><last>Zhang</last></author>
      <author><first>Mark</first><last>Yatskar</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <pages>2167–2179</pages>
      <abstract>Understanding what sequence of steps are needed to complete a goal can help artificial intelligence systems reason about human activities. Past work in NLP has examined the task of goal-step inference for text. We introduce the visual analogue. We propose the Visual Goal-Step Inference (VGSI) task, where a model is given a textual goal and must choose which of four images represents a plausible step towards that goal. With a new dataset harvested from wikiHow consisting of 772,277 images representing human actions, we show that our task is challenging for state-of-the-art multimodal models. Moreover, the multimodal representation learned from our data can be effectively transferred to other datasets like HowTo100m, increasing the VGSI accuracy by 15 - 20%. Our task will facilitate multimodal reasoning about procedural events.</abstract>
      <url hash="ec2dfcbf">2021.emnlp-main.165</url>
      <bibkey>yang-etal-2021-visual</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.165</doi>
      <video href="2021.emnlp-main.165.mp4"/>
      <revision id="1" href="2021.emnlp-main.165v1" hash="307192e8"/>
      <revision id="2" href="2021.emnlp-main.165v2" hash="ec2dfcbf" date="2022-12-26">Corrected the Acknowledgement section.</revision>
      <pwccode url="https://github.com/yueyang1996/wikihow-vgsi" additional="false">yueyang1996/wikihow-vgsi</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihow-image">wikiHow-image</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coin">COIN</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/howto100m">HowTo100M</pwcdataset>
    </paper>
    <paper id="166">
      <title>Systematic Generalization on g<fixed-case>SCAN</fixed-case>: <fixed-case>W</fixed-case>hat is Nearly Solved and What is Next?</title>
      <author><first>Linlu</first><last>Qiu</last></author>
      <author><first>Hexiang</first><last>Hu</last></author>
      <author><first>Bowen</first><last>Zhang</last></author>
      <author><first>Peter</first><last>Shaw</last></author>
      <author><first>Fei</first><last>Sha</last></author>
      <pages>2180–2188</pages>
      <abstract>We analyze the <i>grounded</i> SCAN (gSCAN) benchmark, which was recently proposed to study systematic generalization for grounded language understanding. First, we study which aspects of the original benchmark can be solved by commonly used methods in multi-modal research. We find that a general-purpose Transformer-based model with cross-modal attention achieves strong performance on a majority of the gSCAN splits, surprisingly outperforming more specialized approaches from prior work. Furthermore, our analysis suggests that many of the remaining errors reveal the same fundamental challenge in systematic generalization of linguistic constructs regardless of visual context. Second, inspired by this finding, we propose challenging new tasks for gSCAN by generating data to incorporate relations between objects in the visual environment. Finally, we find that current models are surprisingly data inefficient given the narrow scope of commands in gSCAN, suggesting another challenge for future work.</abstract>
      <url hash="7fb6c527">2021.emnlp-main.166</url>
      <bibkey>qiu-etal-2021-systematic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.166</doi>
      <video href="2021.emnlp-main.166.mp4"/>
      <pwccode url="https://github.com/LauraRuis/groundedSCAN" additional="true">LauraRuis/groundedSCAN</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/scan">SCAN</pwcdataset>
    </paper>
    <paper id="167">
      <title>Effect of Visual Extensions on Natural Language Understanding in Vision-and-Language Models</title>
      <author><first>Taichi</first><last>Iki</last></author>
      <author><first>Akiko</first><last>Aizawa</last></author>
      <pages>2189–2196</pages>
      <abstract>A method for creating a vision-and-language (V&amp;L) model is to extend a language model through structural modifications and V&amp;L pre-training. Such an extension aims to make a V&amp;L model inherit the capability of natural language understanding (NLU) from the original language model. To see how well this is achieved, we propose to evaluate V&amp;L models using an NLU benchmark (GLUE). We compare five V&amp;L models, including single-stream and dual-stream models, trained with the same pre-training. Dual-stream models, with their higher modality independence achieved by approximately doubling the number of parameters, are expected to preserve the NLU capability better. Our main finding is that the dual-stream scores are not much different than the single-stream scores, contrary to expectation. Further analysis shows that pre-training causes the performance drop in NLU tasks with few exceptions. These results suggest that adopting a single-stream structure and devising the pre-training could be an effective method for improving the maintenance of language knowledge in V&amp;L extensions.</abstract>
      <url hash="92545804">2021.emnlp-main.167</url>
      <bibkey>iki-aizawa-2021-effect</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.167</doi>
      <video href="2021.emnlp-main.167.mp4"/>
      <pwccode url="https://github.com/alab-nii/eval_vl_glue" additional="false">alab-nii/eval_vl_glue</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="168">
      <title>Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding</title>
      <author><first>Nouha</first><last>Dziri</last></author>
      <author><first>Andrea</first><last>Madotto</last></author>
      <author><first>Osmar</first><last>Zaïane</last></author>
      <author><first>Avishek Joey</first><last>Bose</last></author>
      <pages>2197–2214</pages>
      <abstract>Dialogue systems powered by large pre-trained language models exhibit an innate ability to deliver fluent and natural-sounding responses. Despite their impressive performance, these models are fitful and can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving faithfulness and reducing hallucination of neural dialogue systems to known facts supplied by a Knowledge Graph (KG). We propose Neural Path Hunter which follows a generate-then-refine strategy whereby a generated response is amended using the KG. Neural Path Hunter leverages a separate token-level fact critic to identify plausible sources of hallucination followed by a refinement stage that retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph. We empirically validate our proposed approach on the OpenDialKG dataset (Moon et al., 2019) against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35% based on FeQA (Durmus et al., 2020). The code is available at https://github.com/nouhadziri/Neural-Path-Hunter.</abstract>
      <url hash="68faeceb">2021.emnlp-main.168</url>
      <bibkey>dziri-etal-2021-neural</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.168</doi>
      <video href="2021.emnlp-main.168.mp4"/>
      <pwccode url="https://github.com/nouhadziri/neural-path-hunter" additional="false">nouhadziri/neural-path-hunter</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/opendialkg">OpenDialKG</pwcdataset>
    </paper>
    <paper id="169">
      <title>Thinking Clearly, Talking Fast: Concept-Guided Non-Autoregressive Generation for Open-Domain Dialogue Systems</title>
      <author><first>Yicheng</first><last>Zou</last></author>
      <author><first>Zhihua</first><last>Liu</last></author>
      <author><first>Xingwu</first><last>Hu</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <pages>2215–2226</pages>
      <abstract>Human dialogue contains evolving concepts, and speakers naturally associate multiple concepts to compose a response. However, current dialogue models with the seq2seq framework lack the ability to effectively manage concept transitions and can hardly introduce multiple concepts to responses in a sequential decoding manner. To facilitate a controllable and coherent dialogue, in this work, we devise a concept-guided non-autoregressive model (CG-nAR) for open-domain dialogue generation. The proposed model comprises a multi-concept planning module that learns to identify multiple associated concepts from a concept graph and a customized Insertion Transformer that performs concept-guided non-autoregressive generation to complete a response. The experimental results on two public datasets show that CG-nAR can produce diverse and coherent responses, outperforming state-of-the-art baselines in both automatic and human evaluations with substantially faster inference speed.</abstract>
      <url hash="24cc0603">2021.emnlp-main.169</url>
      <bibkey>zou-etal-2021-thinking</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.169</doi>
      <video href="2021.emnlp-main.169.mp4"/>
      <pwccode url="https://github.com/rowitzou/cg-nar" additional="false">rowitzou/cg-nar</pwccode>
    </paper>
    <paper id="170">
      <title>Perspective-taking and Pragmatics for Generating Empathetic Responses Focused on Emotion Causes</title>
      <author><first>Hyunwoo</first><last>Kim</last></author>
      <author><first>Byeongchang</first><last>Kim</last></author>
      <author><first>Gunhee</first><last>Kim</last></author>
      <pages>2227–2240</pages>
      <abstract>Empathy is a complex cognitive ability based on the reasoning of others’ affective states. In order to better understand others and express stronger empathy in dialogues, we argue that two issues must be tackled at the same time: (i) identifying which word is the cause for the other’s emotion from his or her utterance and (ii) reflecting those specific words in the response generation. However, previous approaches for recognizing emotion cause words in text require sub-utterance level annotations, which can be demanding. Taking inspiration from social cognition, we leverage a generative estimator to infer emotion cause words from utterances with no word-level label. Also, we introduce a novel method based on pragmatics to make dialogue models focus on targeted words in the input during generation. Our method is applicable to any dialogue models with no additional training on the fly. We show our approach improves multiple best-performing dialogue agents on generating more focused empathetic responses in terms of both automatic and human evaluation.</abstract>
      <url hash="ea891378">2021.emnlp-main.170</url>
      <bibkey>kim-etal-2021-perspective</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.170</doi>
      <video href="2021.emnlp-main.170.mp4"/>
      <pwccode url="https://github.com/skywalker023/focused-empathy" additional="false">skywalker023/focused-empathy</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/emocause">EmoCause</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/empatheticdialogues">EmpatheticDialogues</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reccon">RECCON</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ecpe-xia-and-ding-2019">Xia and Ding, 2019</pwcdataset>
    </paper>
    <paper id="171">
      <title>Generation and Extraction Combined Dialogue State Tracking with Hierarchical Ontology Integration</title>
      <author><first>Xinmeng</first><last>Li</last></author>
      <author><first>Qian</first><last>Li</last></author>
      <author><first>Wansen</first><last>Wu</last></author>
      <author><first>Quanjun</first><last>Yin</last></author>
      <pages>2241–2249</pages>
      <abstract>Recently, the focus of dialogue state tracking has expanded from single domain to multiple domains. The task is characterized by the shared slots between domains. As the scenario gets more complex, the out-of-vocabulary problem also becomes severer. Current models are not satisfactory for solving the challenges of ontology integration between domains and out-of-vocabulary problems. To address the problem, we explore the hierarchical semantic of ontology and enhance the interrelation between slots with masked hierarchical attention. In state value decoding stage, we solve the out-of-vocabulary problem by combining generation method and extraction method together. We evaluate the performance of our model on two representative datasets, MultiWOZ in English and CrossWOZ in Chinese. The results show that our model yields a significant performance gain over current state-of-the-art state tracking model and it is more robust to out-of-vocabulary problem compared with other methods.</abstract>
      <url hash="013d01b9">2021.emnlp-main.171</url>
      <bibkey>li-etal-2021-generation</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.171</doi>
      <video href="2021.emnlp-main.171.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/crosswoz">CrossWOZ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
    </paper>
    <paper id="172">
      <title><fixed-case>C</fixed-case>o<fixed-case>LV</fixed-case>: A Collaborative Latent Variable Model for Knowledge-Grounded Dialogue Generation</title>
      <author><first>Haolan</first><last>Zhan</last></author>
      <author><first>Lei</first><last>Shen</last></author>
      <author><first>Hongshen</first><last>Chen</last></author>
      <author><first>Hainan</first><last>Zhang</last></author>
      <pages>2250–2261</pages>
      <abstract>Knowledge-grounded dialogue generation has achieved promising performance with the engagement of external knowledge sources. Typical approaches towards this task usually perform relatively independent two sub-tasks, i.e., knowledge selection and knowledge-aware response generation. In this paper, in order to improve the diversity of both knowledge selection and knowledge-aware response generation, we propose a collaborative latent variable (CoLV) model to integrate these two aspects simultaneously in separate yet collaborative latent spaces, so as to capture the inherent correlation between knowledge selection and response generation. During generation, our proposed model firstly draws knowledge candidate from the latent space conditioned on the dialogue context, and then samples a response from another collaborative latent space conditioned on both the context and the selected knowledge. Experimental results on two widely-used knowledge-grounded dialogue datasets show that our model outperforms previous methods on both knowledge selection and response generation.</abstract>
      <url hash="a5ef91da">2021.emnlp-main.172</url>
      <bibkey>zhan-etal-2021-colv</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.172</doi>
      <video href="2021.emnlp-main.172.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/holl-e">Holl-E</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="173">
      <title><fixed-case>A</fixed-case> <fixed-case>T</fixed-case>hree-<fixed-case>S</fixed-case>tage <fixed-case>L</fixed-case>earning <fixed-case>F</fixed-case>ramework for <fixed-case>L</fixed-case>ow-<fixed-case>R</fixed-case>esource <fixed-case>K</fixed-case>nowledge-<fixed-case>G</fixed-case>rounded <fixed-case>D</fixed-case>ialogue <fixed-case>G</fixed-case>eneration</title>
      <author><first>Shilei</first><last>Liu</last></author>
      <author><first>Xiaofeng</first><last>Zhao</last></author>
      <author><first>Bochao</first><last>Li</last></author>
      <author><first>Feiliang</first><last>Ren</last></author>
      <author><first>Longhui</first><last>Zhang</last></author>
      <author><first>Shujuan</first><last>Yin</last></author>
      <pages>2262–2272</pages>
      <abstract>Neural conversation models have shown great potentials towards generating fluent and informative responses by introducing external background knowledge. Nevertheless, it is laborious to construct such knowledge-grounded dialogues, and existing models usually perform poorly when transfer to new domains with limited training samples. Therefore, building a knowledge-grounded dialogue system under the low-resource setting is a still crucial issue. In this paper, we propose a novel three-stage learning framework based on weakly supervised learning which benefits from large scale ungrounded dialogues and unstructured knowledge base. To better cooperate with this framework, we devise a variant of Transformer with decoupled decoder which facilitates the disentangled learning of response generation and knowledge incorporation. Evaluation results on two benchmarks indicate that our approach can outperform other state-of-the-art methods with less training data, and even in zero-resource scenario, our approach still performs well.</abstract>
      <url hash="6a74ba56">2021.emnlp-main.173</url>
      <bibkey>liu-etal-2021-three</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.173</doi>
      <video href="2021.emnlp-main.173.mp4"/>
      <pwccode url="https://github.com/neukg/kat-tslf" additional="false">neukg/kat-tslf</pwccode>
    </paper>
    <paper id="174">
      <title>Intention Reasoning Network for Multi-Domain End-to-end Task-Oriented Dialogue</title>
      <author><first>Zhiyuan</first><last>Ma</last></author>
      <author><first>Jianjun</first><last>Li</last></author>
      <author><first>Zezheng</first><last>Zhang</last></author>
      <author><first>Guohui</first><last>Li</last></author>
      <author><first>Yongjing</first><last>Cheng</last></author>
      <pages>2273–2285</pages>
      <abstract>Recent years has witnessed the remarkable success in end-to-end task-oriented dialog system, especially when incorporating external knowledge information. However, the quality of most existing models’ generated response is still limited, mainly due to their lack of fine-grained reasoning on deterministic knowledge (w.r.t. conceptual tokens), which makes them difficult to capture the concept shifts and identify user’s real intention in cross-task scenarios. To address these issues, we propose a novel intention mechanism to better model deterministic entity knowledge. Based on such a mechanism, we further propose an intention reasoning network (IR-Net), which consists of joint and multi-hop reasoning, to obtain intention-aware representations of conceptual tokens that can be used to capture the concept shifts involved in task-oriented conversations, so as to effectively identify user’s intention and generate more accurate responses. Experimental results verify the effectiveness of IR-Net, showing that it achieves the state-of-the-art performance on two representative multi-domain dialog datasets.</abstract>
      <url hash="8252b8c9">2021.emnlp-main.174</url>
      <bibkey>ma-etal-2021-intention</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.174</doi>
      <video href="2021.emnlp-main.174.mp4"/>
    </paper>
    <paper id="175">
      <title>More is Better: Enhancing Open-Domain Dialogue Generation via Multi-Source Heterogeneous Knowledge</title>
      <author><first>Sixing</first><last>Wu</last></author>
      <author><first>Ying</first><last>Li</last></author>
      <author><first>Minghui</first><last>Wang</last></author>
      <author><first>Dawei</first><last>Zhang</last></author>
      <author><first>Yang</first><last>Zhou</last></author>
      <author><first>Zhonghai</first><last>Wu</last></author>
      <pages>2286–2300</pages>
      <abstract>Despite achieving remarkable performance, previous knowledge-enhanced works usually only use a single-source homogeneous knowledge base of limited knowledge coverage. Thus, they often degenerate into traditional methods because not all dialogues can be linked with knowledge entries. This paper proposes a novel dialogue generation model, MSKE-Dialog, to solve this issue with three unique advantages: (1) Rather than only one, MSKE-Dialog can simultaneously leverage multiple heterogeneous knowledge sources (it includes but is not limited to commonsense knowledge facts, text knowledge, infobox knowledge) to improve the knowledge coverage; (2) To avoid the topic conflict among the context and different knowledge sources, we propose a Multi-Reference Selection to better select context/knowledge; (3) We propose a Multi-Reference Generation to generate informative responses by referring to multiple generation references at the same time. Extensive evaluations on a Chinese dataset show the superior performance of this work against various state-of-the-art approaches. To our best knowledge, this work is the first to use the multi-source heterogeneous knowledge in the open-domain knowledge-enhanced dialogue generation.</abstract>
      <url hash="85bea459">2021.emnlp-main.175</url>
      <bibkey>wu-etal-2021-better</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.175</doi>
      <video href="2021.emnlp-main.175.mp4"/>
      <pwccode url="https://github.com/pku-sixing/emnlp2021-mske_dialog" additional="false">pku-sixing/emnlp2021-mske_dialog</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="176">
      <title>Domain-Lifelong Learning for Dialogue State Tracking via Knowledge Preservation Networks</title>
      <author><first>Qingbin</first><last>Liu</last></author>
      <author><first>Pengfei</first><last>Cao</last></author>
      <author><first>Cao</first><last>Liu</last></author>
      <author><first>Jiansong</first><last>Chen</last></author>
      <author><first>Xunliang</first><last>Cai</last></author>
      <author><first>Fan</first><last>Yang</last></author>
      <author><first>Shizhu</first><last>He</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>2301–2311</pages>
      <abstract>Dialogue state tracking (DST), which estimates user goals given a dialogue context, is an essential component of task-oriented dialogue systems. Conventional DST models are usually trained offline, which requires a fixed dataset prepared in advance. This paradigm is often impractical in real-world applications since online dialogue systems usually involve continually emerging new data and domains. Therefore, this paper explores Domain-Lifelong Learning for Dialogue State Tracking (DLL-DST), which aims to continually train a DST model on new data to learn incessantly emerging new domains while avoiding catastrophically forgetting old learned domains. To this end, we propose a novel domain-lifelong learning method, called Knowledge Preservation Networks (KPN), which consists of multi-prototype enhanced retrospection and multi-strategy knowledge distillation, to solve the problems of expression diversity and combinatorial explosion in the DLL-DST task. Experimental results show that KPN effectively alleviates catastrophic forgetting and outperforms previous state-of-the-art lifelong learning methods by 4.25% and 8.27% of whole joint goal accuracy on the MultiWOZ benchmark and the SGD benchmark, respectively.</abstract>
      <url hash="68d73367">2021.emnlp-main.176</url>
      <bibkey>liu-etal-2021-domain</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.176</doi>
      <video href="2021.emnlp-main.176.mp4"/>
      <pwccode url="https://github.com/liuqingbin/knowledge-preservation-networks" additional="false">liuqingbin/knowledge-preservation-networks</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
    </paper>
    <paper id="177">
      <title><fixed-case>CSAGN</fixed-case>: Conversational Structure Aware Graph Network for Conversational Semantic Role Labeling</title>
      <author><first>Han</first><last>Wu</last></author>
      <author><first>Kun</first><last>Xu</last></author>
      <author><first>Linqi</first><last>Song</last></author>
      <pages>2312–2317</pages>
      <abstract>Conversational semantic role labeling (CSRL) is believed to be a crucial step towards dialogue understanding. However, it remains a major challenge for existing CSRL parser to handle conversational structural information. In this paper, we present a simple and effective architecture for CSRL which aims to address this problem. Our model is based on a conversational structure aware graph network which explicitly encodes the speaker dependent information. We also propose a multi-task learning method to further improve the model. Experimental results on benchmark datasets show that our model with our proposed training objectives significantly outperforms previous baselines.</abstract>
      <url hash="62b18cd4">2021.emnlp-main.177</url>
      <bibkey>wu-etal-2021-csagn</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.177</doi>
      <video href="2021.emnlp-main.177.mp4"/>
      <pwccode url="https://github.com/hahahawu/CSAGN" additional="false">hahahawu/CSAGN</pwccode>
    </paper>
    <paper id="178">
      <title>Different Strokes for Different Folks: Investigating Appropriate Further Pre-training Approaches for Diverse Dialogue Tasks</title>
      <author><first>Yao</first><last>Qiu</last></author>
      <author><first>Jinchao</first><last>Zhang</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>2318–2327</pages>
      <abstract>Loading models pre-trained on the large-scale corpus in the general domain and fine-tuning them on specific downstream tasks is gradually becoming a paradigm in Natural Language Processing. Previous investigations prove that introducing a further pre-training phase between pre-training and fine-tuning phases to adapt the model on the domain-specific unlabeled data can bring positive effects. However, most of these further pre-training works just keep running the conventional pre-training task, e.g., masked language model, which can be regarded as the domain adaptation to bridge the data distribution gap. After observing diverse downstream tasks, we suggest that different tasks may also need a further pre-training phase with appropriate training tasks to bridge the task formulation gap. To investigate this, we carry out a study for improving multiple task-oriented dialogue downstream tasks through designing various tasks at the further pre-training phase. The experiment shows that different downstream tasks prefer different further pre-training tasks, which have intrinsic correlation and most further pre-training tasks significantly improve certain target tasks rather than all. Our investigation indicates that it is of great importance and effectiveness to design appropriate further pre-training tasks modeling specific information that benefit downstream tasks. Besides, we present multiple constructive empirical conclusions for enhancing task-oriented dialogues.</abstract>
      <url hash="b72dc2d4">2021.emnlp-main.178</url>
      <bibkey>qiu-etal-2021-different</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.178</doi>
      <video href="2021.emnlp-main.178.mp4"/>
    </paper>
    <paper id="179">
      <title>Knowledge Enhanced Fine-Tuning for Better Handling Unseen Entities in Dialogue Generation</title>
      <author><first>Leyang</first><last>Cui</last></author>
      <author><first>Yu</first><last>Wu</last></author>
      <author><first>Shujie</first><last>Liu</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>2328–2337</pages>
      <abstract>Although pre-training models have achieved great success in dialogue generation, their performance drops dramatically when the input contains an entity that does not appear in pre-training and fine-tuning datasets (unseen entity). To address this issue, existing methods leverage an external knowledge base to generate appropriate responses. In real-world practical, the entity may not be included by the knowledge base or suffer from the precision of knowledge retrieval. To deal with this problem, instead of introducing knowledge base as the input, we force the model to learn a better semantic representation by predicting the information in the knowledge base, only based on the input context. Specifically, with the help of a knowledge base, we introduce two auxiliary training objectives: 1) Interpret Masked Word, which conjectures the meaning of the masked entity given the context; 2) Hypernym Generation, which predicts the hypernym of the entity based on the context. Experiment results on two dialogue corpus verify the effectiveness of our methods under both knowledge available and unavailable settings.</abstract>
      <url hash="d5894ffa">2021.emnlp-main.179</url>
      <bibkey>cui-etal-2021-knowledge</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.179</doi>
      <video href="2021.emnlp-main.179.mp4"/>
      <pwccode url="https://github.com/nealcly/ke-blender" additional="false">nealcly/ke-blender</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="180">
      <title>An Evaluation Dataset and Strategy for Building Robust Multi-turn Response Selection Model</title>
      <author><first>Kijong</first><last>Han</last></author>
      <author><first>Seojin</first><last>Lee</last></author>
      <author><first>Dong-hun</first><last>Lee</last></author>
      <pages>2338–2344</pages>
      <abstract>Multi-turn response selection models have recently shown comparable performance to humans in several benchmark datasets. However, in the real environment, these models often have weaknesses, such as making incorrect predictions based heavily on superficial patterns without a comprehensive understanding of the context. For example, these models often give a high score to the wrong response candidate containing several keywords related to the context but using the inconsistent tense. In this study, we analyze the weaknesses of the open-domain Korean Multi-turn response selection models and publish an adversarial dataset to evaluate these weaknesses. We also suggest a strategy to build a robust model in this adversarial environment.</abstract>
      <url hash="0da7574f">2021.emnlp-main.180</url>
      <bibkey>han-etal-2021-evaluation</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.180</doi>
      <video href="2021.emnlp-main.180.mp4"/>
      <pwccode url="https://github.com/kakaoenterprise/koradvmrstestdata" additional="false">kakaoenterprise/koradvmrstestdata</pwccode>
    </paper>
    <paper id="181">
      <title>Unsupervised Conversation Disentanglement through Co-Training</title>
      <author><first>Hui</first><last>Liu</last></author>
      <author><first>Zhan</first><last>Shi</last></author>
      <author><first>Xiaodan</first><last>Zhu</last></author>
      <pages>2345–2356</pages>
      <abstract>Conversation disentanglement aims to separate intermingled messages into detached sessions, which is a fundamental task in understanding multi-party conversations. Existing work on conversation disentanglement relies heavily upon human-annotated datasets, which is expensive to obtain in practice. In this work, we explore training a conversation disentanglement model without referencing any human annotations. Our method is built upon the deep co-training algorithm, which consists of two neural networks: a message-pair classifier and a session classifier. The former is responsible of retrieving local relations between two messages while the latter categorizes a message to a session by capturing context-aware information. Both the two networks are initialized respectively with pseudo data built from the unannotated corpus. During the deep co-training process, we use the session classifier as a reinforcement learning component to learn a session assigning policy by maximizing the local rewards given by the message-pair classifier. For the message-pair classifier, we enrich its training data by retrieving message pairs with high confidence from the disentangled sessions predicted by the session classifier. Experimental results on the large Movie Dialogue Dataset demonstrate that our proposed approach achieves competitive performance compared to previous supervised methods. Further experiments show that the predicted disentangled conversations can promote the performance on the downstream task of multi-party response selection.</abstract>
      <url hash="4afe3bd1">2021.emnlp-main.181</url>
      <bibkey>liu-etal-2021-unsupervised</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.181</doi>
      <video href="2021.emnlp-main.181.mp4"/>
      <pwccode url="https://github.com/layneins/unsupervised_dialo_disentanglement" additional="false">layneins/unsupervised_dialo_disentanglement</pwccode>
    </paper>
    <paper id="182">
      <title>Don’t be Contradicted with Anything! <fixed-case>CI</fixed-case>-<fixed-case>T</fixed-case>o<fixed-case>D</fixed-case>: Towards Benchmarking Consistency for Task-oriented Dialogue System</title>
      <author><first>Libo</first><last>Qin</last></author>
      <author><first>Tianbao</first><last>Xie</last></author>
      <author><first>Shijue</first><last>Huang</last></author>
      <author><first>Qiguang</first><last>Chen</last></author>
      <author><first>Xiao</first><last>Xu</last></author>
      <author><first>Wanxiang</first><last>Che</last></author>
      <pages>2357–2367</pages>
      <abstract>Consistency Identification has obtained remarkable success on open-domain dialogue, which can be used for preventing inconsistent response generation. However, in contrast to the rapid development in open-domain dialogue, few efforts have been made to the task-oriented dialogue direction. In this paper, we argue that <i>consistency problem</i> is more urgent in task-oriented domain. To facilitate the research, we introduce CI-ToD, a novel dataset for <b>C</b>onsistency <b>I</b>dentification in <b>T</b>ask-<b>o</b>riented <b>D</b>ialog system. In addition, we not only annotate the single label to enable the model to judge whether the system response is contradictory, but also provide more fine-grained labels (i.e., Dialogue History Inconsistency, User Query Inconsistency and Knowledge Base Inconsistency) to encourage model to know what inconsistent sources lead to it. Empirical results show that state-of-the-art methods only achieve 51.3%, which is far behind the human performance of 93.2%, indicating that there is ample room for improving consistency identification ability. Finally, we conduct exhaustive experiments and qualitative analysis to comprehend key challenges and provide guidance for future directions. All datasets and models are publicly available at <url>https://github.com/yizhen20133868/CI-ToD</url>.</abstract>
      <url hash="ab1c6973">2021.emnlp-main.182</url>
      <bibkey>qin-etal-2021-dont</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.182</doi>
      <video href="2021.emnlp-main.182.mp4"/>
      <pwccode url="https://github.com/yizhen20133868/ci-tod" additional="false">yizhen20133868/ci-tod</pwccode>
    </paper>
    <paper id="183">
      <title>Transferable Persona-Grounded Dialogues via Grounded Minimal Edits</title>
      <author><first>Chen Henry</first><last>Wu</last></author>
      <author><first>Yinhe</first><last>Zheng</last></author>
      <author><first>Xiaoxi</first><last>Mao</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>2368–2382</pages>
      <abstract>Grounded dialogue models generate responses that are grounded on certain concepts. Limited by the distribution of grounded dialogue data, models trained on such data face the <i>transferability</i> challenges in terms of the data distribution and the type of grounded concepts. To address the challenges, we propose the <i>grounded minimal editing</i> framework, which minimally edits existing responses to be grounded on the given concept. Focusing on personas, we propose Grounded Minimal Editor (GME), which learns to edit by disentangling and recombining persona-related and persona-agnostic parts of the response. To evaluate persona-grounded minimal editing, we present the PersonaMi-nEdit dataset, and experimental results show that GME outperforms competitive baselines by a large margin. To evaluate the transferability, we experiment on the test set of BlendedSkillTalk and show that GME can edit dialogue models’ responses to largely improve their persona consistency while preserving the use of knowledge and empathy.</abstract>
      <url hash="c25e641a">2021.emnlp-main.183</url>
      <bibkey>wu-etal-2021-transferable</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.183</doi>
      <video href="2021.emnlp-main.183.mp4"/>
      <pwccode url="https://github.com/thu-coai/grounded-minimal-edit" additional="false">thu-coai/grounded-minimal-edit</pwccode>
    </paper>
    <paper id="184">
      <title><fixed-case>EARL</fixed-case>: Informative Knowledge-Grounded Conversation Generation with Entity-Agnostic Representation Learning</title>
      <author><first>Hao</first><last>Zhou</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <author><first>Yong</first><last>Liu</last></author>
      <author><first>Wei</first><last>Chen</last></author>
      <author><first>Xiaoyan</first><last>Zhu</last></author>
      <pages>2383–2395</pages>
      <abstract>Generating informative and appropriate responses is challenging but important for building human-like dialogue systems. Although various knowledge-grounded conversation models have been proposed, these models have limitations in utilizing knowledge that infrequently occurs in the training data, not to mention integrating unseen knowledge into conversation generation. In this paper, we propose an Entity-Agnostic Representation Learning (EARL) method to introduce knowledge graphs to informative conversation generation. Unlike traditional approaches that parameterize the specific representation for each entity, EARL utilizes the context of conversations and the relational structure of knowledge graphs to learn the category representation for entities, which is generalized to incorporating unseen entities in knowledge graphs into conversation generation. Automatic and manual evaluations demonstrate that our model can generate more informative, coherent, and natural responses than baseline models.</abstract>
      <url hash="257d476e">2021.emnlp-main.184</url>
      <bibkey>zhou-etal-2021-earl</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.184</doi>
      <video href="2021.emnlp-main.184.mp4"/>
      <pwccode url="https://github.com/thu-coai/earl" additional="false">thu-coai/earl</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/opendialkg">OpenDialKG</pwcdataset>
    </paper>
    <paper id="185">
      <title><fixed-case>D</fixed-case>ialogue<fixed-case>CSE</fixed-case>: Dialogue-based Contrastive Learning of Sentence Embeddings</title>
      <author><first>Che</first><last>Liu</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Jinghua</first><last>Liu</last></author>
      <author><first>Jian</first><last>Sun</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <pages>2396–2406</pages>
      <abstract>Learning sentence embeddings from dialogues has drawn increasing attention due to its low annotation cost and high domain adaptability. Conventional approaches employ the siamese-network for this task, which obtains the sentence embeddings through modeling the context-response semantic relevance by applying a feed-forward network on top of the sentence encoders. However, as the semantic textual similarity is commonly measured through the element-wise distance metrics (e.g. cosine and L2 distance), such architecture yields a large gap between training and evaluating. In this paper, we propose DialogueCSE, a dialogue-based contrastive learning approach to tackle this issue. DialogueCSE first introduces a novel matching-guided embedding (MGE) mechanism, which generates a context-aware embedding for each candidate response embedding (i.e. the context-free embedding) according to the guidance of the multi-turn context-response matching matrices. Then it pairs each context-aware embedding with its corresponding context-free embedding and finally minimizes the contrastive loss across all pairs. We evaluate our model on three multi-turn dialogue datasets: the Microsoft Dialogue Corpus, the Jing Dong Dialogue Corpus, and the E-commerce Dialogue Corpus. Evaluation results show that our approach significantly outperforms the baselines across all three datasets in terms of MAP and Spearman’s correlation measures, demonstrating its effectiveness. Further quantitative experiments show that our approach achieves better performance when leveraging more dialogue context and remains robust when less training data is provided.</abstract>
      <url hash="eeb39f37">2021.emnlp-main.185</url>
      <bibkey>liu-etal-2021-dialoguecse</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.185</doi>
      <video href="2021.emnlp-main.185.mp4"/>
      <pwccode url="https://github.com/wangruicn/dialoguecse" additional="false">wangruicn/dialoguecse</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="186">
      <title>Improving Graph-based Sentence Ordering with Iteratively Predicted Pairwise Orderings</title>
      <author><first>Shaopeng</first><last>Lai</last></author>
      <author><first>Ante</first><last>Wang</last></author>
      <author><first>Fandong</first><last>Meng</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Yubin</first><last>Ge</last></author>
      <author><first>Jiali</first><last>Zeng</last></author>
      <author><first>Junfeng</first><last>Yao</last></author>
      <author><first>Degen</first><last>Huang</last></author>
      <author><first>Jinsong</first><last>Su</last></author>
      <pages>2407–2417</pages>
      <abstract>Dominant sentence ordering models can be classified into pairwise ordering models and set-to-sequence models. However, there is little attempt to combine these two types of models, which inituitively possess complementary advantages. In this paper, we propose a novel sentence ordering framework which introduces two classifiers to make better use of pairwise orderings for graph-based sentence ordering (Yin et al. 2019, 2021). Specially, given an initial sentence-entity graph, we first introduce a graph-based classifier to predict pairwise orderings between linked sentences. Then, in an iterative manner, based on the graph updated by previously predicted high-confident pairwise orderings, another classifier is used to predict the remaining uncertain pairwise orderings. At last, we adapt a GRN-based sentence ordering model (Yin et al. 2019, 2021) on the basis of final graph. Experiments on five commonly-used datasets demonstrate the effectiveness and generality of our model. Particularly, when equipped with BERT (Devlin et al. 2019) and FHDecoder (Yin et al. 2020), our model achieves state-of-the-art performance. Our code is available at https://github.com/DeepLearnXMU/IRSEG.</abstract>
      <url hash="01af5774">2021.emnlp-main.186</url>
      <bibkey>lai-etal-2021-improving</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.186</doi>
      <video href="2021.emnlp-main.186.mp4"/>
      <pwccode url="https://github.com/deeplearnxmu/irseg" additional="false">deeplearnxmu/irseg</pwccode>
    </paper>
    <paper id="187">
      <title>Not Just Classification: Recognizing Implicit Discourse Relation on Joint Modeling of Classification and Generation</title>
      <author><first>Feng</first><last>Jiang</last></author>
      <author><first>Yaxin</first><last>Fan</last></author>
      <author><first>Xiaomin</first><last>Chu</last></author>
      <author><first>Peifeng</first><last>Li</last></author>
      <author><first>Qiaoming</first><last>Zhu</last></author>
      <pages>2418–2431</pages>
      <abstract>Implicit discourse relation recognition (IDRR) is a critical task in discourse analysis. Previous studies only regard it as a classification task and lack an in-depth understanding of the semantics of different relations. Therefore, we first view IDRR as a generation task and further propose a method joint modeling of the classification and generation. Specifically, we propose a joint model, CG-T5, to recognize the relation label and generate the target sentence containing the meaning of relations simultaneously. Furthermore, we design three target sentence forms, including the question form, for the generation model to incorporate prior knowledge. To address the issue that large discourse units are hardly embedded into the target sentence, we also propose a target sentence construction mechanism that automatically extracts core sentences from those large discourse units. Experimental results both on Chinese MCDTB and English PDTB datasets show that our model CG-T5 achieves the best performance against several state-of-the-art systems.</abstract>
      <url hash="40e9958a">2021.emnlp-main.187</url>
      <bibkey>jiang-etal-2021-just</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.187</doi>
      <video href="2021.emnlp-main.187.mp4"/>
    </paper>
    <paper id="188">
      <title>A Language Model-based Generative Classifier for Sentence-level Discourse Parsing</title>
      <author><first>Ying</first><last>Zhang</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <pages>2432–2446</pages>
      <abstract>Discourse segmentation and sentence-level discourse parsing play important roles for various NLP tasks to consider textual coherence. Despite recent achievements in both tasks, there is still room for improvement due to the scarcity of labeled data. To solve the problem, we propose a language model-based generative classifier (LMGC) for using more information from labels by treating the labels as an input while enhancing label representations by embedding descriptions for each label. Moreover, since this enables LMGC to make ready the representations for labels, unseen in the pre-training step, we can effectively use a pre-trained language model in LMGC. Experimental results on the RST-DT dataset show that our LMGC achieved the state-of-the-art F1 score of 96.72 in discourse segmentation. It further achieved the state-of-the-art relation F1 scores of 84.69 with gold EDU boundaries and 81.18 with automatically segmented boundaries, respectively, in sentence-level discourse parsing.</abstract>
      <url hash="23efabaf">2021.emnlp-main.188</url>
      <bibkey>zhang-etal-2021-language</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.188</doi>
      <video href="2021.emnlp-main.188.mp4"/>
    </paper>
    <paper id="189">
      <title>Multimodal Phased Transformer for Sentiment Analysis</title>
      <author><first>Junyan</first><last>Cheng</last></author>
      <author><first>Iordanis</first><last>Fostiropoulos</last></author>
      <author><first>Barry</first><last>Boehm</last></author>
      <author><first>Mohammad</first><last>Soleymani</last></author>
      <pages>2447–2458</pages>
      <abstract>Multimodal Transformers achieve superior performance in multimodal learning tasks. However, the quadratic complexity of the self-attention mechanism in Transformers limits their deployment in low-resource devices and makes their inference and training computationally expensive. We propose multimodal Sparse Phased Transformer (SPT) to alleviate the problem of self-attention complexity and memory footprint. SPT uses a sampling function to generate a sparse attention matrix and compress a long sequence to a shorter sequence of hidden states. SPT concurrently captures interactions between the hidden states of different modalities at every layer. To further improve the efficiency of our method, we use Layer-wise parameter sharing and Factorized Co-Attention that share parameters between Cross Attention Blocks, with minimal impact on task performance. We evaluate our model with three sentiment analysis datasets and achieve comparable or superior performance compared with the existing methods, with a 90% reduction in the number of parameters. We conclude that (SPT) along with parameter sharing can capture multimodal interactions with reduced model size and improved sample efficiency.</abstract>
      <url hash="c7a17995">2021.emnlp-main.189</url>
      <attachment type="Software" hash="4757fc0f">2021.emnlp-main.189.Software.zip</attachment>
      <bibkey>cheng-etal-2021-multimodal</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.189</doi>
      <video href="2021.emnlp-main.189.mp4"/>
      <pwccode url="https://github.com/chengjunyan1/sp-transformer" additional="false">chengjunyan1/sp-transformer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multimodal-opinionlevel-sentiment-intensity">Multimodal Opinionlevel Sentiment Intensity</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ur-funny">UR-FUNNY</pwcdataset>
    </paper>
    <paper id="190">
      <title>Hierarchical Multi-label Text Classification with Horizontal and Vertical Category Correlations</title>
      <author><first>Linli</first><last>Xu</last></author>
      <author><first>Sijie</first><last>Teng</last></author>
      <author><first>Ruoyu</first><last>Zhao</last></author>
      <author><first>Junliang</first><last>Guo</last></author>
      <author><first>Chi</first><last>Xiao</last></author>
      <author><first>Deqiang</first><last>Jiang</last></author>
      <author><first>Bo</first><last>Ren</last></author>
      <pages>2459–2468</pages>
      <abstract>Hierarchical multi-label text classification (HMTC) deals with the challenging task where an instance can be assigned to multiple hierarchically structured categories at the same time. The majority of prior studies either focus on reducing the HMTC task into a flat multi-label problem ignoring the vertical category correlations or exploiting the dependencies across different hierarchical levels without considering the horizontal correlations among categories at the same level, which inevitably leads to fundamental information loss. In this paper, we propose a novel HMTC framework that considers both vertical and horizontal category correlations. Specifically, we first design a loosely coupled graph convolutional neural network as the representation extractor to obtain representations for words, documents, and, more importantly, level-wise representations for categories, which are not considered in previous works. Then, the learned category representations are adopted to capture the vertical dependencies among levels of category hierarchy and model the horizontal correlations. Finally, based on the document embeddings and category embeddings, we design a hybrid algorithm to predict the categories of the entire hierarchical structure. Extensive experiments conducted on real-world HMTC datasets validate the effectiveness of the proposed framework with significant improvements over the baselines.</abstract>
      <url hash="51924be9">2021.emnlp-main.190</url>
      <attachment type="Software" hash="5291a014">2021.emnlp-main.190.Software.zip</attachment>
      <bibkey>xu-etal-2021-hierarchical</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.190</doi>
      <video href="2021.emnlp-main.190.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/web-of-science-dataset">WOS</pwcdataset>
    </paper>
    <paper id="191">
      <title><fixed-case>R</fixed-case>ank<fixed-case>NAS</fixed-case>: Efficient Neural Architecture Search by Pairwise Ranking</title>
      <author><first>Chi</first><last>Hu</last></author>
      <author><first>Chenglong</first><last>Wang</last></author>
      <author><first>Xiangnan</first><last>Ma</last></author>
      <author><first>Xia</first><last>Meng</last></author>
      <author><first>Yinqiao</first><last>Li</last></author>
      <author><first>Tong</first><last>Xiao</last></author>
      <author><first>Jingbo</first><last>Zhu</last></author>
      <author><first>Changliang</first><last>Li</last></author>
      <pages>2469–2480</pages>
      <abstract>This paper addresses the efficiency challenge of Neural Architecture Search (NAS) by formulating the task as a ranking problem. Previous methods require numerous training examples to estimate the accurate performance of architectures, although the actual goal is to find the distinction between “good” and “bad” candidates. Here we do not resort to performance predictors. Instead, we propose a performance ranking method (RankNAS) via pairwise ranking. It enables efficient architecture search using much fewer training examples. Moreover, we develop an architecture selection method to prune the search space and concentrate on more promising candidates. Extensive experiments on machine translation and language modeling tasks show that RankNAS can design high-performance architectures while being orders of magnitude faster than state-of-the-art NAS systems.</abstract>
      <url hash="d90e5eb9">2021.emnlp-main.191</url>
      <bibkey>hu-etal-2021-ranknas</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.191</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="192">
      <title><fixed-case>FL</fixed-case>i<fixed-case>T</fixed-case>ext: A Faster and Lighter Semi-Supervised Text Classification with Convolution Networks</title>
      <author><first>Chen</first><last>Liu</last></author>
      <author><first>Zhang</first><last>Mengchao</last></author>
      <author><first>Fu</first><last>Zhibing</last></author>
      <author><first>Panpan</first><last>Hou</last></author>
      <author><first>Yu</first><last>Li</last></author>
      <pages>2481–2491</pages>
      <abstract>In natural language processing (NLP), state-of-the-art (SOTA) semi-supervised learning (SSL) frameworks have shown great performance on deep pre-trained language models such as BERT, and are expected to significantly reduce the demand for manual labeling. However, our empirical studies indicate that these frameworks are not suitable for lightweight models such as TextCNN, LSTM and etc. In this work, we develop a new SSL framework called FLiText, which stands for Faster and Lighter semi-supervised Text classification. FLiText introduces an inspirer network together with the consistency regularization framework, which leverages a generalized regular constraint on the lightweight models for efficient SSL. As a result, FLiText obtains new SOTA performance for lightweight models across multiple SSL benchmarks on text classification. Compared with existing SOTA SSL methods on TextCNN, FLiText improves the accuracy of lightweight model TextCNN from 51.00% to 90.49% on IMDb, 39.8% to 58.06% on Yelp-5, and from 55.3% to 65.08% on Yahoo! Answer. In addition, compared with the fully supervised method on the full dataset, FLiText just uses less than 1% of labeled data to improve the accuracy by 6.59%, 3.94%, and 3.22% on the datasets of IMDb, Yelp-5, and Yahoo! Answer respectively.</abstract>
      <url hash="965d036a">2021.emnlp-main.192</url>
      <bibkey>liu-etal-2021-flitext</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.192</doi>
      <video href="2021.emnlp-main.192.mp4"/>
      <pwccode url="https://github.com/valuesimplex/flitext" additional="false">valuesimplex/flitext</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="193">
      <title>Evaluating Debiasing Techniques for Intersectional Biases</title>
      <author><first>Shivashankar</first><last>Subramanian</last></author>
      <author><first>Xudong</first><last>Han</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <author><first>Lea</first><last>Frermann</last></author>
      <pages>2492–2498</pages>
      <abstract>Bias is pervasive for NLP models, motivating the development of automatic debiasing techniques. Evaluation of NLP debiasing methods has largely been limited to binary attributes in isolation, e.g., debiasing with respect to binary gender or race, however many corpora involve multiple such attributes, possibly with higher cardinality. In this paper we argue that a truly fair model must consider ‘gerrymandering’ groups which comprise not only single attributes, but also intersectional groups. We evaluate a form of bias-constrained model which is new to NLP, as well an extension of the iterative nullspace projection technique which can handle multiple identities.</abstract>
      <url hash="c947702f">2021.emnlp-main.193</url>
      <bibkey>subramanian-etal-2021-evaluating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.193</doi>
      <video href="2021.emnlp-main.193.mp4"/>
    </paper>
    <paper id="194">
      <title>Definition Modelling for Appropriate Specificity</title>
      <author><first>Han</first><last>Huang</last></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last></author>
      <author><first>Yuki</first><last>Arase</last></author>
      <pages>2499–2509</pages>
      <abstract>Definition generation techniques aim to generate a definition of a target word or phrase given a context. In previous studies, researchers have faced various issues such as the out-of-vocabulary problem and over/under-specificity problems. Over-specific definitions present narrow word meanings, whereas under-specific definitions present general and context-insensitive meanings. Herein, we propose a method for definition generation with appropriate specificity. The proposed method addresses the aforementioned problems by leveraging a pre-trained encoder-decoder model, namely Text-to-Text Transfer Transformer, and introducing a re-ranking mechanism to model specificity in definitions. Experimental results on standard evaluation datasets indicate that our method significantly outperforms the previous state-of-the-art method. Moreover, manual evaluation confirms that our method effectively addresses the over/under-specificity problems.</abstract>
      <url hash="2859225b">2021.emnlp-main.194</url>
      <bibkey>huang-etal-2021-definition</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.194</doi>
      <video href="2021.emnlp-main.194.mp4"/>
    </paper>
    <paper id="195">
      <title>Transductive Learning for Unsupervised Text Style Transfer</title>
      <author><first>Fei</first><last>Xiao</last></author>
      <author><first>Liang</first><last>Pang</last></author>
      <author><first>Yanyan</first><last>Lan</last></author>
      <author><first>Yan</first><last>Wang</last></author>
      <author><first>Huawei</first><last>Shen</last></author>
      <author><first>Xueqi</first><last>Cheng</last></author>
      <pages>2510–2521</pages>
      <abstract>Unsupervised style transfer models are mainly based on an inductive learning approach, which represents the style as embeddings, decoder parameters, or discriminator parameters and directly applies these general rules to the test cases. However, the lacking of parallel corpus hinders the ability of these inductive learning methods on this task. As a result, it is likely to cause severe inconsistent style expressions, like ‘the salad is rude’. To tackle this problem, we propose a novel transductive learning approach in this paper, based on a retrieval-based context-aware style representation. Specifically, an attentional encoder-decoder with a retriever framework is utilized. It involves top-K relevant sentences in the target style in the transfer process. In this way, we can learn a context-aware style embedding to alleviate the above inconsistency problem. In this paper, both sparse (BM25) and dense retrieval functions (MIPS) are used, and two objective functions are designed to facilitate joint learning. Experimental results show that our method outperforms several strong baselines. The proposed transductive learning approach is general and effective to the task of unsupervised style transfer, and we will apply it to the other two typical methods in the future.</abstract>
      <url hash="409db797">2021.emnlp-main.195</url>
      <attachment type="Software" hash="f7d33825">2021.emnlp-main.195.Software.zip</attachment>
      <bibkey>xiao-etal-2021-transductive</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.195</doi>
      <video href="2021.emnlp-main.195.mp4"/>
      <pwccode url="https://github.com/xiaofei05/tsst" additional="false">xiaofei05/tsst</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
    </paper>
    <paper id="196">
      <title>Integrating Semantic Scenario and Word Relations for Abstractive Sentence Summarization</title>
      <author><first>Yong</first><last>Guan</last></author>
      <author><first>Shaoru</first><last>Guo</last></author>
      <author><first>Ru</first><last>Li</last></author>
      <author><first>Xiaoli</first><last>Li</last></author>
      <author><first>Hu</first><last>Zhang</last></author>
      <pages>2522–2529</pages>
      <abstract>Recently graph-based methods have been adopted for Abstractive Text Summarization. However, existing graph-based methods only consider either word relations or structure information, which neglect the correlation between them. To simultaneously capture the word relations and structure information from sentences, we propose a novel Dual Graph network for Abstractive Sentence Summarization. Specifically, we first construct semantic scenario graph and semantic word relation graph based on FrameNet, and subsequently learn their representations and design graph fusion method to enhance their correlation and obtain better semantic representation for summary generation. Experimental results show our model outperforms existing state-of-the-art methods on two popular benchmark datasets, i.e., Gigaword and DUC 2004.</abstract>
      <url hash="525f2c13">2021.emnlp-main.196</url>
      <bibkey>guan-etal-2021-integrating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.196</doi>
      <video href="2021.emnlp-main.196.mp4"/>
    </paper>
    <paper id="197">
      <title>Coupling Context Modeling with Zero Pronoun Recovering for Document-Level Natural Language Generation</title>
      <author><first>Xin</first><last>Tan</last></author>
      <author><first>Longyin</first><last>Zhang</last></author>
      <author><first>Guodong</first><last>Zhou</last></author>
      <pages>2530–2540</pages>
      <abstract>Natural language generation (NLG) tasks on pro-drop languages are known to suffer from zero pronoun (ZP) problems, and the problems remain challenging due to the scarcity of ZP-annotated NLG corpora. In this case, we propose a highly adaptive two-stage approach to couple context modeling with ZP recovering to mitigate the ZP problem in NLG tasks. Notably, we frame the recovery process in a task-supervised fashion where the ZP representation recovering capability is learned during the NLG task learning process, thus our method does not require NLG corpora annotated with ZPs. For system enhancement, we learn an adversarial bot to adjust our model outputs to alleviate the error propagation caused by mis-recovered ZPs. Experiments on three document-level NLG tasks, i.e., machine translation, question answering, and summarization, show that our approach can improve the performance to a great extent, and the improvement on pronoun translation is very impressive.</abstract>
      <url hash="fc13cfe4">2021.emnlp-main.197</url>
      <bibkey>tan-etal-2021-coupling</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.197</doi>
      <video href="2021.emnlp-main.197.mp4"/>
      <pwccode url="https://github.com/txannie/zp-dnlg" additional="false">txannie/zp-dnlg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/matinf">MATINF</pwcdataset>
    </paper>
    <paper id="198">
      <title>Adaptive Bridge between Training and Inference for Dialogue Generation</title>
      <author><first>Haoran</first><last>Xu</last></author>
      <author><first>Hainan</first><last>Zhang</last></author>
      <author><first>Yanyan</first><last>Zou</last></author>
      <author><first>Hongshen</first><last>Chen</last></author>
      <author><first>Zhuoye</first><last>Ding</last></author>
      <author><first>Yanyan</first><last>Lan</last></author>
      <pages>2541–2550</pages>
      <abstract>Although exposure bias has been widely studied in some NLP tasks, it faces its unique challenges in dialogue response generation, the representative one-to-various generation scenario.In real human dialogue, there are many appropriate responses for the same context, not only with different expressions, but also with different topics. Therefore, due to the much bigger gap between various ground-truth responses and the generated synthetic response, exposure bias is more challenging in dialogue generation task.What’s more, as MLE encourages the model to only learn the common words among different ground-truth responses, but ignores the interesting and specific parts, exposure bias may further lead to the common response generation problem, such as “I don’t know” and “HaHa?” In this paper, we propose a novel adaptive switching mechanism, which learns to automatically transit between ground-truth learning and generated learning regarding the word-level matching score, such as the cosine similarity. Experimental results on both Chinese STC dataset and English Reddit dataset, show that our adaptive method achieves a significant improvement in terms of metric-based evaluation and human evaluation, as compared with the state-of-the-art exposure bias approaches. Further analysis on NMT task also shows that our model can achieve a significant improvement.</abstract>
      <url hash="4372de17">2021.emnlp-main.198</url>
      <attachment type="Software" hash="42a862a4">2021.emnlp-main.198.Software.zip</attachment>
      <bibkey>xu-etal-2021-adaptive</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.198</doi>
      <video href="2021.emnlp-main.198.mp4"/>
    </paper>
    <paper id="199">
      <title><fixed-case>C</fixed-case>on<fixed-case>RPG</fixed-case>: Paraphrase Generation using Contexts as Regularizer</title>
      <author><first>Yuxian</first><last>Meng</last></author>
      <author><first>Xiang</first><last>Ao</last></author>
      <author><first>Qing</first><last>He</last></author>
      <author><first>Xiaofei</first><last>Sun</last></author>
      <author><first>Qinghong</first><last>Han</last></author>
      <author><first>Fei</first><last>Wu</last></author>
      <author><first>Chun</first><last>Fan</last></author>
      <author><first>Jiwei</first><last>Li</last></author>
      <pages>2551–2562</pages>
      <abstract>A long-standing issue with paraphrase generation is the lack of reliable supervision signals. In this paper, we propose a new unsupervised paradigm for paraphrase generation based on the assumption that the probabilities of generating two sentences with the same meaning given the same context should be the same. Inspired by this fundamental idea, we propose a pipelined system which consists of paraphrase candidate generation based on contextual language models, candidate filtering using scoring functions, and paraphrase model training based on the selected candidates. The proposed paradigm offers merits over existing paraphrase generation methods: (1) using the context regularizer on meanings, the model is able to generate massive amounts of high-quality paraphrase pairs; (2) the combination of the huge amount of paraphrase candidates and further diversity-promoting filtering yields paraphrases with more lexical and syntactic diversity; and (3) using human-interpretable scoring functions to select paraphrase pairs from candidates, the proposed framework provides a channel for developers to intervene with the data generation process, leading to a more controllable model. Experimental results across different tasks and datasets demonstrate that the proposed paradigm significantly outperforms existing paraphrase approaches in both supervised and unsupervised setups.</abstract>
      <url hash="f4b77498">2021.emnlp-main.199</url>
      <bibkey>meng-etal-2021-conrpg</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.199</doi>
      <video href="2021.emnlp-main.199.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="200">
      <title>Building the Directed Semantic Graph for Coherent Long Text Generation</title>
      <author><first>Ziao</first><last>Wang</last></author>
      <author><first>Xiaofeng</first><last>Zhang</last></author>
      <author><first>Hongwei</first><last>Du</last></author>
      <pages>2563–2572</pages>
      <abstract>Generating long text conditionally depending on the short input text has recently attracted more and more research efforts. Most existing approaches focus more on introducing extra knowledge to supplement the short input text, but ignore the coherence issue of the generated texts. To address aforementioned research issue, this paper proposes a novel two-stage approach to generate coherent long text. Particularly, we first build a document-level path for each output text with each sentence embedding as its node, and a revised self-organising map (SOM) is proposed to cluster similar nodes of a family of document-level paths to construct the directed semantic graph. Then, three subgraph alignment methods are proposed to extract the maximum matching paths or subgraphs. These directed subgraphs are considered to well preserve extra but relevant content to the short input text, and then they are decoded by the employed pre-trained model to generate coherent long text. Extensive experiments have been performed on three real-world datasets, and the promising results demonstrate that the proposed approach is superior to the state-of-the-art approaches w.r.t. a number of evaluation criteria.</abstract>
      <url hash="b4691e10">2021.emnlp-main.200</url>
      <bibkey>wang-etal-2021-building</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.200</doi>
      <video href="2021.emnlp-main.200.mp4"/>
    </paper>
    <paper id="201">
      <title>Iterative <fixed-case>GNN</fixed-case>-based Decoder for Question Generation</title>
      <author><first>Zichu</first><last>Fei</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Yaqian</first><last>Zhou</last></author>
      <pages>2573–2582</pages>
      <abstract>Natural question generation (QG) aims to generate questions from a passage, and generated questions are answered from the passage. Most models with state-of-the-art performance model the previously generated text at each decoding step. However, (1) they ignore the rich structure information that is hidden in the previously generated text. (2) they ignore the impact of copied words on the passage. We perceive that information in previously generated words serves as auxiliary information in subsequent generation. To address these problems, we design the Iterative Graph Network-based Decoder (IGND) to model the previous generation using a Graph Neural Network at each decoding step. Moreover, our graph model captures dependency relations in the passage that boost the generation. Experimental results demonstrate that our model outperforms the state-of-the-art models with sentence-level QG tasks on SQuAD and MARCO datasets.</abstract>
      <url hash="e6ad3706">2021.emnlp-main.201</url>
      <bibkey>fei-etal-2021-iterative</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.201</doi>
      <video href="2021.emnlp-main.201.mp4"/>
      <pwccode url="https://github.com/sion-zcfei/ignd" additional="false">sion-zcfei/ignd</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="202">
      <title>Asking Questions Like Educational Experts: <fixed-case>A</fixed-case>utomatically Generating Question-Answer Pairs on Real-World Examination Data</title>
      <author><first>Fanyi</first><last>Qu</last></author>
      <author><first>Xin</first><last>Jia</last></author>
      <author><first>Yunfang</first><last>Wu</last></author>
      <pages>2583–2593</pages>
      <abstract>Generating high quality question-answer pairs is a hard but meaningful task. Although previous works have achieved great results on answer-aware question generation, it is difficult to apply them into practical application in the education field. This paper for the first time addresses the question-answer pair generation task on the real-world examination data, and proposes a new unified framework on RACE. To capture the important information of the input passage we first automatically generate (rather than extracting) keyphrases, thus this task is reduced to keyphrase-question-answer triplet joint generation. Accordingly, we propose a multi-agent communication model to generate and optimize the question and keyphrases iteratively, and then apply the generated question and keyphrases to guide the generation of answers. To establish a solid benchmark, we build our model on the strong generative pre-training model. Experimental results show that our model makes great breakthroughs in the question-answer pair generation task. Moreover, we make a comprehensive analysis on our model, suggesting new directions for this challenging task.</abstract>
      <url hash="edcc18b7">2021.emnlp-main.202</url>
      <bibkey>qu-etal-2021-asking</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.202</doi>
      <video href="2021.emnlp-main.202.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="203">
      <title>Syntactically-Informed Unsupervised Paraphrasing with Non-Parallel Data</title>
      <author><first>Erguang</first><last>Yang</last></author>
      <author><first>Mingtong</first><last>Liu</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <author><first>Yujie</first><last>Zhang</last></author>
      <author><first>Yao</first><last>Meng</last></author>
      <author><first>Changjian</first><last>Hu</last></author>
      <author><first>Jinan</first><last>Xu</last></author>
      <author><first>Yufeng</first><last>Chen</last></author>
      <pages>2594–2604</pages>
      <abstract>Previous works on syntactically controlled paraphrase generation heavily rely on large-scale parallel paraphrase data that is not easily available for many languages and domains. In this paper, we take this research direction to the extreme and investigate whether it is possible to learn syntactically controlled paraphrase generation with nonparallel data. We propose a syntactically-informed unsupervised paraphrasing model based on conditional variational auto-encoder (VAE) which can generate texts in a specified syntactic structure. Particularly, we design a two-stage learning method to effectively train the model using non-parallel data. The conditional VAE is trained to reconstruct the input sentence according to the given input and its syntactic structure. Furthermore, to improve the syntactic controllability and semantic consistency of the pre-trained conditional VAE, we fine-tune it using syntax controlling and cycle reconstruction learning objectives, and employ Gumbel-Softmax to combine these new learning objectives. Experiment results demonstrate that the proposed model trained only on non-parallel data is capable of generating diverse paraphrases with specified syntactic structure. Additionally, we validate the effectiveness of our method for generating syntactically adversarial examples on the sentiment analysis task.</abstract>
      <url hash="3b80fcdd">2021.emnlp-main.203</url>
      <bibkey>yang-etal-2021-syntactically</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.203</doi>
      <video href="2021.emnlp-main.203.mp4"/>
      <pwccode url="https://github.com/lanse-sir/sup" additional="false">lanse-sir/sup</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="204">
      <title>Exploring Task Difficulty for Few-Shot Relation Extraction</title>
      <author><first>Jiale</first><last>Han</last></author>
      <author><first>Bo</first><last>Cheng</last></author>
      <author><first>Wei</first><last>Lu</last></author>
      <pages>2605–2616</pages>
      <abstract>Few-shot relation extraction (FSRE) focuses on recognizing novel relations by learning with merely a handful of annotated instances. Meta-learning has been widely adopted for such a task, which trains on randomly generated few-shot tasks to learn generic data representations. Despite impressive results achieved, existing models still perform suboptimally when handling hard FSRE tasks, where the relations are fine-grained and similar to each other. We argue this is largely because existing models do not distinguish hard tasks from easy ones in the learning process. In this paper, we introduce a novel approach based on contrastive learning that learns better representations by exploiting relation label information. We further design a method that allows the model to adaptively learn how to focus on hard tasks. Experiments on two standard datasets demonstrate the effectiveness of our method.</abstract>
      <url hash="a582ceff">2021.emnlp-main.204</url>
      <bibkey>han-etal-2021-exploring</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.204</doi>
      <video href="2021.emnlp-main.204.mp4"/>
      <pwccode url="https://github.com/hanjiale/hcrp" additional="false">hanjiale/hcrp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel-2-0">FewRel 2.0</pwcdataset>
    </paper>
    <paper id="205">
      <title><fixed-case>M</fixed-case>u<fixed-case>VER</fixed-case>: <fixed-case>I</fixed-case>mproving First-Stage Entity Retrieval with Multi-View Entity Representations</title>
      <author><first>Xinyin</first><last>Ma</last></author>
      <author><first>Yong</first><last>Jiang</last></author>
      <author><first>Nguyen</first><last>Bach</last></author>
      <author><first>Tao</first><last>Wang</last></author>
      <author><first>Zhongqiang</first><last>Huang</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Weiming</first><last>Lu</last></author>
      <pages>2617–2624</pages>
      <abstract>Entity retrieval, which aims at disambiguating mentions to canonical entities from massive KBs, is essential for many tasks in natural language processing. Recent progress in entity retrieval shows that the dual-encoder structure is a powerful and efficient framework to nominate candidates if entities are only identified by descriptions. However, they ignore the property that meanings of entity mentions diverge in different contexts and are related to various portions of descriptions, which are treated equally in previous works. In this work, we propose Multi-View Entity Representations (MuVER), a novel approach for entity retrieval that constructs multi-view representations for entity descriptions and approximates the optimal view for mentions via a heuristic searching method. Our method achieves the state-of-the-art performance on ZESHEL and improves the quality of candidates on three standard Entity Linking datasets.</abstract>
      <url hash="e45cddaa">2021.emnlp-main.205</url>
      <bibkey>ma-etal-2021-muver</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.205</doi>
      <video href="2021.emnlp-main.205.mp4"/>
      <pwccode url="https://github.com/alibaba-nlp/muver" additional="false">alibaba-nlp/muver</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/zeshel">ZESHEL</pwcdataset>
    </paper>
    <paper id="206">
      <title>Treasures Outside Contexts: Improving Event Detection via Global Statistics</title>
      <author><first>Rui</first><last>Li</last></author>
      <author><first>Wenlin</first><last>Zhao</last></author>
      <author><first>Cheng</first><last>Yang</last></author>
      <author><first>Sen</first><last>Su</last></author>
      <pages>2625–2635</pages>
      <abstract>Event detection (ED) aims at identifying event instances of specified types in given texts, which has been formalized as a sequence labeling task. As far as we know, existing neural-based ED models make decisions relying entirely on the contextual semantic features of each word in the inputted text, which we find is easy to be confused by the varied contexts in the test stage. To this end, we come up with the idea of introducing a set of statistical features from word-event co-occurrence frequencies in the entire training set to cooperate with contextual features. Specifically, we propose a Semantic and Statistic-Joint Discriminative Network (SS-JDN) consisting of a semantic feature extractor, a statistical feature extractor, and a joint event discriminator. In experiments, SS-JDN effectively exceeds ten recent strong baselines on ACE2005 and KBP2015 datasets. Further, we perform extensive experiments to comprehensively probe SS-JDN.</abstract>
      <url hash="e908f3d5">2021.emnlp-main.206</url>
      <bibkey>li-etal-2021-treasures</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.206</doi>
      <video href="2021.emnlp-main.206.mp4"/>
      <pwccode url="https://github.com/buted/ssjdn" additional="false">buted/ssjdn</pwccode>
    </paper>
    <paper id="207">
      <title>Uncertain Local-to-Global Networks for Document-Level Event Factuality Identification</title>
      <author><first>Pengfei</first><last>Cao</last></author>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Yuqing</first><last>Yang</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>2636–2645</pages>
      <abstract>Event factuality indicates the degree of certainty about whether an event occurs in the real world. Existing studies mainly focus on identifying event factuality at sentence level, which easily leads to conflicts between different mentions of the same event. To this end, we study the problem of document-level event factuality identification, which determines the event factuality from the view of a document. For this task, we need to consider two important characteristics: Local Uncertainty and Global Structure, which can be utilized to improve performance. In this paper, we propose an Uncertain Local-to-Global Network (ULGN) to make use of these two characteristics. Specifically, we devise a Local Uncertainty Estimation module to model the uncertainty of local information. Moreover, we propose an Uncertain Information Aggregation module to leverage the global structure for integrating the local information. Experimental results demonstrate the effectiveness of our proposed method, outperforming the previous state-of-the-art model by 8.4% and 11.45% of F1 score on two widely used datasets.</abstract>
      <url hash="dbc7619b">2021.emnlp-main.207</url>
      <bibkey>cao-etal-2021-uncertain</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.207</doi>
      <video href="2021.emnlp-main.207.mp4"/>
      <pwccode url="https://github.com/cpf-nlpr/ulgn4docefi" additional="false">cpf-nlpr/ulgn4docefi</pwccode>
    </paper>
    <paper id="208">
      <title>A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling</title>
      <author><first>Feiliang</first><last>Ren</last></author>
      <author><first>Longhui</first><last>Zhang</last></author>
      <author><first>Shujuan</first><last>Yin</last></author>
      <author><first>Xiaofeng</first><last>Zhao</last></author>
      <author><first>Shilei</first><last>Liu</last></author>
      <author><first>Bochao</first><last>Li</last></author>
      <author><first>Yaduo</first><last>Liu</last></author>
      <pages>2646–2656</pages>
      <abstract>Table filling based relational triple extraction methods are attracting growing research interests due to their promising performance and their abilities on extracting triples from complex sentences. However, this kind of methods are far from their full potential because most of them only focus on using local features but ignore the global associations of relations and of token pairs, which increases the possibility of overlooking some important information during triple extraction. To overcome this deficiency, we propose a global feature-oriented triple extraction model that makes full use of the mentioned two kinds of global associations. Specifically, we first generate a table feature for each relation. Then two kinds of global associations are mined from the generated table features. Next, the mined global associations are integrated into the table feature of each relation. This “generate-mine-integrate” process is performed multiple times so that the table feature of each relation is refined step by step. Finally, each relation’s table is filled based on its refined table feature, and all triples linked to this relation are extracted based on its filled table. We evaluate the proposed model on three benchmark datasets. Experimental results show our model is effective and it achieves state-of-the-art results on all of these datasets. The source code of our work is available at: https://github.com/neukg/GRTE.</abstract>
      <url hash="48df7407">2021.emnlp-main.208</url>
      <bibkey>ren-etal-2021-novel</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.208</doi>
      <video href="2021.emnlp-main.208.mp4"/>
      <pwccode url="https://github.com/neukg/GRTE" additional="false">neukg/GRTE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/webnlg">WebNLG</pwcdataset>
    </paper>
    <paper id="209">
      <title>Structure-Augmented Keyphrase Generation</title>
      <author><first>Jihyuk</first><last>Kim</last></author>
      <author><first>Myeongho</first><last>Jeong</last></author>
      <author><first>Seungtaek</first><last>Choi</last></author>
      <author><first>Seung-won</first><last>Hwang</last></author>
      <pages>2657–2667</pages>
      <abstract>This paper studies the keyphrase generation (KG) task for scenarios where structure plays an important role. For example, a scientific publication consists of a short title and a long body, where the title can be used for de-emphasizing unimportant details in the body. Similarly, for short social media posts (, tweets), scarce context can be augmented from titles, though often missing. Our contribution is generating/augmenting structure then injecting these information in the encoding, using existing keyphrases of other documents, complementing missing/incomplete titles. We propose novel structure-augmented document encoding approaches that consist of the following two phases: The first phase, generating structure, extends the given document with related but absent keyphrases, augmenting missing context. The second phase, encoding structure, builds a graph of keyphrases and the given document to obtain the structure-aware representation of the augmented text. Our empirical results validate that our proposed structure augmentation and augmentation-aware encoding/decoding can improve KG for both scenarios, outperforming the state-of-the-art.</abstract>
      <url hash="2d5c9e7e">2021.emnlp-main.209</url>
      <bibkey>kim-etal-2021-structure</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.209</doi>
      <video href="2021.emnlp-main.209.mp4"/>
      <pwccode url="https://github.com/jihyukkim-nlp/straugkg" additional="false">jihyukkim-nlp/straugkg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/kp20k">KP20k</pwcdataset>
    </paper>
    <paper id="210">
      <title>An Empirical Study on Multiple Information Sources for Zero-Shot Fine-Grained Entity Typing</title>
      <author><first>Yi</first><last>Chen</last></author>
      <author><first>Haiyun</first><last>Jiang</last></author>
      <author><first>Lemao</first><last>Liu</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <author><first>Chuang</first><last>Fan</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <pages>2668–2678</pages>
      <abstract>Auxiliary information from multiple sources has been demonstrated to be effective in zero-shot fine-grained entity typing (ZFET). However, there lacks a comprehensive understanding about how to make better use of the existing information sources and how they affect the performance of ZFET. In this paper, we empirically study three kinds of auxiliary information: context consistency, type hierarchy and background knowledge (e.g., prototypes and descriptions) of types, and propose a multi-source fusion model (MSF) targeting these sources. The performance obtains up to 11.42% and 22.84% absolute gains over state-of-the-art baselines on BBN and Wiki respectively with regard to macro F1 scores. More importantly, we further discuss the characteristics, merits and demerits of each information source and provide an intuitive understanding of the complementarity among them.</abstract>
      <url hash="25bf15b4">2021.emnlp-main.210</url>
      <bibkey>chen-etal-2021-empirical</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.210</doi>
      <video href="2021.emnlp-main.210.mp4"/>
    </paper>
    <paper id="211">
      <title><fixed-case>D</fixed-case>y<fixed-case>L</fixed-case>ex: Incorporating Dynamic Lexicons into <fixed-case>BERT</fixed-case> for Sequence Labeling</title>
      <author><first>Baojun</first><last>Wang</last></author>
      <author><first>Zhao</first><last>Zhang</last></author>
      <author><first>Kun</first><last>Xu</last></author>
      <author><first>Guang-Yuan</first><last>Hao</last></author>
      <author><first>Yuyang</first><last>Zhang</last></author>
      <author><first>Lifeng</first><last>Shang</last></author>
      <author><first>Linlin</first><last>Li</last></author>
      <author><first>Xiao</first><last>Chen</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>2679–2693</pages>
      <abstract>Incorporating lexical knowledge into deep learning models has been proved to be very effective for sequence labeling tasks. However, previous works commonly have difficulty dealing with large-scale dynamic lexicons which often cause excessive matching noise and problems of frequent updates. In this paper, we propose DyLex, a plug-in lexicon incorporation approach for BERT based sequence labeling tasks. Instead of leveraging embeddings of words in the lexicon as in conventional methods, we adopt word-agnostic tag embeddings to avoid re-training the representation while updating the lexicon. Moreover, we employ an effective supervised lexical knowledge denoising method to smooth out matching noise. Finally, we introduce a col-wise attention based knowledge fusion mechanism to guarantee the pluggability of the proposed framework. Experiments on ten datasets of three tasks show that the proposed framework achieves new SOTA, even with very large scale lexicons.</abstract>
      <url hash="8512cced">2021.emnlp-main.211</url>
      <bibkey>wang-etal-2021-dylex</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.211</doi>
      <pwccode url="https://github.com/huawei-noah/noah-research/tree/master/NLP/dylex" additional="false">huawei-noah/noah-research</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="212">
      <title><fixed-case>M</fixed-case>ap<fixed-case>RE</fixed-case>: An Effective Semantic Mapping Approach for Low-resource Relation Extraction</title>
      <author><first>Manqing</first><last>Dong</last></author>
      <author><first>Chunguang</first><last>Pan</last></author>
      <author><first>Zhipeng</first><last>Luo</last></author>
      <pages>2694–2704</pages>
      <abstract>Neural relation extraction models have shown promising results in recent years; however, the model performance drops dramatically given only a few training samples. Recent works try leveraging the advance in few-shot learning to solve the low resource problem, where they train label-agnostic models to directly compare the semantic similarities among context sentences in the embedding space. However, the label-aware information, i.e., the relation label that contains the semantic knowledge of the relation itself, is often neglected for prediction. In this work, we propose a framework considering both label-agnostic and label-aware semantic mapping information for low resource relation extraction. We show that incorporating the above two types of mapping information in both pretraining and fine-tuning can significantly improve the model performance on low-resource relation extraction tasks.</abstract>
      <url hash="13bb9bc0">2021.emnlp-main.212</url>
      <bibkey>dong-etal-2021-mapre</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.212</doi>
      <video href="2021.emnlp-main.212.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="213">
      <title>Heterogeneous Graph Neural Networks for Keyphrase Generation</title>
      <author><first>Jiacheng</first><last>Ye</last></author>
      <author><first>Ruijian</first><last>Cai</last></author>
      <author><first>Tao</first><last>Gui</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <pages>2705–2715</pages>
      <abstract>The encoder–decoder framework achieves state-of-the-art results in keyphrase generation (KG) tasks by predicting both present keyphrases that appear in the source document and absent keyphrases that do not. However, relying solely on the source document can result in generating uncontrollable and inaccurate absent keyphrases. To address these problems, we propose a novel graph-based method that can capture explicit knowledge from related references. Our model first retrieves some document-keyphrases pairs similar to the source document from a pre-defined index as references. Then a heterogeneous graph is constructed to capture relations with different levels of granularity of the source document and its retrieved references. To guide the decoding process, a hierarchical attention and copy mechanism is introduced, which directly copies appropriate words from both source document and its references based on their relevance and significance. The experimental results on multiple KG benchmarks show that the proposed model achieves significant improvements against other baseline models, especially with regard to the absent keyphrase prediction.</abstract>
      <url hash="853fd24b">2021.emnlp-main.213</url>
      <bibkey>ye-etal-2021-heterogeneous</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.213</doi>
      <video href="2021.emnlp-main.213.mp4"/>
      <pwccode url="https://github.com/jiacheng-ye/kg_gater" additional="false">jiacheng-ye/kg_gater</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/kp20k">KP20k</pwcdataset>
    </paper>
    <paper id="214">
      <title>Machine Reading Comprehension as Data Augmentation: A Case Study on Implicit Event Argument Extraction</title>
      <author><first>Jian</first><last>Liu</last></author>
      <author><first>Yufeng</first><last>Chen</last></author>
      <author><first>Jinan</first><last>Xu</last></author>
      <pages>2716–2725</pages>
      <abstract>Implicit event argument extraction (EAE) is a crucial document-level information extraction task that aims to identify event arguments beyond the sentence level. Despite many efforts for this task, the lack of enough training data has long impeded the study. In this paper, we take a new perspective to address the data sparsity issue faced by implicit EAE, by bridging the task with machine reading comprehension (MRC). Particularly, we devise two data augmentation regimes via MRC, including: 1) implicit knowledge transfer, which enables knowledge transfer from other tasks, by building a unified training framework in the MRC formulation, and 2) explicit data augmentation, which can explicitly generate new training examples, by treating MRC models as an annotator. The extensive experiments have justified the effectiveness of our approach — it not only obtains state-of-the-art performance on two benchmarks, but also demonstrates superior results in a data-low scenario.</abstract>
      <url hash="7a22ed6d">2021.emnlp-main.214</url>
      <bibkey>liu-etal-2021-machine</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.214</doi>
      <revision id="1" href="2021.emnlp-main.214v1" hash="981943cb"/>
      <revision id="2" href="2021.emnlp-main.214v2" hash="7a22ed6d" date="2022-01-02">Corrected sponsor number in the Acknowledgments section.</revision>
      <video href="2021.emnlp-main.214.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/wikievents">WikiEvents</pwcdataset>
    </paper>
    <paper id="215">
      <title><fixed-case>I</fixed-case>mportance <fixed-case>E</fixed-case>stimation from <fixed-case>M</fixed-case>ultiple <fixed-case>P</fixed-case>erspectives for <fixed-case>K</fixed-case>eyphrase <fixed-case>E</fixed-case>xtraction</title>
      <author><first>Mingyang</first><last>Song</last></author>
      <author><first>Liping</first><last>Jing</last></author>
      <author><first>Lin</first><last>Xiao</last></author>
      <pages>2726–2736</pages>
      <abstract>Keyphrase extraction is a fundamental task in Natural Language Processing, which usually contains two main parts: candidate keyphrase extraction and keyphrase importance estimation. From the view of human understanding documents, we typically measure the importance of phrase according to its syntactic accuracy, information saliency, and concept consistency simultaneously. However, most existing keyphrase extraction approaches only focus on the part of them, which leads to biased results. In this paper, we propose a new approach to estimate the importance of keyphrase from multiple perspectives (called as <i>KIEMP</i>) and further improve the performance of keyphrase extraction. Specifically, <i>KIEMP</i> estimates the importance of phrase with three modules: a chunking module to measure its syntactic accuracy, a ranking module to check its information saliency, and a matching module to judge the concept (i.e., topic) consistency between phrase and the whole document. These three modules are seamlessly jointed together via an end-to-end multi-task learning model, which is helpful for three parts to enhance each other and balance the effects of three perspectives. Experimental results on six benchmark datasets show that <i>KIEMP</i> outperforms the existing state-of-the-art keyphrase extraction approaches in most cases.</abstract>
      <url hash="134adfb5">2021.emnlp-main.215</url>
      <bibkey>song-etal-2021-importance</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.215</doi>
      <video href="2021.emnlp-main.215.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/kp20k">KP20k</pwcdataset>
    </paper>
    <paper id="216">
      <title>Gradient Imitation Reinforcement Learning for Low Resource Relation Extraction</title>
      <author><first>Xuming</first><last>Hu</last></author>
      <author><first>Chenwei</first><last>Zhang</last></author>
      <author><first>Yawen</first><last>Yang</last></author>
      <author><first>Xiaohe</first><last>Li</last></author>
      <author><first>Li</first><last>Lin</last></author>
      <author><first>Lijie</first><last>Wen</last></author>
      <author><first>Philip S.</first><last>Yu</last></author>
      <pages>2737–2746</pages>
      <abstract>Low-resource Relation Extraction (LRE) aims to extract relation facts from limited labeled corpora when human annotation is scarce. Existing works either utilize self-training scheme to generate pseudo labels that will cause the gradual drift problem, or leverage meta-learning scheme which does not solicit feedback explicitly. To alleviate selection bias due to the lack of feedback loops in existing LRE learning paradigms, we developed a Gradient Imitation Reinforcement Learning method to encourage pseudo label data to imitate the gradient descent direction on labeled data and bootstrap its optimization capability through trial and error. We also propose a framework called GradLRE, which handles two major scenarios in low-resource relation extraction. Besides the scenario where unlabeled data is sufficient, GradLRE handles the situation where no unlabeled data is available, by exploiting a contextualized augmentation method to generate data. Experimental results on two public datasets demonstrate the effectiveness of GradLRE on low resource relation extraction when comparing with baselines.</abstract>
      <url hash="8a08b757">2021.emnlp-main.216</url>
      <bibkey>hu-etal-2021-gradient</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.216</doi>
      <video href="2021.emnlp-main.216.mp4"/>
      <pwccode url="https://github.com/thu-bpm/gradlre" additional="false">thu-bpm/gradlre</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2010-task-8">SemEval-2010 Task 8</pwcdataset>
    </paper>
    <paper id="217">
      <title>Low-resource Taxonomy Enrichment with Pretrained Language Models</title>
      <author><first>Kunihiro</first><last>Takeoka</last></author>
      <author><first>Kosuke</first><last>Akimoto</last></author>
      <author><first>Masafumi</first><last>Oyamada</last></author>
      <pages>2747–2758</pages>
      <abstract>Taxonomies are symbolic representations of hierarchical relationships between terms or entities. While taxonomies are useful in broad applications, manually updating or maintaining them is labor-intensive and difficult to scale in practice. Conventional supervised methods for this enrichment task fail to find optimal parents of new terms in low-resource settings where only small taxonomies are available because of overfitting to hierarchical relationships in the taxonomies. To tackle the problem of low-resource taxonomy enrichment, we propose Musubu, an efficient framework for taxonomy enrichment in low-resource settings with pretrained language models (LMs) as knowledge bases to compensate for the shortage of information. Musubu leverages an LM-based classifier to determine whether or not inputted term pairs have hierarchical relationships. Musubu also utilizes Hearst patterns to generate queries to leverage implicit knowledge from the LM efficiently for more accurate prediction. We empirically demonstrate the effectiveness of our method in extensive experiments on taxonomies from both a SemEval task and real-world retailer datasets.</abstract>
      <url hash="a6756f7d">2021.emnlp-main.217</url>
      <bibkey>takeoka-etal-2021-low</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.217</doi>
      <video href="2021.emnlp-main.217.mp4"/>
    </paper>
    <paper id="218">
      <title>Entity Relation Extraction as Dependency Parsing in Visually Rich Documents</title>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Zhang</first><last>Bo</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Junjie</first><last>Cao</last></author>
      <author><first>Chen</first><last>Li</last></author>
      <author><first>Zuyi</first><last>Bao</last></author>
      <pages>2759–2768</pages>
      <abstract>Previous works on key information extraction from visually rich documents (VRDs) mainly focus on labeling the text within each bounding box (i.e.,semantic entity), while the relations in-between are largely unexplored. In this paper, we adapt the popular dependency parsing model, the biaffine parser, to this entity relation extraction task. Being different from the original dependency parsing model which recognizes dependency relations between words, we identify relations between groups of words with layout information instead. We have compared different representations of the semantic entity, different VRD encoders, and different relation decoders. For the model training, we explore multi-task learning to combine entity labeling and relation extraction tasks; and for the evaluation, we conduct experiments on different datasets with filtering and augmentation. The results demonstrate that our proposed model achieves 65.96% F1 score on the FUNSD dataset. As for the real-world application, our model has been applied to the in-house customs data, achieving reliable performance in the production setting.</abstract>
      <url hash="b149fbb2">2021.emnlp-main.218</url>
      <bibkey>zhang-etal-2021-entity</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.218</doi>
      <video href="2021.emnlp-main.218.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/funsd">FUNSD</pwcdataset>
    </paper>
    <paper id="219">
      <title>Synchronous Dual Network with Cross-Type Attention for Joint Entity and Relation Extraction</title>
      <author><first>Hui</first><last>Wu</last></author>
      <author><first>Xiaodong</first><last>Shi</last></author>
      <pages>2769–2779</pages>
      <abstract>Joint entity and relation extraction is challenging due to the complex interaction of interaction between named entity recognition and relation extraction. Although most existing works tend to jointly train these two tasks through a shared network, they fail to fully utilize the interdependence between entity types and relation types. In this paper, we design a novel synchronous dual network (SDN) with cross-type attention via separately and interactively considering the entity types and relation types. On the one hand, SDN adopts two isomorphic bi-directional type-attention LSTM to encode the entity type enhanced representations and the relation type enhanced representations, respectively. On the other hand, SDN explicitly models the interdependence between entity types and relation types via cross-type attention mechanism. In addition, we also propose a new multi-task learning strategy via modeling the interaction of two types of information. Experiments on NYT and WebNLG datasets verify the effectiveness of the proposed model, achieving state-of-the-art performance.</abstract>
      <url hash="196e4033">2021.emnlp-main.219</url>
      <bibkey>wu-shi-2021-synchronous</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.219</doi>
      <video href="2021.emnlp-main.219.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/webnlg">WebNLG</pwcdataset>
    </paper>
    <paper id="220">
      <title>Less is More: Pretrain a Strong <fixed-case>S</fixed-case>iamese Encoder for Dense Text Retrieval Using a Weak Decoder</title>
      <author><first>Shuqi</first><last>Lu</last></author>
      <author><first>Di</first><last>He</last></author>
      <author><first>Chenyan</first><last>Xiong</last></author>
      <author><first>Guolin</first><last>Ke</last></author>
      <author><first>Waleed</first><last>Malik</last></author>
      <author><first>Zhicheng</first><last>Dou</last></author>
      <author><first>Paul</first><last>Bennett</last></author>
      <author><first>Tie-Yan</first><last>Liu</last></author>
      <author><first>Arnold</first><last>Overwijk</last></author>
      <pages>2780–2791</pages>
      <abstract>Dense retrieval requires high-quality text sequence embeddings to support effective search in the representation space. Autoencoder-based language models are appealing in dense retrieval as they train the encoder to output high-quality embedding that can reconstruct the input texts. However, in this paper, we provide theoretical analyses and show empirically that an autoencoder language model with a low reconstruction loss may not provide good sequence representations because the decoder may take shortcuts by exploiting language patterns. To address this, we propose a new self-learning method that pre-trains the autoencoder using a <i>weak</i> decoder, with restricted capacity and attention flexibility to push the encoder to provide better text representations. Our experiments on web search, news recommendation, and open domain question answering show that our pre-trained model significantly boosts the effectiveness and few-shot ability of dense retrieval models. Our code is available at https://github.com/microsoft/SEED-Encoder/.</abstract>
      <url hash="dcdc6015">2021.emnlp-main.220</url>
      <bibkey>lu-etal-2021-less</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.220</doi>
      <video href="2021.emnlp-main.220.mp4"/>
      <pwccode url="https://github.com/microsoft/seed-encoder" additional="false">microsoft/seed-encoder</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mind">MIND</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="221">
      <title><fixed-case>T</fixed-case>rans<fixed-case>P</fixed-case>rompt: Towards an Automatic Transferable Prompting Framework for Few-shot Text Classification</title>
      <author><first>Chengyu</first><last>Wang</last></author>
      <author><first>Jianing</first><last>Wang</last></author>
      <author><first>Minghui</first><last>Qiu</last></author>
      <author><first>Jun</first><last>Huang</last></author>
      <author><first>Ming</first><last>Gao</last></author>
      <pages>2792–2802</pages>
      <abstract>Recent studies have shown that prompts improve the performance of large pre-trained language models for few-shot text classification. Yet, it is unclear how the prompting knowledge can be transferred across similar NLP tasks for the purpose of mutual reinforcement. Based on continuous prompt embeddings, we propose TransPrompt, a transferable prompting framework for few-shot learning across similar tasks. In TransPrompt, we employ a multi-task meta-knowledge acquisition procedure to train a meta-learner that captures cross-task transferable knowledge. Two de-biasing techniques are further designed to make it more task-agnostic and unbiased towards any tasks. After that, the meta-learner can be adapted to target tasks with high accuracy. Extensive experiments show that TransPrompt outperforms single-task and cross-task strong baselines over multiple NLP tasks and datasets. We further show that the meta-learner can effectively improve the performance on previously unseen tasks; and TransPrompt also outperforms strong fine-tuning baselines when learning with full training sets.</abstract>
      <url hash="a897e977">2021.emnlp-main.221</url>
      <bibkey>wang-etal-2021-transprompt</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.221</doi>
      <video href="2021.emnlp-main.221.mp4"/>
      <pwccode url="https://github.com/wjn1996/TransPrompt" additional="true">wjn1996/TransPrompt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mr">MR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="222">
      <title>Weakly-supervised Text Classification Based on Keyword Graph</title>
      <author><first>Lu</first><last>Zhang</last></author>
      <author><first>Jiandong</first><last>Ding</last></author>
      <author><first>Yi</first><last>Xu</last></author>
      <author><first>Yingyao</first><last>Liu</last></author>
      <author><first>Shuigeng</first><last>Zhou</last></author>
      <pages>2803–2813</pages>
      <abstract>Weakly-supervised text classification has received much attention in recent years for it can alleviate the heavy burden of annotating massive data. Among them, keyword-driven methods are the mainstream where user-provided keywords are exploited to generate pseudo-labels for unlabeled texts. However, existing methods treat keywords independently, thus ignore the correlation among them, which should be useful if properly exploited. In this paper, we propose a novel framework called ClassKG to explore keyword-keyword correlation on keyword graph by GNN. Our framework is an iterative process. In each iteration, we first construct a keyword graph, so the task of assigning pseudo labels is transformed to annotating keyword subgraphs. To improve the annotation quality, we introduce a self-supervised task to pretrain a subgraph annotator, and then finetune it. With the pseudo labels generated by the subgraph annotator, we then train a text classifier to classify the unlabeled texts. Finally, we re-extract keywords from the classified texts. Extensive experiments on both long-text and short-text datasets show that our method substantially outperforms the existing ones.</abstract>
      <url hash="fbf2cffe">2021.emnlp-main.222</url>
      <bibkey>zhang-etal-2021-weakly</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.222</doi>
      <video href="2021.emnlp-main.222.mp4"/>
      <pwccode url="https://github.com/zhanglu-cst/classkg" additional="false">zhanglu-cst/classkg</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="223">
      <title>Efficient-<fixed-case>F</fixed-case>ed<fixed-case>R</fixed-case>ec: Efficient Federated Learning Framework for Privacy-Preserving News Recommendation</title>
      <author><first>Jingwei</first><last>Yi</last></author>
      <author><first>Fangzhao</first><last>Wu</last></author>
      <author><first>Chuhan</first><last>Wu</last></author>
      <author><first>Ruixuan</first><last>Liu</last></author>
      <author><first>Guangzhong</first><last>Sun</last></author>
      <author><first>Xing</first><last>Xie</last></author>
      <pages>2814–2824</pages>
      <abstract>News recommendation is critical for personalized news access. Most existing news recommendation methods rely on centralized storage of users’ historical news click behavior data, which may lead to privacy concerns and hazards. Federated Learning is a privacy-preserving framework for multiple clients to collaboratively train models without sharing their private data. However, the computation and communication cost of directly learning many existing news recommendation models in a federated way are unacceptable for user clients. In this paper, we propose an efficient federated learning framework for privacy-preserving news recommendation. Instead of training and communicating the whole model, we decompose the news recommendation model into a large news model maintained in the server and a light-weight user model shared on both server and clients, where news representations and user model are communicated between server and clients. More specifically, the clients request the user model and news representations from the server, and send their locally computed gradients to the server for aggregation. The server updates its global user model with the aggregated gradients, and further updates its news model to infer updated news representations. Since the local gradients may contain private information, we propose a secure aggregation method to aggregate gradients in a privacy-preserving way. Experiments on two real-world datasets show that our method can reduce the computation and communication cost on clients while keep promising model performance.</abstract>
      <url hash="5c3d6b96">2021.emnlp-main.223</url>
      <bibkey>yi-etal-2021-efficient</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.223</doi>
      <video href="2021.emnlp-main.223.mp4"/>
      <pwccode url="https://github.com/yjw1029/efficient-fedrec" additional="false">yjw1029/efficient-fedrec</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mind">MIND</pwcdataset>
    </paper>
    <paper id="224">
      <title><fixed-case>R</fixed-case>ocket<fixed-case>QA</fixed-case>v2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking</title>
      <author><first>Ruiyang</first><last>Ren</last></author>
      <author><first>Yingqi</first><last>Qu</last></author>
      <author><first>Jing</first><last>Liu</last></author>
      <author><first>Wayne Xin</first><last>Zhao</last></author>
      <author><first>QiaoQiao</first><last>She</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <author><first>Ji-Rong</first><last>Wen</last></author>
      <pages>2825–2835</pages>
      <abstract>In various natural language processing tasks, passage retrieval and passage re-ranking are two key procedures in finding and ranking relevant information. Since both the two procedures contribute to the final performance, it is important to jointly optimize them in order to achieve mutual improvement. In this paper, we propose a novel joint training approach for dense passage retrieval and passage reranking. A major contribution is that we introduce the dynamic listwise distillation, where we design a unified listwise training approach for both the retriever and the re-ranker. During the dynamic distillation, the retriever and the re-ranker can be adaptively improved according to each other’s relevance information. We also propose a hybrid data augmentation strategy to construct diverse training instances for listwise training approach. Extensive experiments show the effectiveness of our approach on both MSMARCO and Natural Questions datasets. Our code is available at https://github.com/PaddlePaddle/RocketQA.</abstract>
      <url hash="f214ef52">2021.emnlp-main.224</url>
      <bibkey>ren-etal-2021-rocketqav2</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.224</doi>
      <video href="2021.emnlp-main.224.mp4"/>
      <pwccode url="https://github.com/paddlepaddle/rocketqa" additional="false">paddlepaddle/rocketqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="225">
      <title>Dealing with Typos for <fixed-case>BERT</fixed-case>-based Passage Retrieval and Ranking</title>
      <author><first>Shengyao</first><last>Zhuang</last></author>
      <author><first>Guido</first><last>Zuccon</last></author>
      <pages>2836–2842</pages>
      <abstract>Passage retrieval and ranking is a key task in open-domain question answering and information retrieval. Current effective approaches mostly rely on pre-trained deep language model-based retrievers and rankers. These methods have been shown to effectively model the semantic matching between queries and passages, also in presence of keyword mismatch, i.e. passages that are relevant to a query but do not contain important query keywords. In this paper we consider the Dense Retriever (DR), a passage retrieval method, and the BERT re-ranker, a popular passage re-ranking method. In this context, we formally investigate how these models respond and adapt to a specific type of keyword mismatch – that caused by keyword typos occurring in queries. Through empirical investigation, we find that typos can lead to a significant drop in retrieval and ranking effectiveness. We then propose a simple typos-aware training framework for DR and BERT re-ranker to address this issue. Our experimental results on the MS MARCO passage ranking dataset show that, with our proposed typos-aware training, DR and BERT re-ranker can become robust to typos in queries, resulting in significantly improved effectiveness compared to models trained without appropriately accounting for typos.</abstract>
      <url hash="cfa52b4f">2021.emnlp-main.225</url>
      <bibkey>zhuang-zuccon-2021-dealing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.225</doi>
      <video href="2021.emnlp-main.225.mp4"/>
      <pwccode url="https://github.com/ielab/typos-aware-bert" additional="true">ielab/typos-aware-bert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
    </paper>
    <paper id="226">
      <title>From Alignment to Assignment: Frustratingly Simple Unsupervised Entity Alignment</title>
      <author><first>Xin</first><last>Mao</last></author>
      <author><first>Wenting</first><last>Wang</last></author>
      <author><first>Yuanbin</first><last>Wu</last></author>
      <author><first>Man</first><last>Lan</last></author>
      <pages>2843–2853</pages>
      <abstract>Cross-lingual entity alignment (EA) aims to find the equivalent entities between crosslingual KGs (Knowledge Graphs), which is a crucial step for integrating KGs. Recently, many GNN-based EA methods are proposed and show decent performance improvements on several public datasets. However, existing GNN-based EA methods inevitably inherit poor interpretability and low efficiency from neural networks. Motivated by the isomorphic assumption of GNN-based methods, we successfully transform the cross-lingual EA problem into an assignment problem. Based on this re-definition, we propose a frustratingly Simple but Effective Unsupervised entity alignment method (SEU) without neural networks. Extensive experiments have been conducted to show that our proposed unsupervised approach even beats advanced supervised methods across all public datasets while having high efficiency, interpretability, and stability.</abstract>
      <url hash="ff462661">2021.emnlp-main.226</url>
      <attachment type="Software" hash="d5f38a73">2021.emnlp-main.226.Software.zip</attachment>
      <bibkey>mao-etal-2021-alignment</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.226</doi>
      <video href="2021.emnlp-main.226.mp4"/>
      <pwccode url="https://github.com/maoxinn/seu" additional="false">maoxinn/seu</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dbp15k">DBP15K</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mmkg">MMKG</pwcdataset>
    </paper>
    <paper id="227">
      <title>Simple and Effective Unsupervised Redundancy Elimination to Compress Dense Vectors for Passage Retrieval</title>
      <author><first>Xueguang</first><last>Ma</last></author>
      <author><first>Minghan</first><last>Li</last></author>
      <author><first>Kai</first><last>Sun</last></author>
      <author><first>Ji</first><last>Xin</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>2854–2859</pages>
      <abstract>Recent work has shown that dense passage retrieval techniques achieve better ranking accuracy in open-domain question answering compared to sparse retrieval techniques such as BM25, but at the cost of large space and memory requirements. In this paper, we analyze the redundancy present in encoded dense vectors and show that the default dimension of 768 is unnecessarily large. To improve space efficiency, we propose a simple unsupervised compression pipeline that consists of principal component analysis (PCA), product quantization, and hybrid search. We further investigate other supervised baselines and find surprisingly that unsupervised PCA outperforms them in some settings. We perform extensive experiments on five question answering datasets and demonstrate that our best pipeline achieves good accuracy–space trade-offs, for example, <tex-math>48\times</tex-math> compression with less than 3% drop in top-100 retrieval accuracy on average or <tex-math>96\times</tex-math> compression with less than 4% drop. Code and data are available at <url>http://pyserini.io/</url>.</abstract>
      <url hash="44e2c522">2021.emnlp-main.227</url>
      <bibkey>ma-etal-2021-simple</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.227</doi>
      <video href="2021.emnlp-main.227.mp4"/>
    </paper>
    <paper id="228">
      <title>Relation Extraction with Word Graphs from N-grams</title>
      <author><first>Han</first><last>Qin</last></author>
      <author><first>Yuanhe</first><last>Tian</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <pages>2860–2868</pages>
      <abstract>Most recent studies for relation extraction (RE) leverage the dependency tree of the input sentence to incorporate syntax-driven contextual information to improve model performance, with little attention paid to the limitation where high-quality dependency parsers in most cases unavailable, especially for in-domain scenarios. To address this limitation, in this paper, we propose attentive graph convolutional networks (A-GCN) to improve neural RE methods with an unsupervised manner to build the context graph, without relying on the existence of a dependency parser. Specifically, we construct the graph from n-grams extracted from a lexicon built from pointwise mutual information (PMI) and apply attention over the graph. Therefore, different word pairs from the contexts within and across n-grams are weighted in the model and facilitate RE accordingly. Experimental results with further analyses on two English benchmark datasets for RE demonstrate the effectiveness of our approach, where state-of-the-art performance is observed on both datasets.</abstract>
      <url hash="91258c4e">2021.emnlp-main.228</url>
      <bibkey>qin-etal-2021-relation</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.228</doi>
      <video href="2021.emnlp-main.228.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2010-task-8">SemEval-2010 Task 8</pwcdataset>
    </paper>
    <paper id="229">
      <title>A <fixed-case>B</fixed-case>ayesian Framework for Information-Theoretic Probing</title>
      <author><first>Tiago</first><last>Pimentel</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <pages>2869–2887</pages>
      <abstract>Pimentel et al. (2020) recently analysed probing from an information-theoretic perspective. They argue that probing should be seen as approximating a mutual information. This led to the rather unintuitive conclusion that representations encode exactly the same information about a target task as the original sentences. The mutual information, however, assumes the true probability distribution of a pair of random variables is known, leading to unintuitive results in settings where it is not. This paper proposes a new framework to measure what we term Bayesian mutual information, which analyses information from the perspective of Bayesian agents—allowing for more intuitive findings in scenarios with finite data. For instance, under Bayesian MI we have that data can add information, processing can help, and information can hurt, which makes it more intuitive for machine learning applications. Finally, we apply our framework to probing where we believe Bayesian mutual information naturally operationalises ease of extraction by explicitly limiting the available background knowledge to solve a task.</abstract>
      <url hash="5c30af52">2021.emnlp-main.229</url>
      <bibkey>pimentel-cotterell-2021-bayesian</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.229</doi>
      <video href="2021.emnlp-main.229.mp4"/>
      <pwccode url="https://github.com/rycolab/bayesian-mi" additional="false">rycolab/bayesian-mi</pwccode>
    </paper>
    <paper id="230">
      <title>Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little</title>
      <author><first>Koustuv</first><last>Sinha</last></author>
      <author><first>Robin</first><last>Jia</last></author>
      <author><first>Dieuwke</first><last>Hupkes</last></author>
      <author><first>Joelle</first><last>Pineau</last></author>
      <author><first>Adina</first><last>Williams</last></author>
      <author><first>Douwe</first><last>Kiela</last></author>
      <pages>2888–2913</pages>
      <abstract>A possible explanation for the impressive performance of masked language model (MLM) pre-training is that such models have learned to represent the syntactic structures prevalent in classical NLP pipelines. In this paper, we propose a different explanation: MLMs succeed on downstream tasks almost entirely due to their ability to model higher-order word co-occurrence statistics. To demonstrate this, we pre-train MLMs on sentences with randomly shuffled word order, and show that these models still achieve high accuracy after fine-tuning on many downstream tasks—including tasks specifically designed to be challenging for models that ignore word order. Our models perform surprisingly well according to some parametric syntactic probes, indicating possible deficiencies in how we test representations for syntactic information. Overall, our results show that purely distributional information largely explains the success of pre-training, and underscore the importance of curating challenging evaluation datasets that require deeper linguistic knowledge.</abstract>
      <url hash="3799ae01">2021.emnlp-main.230</url>
      <bibkey>sinha-etal-2021-masked</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.230</doi>
      <video href="2021.emnlp-main.230.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws">PAWS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="231">
      <title>What’s Hidden in a One-layer Randomly Weighted Transformer?</title>
      <author><first>Sheng</first><last>Shen</last></author>
      <author><first>Zhewei</first><last>Yao</last></author>
      <author><first>Douwe</first><last>Kiela</last></author>
      <author><first>Kurt</first><last>Keutzer</last></author>
      <author><first>Michael</first><last>Mahoney</last></author>
      <pages>2914–2921</pages>
      <abstract>We demonstrate that, hidden within one-layer randomly weighted neural networks, there exist subnetworks that can achieve impressive performance, without ever modifying the weight initializations, on machine translation tasks. To find subnetworks for one-layer randomly weighted neural networks, we apply different binary masks to the same weight matrix to generate different layers. Hidden within a one-layer randomly weighted Transformer, we find that subnetworks that can achieve 29.45/17.29 BLEU on IWSLT14/WMT14. Using a fixed pre-trained embedding layer, the previously found subnetworks are smaller than, but can match 98%/92% (34.14/25.24 BLEU) of the performance of, a trained Transformer<tex-math>_\text{small/base}</tex-math> on IWSLT14/WMT14. Furthermore, we demonstrate the effectiveness of larger and deeper transformers in this setting, as well as the impact of different initialization methods.</abstract>
      <url hash="3a1fa07a">2021.emnlp-main.231</url>
      <bibkey>shen-etal-2021-whats</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.231</doi>
      <video href="2021.emnlp-main.231.mp4"/>
      <pwccode url="https://github.com/sincerass/one_layer_lottery_ticket" additional="false">sincerass/one_layer_lottery_ticket</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="232">
      <title>Rethinking Denoised Auto-Encoding in Language Pre-Training</title>
      <author><first>Fuli</first><last>Luo</last></author>
      <author><first>Pengcheng</first><last>Yang</last></author>
      <author><first>Shicheng</first><last>Li</last></author>
      <author><first>Xuancheng</first><last>Ren</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <author><first>Songfang</first><last>Huang</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <pages>2922–2932</pages>
      <abstract>Pre-trained self-supervised models such as BERT have achieved striking success in learning sequence representations, especially for natural language processing. These models typically corrupt the given sequences with certain types of noise, such as masking, shuffling, or substitution, and then try to recover the original input. However, such pre-training approaches are prone to learning representations that are covariant with the noise, leading to the discrepancy between the pre-training and fine-tuning stage. To remedy this, we present ContrAstive Pre-Training (CAPT) to learn noise invariant sequence representations. The proposed CAPT encourages the consistency between representations of the original sequence and its corrupted version via unsupervised instance-wise training signals. In this way, it not only alleviates the pretrain-finetune discrepancy induced by the noise of pre-training, but also aids the pre-trained model in better capturing global semantics of the input via more effective sentence-level supervision. Different from most prior work that focuses on a particular modality, comprehensive empirical evidence on 11 natural language understanding and cross-modal tasks illustrates that CAPT is applicable for both language and vision-language tasks, and obtains surprisingly consistent improvement, including 0.6% absolute gain on GLUE benchmarks and 0.8% absolute increment on NLVR2.</abstract>
      <url hash="6dc6ac5b">2021.emnlp-main.232</url>
      <bibkey>luo-etal-2021-rethinking</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.232</doi>
      <video href="2021.emnlp-main.232.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gqa">GQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="233">
      <title>Lifelong Explainer for Lifelong Learners</title>
      <author><first>Xuelin</first><last>Situ</last></author>
      <author><first>Sameen</first><last>Maruf</last></author>
      <author><first>Ingrid</first><last>Zukerman</last></author>
      <author><first>Cecile</first><last>Paris</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <pages>2933–2940</pages>
      <abstract>Lifelong Learning (LL) black-box models are dynamic in that they keep learning from new tasks and constantly update their parameters. Owing to the need to utilize information from previously seen tasks, and capture commonalities in potentially diverse data, it is hard for automatic explanation methods to explain the outcomes of these models. In addition, existing explanation methods, e.g., LIME, which are computationally expensive when explaining a static black-box model, are even more inefficient in the LL setting. In this paper, we propose a novel Lifelong Explanation (LLE) approach that continuously trains a student explainer under the supervision of a teacher – an arbitrary explanation algorithm – on different tasks undertaken in LL. We also leverage the Experience Replay (ER) mechanism to prevent catastrophic forgetting in the student explainer. Our experiments comparing LLE to three baselines on text classification tasks show that LLE can enhance the stability of the explanations for all seen tasks and maintain the same level of faithfulness to the black-box model as the teacher, while being up to 10ˆ2 times faster at test time. Our ablation study shows that the ER mechanism in our LLE approach enhances the learning capabilities of the student explainer. Our code is available at https://github.com/situsnow/LLE.</abstract>
      <url hash="db337028">2021.emnlp-main.233</url>
      <bibkey>situ-etal-2021-lifelong</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.233</doi>
      <video href="2021.emnlp-main.233.mp4"/>
      <pwccode url="https://github.com/situsnow/lle" additional="false">situsnow/lle</pwccode>
    </paper>
    <paper id="234">
      <title>Linguistic Dependencies and Statistical Dependence</title>
      <author><first>Jacob Louis</first><last>Hoover</last></author>
      <author><first>Wenyu</first><last>Du</last></author>
      <author><first>Alessandro</first><last>Sordoni</last></author>
      <author><first>Timothy J.</first><last>O’Donnell</last></author>
      <pages>2941–2963</pages>
      <abstract>Are pairs of words that tend to occur together also likely to stand in a linguistic dependency? This empirical question is motivated by a long history of literature in cognitive science, psycholinguistics, and NLP. In this work we contribute an extensive analysis of the relationship between linguistic dependencies and statistical dependence between words. Improving on previous work, we introduce the use of large pretrained language models to compute contextualized estimates of the pointwise mutual information between words (CPMI). For multiple models and languages, we extract dependency trees which maximize CPMI, and compare to gold standard linguistic dependencies. Overall, we find that CPMI dependencies achieve an unlabelled undirected attachment score of at most <tex-math>\approx 0.5</tex-math>. While far above chance, and consistently above a non-contextualized PMI baseline, this score is generally comparable to a simple baseline formed by connecting adjacent words. We analyze which kinds of linguistic dependencies are best captured in CPMI dependencies, and also find marked differences between the estimates of the large pretrained language models, illustrating how their different training schemes affect the type of dependencies they capture.</abstract>
      <url hash="48121ef9">2021.emnlp-main.234</url>
      <bibkey>hoover-etal-2021-linguistic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.234</doi>
      <video href="2021.emnlp-main.234.mp4"/>
      <pwccode url="https://github.com/mcqll/cpmi-dependencies" additional="false">mcqll/cpmi-dependencies</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="235">
      <title>Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars</title>
      <author><first>Ryo</first><last>Yoshida</last></author>
      <author><first>Hiroshi</first><last>Noji</last></author>
      <author><first>Yohei</first><last>Oseki</last></author>
      <pages>2964–2973</pages>
      <abstract>In computational linguistics, it has been shown that hierarchical structures make language models (LMs) more human-like. However, the previous literature has been agnostic about a parsing strategy of the hierarchical models. In this paper, we investigated whether hierarchical structures make LMs more human-like, and if so, which parsing strategy is most cognitively plausible. In order to address this question, we evaluated three LMs against human reading times in Japanese with head-final left-branching structures: Long Short-Term Memory (LSTM) as a sequential model and Recurrent Neural Network Grammars (RNNGs) with top-down and left-corner parsing strategies as hierarchical models. Our computational modeling demonstrated that left-corner RNNGs outperformed top-down RNNGs and LSTM, suggesting that hierarchical and left-corner architectures are more cognitively plausible than top-down or sequential architectures. In addition, the relationships between the cognitive plausibility and (i) perplexity, (ii) parsing, and (iii) beam size will also be discussed.</abstract>
      <url hash="11d3baf4">2021.emnlp-main.235</url>
      <bibkey>yoshida-etal-2021-modeling</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.235</doi>
      <video href="2021.emnlp-main.235.mp4"/>
    </paper>
    <paper id="236">
      <title>A Simple and Effective Positional Encoding for Transformers</title>
      <author><first>Pu-Chin</first><last>Chen</last></author>
      <author><first>Henry</first><last>Tsai</last></author>
      <author><first>Srinadh</first><last>Bhojanapalli</last></author>
      <author><first>Hyung Won</first><last>Chung</last></author>
      <author><first>Yin-Wen</first><last>Chang</last></author>
      <author><first>Chun-Sung</first><last>Ferng</last></author>
      <pages>2974–2988</pages>
      <abstract>Transformer models are permutation equivariant. To supply the order and type information of the input tokens, position and segment embeddings are usually added to the input. Recent works proposed variations of positional encodings with relative position encodings achieving better performance. Our analysis shows that the gain actually comes from moving positional information to attention layer from the input. Motivated by this, we introduce Decoupled Positional Attention for Transformers (DIET), a simple yet effective mechanism to encode position and segment information into the Transformer models. The proposed method has faster training and inference time, while achieving competitive performance on GLUE, XTREME and WMT benchmarks. We further generalize our method to long-range transformers and show performance gain.</abstract>
      <url hash="3216324f">2021.emnlp-main.236</url>
      <bibkey>chen-etal-2021-simple</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.236</doi>
      <video href="2021.emnlp-main.236.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xtreme">XTREME</pwcdataset>
    </paper>
    <paper id="237">
      <title>Explore Better Relative Position Embeddings from Encoding Perspective for Transformer Models</title>
      <author><first>Anlin</first><last>Qu</last></author>
      <author><first>Jianwei</first><last>Niu</last></author>
      <author><first>Shasha</first><last>Mo</last></author>
      <pages>2989–2997</pages>
      <abstract>Relative position embedding (RPE) is a successful method to explicitly and efficaciously encode position information into Transformer models. In this paper, we investigate the potential problems in Shaw-RPE and XL-RPE, which are the most representative and prevalent RPEs, and propose two novel RPEs called Low-level Fine-grained High-level Coarse-grained (LFHC) RPE and Gaussian Cumulative Distribution Function (GCDF) RPE. LFHC-RPE is an improvement of Shaw-RPE, which enhances the perception ability at medium and long relative positions. GCDF-RPE utilizes the excellent properties of the Gaussian function to amend the prior encoding mechanism in XL-RPE. Experimental results on nine authoritative datasets demonstrate the effectiveness of our methods empirically. Furthermore, GCDF-RPE achieves the best overall performance among five different RPEs.</abstract>
      <url hash="9bc84909">2021.emnlp-main.237</url>
      <bibkey>qu-etal-2021-explore</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.237</doi>
      <video href="2021.emnlp-main.237.mp4"/>
      <pwccode url="https://github.com/menghuanlater/lfhc-gcdf-rpe" additional="false">menghuanlater/lfhc-gcdf-rpe</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cmrc">CMRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="238">
      <title>Adversarial Mixing Policy for Relaxing Locally Linear Constraints in Mixup</title>
      <author><first>Guang</first><last>Liu</last></author>
      <author><first>Yuzhao</first><last>Mao</last></author>
      <author><first>Huang</first><last>Hailong</last></author>
      <author><first>Gao</first><last>Weiguo</last></author>
      <author><first>Li</first><last>Xuan</last></author>
      <pages>2998–3008</pages>
      <abstract>Mixup is a recent regularizer for current deep classification networks. Through training a neural network on convex combinations of pairs of examples and their labels, it imposes locally linear constraints on the model’s input space. However, such strict linear constraints often lead to under-fitting which degrades the effects of regularization. Noticeably, this issue is getting more serious when the resource is extremely limited. To address these issues, we propose the Adversarial Mixing Policy (AMP), organized in a “min-max-rand” formulation, to relax the Locally Linear Constraints in Mixup. Specifically, AMP adds a small adversarial perturbation to the mixing coefficients rather than the examples. Thus, slight non-linearity is injected in-between the synthetic examples and synthetic labels. By training on these data, the deep networks are further regularized, and thus achieve a lower predictive error rate. Experiments on five text classification benchmarks and five backbone models have empirically shown that our methods reduce the error rate over Mixup variants in a significant margin (up to 31.3%), especially in low-resource conditions (up to 17.5%).</abstract>
      <url hash="d239d82a">2021.emnlp-main.238</url>
      <bibkey>liu-etal-2021-adversarial</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.238</doi>
      <video href="2021.emnlp-main.238.mp4"/>
      <pwccode url="https://github.com/pai-smallisallyourneed/mixup-amp" additional="false">pai-smallisallyourneed/mixup-amp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="239">
      <title>Is this the end of the gold standard? A straightforward reference-less grammatical error correction metric</title>
      <author><first>Md Asadul</first><last>Islam</last></author>
      <author><first>Enrico</first><last>Magnani</last></author>
      <pages>3009–3015</pages>
      <abstract>It is difficult to rank and evaluate the performance of grammatical error correction (GEC) systems, as a sentence can be rewritten in numerous correct ways. A number of GEC metrics have been used to evaluate proposed GEC systems; however, each system relies on either a comparison with one or more reference texts—in what is known as the gold standard for reference-based metrics—or a separate annotated dataset to fine-tune the reference-less metric. Reference-based systems have a low correlation with human judgement, cannot capture all the ways in which a sentence can be corrected, and require substantial work to develop a test dataset. We propose a reference-less GEC evaluation system that is strongly correlated with human judgement, solves the issues related to the use of a reference, and does not need another annotated dataset for fine-tuning. The proposed system relies solely on commonly available tools. Additionally, currently available reference-less metrics do not work properly when part of a sentence is repeated as opposed to reference-based metrics. In our proposed system, we look to address issues inherent in reference-less metrics and reference-based metrics.</abstract>
      <url hash="fc7b3e03">2021.emnlp-main.239</url>
      <bibkey>islam-magnani-2021-end</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.239</doi>
      <video href="2021.emnlp-main.239.mp4"/>
    </paper>
    <paper id="240">
      <title>Augmenting <fixed-case>BERT</fixed-case>-style Models with Predictive Coding to Improve Discourse-level Representations</title>
      <author><first>Vladimir</first><last>Araujo</last></author>
      <author><first>Andrés</first><last>Villa</last></author>
      <author><first>Marcelo</first><last>Mendoza</last></author>
      <author><first>Marie-Francine</first><last>Moens</last></author>
      <author><first>Alvaro</first><last>Soto</last></author>
      <pages>3016–3022</pages>
      <abstract>Current language models are usually trained using a self-supervised scheme, where the main focus is learning representations at the word or sentence level. However, there has been limited progress in generating useful discourse-level representations. In this work, we propose to use ideas from predictive coding theory to augment BERT-style language models with a mechanism that allows them to learn suitable discourse-level representations. As a result, our proposed approach is able to predict future sentences using explicit top-down connections that operate at the intermediate layers of the network. By experimenting with benchmarks designed to evaluate discourse-related knowledge using pre-trained sentence representations, we demonstrate that our approach improves performance in 6 out of 11 tasks by excelling in discourse relationship detection.</abstract>
      <url hash="8ad1c2d5">2021.emnlp-main.240</url>
      <bibkey>araujo-etal-2021-augmenting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.240</doi>
      <video href="2021.emnlp-main.240.mp4"/>
    </paper>
    <paper id="241">
      <title>Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning</title>
      <author><first>Linyang</first><last>Li</last></author>
      <author><first>Demin</first><last>Song</last></author>
      <author><first>Xiaonan</first><last>Li</last></author>
      <author><first>Jiehang</first><last>Zeng</last></author>
      <author><first>Ruotian</first><last>Ma</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <pages>3023–3032</pages>
      <abstract>
        <b>P</b>re-<b>T</b>rained <b>M</b>odel<b>s</b> have been widely applied and recently proved vulnerable under backdoor attacks: the released pre-trained weights can be maliciously poisoned with certain triggers. When the triggers are activated, even the fine-tuned model will predict pre-defined labels, causing a security threat. These backdoors generated by the poisoning methods can be erased by changing hyper-parameters during fine-tuning or detected by finding the triggers. In this paper, we propose a stronger weight-poisoning attack method that introduces a layerwise weight poisoning strategy to plant deeper backdoors; we also introduce a combinatorial trigger that cannot be easily detected. The experiments on text classification tasks show that previous defense methods cannot resist our weight-poisoning method, which indicates that our method can be widely applied and may provide hints for future model robustness studies.</abstract>
      <url hash="72226ab2">2021.emnlp-main.241</url>
      <bibkey>li-etal-2021-backdoor</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.241</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="242">
      <title><fixed-case>GAML</fixed-case>-<fixed-case>BERT</fixed-case>: Improving <fixed-case>BERT</fixed-case> Early Exiting by Gradient Aligned Mutual Learning</title>
      <author><first>Wei</first><last>Zhu</last></author>
      <author><first>Xiaoling</first><last>Wang</last></author>
      <author><first>Yuan</first><last>Ni</last></author>
      <author><first>Guotong</first><last>Xie</last></author>
      <pages>3033–3044</pages>
      <abstract>In this work, we propose a novel framework, Gradient Aligned Mutual Learning BERT (GAML-BERT), for improving the early exiting of BERT. GAML-BERT’s contributions are two-fold. We conduct a set of pilot experiments, which shows that mutual knowledge distillation between a shallow exit and a deep exit leads to better performances for both. From this observation, we use mutual learning to improve BERT’s early exiting performances, that is, we ask each exit of a multi-exit BERT to distill knowledge from each other. Second, we propose GA, a novel training method that aligns the gradients from knowledge distillation to cross-entropy losses. Extensive experiments are conducted on the GLUE benchmark, which shows that our GAML-BERT can significantly outperform the state-of-the-art (SOTA) BERT early exiting methods.</abstract>
      <url hash="057f2090">2021.emnlp-main.242</url>
      <bibkey>zhu-etal-2021-gaml</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.242</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="243">
      <title>The Power of Scale for Parameter-Efficient Prompt Tuning</title>
      <author><first>Brian</first><last>Lester</last></author>
      <author><first>Rami</first><last>Al-Rfou</last></author>
      <author><first>Noah</first><last>Constant</last></author>
      <pages>3045–3059</pages>
      <abstract>In this work, we explore “prompt tuning,” a simple yet effective mechanism for learning “soft prompts” to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3’s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method “closes the gap” and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed “prefix tuning” of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient “prompt ensembling.” We release code and model checkpoints to reproduce our experiments.</abstract>
      <url hash="b7481d5c">2021.emnlp-main.243</url>
      <bibkey>lester-etal-2021-power</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.243</doi>
      <video href="2021.emnlp-main.243.mp4"/>
      <pwccode url="https://github.com/google-research/prompt-tuning" additional="true">google-research/prompt-tuning</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/duorc">DuoRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multirc">MultiRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/record">ReCoRD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
    </paper>
    <paper id="244">
      <title>Scalable Font Reconstruction with Dual Latent Manifolds</title>
      <author><first>Nikita</first><last>Srivatsan</last></author>
      <author><first>Si</first><last>Wu</last></author>
      <author><first>Jonathan</first><last>Barron</last></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last></author>
      <pages>3060–3072</pages>
      <abstract>We propose a deep generative model that performs typography analysis and font reconstruction by learning disentangled manifolds of both font style and character shape. Our approach enables us to massively scale up the number of character types we can effectively model compared to previous methods. Specifically, we infer separate latent variables representing character and font via a pair of inference networks which take as input sets of glyphs that either all share a character type, or belong to the same font. This design allows our model to generalize to characters that were not observed during training time, an important task in light of the relative sparsity of most fonts. We also put forward a new loss, adapted from prior work that measures likelihood using an adaptive distribution in a projected space, resulting in more natural images without requiring a discriminator. We evaluate on the task of font reconstruction over various datasets representing character types of many languages, and compare favorably to modern style transfer systems according to both automatic and manually-evaluated metrics.</abstract>
      <url hash="436dac35">2021.emnlp-main.244</url>
      <bibkey>srivatsan-etal-2021-scalable</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.244</doi>
      <video href="2021.emnlp-main.244.mp4"/>
    </paper>
    <paper id="245">
      <title>Neuro-Symbolic Approaches for Text-Based Policy Learning</title>
      <author><first>Subhajit</first><last>Chaudhury</last></author>
      <author><first>Prithviraj</first><last>Sen</last></author>
      <author><first>Masaki</first><last>Ono</last></author>
      <author><first>Daiki</first><last>Kimura</last></author>
      <author><first>Michiaki</first><last>Tatsubori</last></author>
      <author><first>Asim</first><last>Munawar</last></author>
      <pages>3073–3078</pages>
      <abstract>Text-Based Games (TBGs) have emerged as important testbeds for reinforcement learning (RL) in the natural language domain. Previous methods using LSTM-based action policies are uninterpretable and often overfit the training games showing poor performance to unseen test games. We present SymboLic Action policy for Textual Environments (SLATE), that learns interpretable action policy rules from symbolic abstractions of textual observations for improved generalization. We outline a method for end-to-end differentiable symbolic rule learning and show that such symbolic policies outperform previous state-of-the-art methods in text-based RL for the coin collector environment from 5-10x fewer training games. Additionally, our method provides human-understandable policy rules that can be readily verified for their logical consistency and can be easily debugged.</abstract>
      <url hash="028d470d">2021.emnlp-main.245</url>
      <bibkey>chaudhury-etal-2021-neuro</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.245</doi>
      <video href="2021.emnlp-main.245.mp4"/>
      <pwccode url="https://github.com/subhajit1411/slate-text-based-rl" additional="false">subhajit1411/slate-text-based-rl</pwccode>
    </paper>
    <paper id="246">
      <title>Layer-wise Model Pruning based on Mutual Information</title>
      <author><first>Chun</first><last>Fan</last></author>
      <author><first>Jiwei</first><last>Li</last></author>
      <author><first>Tianwei</first><last>Zhang</last></author>
      <author><first>Xiang</first><last>Ao</last></author>
      <author><first>Fei</first><last>Wu</last></author>
      <author><first>Yuxian</first><last>Meng</last></author>
      <author><first>Xiaofei</first><last>Sun</last></author>
      <pages>3079–3090</pages>
      <abstract>Inspired by mutual information (MI) based feature selection in SVMs and logistic regression, in this paper, we propose MI-based layer-wise pruning: for each layer of a multi-layer neural network, neurons with higher values of MI with respect to preserved neurons in the upper layer are preserved. Starting from the top softmax layer, layer-wise pruning proceeds in a top-down fashion until reaching the bottom word embedding layer. The proposed pruning strategy offers merits over weight-based pruning techniques: (1) it avoids irregular memory access since representations and matrices can be squeezed into their smaller but dense counterparts, leading to greater speedup; (2) in a manner of top-down pruning, the proposed method operates from a more global perspective based on training signals in the top layer, and prunes each layer by propagating the effect of global signals through layers, leading to better performances at the same sparsity level. Extensive experiments show that at the same sparsity level, the proposed strategy offers both greater speedup and higher performances than weight-based pruning methods (e.g., magnitude pruning, movement pruning).</abstract>
      <url hash="8aea531e">2021.emnlp-main.246</url>
      <bibkey>fan-etal-2021-layer</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.246</doi>
      <video href="2021.emnlp-main.246.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="247">
      <title>Hierarchical Heterogeneous Graph Representation Learning for Short Text Classification</title>
      <author><first>Yaqing</first><last>Wang</last></author>
      <author><first>Song</first><last>Wang</last></author>
      <author><first>Quanming</first><last>Yao</last></author>
      <author><first>Dejing</first><last>Dou</last></author>
      <pages>3091–3101</pages>
      <abstract>Short text classification is a fundamental task in natural language processing. It is hard due to the lack of context information and labeled data in practice. In this paper, we propose a new method called SHINE, which is based on graph neural network (GNN), for short text classification. First, we model the short text dataset as a hierarchical heterogeneous graph consisting of word-level component graphs which introduce more semantic and syntactic information. Then, we dynamically learn a short document graph that facilitates effective label propagation among similar short texts. Thus, comparing with existing GNN-based methods, SHINE can better exploit interactions between nodes of the same types and capture similarities between short texts. Extensive experiments on various benchmark short text datasets show that SHINE consistently outperforms state-of-the-art methods, especially with fewer labels.</abstract>
      <url hash="9ac55a8c">2021.emnlp-main.247</url>
      <bibkey>wang-etal-2021-hierarchical</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.247</doi>
      <video href="2021.emnlp-main.247.mp4"/>
      <pwccode url="https://github.com/tata1661/shine-emnlp21" additional="false">tata1661/shine-emnlp21</pwccode>
    </paper>
    <paper id="248">
      <title><tex-math>k</tex-math><fixed-case>F</fixed-case>olden: <tex-math>k</tex-math>-Fold Ensemble for Out-Of-Distribution Detection</title>
      <author><first>Xiaoya</first><last>Li</last></author>
      <author><first>Jiwei</first><last>Li</last></author>
      <author><first>Xiaofei</first><last>Sun</last></author>
      <author><first>Chun</first><last>Fan</last></author>
      <author><first>Tianwei</first><last>Zhang</last></author>
      <author><first>Fei</first><last>Wu</last></author>
      <author><first>Yuxian</first><last>Meng</last></author>
      <author><first>Jun</first><last>Zhang</last></author>
      <pages>3102–3115</pages>
      <abstract>Out-of-Distribution (OOD) detection is an important problem in natural language processing (NLP). In this work, we propose a simple yet effective framework <tex-math>k</tex-math>Folden, which mimics the behaviors of OOD detection during training without the use of any external data. For a task with <tex-math>k</tex-math> training labels, <tex-math>k</tex-math>Folden induces <tex-math>k</tex-math> sub-models, each of which is trained on a subset with <tex-math>k-1</tex-math> categories with the left category masked unknown to the sub-model. Exposing an unknown label to the sub-model during training, the model is encouraged to learn to equally attribute the probability to the seen <tex-math>k-1</tex-math> labels for the unknown label, enabling this framework to simultaneously resolve in- and out-distribution examples in a natural way via OOD simulations. Taking text classification as an archetype, we develop benchmarks for OOD detection using existing text classification datasets. By conducting comprehensive comparisons and analyses on the developed benchmarks, we demonstrate the superiority of <tex-math>k</tex-math>Folden against current methods in terms of improving OOD detection performances while maintaining improved in-domain classification accuracy.</abstract>
      <url hash="64045f26">2021.emnlp-main.248</url>
      <bibkey>li-etal-2021-kfolden</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.248</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reuters-21578">Reuters-21578</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/yahoo-answers">Yahoo! Answers</pwcdataset>
    </paper>
    <paper id="249">
      <title>Frustratingly Simple Pretraining Alternatives to Masked Language Modeling</title>
      <author><first>Atsuki</first><last>Yamaguchi</last></author>
      <author><first>George</first><last>Chrysostomou</last></author>
      <author><first>Katerina</first><last>Margatina</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <pages>3116–3125</pages>
      <abstract>Masked language modeling (MLM), a self-supervised pretraining objective, is widely used in natural language processing for learning text representations. MLM trains a model to predict a random sample of input tokens that have been replaced by a [MASK] placeholder in a multi-class setting over the entire vocabulary. When pretraining, it is common to use alongside MLM other auxiliary objectives on the token or sequence level to improve downstream performance (e.g. next sentence prediction). However, no previous work so far has attempted in examining whether other simpler linguistically intuitive or not objectives can be used standalone as main pretraining objectives. In this paper, we explore five simple pretraining objectives based on token-level classification tasks as replacements of MLM. Empirical results on GLUE and SQUAD show that our proposed methods achieve comparable or better performance to MLM using a BERT-BASE architecture. We further validate our methods using smaller models, showing that pretraining a model with 41% of the BERT-BASE’s parameters, BERT-MEDIUM results in only a 1% drop in GLUE scores with our best objective.</abstract>
      <url hash="113f5d0c">2021.emnlp-main.249</url>
      <bibkey>yamaguchi-etal-2021-frustratingly</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.249</doi>
      <video href="2021.emnlp-main.249.mp4"/>
      <pwccode url="https://github.com/gucci-j/light-transformer-emnlp2021" additional="false">gucci-j/light-transformer-emnlp2021</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="250">
      <title><fixed-case>HRKD</fixed-case>: Hierarchical Relational Knowledge Distillation for Cross-domain Language Model Compression</title>
      <author><first>Chenhe</first><last>Dong</last></author>
      <author><first>Yaliang</first><last>Li</last></author>
      <author><first>Ying</first><last>Shen</last></author>
      <author><first>Minghui</first><last>Qiu</last></author>
      <pages>3126–3136</pages>
      <abstract>On many natural language processing tasks, large pre-trained language models (PLMs) have shown overwhelming performances compared with traditional neural network methods. Nevertheless, their huge model size and low inference speed have hindered the deployment on resource-limited devices in practice. In this paper, we target to compress PLMs with knowledge distillation, and propose a hierarchical relational knowledge distillation (HRKD) method to capture both hierarchical and domain relational information. Specifically, to enhance the model capability and transferability, we leverage the idea of meta-learning and set up domain-relational graphs to capture the relational information across different domains. And to dynamically select the most representative prototypes for each domain, we propose a hierarchical compare-aggregate mechanism to capture hierarchical relationships. Extensive experiments on public multi-domain datasets demonstrate the superior performance of our HRKD method as well as its strong few-shot learning ability. For reproducibility, we release the code at <url>https://github.com/cheneydon/hrkd</url>.</abstract>
      <url hash="4c387f4d">2021.emnlp-main.250</url>
      <bibkey>dong-etal-2021-hrkd</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.250</doi>
      <video href="2021.emnlp-main.250.mp4"/>
      <pwccode url="https://github.com/cheneydon/hrkd" additional="false">cheneydon/hrkd</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="251">
      <title>Searching for an Effective Defender: Benchmarking Defense against Adversarial Word Substitution</title>
      <author><first>Zongyi</first><last>Li</last></author>
      <author><first>Jianhan</first><last>Xu</last></author>
      <author><first>Jiehang</first><last>Zeng</last></author>
      <author><first>Linyang</first><last>Li</last></author>
      <author><first>Xiaoqing</first><last>Zheng</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Cho-Jui</first><last>Hsieh</last></author>
      <pages>3137–3147</pages>
      <abstract>Recent studies have shown that deep neural network-based models are vulnerable to intentionally crafted adversarial examples, and various methods have been proposed to defend against adversarial word-substitution attacks for neural NLP models. However, there is a lack of systematic study on comparing different defense approaches under the same attacking setting. In this paper, we seek to fill the gap of systematic studies through comprehensive researches on understanding the behavior of neural text classifiers trained by various defense methods under representative adversarial attacks. In addition, we propose an effective method to further improve the robustness of neural text classifiers against such attacks, and achieved the highest accuracy on both clean and adversarial examples on AGNEWS and IMDB datasets by a significant margin. We hope this study could provide useful clues for future research on text adversarial defense. Codes are available at https://github.com/RockyLzy/TextDefender.</abstract>
      <url hash="54279453">2021.emnlp-main.251</url>
      <bibkey>li-etal-2021-searching</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.251</doi>
      <video href="2021.emnlp-main.251.mp4"/>
      <pwccode url="https://github.com/rockylzy/textdefender" additional="false">rockylzy/textdefender</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="252">
      <title>Re-embedding Difficult Samples via Mutual Information Constrained Semantically Oversampling for Imbalanced Text Classification</title>
      <author><first>Jiachen</first><last>Tian</last></author>
      <author><first>Shizhan</first><last>Chen</last></author>
      <author><first>Xiaowang</first><last>Zhang</last></author>
      <author><first>Zhiyong</first><last>Feng</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <author><first>Shaojuan</first><last>Wu</last></author>
      <author><first>Chunliu</first><last>Dou</last></author>
      <pages>3148–3161</pages>
      <abstract>Difficult samples of the minority class in imbalanced text classification are usually hard to be classified as they are embedded into an overlapping semantic region with the majority class. In this paper, we propose a Mutual Information constrained Semantically Oversampling framework (MISO) that can generate anchor instances to help the backbone network determine the re-embedding position of a non-overlapping representation for each difficult sample. MISO consists of (1) a semantic fusion module that learns entangled semantics among difficult and majority samples with an adaptive multi-head attention mechanism, (2) a mutual information loss that forces our model to learn new representations of entangled semantics in the non-overlapping region of the minority class, and (3) a coupled adversarial encoder-decoder that fine-tunes disentangled semantic representations to remain their correlations with the minority class, and then using these disentangled semantic representations to generate anchor instances for each difficult sample. Experiments on a variety of imbalanced text classification tasks demonstrate that anchor instances help classifiers achieve significant improvements over strong baselines.</abstract>
      <url hash="6ee8a699">2021.emnlp-main.252</url>
      <bibkey>tian-etal-2021-embedding</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.252</doi>
      <video href="2021.emnlp-main.252.mp4"/>
    </paper>
    <paper id="253">
      <title>Beyond Text: Incorporating Metadata and Label Structure for Multi-Label Document Classification using Heterogeneous Graphs</title>
      <author><first>Chenchen</first><last>Ye</last></author>
      <author><first>Linhai</first><last>Zhang</last></author>
      <author><first>Yulan</first><last>He</last></author>
      <author><first>Deyu</first><last>Zhou</last></author>
      <author><first>Jie</first><last>Wu</last></author>
      <pages>3162–3171</pages>
      <abstract>Multi-label document classification, associating one document instance with a set of relevant labels, is attracting more and more research attention. Existing methods explore the incorporation of information beyond text, such as document metadata or label structure. These approaches however either simply utilize the semantic information of metadata or employ the predefined parent-child label hierarchy, ignoring the heterogeneous graphical structures of metadata and labels, which we believe are crucial for accurate multi-label document classification. Therefore, in this paper, we propose a novel neural network based approach for multi-label document classification, in which two heterogeneous graphs are constructed and learned using heterogeneous graph transformers. One is metadata heterogeneous graph, which models various types of metadata and their topological relations. The other is label heterogeneous graph, which is constructed based on both the labels’ hierarchy and their statistical dependencies. Experimental results on two benchmark datasets show the proposed approach outperforms several state-of-the-art baselines.</abstract>
      <url hash="3205b06e">2021.emnlp-main.253</url>
      <bibkey>ye-etal-2021-beyond</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.253</doi>
      <video href="2021.emnlp-main.253.mp4"/>
    </paper>
    <paper id="254">
      <title>Natural Language Processing Meets Quantum Physics: A Survey and Categorization</title>
      <author><first>Sixuan</first><last>Wu</last></author>
      <author><first>Jian</first><last>Li</last></author>
      <author><first>Peng</first><last>Zhang</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>3172–3182</pages>
      <abstract>Recent research has investigated quantum NLP, designing algorithms that process natural language in quantum computers, and also quantum-inspired algorithms that improve NLP performance on classical computers. In this survey, we review representative methods at the intersection of NLP and quantum physics in the past ten years, categorizing them according to the use of quantum theory, the linguistic targets that are modeled, and the downstream application. The literature review ends with a discussion on the key factors to the success that has been achieved by existing work, as well as challenges ahead, with the goal of better understanding the promises and further directions.</abstract>
      <url hash="77b35949">2021.emnlp-main.254</url>
      <bibkey>wu-etal-2021-natural</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.254</doi>
      <video href="2021.emnlp-main.254.mp4"/>
    </paper>
    <paper id="255">
      <title><fixed-case>M</fixed-case>eta<fixed-case>TS</fixed-case>: Meta Teacher-Student Network for Multilingual Sequence Labeling with Minimal Supervision</title>
      <author><first>Zheng</first><last>Li</last></author>
      <author><first>Danqing</first><last>Zhang</last></author>
      <author><first>Tianyu</first><last>Cao</last></author>
      <author><first>Ying</first><last>Wei</last></author>
      <author><first>Yiwei</first><last>Song</last></author>
      <author><first>Bing</first><last>Yin</last></author>
      <pages>3183–3196</pages>
      <abstract>Sequence labeling aims to predict a fine-grained sequence of labels for the text. However, such formulation hinders the effectiveness of supervised methods due to the lack of token-level annotated data. This is exacerbated when we meet a diverse range of languages. In this work, we explore multilingual sequence labeling with minimal supervision using a single unified model for multiple languages. Specifically, we propose a Meta Teacher-Student (MetaTS) Network, a novel meta learning method to alleviate data scarcity by leveraging large multilingual unlabeled data. Prior teacher-student frameworks of self-training rely on rigid teaching strategies, which may hardly produce high-quality pseudo-labels for consecutive and interdependent tokens. On the contrary, MetaTS allows the teacher to dynamically adapt its pseudo-annotation strategies by the student’s feedback on the generated pseudo-labeled data of each language and thus mitigate error propagation from noisy pseudo-labels. Extensive experiments on both public and real-world multilingual sequence labeling datasets empirically demonstrate the effectiveness of MetaTS.</abstract>
      <url hash="212fd56e">2021.emnlp-main.255</url>
      <bibkey>li-etal-2021-metats</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.255</doi>
      <video href="2021.emnlp-main.255.mp4"/>
    </paper>
    <paper id="256">
      <title>Neural Machine Translation with Heterogeneous Topic Knowledge Embeddings</title>
      <author><first>Weixuan</first><last>Wang</last></author>
      <author><first>Wei</first><last>Peng</last></author>
      <author><first>Meng</first><last>Zhang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>3197–3202</pages>
      <abstract>Neural Machine Translation (NMT) has shown a strong ability to utilize local context to disambiguate the meaning of words. However, it remains a challenge for NMT to leverage broader context information like topics. In this paper, we propose heterogeneous ways of embedding topic information at the sentence level into an NMT model to improve translation performance. Specifically, the topic information can be incorporated as pre-encoder topic embedding, post-encoder topic embedding, and decoder topic embedding to increase the likelihood of selecting target words from the same topic of the source sentence. Experimental results show that NMT models with the proposed topic knowledge embedding outperform the baselines on the English -&gt; German and English -&gt; French translation tasks.</abstract>
      <url hash="5b6c5354">2021.emnlp-main.256</url>
      <bibkey>wang-etal-2021-neural</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.256</doi>
      <video href="2021.emnlp-main.256.mp4"/>
      <pwccode url="https://github.com/Vicky-Wil/topic-NMT" additional="false">Vicky-Wil/topic-NMT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="257">
      <title>Allocating Large Vocabulary Capacity for Cross-Lingual Language Model Pre-Training</title>
      <author><first>Bo</first><last>Zheng</last></author>
      <author><first>Li</first><last>Dong</last></author>
      <author><first>Shaohan</first><last>Huang</last></author>
      <author><first>Saksham</first><last>Singhal</last></author>
      <author><first>Wanxiang</first><last>Che</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <author><first>Xia</first><last>Song</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <pages>3203–3215</pages>
      <abstract>Compared to monolingual models, cross-lingual models usually require a more expressive vocabulary to represent all languages adequately. We find that many languages are under-represented in recent cross-lingual language models due to the limited vocabulary capacity. To this end, we propose an algorithm VoCap to determine the desired vocabulary capacity of each language. However, increasing the vocabulary size significantly slows down the pre-training speed. In order to address the issues, we propose k-NN-based target sampling to accelerate the expensive softmax. Our experiments show that the multilingual vocabulary learned with VoCap benefits cross-lingual language model pre-training. Moreover, k-NN-based target sampling mitigates the side-effects of increasing the vocabulary size while achieving comparable performance and faster pre-training speed. The code and the pretrained multilingual vocabularies are available at https://github.com/bozheng-hit/VoCapXLM.</abstract>
      <url hash="74d3e7fe">2021.emnlp-main.257</url>
      <bibkey>zheng-etal-2021-allocating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.257</doi>
      <video href="2021.emnlp-main.257.mp4"/>
      <pwccode url="https://github.com/bozheng-hit/vocapxlm" additional="false">bozheng-hit/vocapxlm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws-x">PAWS-X</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDi QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="258">
      <title>Recurrent Attention for Neural Machine Translation</title>
      <author><first>Jiali</first><last>Zeng</last></author>
      <author><first>Shuangzhi</first><last>Wu</last></author>
      <author><first>Yongjing</first><last>Yin</last></author>
      <author><first>Yufan</first><last>Jiang</last></author>
      <author><first>Mu</first><last>Li</last></author>
      <pages>3216–3225</pages>
      <abstract>Recent research questions the importance of the dot-product self-attention in Transformer models and shows that most attention heads learn simple positional patterns. In this paper, we push further in this research line and propose a novel substitute mechanism for self-attention: Recurrent AtteNtion (RAN) . RAN directly learns attention weights without any token-to-token interaction and further improves their capacity by layer-to-layer interaction. Across an extensive set of experiments on 10 machine translation tasks, we find that RAN models are competitive and outperform their Transformer counterpart in certain scenarios, with fewer parameters and inference time. Particularly, when apply RAN to the decoder of Transformer, there brings consistent improvements by about +0.5 BLEU on 6 translation tasks and +1.0 BLEU on Turkish-English translation task. In addition, we conduct extensive analysis on the attention weights of RAN to confirm their reasonableness. Our RAN is a promising alternative to build more effective and efficient NMT models.</abstract>
      <url hash="698d050a">2021.emnlp-main.258</url>
      <bibkey>zeng-etal-2021-recurrent</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.258</doi>
      <video href="2021.emnlp-main.258.mp4"/>
      <pwccode url="https://github.com/lemon0830/ran" additional="false">lemon0830/ran</pwccode>
    </paper>
    <paper id="259">
      <title>Learning from Multiple Noisy Augmented Data Sets for Better Cross-Lingual Spoken Language Understanding</title>
      <author><first>Yingmei</first><last>Guo</last></author>
      <author><first>Linjun</first><last>Shou</last></author>
      <author><first>Jian</first><last>Pei</last></author>
      <author><first>Ming</first><last>Gong</last></author>
      <author><first>Mingxing</first><last>Xu</last></author>
      <author><first>Zhiyong</first><last>Wu</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <pages>3226–3237</pages>
      <abstract>Lack of training data presents a grand challenge to scaling out spoken language understanding (SLU) to low-resource languages. Although various data augmentation approaches have been proposed to synthesize training data in low-resource target languages, the augmented data sets are often noisy, and thus impede the performance of SLU models. In this paper we focus on mitigating noise in augmented data. We develop a denoising training approach. Multiple models are trained with data produced by various augmented methods. Those models provide supervision signals to each other. The experimental results show that our method outperforms the existing state of the art by 3.05 and 4.24 percentage points on two benchmark datasets, respectively. The code will be made open sourced on github.</abstract>
      <url hash="0e80477e">2021.emnlp-main.259</url>
      <bibkey>guo-etal-2021-learning</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.259</doi>
      <video href="2021.emnlp-main.259.mp4"/>
    </paper>
    <paper id="260">
      <title>Enlivening Redundant Heads in Multi-head Self-attention for Machine Translation</title>
      <author><first>Tianfu</first><last>Zhang</last></author>
      <author><first>Heyan</first><last>Huang</last></author>
      <author><first>Chong</first><last>Feng</last></author>
      <author><first>Longbing</first><last>Cao</last></author>
      <pages>3238–3248</pages>
      <abstract>Multi-head self-attention recently attracts enormous interest owing to its specialized functions, significant parallelizable computation, and flexible extensibility. However, very recent empirical studies show that some self-attention heads make little contribution and can be pruned as redundant heads. This work takes a novel perspective of identifying and then vitalizing redundant heads. We propose a redundant head enlivening (RHE) method to precisely identify redundant heads, and then vitalize their potential by learning syntactic relations and prior knowledge in the text without sacrificing the roles of important heads. Two novel syntax-enhanced attention (SEA) mechanisms: a dependency mask bias and a relative local-phrasal position bias, are introduced to revise self-attention distributions for syntactic enhancement in machine translation. The importance of individual heads is dynamically evaluated during the redundant heads identification, on which we apply SEA to vitalize redundant heads while maintaining the strength of important heads. Experimental results on widely adopted WMT14 and WMT16 English to German and English to Czech language machine translation validate the RHE effectiveness.</abstract>
      <url hash="69019f53">2021.emnlp-main.260</url>
      <bibkey>zhang-etal-2021-enlivening</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.260</doi>
      <video href="2021.emnlp-main.260.mp4"/>
    </paper>
    <paper id="261">
      <title>Unsupervised Neural Machine Translation with Universal Grammar</title>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <pages>3249–3264</pages>
      <abstract>Machine translation usually relies on parallel corpora to provide parallel signals for training. The advent of unsupervised machine translation has brought machine translation away from this reliance, though performance still lags behind traditional supervised machine translation. In unsupervised machine translation, the model seeks symmetric language similarities as a source of weak parallel signal to achieve translation. Chomsky’s Universal Grammar theory postulates that grammar is an innate form of knowledge to humans and is governed by universal principles and constraints. Therefore, in this paper, we seek to leverage such shared grammar clues to provide more explicit language parallel signals to enhance the training of unsupervised machine translation models. Through experiments on multiple typical language pairs, we demonstrate the effectiveness of our proposed approaches.</abstract>
      <url hash="2b55a15a">2021.emnlp-main.261</url>
      <bibkey>li-etal-2021-unsupervised-neural</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.261</doi>
      <video href="2021.emnlp-main.261.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="262">
      <title>Encouraging Lexical Translation Consistency for Document-Level Neural Machine Translation</title>
      <author><first>Xinglin</first><last>Lyu</last></author>
      <author><first>Junhui</first><last>Li</last></author>
      <author><first>Zhengxian</first><last>Gong</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>3265–3277</pages>
      <abstract>Recently a number of approaches have been proposed to improve translation performance for document-level neural machine translation (NMT). However, few are focusing on the subject of lexical translation consistency. In this paper we apply “one translation per discourse” in NMT, and aim to encourage lexical translation consistency for document-level NMT. This is done by first obtaining a word link for each source word in a document, which tells the positions where the source word appears. Then we encourage the translation of those words within a link to be consistent in two ways. On the one hand, when encoding sentences within a document we properly share context information of those words. On the other hand, we propose an auxiliary loss function to better constrain that their translation should be consistent. Experimental results on Chinese↔English and English→French translation tasks show that our approach not only achieves state-of-the-art performance in BLEU scores, but also greatly improves lexical consistency in translation.</abstract>
      <url hash="bac39b58">2021.emnlp-main.262</url>
      <bibkey>lyu-etal-2021-encouraging</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.262</doi>
      <video href="2021.emnlp-main.262.mp4"/>
    </paper>
    <paper id="263">
      <title>Improving Neural Machine Translation by Bidirectional Training</title>
      <author><first>Liang</first><last>Ding</last></author>
      <author><first>Di</first><last>Wu</last></author>
      <author><first>Dacheng</first><last>Tao</last></author>
      <pages>3278–3284</pages>
      <abstract>We present a simple and effective pretraining strategy – bidirectional training (BiT) for neural machine translation. Specifically, we bidirectionally update the model parameters at the early stage and then tune the model normally. To achieve bidirectional updating, we simply reconstruct the training samples from “src<tex-math>\rightarrow</tex-math>tgt” to “src+tgt<tex-math>\rightarrow</tex-math>tgt+src” without any complicated model modifications. Notably, our approach does not increase any parameters or training steps, requiring the parallel data merely. Experimental results show that BiT pushes the SOTA neural machine translation performance across 15 translation tasks on 8 language pairs (data sizes range from 160K to 38M) significantly higher. Encouragingly, our proposed model can complement existing data manipulation strategies, i.e. back translation, data distillation, and data diversification. Extensive analyses show that our approach functions as a novel bilingual code-switcher, obtaining better bilingual alignment.</abstract>
      <url hash="a4fa3f94">2021.emnlp-main.263</url>
      <bibkey>ding-etal-2021-improving</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.263</doi>
    </paper>
    <paper id="264">
      <title>Scheduled Sampling Based on Decoding Steps for Neural Machine Translation</title>
      <author><first>Yijin</first><last>Liu</last></author>
      <author><first>Fandong</first><last>Meng</last></author>
      <author><first>Yufeng</first><last>Chen</last></author>
      <author><first>Jinan</first><last>Xu</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>3285–3296</pages>
      <abstract>Scheduled sampling is widely used to mitigate the exposure bias problem for neural machine translation. Its core motivation is to simulate the inference scene during training by replacing ground-truth tokens with predicted tokens, thus bridging the gap between training and inference. However, vanilla scheduled sampling is merely based on training steps and equally treats all decoding steps. Namely, it simulates an inference scene with uniform error rates, which disobeys the real inference scene, where larger decoding steps usually have higher error rates due to error accumulations. To alleviate the above discrepancy, we propose scheduled sampling methods based on decoding steps, increasing the selection chance of predicted tokens with the growth of decoding steps. Consequently, we can more realistically simulate the inference scene during training, thus better bridging the gap between training and inference. Moreover, we investigate scheduled sampling based on both training steps and decoding steps for further improvements. Experimentally, our approaches significantly outperform the Transformer baseline and vanilla scheduled sampling on three large-scale WMT tasks. Additionally, our approaches also generalize well to the text summarization task on two popular benchmarks.</abstract>
      <url hash="36d6002e">2021.emnlp-main.264</url>
      <bibkey>liu-etal-2021-scheduled-sampling</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.264</doi>
      <video href="2021.emnlp-main.264.mp4"/>
      <pwccode url="https://github.com/adaxry/ss_on_decoding_steps" additional="false">adaxry/ss_on_decoding_steps</pwccode>
    </paper>
    <paper id="265">
      <title>Learning to Rewrite for Non-Autoregressive Neural Machine Translation</title>
      <author><first>Xinwei</first><last>Geng</last></author>
      <author><first>Xiaocheng</first><last>Feng</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <pages>3297–3308</pages>
      <abstract>Non-autoregressive neural machine translation, which decomposes the dependence on previous target tokens from the inputs of the decoder, has achieved impressive inference speedup but at the cost of inferior accuracy. Previous works employ iterative decoding to improve the translation by applying multiple refinement iterations. However, a serious drawback is that these approaches expose the serious weakness in recognizing the erroneous translation pieces. In this paper, we propose an architecture named RewriteNAT to explicitly learn to rewrite the erroneous translation pieces. Specifically, RewriteNAT utilizes a locator module to locate the erroneous ones, which are then revised into the correct ones by a revisor module. Towards keeping the consistency of data distribution with iterative decoding, an iterative training strategy is employed to further improve the capacity of rewriting. Extensive experiments conducted on several widely-used benchmarks show that RewriteNAT can achieve better performance while significantly reducing decoding time, compared with previous iterative decoding strategies. In particular, RewriteNAT can obtain competitive results with autoregressive translation on WMT14 En-De, En-Fr and WMT16 Ro-En translation benchmarks.</abstract>
      <url hash="398efbbb">2021.emnlp-main.265</url>
      <bibkey>geng-etal-2021-learning</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.265</doi>
      <video href="2021.emnlp-main.265.mp4"/>
      <pwccode url="https://github.com/xwgeng/rewritenat" additional="false">xwgeng/rewritenat</pwccode>
    </paper>
    <paper id="266">
      <title><fixed-case>SHAPE</fixed-case>: <fixed-case>S</fixed-case>hifted Absolute Position Embedding for Transformers</title>
      <author><first>Shun</first><last>Kiyono</last></author>
      <author><first>Sosuke</first><last>Kobayashi</last></author>
      <author><first>Jun</first><last>Suzuki</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>3309–3321</pages>
      <abstract>Position representation is crucial for building position-aware representations in Transformers. Existing position representations suffer from a lack of generalization to test data with unseen lengths or high computational cost. We investigate shifted absolute position embedding (SHAPE) to address both issues. The basic idea of SHAPE is to achieve shift invariance, which is a key property of recent successful position representations, by randomly shifting absolute positions during training. We demonstrate that SHAPE is empirically comparable to its counterpart while being simpler and faster.</abstract>
      <url hash="08df10e5">2021.emnlp-main.266</url>
      <bibkey>kiyono-etal-2021-shape</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.266</doi>
      <video href="2021.emnlp-main.266.mp4"/>
    </paper>
    <paper id="267">
      <title>Self-Supervised Quality Estimation for Machine Translation</title>
      <author><first>Yuanhang</first><last>Zheng</last></author>
      <author><first>Zhixing</first><last>Tan</last></author>
      <author><first>Meng</first><last>Zhang</last></author>
      <author><first>Mieradilijiang</first><last>Maimaiti</last></author>
      <author><first>Huanbo</first><last>Luan</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <author id="yang-liu-ict"><first>Yang</first><last>Liu</last></author>
      <pages>3322–3334</pages>
      <abstract>Quality estimation (QE) of machine translation (MT) aims to evaluate the quality of machine-translated sentences without references and is important in practical applications of MT. Training QE models require massive parallel data with hand-crafted quality annotations, which are time-consuming and labor-intensive to obtain. To address the issue of the absence of annotated training data, previous studies attempt to develop unsupervised QE methods. However, very few of them can be applied to both sentence- and word-level QE tasks, and they may suffer from noises in the synthetic data. To reduce the negative impact of noises, we propose a self-supervised method for both sentence- and word-level QE, which performs quality estimation by recovering the masked target words. Experimental results show that our method outperforms previous unsupervised methods on several QE tasks in different language pairs and domains.</abstract>
      <url hash="b203d7cb">2021.emnlp-main.267</url>
      <bibkey>zheng-etal-2021-self</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.267</doi>
      <video href="2021.emnlp-main.267.mp4"/>
    </paper>
    <paper id="268">
      <title>Generalised Unsupervised Domain Adaptation of Neural Machine Translation with Cross-Lingual Data Selection</title>
      <author><first>Thuy-Trang</first><last>Vu</last></author>
      <author><first>Xuanli</first><last>He</last></author>
      <author><first>Dinh</first><last>Phung</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <pages>3335–3346</pages>
      <abstract>This paper considers the unsupervised domain adaptation problem for neural machine translation (NMT), where we assume the access to only monolingual text in either the source or target language in the new domain. We propose a cross-lingual data selection method to extract in-domain sentences in the missing language side from a large generic monolingual corpus. Our proposed method trains an adaptive layer on top of multilingual BERT by contrastive learning to align the representation between the source and target language. This then enables the transferability of the domain classifier between the languages in a zero-shot manner. Once the in-domain data is detected by the classifier, the NMT model is then adapted to the new domain by jointly learning translation and domain discrimination tasks. We evaluate our cross-lingual data selection method on NMT across five diverse domains in three language pairs, as well as a real-world scenario of translation for COVID-19. The results show that our proposed method outperforms other selection baselines up to +1.5 BLEU score.</abstract>
      <url hash="c461d256">2021.emnlp-main.268</url>
      <bibkey>vu-etal-2021-generalised</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.268</doi>
      <video href="2021.emnlp-main.268.mp4"/>
      <pwccode url="https://github.com/trangvu/guda" additional="false">trangvu/guda</pwccode>
    </paper>
    <paper id="269">
      <title><fixed-case>STANKER</fixed-case>: Stacking Network based on Level-grained Attention-masked <fixed-case>BERT</fixed-case> for Rumor Detection on Social Media</title>
      <author><first>Dongning</first><last>Rao</last></author>
      <author><first>Xin</first><last>Miao</last></author>
      <author><first>Zhihua</first><last>Jiang</last></author>
      <author><first>Ran</first><last>Li</last></author>
      <pages>3347–3363</pages>
      <abstract>Rumor detection on social media puts pre-trained language models (LMs), such as BERT, and auxiliary features, such as comments, into use. However, on the one hand, rumor detection datasets in Chinese companies with comments are rare; on the other hand, intensive interaction of attention on Transformer-based models like BERT may hinder performance improvement. To alleviate these problems, we build a new Chinese microblog dataset named Weibo20 by collecting posts and associated comments from Sina Weibo and propose a new ensemble named STANKER (Stacking neTwork bAsed-on atteNtion-masKed BERT). STANKER adopts two level-grained attention-masked BERT (LGAM-BERT) models as base encoders. Unlike the original BERT, our new LGAM-BERT model takes comments as important auxiliary features and masks co-attention between posts and comments on lower-layers. Experiments on Weibo20 and three existing social media datasets showed that STANKER outperformed all compared models, especially beating the old state-of-the-art on Weibo dataset.</abstract>
      <url hash="aec4e876">2021.emnlp-main.269</url>
      <attachment type="Software" hash="91e89325">2021.emnlp-main.269.Software.zip</attachment>
      <bibkey>rao-etal-2021-stanker</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.269</doi>
      <video href="2021.emnlp-main.269.mp4"/>
      <pwccode url="https://github.com/fip-lab/stanker" additional="false">fip-lab/stanker</pwccode>
    </paper>
    <paper id="270">
      <title><fixed-case>A</fixed-case>ctive<fixed-case>EA</fixed-case>: Active Learning for Neural Entity Alignment</title>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Harrisen</first><last>Scells</last></author>
      <author><first>Guido</first><last>Zuccon</last></author>
      <author><first>Wen</first><last>Hua</last></author>
      <author><first>Genghong</first><last>Zhao</last></author>
      <pages>3364–3374</pages>
      <abstract>Entity Alignment (EA) aims to match equivalent entities across different Knowledge Graphs (KGs) and is an essential step of KG fusion. Current mainstream methods – neural EA models – rely on training with seed alignment, i.e., a set of pre-aligned entity pairs which are very costly to annotate. In this paper, we devise a novel Active Learning (AL) framework for neural EA, aiming to create highly informative seed alignment to obtain more effective EA models with less annotation cost. Our framework tackles two main challenges encountered when applying AL to EA: (1) How to exploit dependencies between entities within the AL strategy. Most AL strategies assume that the data instances to sample are independent and identically distributed. However, entities in KGs are related. To address this challenge, we propose a structure-aware uncertainty sampling strategy that can measure the uncertainty of each entity as well as its impact on its neighbour entities in the KG. (2) How to recognise entities that appear in one KG but not in the other KG (i.e., bachelors). Identifying bachelors would likely save annotation budget. To address this challenge, we devise a bachelor recognizer paying attention to alleviate the effect of sampling bias. Empirical results show that our proposed AL strategy can significantly improve sampling quality with good generality across different datasets, EA models and amount of bachelors.</abstract>
      <url hash="5de60a10">2021.emnlp-main.270</url>
      <bibkey>liu-etal-2021-activeea</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.270</doi>
      <video href="2021.emnlp-main.270.mp4"/>
      <pwccode url="https://github.com/uq-neusoft-health-data-science/activeea" additional="false">uq-neusoft-health-data-science/activeea</pwccode>
    </paper>
    <paper id="271">
      <title>Cost-effective End-to-end Information Extraction for Semi-structured Document Images</title>
      <author><first>Wonseok</first><last>Hwang</last></author>
      <author><first>Hyunji</first><last>Lee</last></author>
      <author><first>Jinyeong</first><last>Yim</last></author>
      <author><first>Geewook</first><last>Kim</last></author>
      <author><first>Minjoon</first><last>Seo</last></author>
      <pages>3375–3383</pages>
      <abstract>A real-world information extraction (IE) system for semi-structured document images often involves a long pipeline of multiple modules, whose complexity dramatically increases its development and maintenance cost. One can instead consider an end-to-end model that directly maps the input to the target output and simplify the entire process. However, such generation approach is known to lead to unstable performance if not designed carefully. Here we present our recent effort on transitioning from our existing pipeline-based IE system to an end-to-end system focusing on practical challenges that are associated with replacing and deploying the system in real, large-scale production. By carefully formulating document IE as a sequence generation task, we show that a single end-to-end IE system can be built and still achieve competent performance.</abstract>
      <url hash="6418ff11">2021.emnlp-main.271</url>
      <bibkey>hwang-etal-2021-cost</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.271</doi>
      <video href="2021.emnlp-main.271.mp4"/>
    </paper>
    <paper id="272">
      <title>Improving Math Word Problems with Pre-trained Knowledge and Hierarchical Reasoning</title>
      <author><first>Weijiang</first><last>Yu</last></author>
      <author><first>Yingpeng</first><last>Wen</last></author>
      <author><first>Fudan</first><last>Zheng</last></author>
      <author><first>Nong</first><last>Xiao</last></author>
      <pages>3384–3394</pages>
      <abstract>The recent algorithms for math word problems (MWP) neglect to use outside knowledge not present in the problems. Most of them only capture the word-level relationship and ignore to build hierarchical reasoning like the human being for mining the contextual structure between words and sentences. In this paper, we propose a <b>R</b>easoning with <b>P</b>re-trained <b>K</b>nowledge and <b>H</b>ierarchical <b>S</b>tructure (<b>RPKHS</b>) network, which contains a pre-trained knowledge encoder and a hierarchical reasoning encoder. Firstly, our pre-trained knowledge encoder aims at reasoning the MWP by using outside knowledge from the pre-trained transformer-based models. Secondly, the hierarchical reasoning encoder is presented for seamlessly integrating the word-level and sentence-level reasoning to bridge the entity and context domain on MWP. Extensive experiments show that our RPKHS significantly outperforms state-of-the-art approaches on two large-scale commonly-used datasets, and boosts performance from 77.4% to 83.9% on Math23K, from 75.5 to 82.2% on Math23K with 5-fold cross-validation and from 83.7% to 89.8% on MAWPS. More extensive ablations are shown to demonstrate the effectiveness and interpretability of our proposed method.</abstract>
      <url hash="877db22b">2021.emnlp-main.272</url>
      <bibkey>yu-etal-2021-improving</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.272</doi>
      <video href="2021.emnlp-main.272.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mawps">MAWPS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/math23k">Math23K</pwcdataset>
    </paper>
    <paper id="273">
      <title><fixed-case>G</fixed-case>raph<fixed-case>MR</fixed-case>: Graph Neural Network for Mathematical Reasoning</title>
      <author><first>Weijie</first><last>Feng</last></author>
      <author><first>Binbin</first><last>Liu</last></author>
      <author><first>Dongpeng</first><last>Xu</last></author>
      <author><first>Qilong</first><last>Zheng</last></author>
      <author><first>Yun</first><last>Xu</last></author>
      <pages>3395–3404</pages>
      <abstract>Mathematical reasoning aims to infer satisfiable solutions based on the given mathematics questions. Previous natural language processing researches have proven the effectiveness of sequence-to-sequence (Seq2Seq) or related variants on mathematics solving. However, few works have been able to explore structural or syntactic information hidden in expressions (e.g., precedence and associativity). This dissertation set out to investigate the usefulness of such untapped information for neural architectures. Firstly, mathematical questions are represented in the format of graphs within syntax analysis. The structured nature of graphs allows them to represent relations of variables or operators while preserving the semantics of the expressions. Having transformed to the new representations, we proposed a graph-to-sequence neural network GraphMR, which can effectively learn the hierarchical information of graphs inputs to solve mathematics and speculate answers. A complete experimental scenario with four classes of mathematical tasks and three Seq2Seq baselines is built to conduct a comprehensive analysis, and results show that GraphMR outperforms others in hidden information learning and mathematics resolving.</abstract>
      <url hash="45cbf0ac">2021.emnlp-main.273</url>
      <bibkey>feng-etal-2021-graphmr</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.273</doi>
      <video href="2021.emnlp-main.273.mp4"/>
    </paper>
    <paper id="274">
      <title>What Changes Can Large-scale Language Models Bring? Intensive Study on <fixed-case>H</fixed-case>yper<fixed-case>CLOVA</fixed-case>: Billions-scale <fixed-case>K</fixed-case>orean Generative Pretrained Transformers</title>
      <author><first>Boseop</first><last>Kim</last></author>
      <author><first>HyoungSeok</first><last>Kim</last></author>
      <author><first>Sang-Woo</first><last>Lee</last></author>
      <author><first>Gichang</first><last>Lee</last></author>
      <author><first>Donghyun</first><last>Kwak</last></author>
      <author><first>Jeon</first><last>Dong Hyeon</last></author>
      <author><first>Sunghyun</first><last>Park</last></author>
      <author><first>Sungju</first><last>Kim</last></author>
      <author><first>Seonhoon</first><last>Kim</last></author>
      <author><first>Dongpil</first><last>Seo</last></author>
      <author><first>Heungsub</first><last>Lee</last></author>
      <author><first>Minyoung</first><last>Jeong</last></author>
      <author><first>Sungjae</first><last>Lee</last></author>
      <author><first>Minsub</first><last>Kim</last></author>
      <author><first>Suk Hyun</first><last>Ko</last></author>
      <author><first>Seokhun</first><last>Kim</last></author>
      <author><first>Taeyong</first><last>Park</last></author>
      <author><first>Jinuk</first><last>Kim</last></author>
      <author><first>Soyoung</first><last>Kang</last></author>
      <author><first>Na-Hyeon</first><last>Ryu</last></author>
      <author><first>Kang Min</first><last>Yoo</last></author>
      <author><first>Minsuk</first><last>Chang</last></author>
      <author><first>Soobin</first><last>Suh</last></author>
      <author><first>Sookyo</first><last>In</last></author>
      <author><first>Jinseong</first><last>Park</last></author>
      <author><first>Kyungduk</first><last>Kim</last></author>
      <author><first>Hiun</first><last>Kim</last></author>
      <author><first>Jisu</first><last>Jeong</last></author>
      <author><first>Yong Goo</first><last>Yeo</last></author>
      <author><first>Donghoon</first><last>Ham</last></author>
      <author><first>Dongju</first><last>Park</last></author>
      <author><first>Min Young</first><last>Lee</last></author>
      <author><first>Jaewook</first><last>Kang</last></author>
      <author><first>Inho</first><last>Kang</last></author>
      <author><first>Jung-Woo</first><last>Ha</last></author>
      <author><first>Woomyoung</first><last>Park</last></author>
      <author><first>Nako</first><last>Sung</last></author>
      <pages>3405–3424</pages>
      <abstract>GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean. Also, we show the performance benefits of prompt-based learning and demonstrate how it can be integrated into the prompt engineering pipeline. Then we discuss the possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio, an interactive prompt engineering interface. Lastly, we demonstrate the potential of our methods with three successful in-house applications.</abstract>
      <url hash="7d38683d">2021.emnlp-main.274</url>
      <bibkey>kim-etal-2021-changes</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.274</doi>
      <video href="2021.emnlp-main.274.mp4"/>
      <pwccode url="" additional="true"/>
      <pwcdataset url="https://paperswithcode.com/dataset/klue">KLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/korquad">KorQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="275">
      <title><fixed-case>APIR</fixed-case>ec<fixed-case>X</fixed-case>: Cross-Library <fixed-case>API</fixed-case> Recommendation via Pre-Trained Language Model</title>
      <author><first>Yuning</first><last>Kang</last></author>
      <author><first>Zan</first><last>Wang</last></author>
      <author><first>Hongyu</first><last>Zhang</last></author>
      <author><first>Junjie</first><last>Chen</last></author>
      <author><first>Hanmo</first><last>You</last></author>
      <pages>3425–3436</pages>
      <abstract>For programmers, learning the usage of APIs (Application Programming Interfaces) of a software library is important yet difficult. API recommendation tools can help developers use APIs by recommending which APIs to be used next given the APIs that have been written. Traditionally, language models such as N-gram are applied to API recommendation. However, because the software libraries keep changing and new libraries keep emerging, new APIs are common. These new APIs can be seen as OOV (out of vocabulary) words and cannot be handled well by existing API recommendation approaches due to the lack of training data. In this paper, we propose APIRecX, the first cross-library API recommendation approach, which uses BPE to split each API call in each API sequence and pre-trains a GPT based language model. It then recommends APIs by fine-tuning the pre-trained model. APIRecX can migrate the knowledge of existing libraries to a new library, and can recommend APIs that are previously regarded as OOV. We evaluate APIRecX on six libraries and the results confirm its effectiveness by comparing with two typical API recommendation approaches.</abstract>
      <url hash="7f8992d5">2021.emnlp-main.275</url>
      <bibkey>kang-etal-2021-apirecx</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.275</doi>
      <video href="2021.emnlp-main.275.mp4"/>
    </paper>
    <paper id="276">
      <title><fixed-case>GMH</fixed-case>: A General Multi-hop Reasoning Model for <fixed-case>KG</fixed-case> Completion</title>
      <author><first>Yao</first><last>Zhang</last></author>
      <author><first>Hongru</first><last>Liang</last></author>
      <author><first>Adam</first><last>Jatowt</last></author>
      <author><first>Wenqiang</first><last>Lei</last></author>
      <author><first>Xin</first><last>Wei</last></author>
      <author><first>Ning</first><last>Jiang</last></author>
      <author><first>Zhenglu</first><last>Yang</last></author>
      <pages>3437–3446</pages>
      <abstract>Knowledge graphs are essential for numerous downstream natural language processing applications, but are typically incomplete with many facts missing. This results in research efforts on multi-hop reasoning task, which can be formulated as a search process and current models typically perform short distance reasoning. However, the long-distance reasoning is also vital with the ability to connect the superficially unrelated entities. To the best of our knowledge, there lacks a general framework that approaches multi-hop reasoning in mixed long-short distance reasoning scenarios. We argue that there are two key issues for a general multi-hop reasoning model: i) where to go, and ii) when to stop. Therefore, we propose a general model which resolves the issues with three modules: 1) the local-global knowledge module to estimate the possible paths, 2) the differentiated action dropout module to explore a diverse set of paths, and 3) the adaptive stopping search module to avoid over searching. The comprehensive results on three datasets demonstrate the superiority of our model with significant improvements against baselines in both short and long distance reasoning scenarios.</abstract>
      <url hash="6efbf08b">2021.emnlp-main.276</url>
      <bibkey>zhang-etal-2021-gmh</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.276</doi>
      <video href="2021.emnlp-main.276.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/umls">UMLS</pwcdataset>
    </paper>
    <paper id="277">
      <title><fixed-case>BPM</fixed-case>_<fixed-case>MT</fixed-case>: Enhanced Backchannel Prediction Model using Multi-Task Learning</title>
      <author><first>Jin Yea</first><last>Jang</last></author>
      <author><first>San</first><last>Kim</last></author>
      <author><first>Minyoung</first><last>Jung</last></author>
      <author><first>Saim</first><last>Shin</last></author>
      <author><first>Gahgene</first><last>Gweon</last></author>
      <pages>3447–3452</pages>
      <abstract>Backchannel (BC), a short reaction signal of a listener to a speaker’s utterances, helps to improve the quality of the conversation. Several studies have been conducted to predict BC in conversation; however, the utilization of advanced natural language processing techniques using lexical information presented in the utterances of a speaker has been less considered. To address this limitation, we present a BC prediction model called BPM_MT (Backchannel prediction model with multitask learning), which utilizes KoBERT, a pre-trained language model. The BPM_MT simultaneously carries out two tasks at learning: 1) BC category prediction using acoustic and lexical features, and 2) sentiment score prediction based on sentiment cues. BPM_MT exhibited 14.24% performance improvement compared to the existing baseline in the four BC categories: continuer, understanding, empathic response, and No BC. In particular, for empathic response category, a performance improvement of 17.14% was achieved.</abstract>
      <url hash="ebe35ccd">2021.emnlp-main.277</url>
      <bibkey>jang-etal-2021-bpm</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.277</doi>
      <video href="2021.emnlp-main.277.mp4"/>
    </paper>
    <paper id="278">
      <title>Graphine: A Dataset for Graph-aware Terminology Definition Generation</title>
      <author><first>Zequn</first><last>Liu</last></author>
      <author><first>Shukai</first><last>Wang</last></author>
      <author><first>Yiyang</first><last>Gu</last></author>
      <author><first>Ruiyi</first><last>Zhang</last></author>
      <author><first>Ming</first><last>Zhang</last></author>
      <author><first>Sheng</first><last>Wang</last></author>
      <pages>3453–3463</pages>
      <abstract>Precisely defining the terminology is the first step in scientific communication. Developing neural text generation models for definition generation can circumvent the labor-intensity curation, further accelerating scientific discovery. Unfortunately, the lack of large-scale terminology definition dataset hinders the process toward definition generation. In this paper, we present a large-scale terminology definition dataset Graphine covering 2,010,648 terminology definition pairs, spanning 227 biomedical subdisciplines. Terminologies in each subdiscipline further form a directed acyclic graph, opening up new avenues for developing graph-aware text generation models. We then proposed a novel graph-aware definition generation model Graphex that integrates transformer with graph neural network. Our model outperforms existing text generation models by exploiting the graph structure of terminologies. We further demonstrated how Graphine can be used to evaluate pretrained language models, compare graph representation learning methods and predict sentence granularity. We envision Graphine to be a unique resource for definition generation and many other NLP tasks in biomedicine.</abstract>
      <url hash="1ba7fce2">2021.emnlp-main.278</url>
      <bibkey>liu-etal-2021-graphine</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.278</doi>
      <video href="2021.emnlp-main.278.mp4"/>
      <pwccode url="https://github.com/zequnl/graphex" additional="false">zequnl/graphex</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/graphine">Graphine</pwcdataset>
    </paper>
    <paper id="279">
      <title>Leveraging Order-Free Tag Relations for Context-Aware Recommendation</title>
      <author><first>Junmo</first><last>Kang</last></author>
      <author><first>Jeonghwan</first><last>Kim</last></author>
      <author><first>Suwon</first><last>Shin</last></author>
      <author><first>Sung-Hyon</first><last>Myaeng</last></author>
      <pages>3464–3476</pages>
      <abstract>Tag recommendation relies on either a ranking function for top-k tags or an autoregressive generation method. However, the previous methods neglect one of two seemingly conflicting yet desirable characteristics of a tag set: orderlessness and inter-dependency. While the ranking approach fails to address the inter-dependency among tags when they are ranked, the autoregressive approach fails to take orderlessness into account because it is designed to utilize sequential relations among tokens. We propose a sequence-oblivious generation method for tag recommendation, in which the next tag to be generated is independent of the order of the generated tags and the order of the ground truth tags occurring in training data. Empirical results on two different domains, Instagram and Stack Overflow, show that our method is significantly superior to the previous approaches.</abstract>
      <url hash="28ae4fd0">2021.emnlp-main.279</url>
      <bibkey>kang-etal-2021-leveraging</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.279</doi>
      <video href="2021.emnlp-main.279.mp4"/>
    </paper>
    <paper id="280">
      <title>End-to-End Conversational Search for Online Shopping with Utterance Transfer</title>
      <author><first>Liqiang</first><last>Xiao</last></author>
      <author><first>Jun</first><last>Ma</last></author>
      <author><first>Xin Luna</first><last>Dong</last></author>
      <author><first>Pascual</first><last>Martínez-Gómez</last></author>
      <author><first>Nasser</first><last>Zalmout</last></author>
      <author><first>Chenwei</first><last>Zhang</last></author>
      <author><first>Tong</first><last>Zhao</last></author>
      <author><first>Hao</first><last>He</last></author>
      <author><first>Yaohui</first><last>Jin</last></author>
      <pages>3477–3486</pages>
      <abstract>Successful conversational search systems can present natural, adaptive and interactive shopping experience for online shopping customers. However, building such systems from scratch faces real word challenges from both imperfect product schema/knowledge and lack of training dialog data. In this work we first propose ConvSearch, an end-to-end conversational search system that deeply combines the dialog system with search. It leverages the text profile to retrieve products, which is more robust against imperfect product schema/knowledge compared with using product attributes alone. We then address the lack of data challenges by proposing an utterance transfer approach that generates dialogue utterances by using existing dialog from other domains, and leveraging the search behavior data from e-commerce retailer. With utterance transfer, we introduce a new conversational search dataset for online shopping. Experiments show that our utterance transfer method can significantly improve the availability of training dialogue data without crowd-sourcing, and the conversational search system significantly outperformed the best tested baseline.</abstract>
      <url hash="9802f40c">2021.emnlp-main.280</url>
      <bibkey>xiao-etal-2021-end</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.280</doi>
      <video href="2021.emnlp-main.280.mp4"/>
    </paper>
    <paper id="281">
      <title>Self-Supervised Curriculum Learning for Spelling Error Correction</title>
      <author><first>Zifa</first><last>Gan</last></author>
      <author><first>Hongfei</first><last>Xu</last></author>
      <author><first>Hongying</first><last>Zan</last></author>
      <pages>3487–3494</pages>
      <abstract>Spelling Error Correction (SEC) that requires high-level language understanding is a challenging but useful task. Current SEC approaches normally leverage a pre-training then fine-tuning procedure that treats data equally. By contrast, Curriculum Learning (CL) utilizes training data differently during training and has shown its effectiveness in improving both performance and training efficiency in many other NLP tasks. In NMT, a model’s performance has been shown sensitive to the difficulty of training examples, and CL has been shown effective to address this. In SEC, the data from different language learners are naturally distributed at different difficulty levels (some errors made by beginners are obvious to correct while some made by fluent speakers are hard), and we expect that designing a curriculum correspondingly for model learning may also help its training and bring about better performance. In this paper, we study how to further improve the performance of the state-of-the-art SEC method with CL, and propose a Self-Supervised Curriculum Learning (SSCL) approach. Specifically, we directly use the cross-entropy loss as criteria for: 1) scoring the difficulty of training data, and 2) evaluating the competence of the model. In our approach, CL improves the model training, which in return improves the CL measurement. In our experiments on the SIGHAN 2015 Chinese spelling check task, we show that SSCL is superior to previous norm-based and uncertainty-aware approaches, and establish a new state of the art (74.38% F1).</abstract>
      <url hash="81d36531">2021.emnlp-main.281</url>
      <bibkey>gan-etal-2021-self</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.281</doi>
      <video href="2021.emnlp-main.281.mp4"/>
    </paper>
    <paper id="282">
      <title>Fix-Filter-Fix: Intuitively Connect Any Models for Effective Bug Fixing</title>
      <author><first>Haiwen</first><last>Hong</last></author>
      <author><first>Jingfeng</first><last>Zhang</last></author>
      <author><first>Yin</first><last>Zhang</last></author>
      <author><first>Yao</first><last>Wan</last></author>
      <author><first>Yulei</first><last>Sui</last></author>
      <pages>3495–3504</pages>
      <abstract>Locating and fixing bugs is a time-consuming task. Most neural machine translation (NMT) based approaches for automatically bug fixing lack generality and do not make full use of the rich information in the source code. In NMT-based bug fixing, we find some predicted code identical to the input buggy code (called unchanged fix) in NMT-based approaches due to high similarity between buggy and fixed code (e.g., the difference may only appear in one particular line). Obviously, unchanged fix is not the correct fix because it is the same as the buggy code that needs to be fixed. Based on these, we propose an intuitive yet effective general framework (called Fix-Filter-Fix or Fˆ3) for bug fixing. Fˆ3 connects models with our filter mechanism to filter out the last model’s unchanged fix to the next. We propose an Fˆ3 theory that can quantitatively and accurately calculate the Fˆ3 lifting effect. To evaluate, we implement the Seq2Seq Transformer (ST) and the AST2Seq Transformer (AT) to form some basic Fˆ3 instances, called Fˆ3_ST+AT and Fˆ3_AT+ST. Comparing them with single model approaches and many model connection baselines across four datasets validates the effectiveness and generality of Fˆ3 and corroborates our findings and methodology.</abstract>
      <url hash="b364cb3a">2021.emnlp-main.282</url>
      <bibkey>hong-etal-2021-fix</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.282</doi>
      <video href="2021.emnlp-main.282.mp4"/>
    </paper>
    <paper id="283">
      <title>Neuro-Symbolic Reinforcement Learning with First-Order Logic</title>
      <author><first>Daiki</first><last>Kimura</last></author>
      <author><first>Masaki</first><last>Ono</last></author>
      <author><first>Subhajit</first><last>Chaudhury</last></author>
      <author><first>Ryosuke</first><last>Kohita</last></author>
      <author><first>Akifumi</first><last>Wachi</last></author>
      <author><first>Don Joven</first><last>Agravante</last></author>
      <author><first>Michiaki</first><last>Tatsubori</last></author>
      <author><first>Asim</first><last>Munawar</last></author>
      <author><first>Alexander</first><last>Gray</last></author>
      <pages>3505–3511</pages>
      <abstract>Deep reinforcement learning (RL) methods often require many trials before convergence, and no direct interpretability of trained policies is provided. In order to achieve fast convergence and interpretability for the policy in RL, we propose a novel RL method for text-based games with a recent neuro-symbolic framework called Logical Neural Network, which can learn symbolic and interpretable rules in their differentiable network. The method is first to extract first-order logical facts from text observation and external word meaning network (ConceptNet), then train a policy in the network with directly interpretable logical operators. Our experimental results show RL training with the proposed method converges significantly faster than other state-of-the-art neuro-symbolic methods in a TextWorld benchmark.</abstract>
      <url hash="7e53a383">2021.emnlp-main.283</url>
      <bibkey>kimura-etal-2021-neuro</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.283</doi>
      <video href="2021.emnlp-main.283.mp4"/>
    </paper>
    <paper id="284">
      <title>Biomedical Concept Normalization by Leveraging Hypernyms</title>
      <author><first>Cheng</first><last>Yan</last></author>
      <author><first>Yuanzhe</first><last>Zhang</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <author><first>Yafei</first><last>Shi</last></author>
      <author><first>Shengping</first><last>Liu</last></author>
      <pages>3512–3517</pages>
      <abstract>Biomedical Concept Normalization (BCN) is widely used in biomedical text processing as a fundamental module. Owing to numerous surface variants of biomedical concepts, BCN still remains challenging and unsolved. In this paper, we exploit biomedical concept hypernyms to facilitate BCN. We propose Biomedical Concept Normalizer with Hypernyms (BCNH), a novel framework that adopts list-wise training to make use of both hypernyms and synonyms, and also employs norm constraint on the representation of hypernym-hyponym entity pairs. The experimental results show that BCNH outperforms the previous state-of-the-art model on the NCBI dataset.</abstract>
      <url hash="8389c8ba">2021.emnlp-main.284</url>
      <bibkey>yan-etal-2021-biomedical</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.284</doi>
      <video href="2021.emnlp-main.284.mp4"/>
      <pwccode url="https://github.com/yan-cheng/bcnh" additional="false">yan-cheng/bcnh</pwccode>
    </paper>
    <paper id="285">
      <title>Leveraging Capsule Routing to Associate Knowledge with Medical Literature Hierarchically</title>
      <author><first>Xin</first><last>Liu</last></author>
      <author><first>Qingcai</first><last>Chen</last></author>
      <author><first>Junying</first><last>Chen</last></author>
      <author><first>Wenxiu</first><last>Zhou</last></author>
      <author><first>Tingyu</first><last>Liu</last></author>
      <author><first>Xinlan</first><last>Yang</last></author>
      <author><first>Weihua</first><last>Peng</last></author>
      <pages>3518–3532</pages>
      <abstract>Integrating knowledge into text is a promising way to enrich text representation, especially in the medical field. However, undifferentiated knowledge not only confuses the text representation but also imports unexpected noises. In this paper, to alleviate this problem, we propose leveraging capsule routing to associate knowledge with medical literature hierarchically (called HiCapsRKL). Firstly, HiCapsRKL extracts two empirically designed text fragments from medical literature and encodes them into fragment representations respectively. Secondly, the capsule routing algorithm is applied to two fragment representations. Through the capsule computing and dynamic routing, each representation is processed into a new representation (denoted as caps-representation), and we integrate the caps-representations as information gain to associate knowledge with medical literature hierarchically. Finally, HiCapsRKL are validated on relevance prediction and medical literature retrieval test sets. The experimental results and analyses show that HiCapsRKLcan more accurately associate knowledge with medical literature than mainstream methods. In summary, HiCapsRKL can efficiently help selecting the most relevant knowledge to the medical literature, which may be an alternative attempt to improve knowledge-based text representation. Source code is released on GitHub.</abstract>
      <url hash="25fdec70">2021.emnlp-main.285</url>
      <bibkey>liu-etal-2021-leveraging</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.285</doi>
      <video href="2021.emnlp-main.285.mp4"/>
      <pwccode url="https://github.com/gdls/hicapsrkl" additional="false">gdls/hicapsrkl</pwccode>
    </paper>
    <paper id="286">
      <title>Label-Enhanced Hierarchical Contextualized Representation for Sequential Metaphor Identification</title>
      <author><first>Shuqun</first><last>Li</last></author>
      <author><first>Liang</first><last>Yang</last></author>
      <author><first>Weidong</first><last>He</last></author>
      <author><first>Shiqi</first><last>Zhang</last></author>
      <author><first>Jingjie</first><last>Zeng</last></author>
      <author><first>Hongfei</first><last>Lin</last></author>
      <pages>3533–3543</pages>
      <abstract>Recent metaphor identification approaches mainly consider the contextual text features within a sentence or introduce external linguistic features to the model. But they usually ignore the extra information that the data can provide, such as the contextual metaphor information and broader discourse information. In this paper, we propose a model augmented with hierarchical contextualized representation to extract more information from both sentence-level and discourse-level. At the sentence level, we leverage the metaphor information of words that except the target word in the sentence to strengthen the reasoning ability of our model via a novel label-enhanced contextualized representation. At the discourse level, the position-aware global memory network is adopted to learn long-range dependency among the same words within a discourse. Finally, our model combines the representations obtained from these two parts. The experiment results on two tasks of the VUA dataset show that our model outperforms every other state-of-the-art method that also does not use any external knowledge except what the pre-trained language model contains.</abstract>
      <url hash="303b1959">2021.emnlp-main.286</url>
      <bibkey>li-etal-2021-label</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.286</doi>
      <video href="2021.emnlp-main.286.mp4"/>
    </paper>
    <paper id="287">
      <title><fixed-case>S</fixed-case>pell<fixed-case>BERT</fixed-case>: A Lightweight Pretrained Model for <fixed-case>C</fixed-case>hinese Spelling Check</title>
      <author><first>Tuo</first><last>Ji</last></author>
      <author><first>Hang</first><last>Yan</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <pages>3544–3551</pages>
      <abstract>Chinese Spelling Check (CSC) is to detect and correct Chinese spelling errors. Many models utilize a predefined confusion set to learn a mapping between correct characters and its visually similar or phonetically similar misuses but the mapping may be out-of-domain. To that end, we propose SpellBERT, a pretrained model with graph-based extra features and independent on confusion set. To explicitly capture the two erroneous patterns, we employ a graph neural network to introduce radical and pinyin information as visual and phonetic features. For better fusing these features with character representations, we devise masked language model alike pre-training tasks. With this feature-rich pre-training, SpellBERT with only half size of BERT can show competitive performance and make a state-of-the-art result on the OCR dataset where most of the errors are not covered by the existing confusion set.</abstract>
      <url hash="ebce69c2">2021.emnlp-main.287</url>
      <bibkey>ji-etal-2021-spellbert</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.287</doi>
      <revision id="1" href="2021.emnlp-main.287v1" hash="28bee265"/>
      <revision id="2" href="2021.emnlp-main.287v2" hash="ebce69c2" date="2022-03-20">Removed two citations.</revision>
      <pwccode url="https://github.com/benbijituo/spellbert" additional="false">benbijituo/spellbert</pwccode>
    </paper>
    <paper id="288">
      <title>Automated Generation of Accurate &amp; Fluent Medical <fixed-case>X</fixed-case>-ray Reports</title>
      <author><first>Hoang</first><last>Nguyen</last></author>
      <author><first>Dong</first><last>Nie</last></author>
      <author><first>Taivanbat</first><last>Badamdorj</last></author>
      <author><first>Yujie</first><last>Liu</last></author>
      <author><first>Yingying</first><last>Zhu</last></author>
      <author><first>Jason</first><last>Truong</last></author>
      <author><first>Li</first><last>Cheng</last></author>
      <pages>3552–3569</pages>
      <abstract>Our paper aims to automate the generation of medical reports from chest X-ray image inputs, a critical yet time-consuming task for radiologists. Existing medical report generation efforts emphasize producing human-readable reports, yet the generated text may not be well aligned to the clinical facts. Our generated medical reports, on the other hand, are fluent and, more importantly, clinically accurate. This is achieved by our fully differentiable and end-to-end paradigm that contains three complementary modules: taking the chest X-ray images and clinical history document of patients as inputs, our classification module produces an internal checklist of disease-related topics, referred to as enriched disease embedding; the embedding representation is then passed to our transformer-based generator, to produce the medical report; meanwhile, our generator also creates a weighted embedding representation, which is fed to our interpreter to ensure consistency with respect to disease-related topics. Empirical evaluations demonstrate very promising results achieved by our approach on commonly-used metrics concerning language fluency and clinical accuracy. Moreover, noticeable performance gains are consistently observed when additional input information is available, such as the clinical document and extra scans from different views.</abstract>
      <url hash="ff258315">2021.emnlp-main.288</url>
      <bibkey>nguyen-etal-2021-automated</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.288</doi>
      <video href="2021.emnlp-main.288.mp4"/>
      <pwccode url="https://github.com/ginobilinie/xray_report_generation" additional="false">ginobilinie/xray_report_generation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/chexpert">CheXpert</pwcdataset>
    </paper>
    <paper id="289">
      <title>Enhancing Document Ranking with Task-adaptive Training and Segmented Token Recovery Mechanism</title>
      <author><first>Xingwu</first><last>Sun</last></author>
      <author><first>Yanling</first><last>Cui</last></author>
      <author><first>Hongyin</first><last>Tang</last></author>
      <author><first>Fuzheng</first><last>Zhang</last></author>
      <author><first>Beihong</first><last>Jin</last></author>
      <author><first>Shi</first><last>Wang</last></author>
      <pages>3570–3579</pages>
      <abstract>In this paper, we propose a new ranking model DR-BERT, which improves the Document Retrieval (DR) task by a task-adaptive training process and a Segmented Token Recovery Mechanism (STRM). In the task-adaptive training, we first pre-train DR-BERT to be domain-adaptive and then make the two-phase fine-tuning. In the first-phase fine-tuning, the model learns query-document matching patterns regarding different query types in a pointwise way. Next, in the second-phase fine-tuning, the model learns document-level ranking features and ranks documents with regard to a given query in a listwise manner. Such pointwise plus listwise fine-tuning enables the model to minimize errors in the document ranking by incorporating ranking-specific supervisions. Meanwhile, the model derived from pointwise fine-tuning is also used to reduce noise in the training data of the listwise fine-tuning. On the other hand, we present STRM which can compute OOV word representation and contextualization more precisely in BERT-based models. As an effective strategy in DR-BERT, STRM improves the matching perfromance of OOV words between a query and a document. Notably, our DR-BERT model keeps in the top three on the MS MARCO leaderboard since May 20, 2020.</abstract>
      <url hash="a5e4c211">2021.emnlp-main.289</url>
      <bibkey>sun-etal-2021-enhancing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.289</doi>
      <video href="2021.emnlp-main.289.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
    </paper>
    <paper id="290">
      <title>Abstract, Rationale, Stance: A Joint Model for Scientific Claim Verification</title>
      <author><first>Zhiwei</first><last>Zhang</last></author>
      <author><first>Jiyi</first><last>Li</last></author>
      <author><first>Fumiyo</first><last>Fukumoto</last></author>
      <author><first>Yanming</first><last>Ye</last></author>
      <pages>3580–3586</pages>
      <abstract>Scientific claim verification can help the researchers to easily find the target scientific papers with the sentence evidence from a large corpus for the given claim. Some existing works propose pipeline models on the three tasks of abstract retrieval, rationale selection and stance prediction. Such works have the problems of error propagation among the modules in the pipeline and lack of sharing valuable information among modules. We thus propose an approach, named as ARSJoint, that jointly learns the modules for the three tasks with a machine reading comprehension framework by including claim information. In addition, we enhance the information exchanges and constraints among tasks by proposing a regularization term between the sentence attention scores of abstract retrieval and the estimated outputs of rational selection. The experimental results on the benchmark dataset SciFact show that our approach outperforms the existing works.</abstract>
      <url hash="dff4f8d8">2021.emnlp-main.290</url>
      <bibkey>zhang-etal-2021-abstract</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.290</doi>
      <video href="2021.emnlp-main.290.mp4"/>
      <pwccode url="https://github.com/zhiweizhang97/arsjointmodel" additional="false">zhiweizhang97/arsjointmodel</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
    </paper>
    <paper id="291">
      <title>A Fine-Grained Domain Adaption Model for Joint Word Segmentation and <fixed-case>POS</fixed-case> Tagging</title>
      <author><first>Peijie</first><last>Jiang</last></author>
      <author><first>Dingkun</first><last>Long</last></author>
      <author><first>Yueheng</first><last>Sun</last></author>
      <author><first>Meishan</first><last>Zhang</last></author>
      <author><first>Guangwei</first><last>Xu</last></author>
      <author><first>Pengjun</first><last>Xie</last></author>
      <pages>3587–3598</pages>
      <abstract>Domain adaption for word segmentation and POS tagging is a challenging problem for Chinese lexical processing. Self-training is one promising solution for it, which struggles to construct a set of high-quality pseudo training instances for the target domain. Previous work usually assumes a universal source-to-target adaption to collect such pseudo corpus, ignoring the different gaps from the target sentences to the source domain. In this work, we start from joint word segmentation and POS tagging, presenting a fine-grained domain adaption method to model the gaps accurately. We measure the gaps by one simple and intuitive metric, and adopt it to develop a pseudo target domain corpus based on fine-grained subdomains incrementally. A novel domain-mixed representation learning model is proposed accordingly to encode the multiple subdomains effectively. The whole process is performed progressively for both corpus construction and model training. Experimental results on a benchmark dataset show that our method can gain significant improvements over a vary of baselines. Extensive analyses are performed to show the advantages of our final domain adaption model as well.</abstract>
      <url hash="aaa948cb">2021.emnlp-main.291</url>
      <bibkey>jiang-etal-2021-fine</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.291</doi>
      <video href="2021.emnlp-main.291.mp4"/>
      <pwccode url="https://github.com/jzx555/fgda" additional="false">jzx555/fgda</pwccode>
    </paper>
    <paper id="292">
      <title>Answering Open-Domain Questions of Varying Reasoning Steps from Text</title>
      <author><first>Peng</first><last>Qi</last></author>
      <author><first>Haejun</first><last>Lee</last></author>
      <author><first>Tg</first><last>Sido</last></author>
      <author><first>Christopher</first><last>Manning</last></author>
      <pages>3599–3614</pages>
      <abstract>We develop a unified system to answer directly from text open-domain questions that may require a varying number of retrieval steps. We employ a single multi-task transformer model to perform all the necessary subtasks—retrieving supporting facts, reranking them, and predicting the answer from all retrieved documents—in an iterative fashion. We avoid crucial assumptions of previous work that do not transfer well to real-world settings, including exploiting knowledge of the fixed number of retrieval steps required to answer each question or using structured metadata like knowledge bases or web links that have limited availability. Instead, we design a system that can answer open-domain questions on any text collection without prior knowledge of reasoning complexity. To emulate this setting, we construct a new benchmark, called BeerQA, by combining existing one- and two-step datasets with a new collection of 530 questions that require three Wikipedia pages to answer, unifying Wikipedia corpora versions in the process. We show that our model demonstrates competitive performance on both existing benchmarks and this new benchmark. We make the new benchmark available at https://beerqa.github.io/.</abstract>
      <url hash="e7506d3d">2021.emnlp-main.292</url>
      <bibkey>qi-etal-2021-answering</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.292</doi>
      <video href="2021.emnlp-main.292.mp4"/>
      <pwccode url="https://github.com/beerqa/irrr" additional="false">beerqa/irrr</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/kilt">KILT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="293">
      <title>Adaptive Information Seeking for Open-Domain Question Answering</title>
      <author><first>Yunchang</first><last>Zhu</last></author>
      <author><first>Liang</first><last>Pang</last></author>
      <author><first>Yanyan</first><last>Lan</last></author>
      <author><first>Huawei</first><last>Shen</last></author>
      <author><first>Xueqi</first><last>Cheng</last></author>
      <pages>3615–3626</pages>
      <abstract>Information seeking is an essential step for open-domain question answering to efficiently gather evidence from a large corpus. Recently, iterative approaches have been proven to be effective for complex questions, by recursively retrieving new evidence at each step. However, almost all existing iterative approaches use predefined strategies, either applying the same retrieval function multiple times or fixing the order of different retrieval functions, which cannot fulfill the diverse requirements of various questions. In this paper, we propose a novel adaptive information-seeking strategy for open-domain question answering, namely AISO. Specifically, the whole retrieval and answer process is modeled as a partially observed Markov decision process, where three types of retrieval operations (e.g., BM25, DPR, and hyperlink) and one answer operation are defined as actions. According to the learned policy, AISO could adaptively select a proper retrieval action to seek the missing evidence at each step, based on the collected evidence and the reformulated query, or directly output the answer when the evidence set is sufficient for the question. Experiments on SQuAD Open and HotpotQA fullwiki, which serve as single-hop and multi-hop open-domain QA benchmarks, show that AISO outperforms all baseline methods with predefined strategies in terms of both retrieval and answer evaluations.</abstract>
      <url hash="9f3da3d8">2021.emnlp-main.293</url>
      <attachment type="Software" hash="6bf9d68d">2021.emnlp-main.293.Software.zip</attachment>
      <bibkey>zhu-etal-2021-adaptive</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.293</doi>
      <video href="2021.emnlp-main.293.mp4"/>
      <pwccode url="https://github.com/zycdev/aiso" additional="false">zycdev/aiso</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="294">
      <title>Mapping probability word problems to executable representations</title>
      <author><first>Simon</first><last>Suster</last></author>
      <author><first>Pieter</first><last>Fivez</last></author>
      <author><first>Pietro</first><last>Totis</last></author>
      <author><first>Angelika</first><last>Kimmig</last></author>
      <author><first>Jesse</first><last>Davis</last></author>
      <author><first>Luc</first><last>de Raedt</last></author>
      <author><first>Walter</first><last>Daelemans</last></author>
      <pages>3627–3640</pages>
      <abstract>While solving math word problems automatically has received considerable attention in the NLP community, few works have addressed probability word problems specifically. In this paper, we employ and analyse various neural models for answering such word problems. In a two-step approach, the problem text is first mapped to a formal representation in a declarative language using a sequence-to-sequence model, and then the resulting representation is executed using a probabilistic programming system to provide the answer. Our best performing model incorporates general-domain contextualised word representations that were finetuned using transfer learning on another in-domain dataset. We also apply end-to-end models to this task, which bring out the importance of the two-step approach in obtaining correct solutions to probability problems.</abstract>
      <url hash="a8caa0e1">2021.emnlp-main.294</url>
      <bibkey>suster-etal-2021-mapping</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.294</doi>
      <video href="2021.emnlp-main.294.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mathqa">MathQA</pwcdataset>
    </paper>
    <paper id="295">
      <title>Enhancing Multiple-choice Machine Reading Comprehension by Punishing Illogical Interpretations</title>
      <author><first>Yiming</first><last>Ju</last></author>
      <author><first>Yuanzhe</first><last>Zhang</last></author>
      <author><first>Zhixing</first><last>Tian</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Xiaohuan</first><last>Cao</last></author>
      <author><first>Wenting</first><last>Zhao</last></author>
      <author><first>Jinlong</first><last>Li</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>3641–3652</pages>
      <abstract>Machine Reading Comprehension (MRC), which requires a machine to answer questions given the relevant documents, is an important way to test machines’ ability to understand human language. Multiple-choice MRC is one of the most studied tasks in MRC due to the convenience of evaluation and the flexibility of answer format. Post-hoc interpretation aims to explain a trained model and reveal how the model arrives at the prediction. One of the most important interpretation forms is to attribute model decisions to input features. Based on post-hoc interpretation methods, we assess attributions of paragraphs in multiple-choice MRC and improve the model by punishing the illogical attributions. Our method can improve model performance without any external information and model structure change. Furthermore, we also analyze how and why such a self-training method works.</abstract>
      <url hash="e6b9cc09">2021.emnlp-main.295</url>
      <bibkey>ju-etal-2021-enhancing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.295</doi>
      <video href="2021.emnlp-main.295.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/dream">DREAM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multirc">MultiRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="296">
      <title>Large-Scale Relation Learning for Question Answering over Knowledge Bases with Pre-trained Language Models</title>
      <author><first>Yuanmeng</first><last>Yan</last></author>
      <author><first>Rumei</first><last>Li</last></author>
      <author><first>Sirui</first><last>Wang</last></author>
      <author><first>Hongzhi</first><last>Zhang</last></author>
      <author><first>Zan</first><last>Daoguang</last></author>
      <author><first>Fuzheng</first><last>Zhang</last></author>
      <author><first>Wei</first><last>Wu</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <pages>3653–3660</pages>
      <abstract>The key challenge of question answering over knowledge bases (KBQA) is the inconsistency between the natural language questions and the reasoning paths in the knowledge base (KB). Recent graph-based KBQA methods are good at grasping the topological structure of the graph but often ignore the textual information carried by the nodes and edges. Meanwhile, pre-trained language models learn massive open-world knowledge from the large corpus, but it is in the natural language form and not structured. To bridge the gap between the natural language and the structured KB, we propose three relation learning tasks for BERT-based KBQA, including relation extraction, relation matching, and relation reasoning. By relation-augmented training, the model learns to align the natural language expressions to the relations in the KB as well as reason over the missing connections in the KB. Experiments on WebQSP show that our method consistently outperforms other baselines, especially when the KB is incomplete.</abstract>
      <url hash="6bc5ec1b">2021.emnlp-main.296</url>
      <bibkey>yan-etal-2021-large</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.296</doi>
      <video href="2021.emnlp-main.296.mp4"/>
      <pwccode url="https://github.com/yym6472/kbqarelationlearning" additional="false">yym6472/kbqarelationlearning</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
    </paper>
    <paper id="297">
      <title>Phrase Retrieval Learns Passage Retrieval, Too</title>
      <author><first>Jinhyuk</first><last>Lee</last></author>
      <author><first>Alexander</first><last>Wettig</last></author>
      <author><first>Danqi</first><last>Chen</last></author>
      <pages>3661–3672</pages>
      <abstract>Dense retrieval methods have shown great promise over sparse retrieval methods in a range of NLP problems. Among them, dense phrase retrieval—the most fine-grained retrieval unit—is appealing because phrases can be directly used as the output for question answering and slot filling tasks. In this work, we follow the intuition that retrieving phrases naturally entails retrieving larger text blocks and study whether phrase retrieval can serve as the basis for coarse-level retrieval including passages and documents. We first observe that a dense phrase-retrieval system, without any retraining, already achieves better passage retrieval accuracy (+3-5% in top-5 accuracy) compared to passage retrievers, which also helps achieve superior end-to-end QA performance with fewer passages. Then, we provide an interpretation for why phrase-level supervision helps learn better fine-grained entailment compared to passage-level supervision, and also show that phrase retrieval can be improved to achieve competitive performance in document-retrieval tasks such as entity linking and knowledge-grounded dialogue. Finally, we demonstrate how phrase filtering and vector quantization can reduce the size of our index by 4-10x, making dense phrase retrieval a practical and versatile solution in multi-granularity retrieval.</abstract>
      <url hash="19074be1">2021.emnlp-main.297</url>
      <bibkey>lee-etal-2021-phrase</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.297</doi>
      <video href="2021.emnlp-main.297.mp4"/>
      <pwccode url="https://github.com/princeton-nlp/DensePhrases" additional="false">princeton-nlp/DensePhrases</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/kilt">KILT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="298">
      <title>Neural Natural Logic Inference for Interpretable Question Answering</title>
      <author><first>Jihao</first><last>Shi</last></author>
      <author><first>Xiao</first><last>Ding</last></author>
      <author><first>Li</first><last>Du</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <pages>3673–3684</pages>
      <abstract>Many open-domain question answering problems can be cast as a textual entailment task, where a question and candidate answers are concatenated to form hypotheses. A QA system then determines if the supporting knowledge bases, regarded as potential premises, entail the hypotheses. In this paper, we investigate a neural-symbolic QA approach that integrates natural logic reasoning within deep learning architectures, towards developing effective and yet explainable question answering models. The proposed model gradually bridges a hypothesis and candidate premises following natural logic inference steps to build proof paths. Entailment scores between the acquired intermediate hypotheses and candidate premises are measured to determine if a premise entails the hypothesis. As the natural logic reasoning process forms a tree-like, hierarchical structure, we embed hypotheses and premises in a Hyperbolic space rather than Euclidean space to acquire more precise representations. Empirically, our method outperforms prior work on answering multiple-choice science questions, achieving the best results on two publicly available datasets. The natural logic inference process inherently provides evidence to help explain the prediction process.</abstract>
      <url hash="494efaa8">2021.emnlp-main.298</url>
      <bibkey>shi-etal-2021-neural</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.298</doi>
      <video href="2021.emnlp-main.298.mp4"/>
      <pwccode url="https://github.com/shijihao/neunli" additional="false">shijihao/neunli</pwccode>
    </paper>
    <paper id="299">
      <title>Smoothing Dialogue States for Open Conversational Machine Reading</title>
      <author><first>Zhuosheng</first><last>Zhang</last></author>
      <author><first>Siru</first><last>Ouyang</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <pages>3685–3696</pages>
      <abstract>Conversational machine reading (CMR) requires machines to communicate with humans through multi-turn interactions between two salient dialogue states of decision making and question generation processes. In open CMR settings, as the more realistic scenario, the retrieved background knowledge would be noisy, which results in severe challenges in the information transmission. Existing studies commonly train independent or pipeline systems for the two subtasks. However, those methods are trivial by using hard-label decisions to activate question generation, which eventually hinders the model performance. In this work, we propose an effective gating strategy by smoothing the two dialogue states in only one decoder and bridge decision making and question generation to provide a richer dialogue state reference. Experiments on the OR-ShARC dataset show the effectiveness of our method, which achieves new state-of-the-art results.</abstract>
      <url hash="28e9ba53">2021.emnlp-main.299</url>
      <bibkey>zhang-etal-2021-smoothing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.299</doi>
      <video href="2021.emnlp-main.299.mp4"/>
      <pwccode url="https://github.com/ozyyshr/oscar" additional="false">ozyyshr/oscar</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sharc">ShARC</pwcdataset>
    </paper>
    <paper id="300">
      <title><fixed-case>F</fixed-case>in<fixed-case>QA</fixed-case>: A Dataset of Numerical Reasoning over Financial Data</title>
      <author><first>Zhiyu</first><last>Chen</last></author>
      <author><first>Wenhu</first><last>Chen</last></author>
      <author><first>Charese</first><last>Smiley</last></author>
      <author><first>Sameena</first><last>Shah</last></author>
      <author><first>Iana</first><last>Borova</last></author>
      <author><first>Dylan</first><last>Langdon</last></author>
      <author><first>Reema</first><last>Moussa</last></author>
      <author><first>Matt</first><last>Beane</last></author>
      <author><first>Ting-Hao</first><last>Huang</last></author>
      <author><first>Bryan</first><last>Routledge</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>3697–3711</pages>
      <abstract>The sheer volume of financial statements makes it difficult for humans to access and analyze a business’s financials. Robust numerical reasoning likewise faces unique challenges in this domain. In this work, we focus on answering deep questions over financial data, aiming to automate the analysis of a large corpus of financial documents. In contrast to existing tasks on general domain, the finance domain includes complex numerical reasoning and understanding of heterogeneous representations. To facilitate analytical progress, we propose a new large-scale dataset, FinQA, with Question-Answering pairs over Financial reports, written by financial experts. We also annotate the gold reasoning programs to ensure full explainability. We further introduce baselines and conduct comprehensive experiments in our dataset. The results demonstrate that popular, large, pre-trained models fall far short of expert humans in acquiring finance knowledge and in complex multi-step numerical reasoning on that knowledge. Our dataset – the first of its kind – should therefore enable significant, new community research into complex application domains. The dataset and code are publicly available at https://github.com/czyssrs/FinQA.</abstract>
      <url hash="c58cbf87">2021.emnlp-main.300</url>
      <bibkey>chen-etal-2021-finqa</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.300</doi>
      <video href="2021.emnlp-main.300.mp4"/>
      <pwccode url="https://github.com/czyssrs/finqa" additional="false">czyssrs/finqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/finqa">FinQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mathqa">MathQA</pwcdataset>
    </paper>
    <paper id="301">
      <title><fixed-case>F</fixed-case>i<fixed-case>D</fixed-case>-Ex: Improving Sequence-to-Sequence Models for Extractive Rationale Generation</title>
      <author><first>Kushal</first><last>Lakhotia</last></author>
      <author><first>Bhargavi</first><last>Paranjape</last></author>
      <author><first>Asish</first><last>Ghoshal</last></author>
      <author><first>Scott</first><last>Yih</last></author>
      <author><first>Yashar</first><last>Mehdad</last></author>
      <author><first>Srini</first><last>Iyer</last></author>
      <pages>3712–3727</pages>
      <abstract>Natural language (NL) explanations of model predictions are gaining popularity as a means to understand and verify decisions made by large black-box pre-trained models, for tasks such as Question Answering (QA) and Fact Verification. Recently, pre-trained sequence to sequence (seq2seq) models have proven to be very effective in jointly making predictions, as well as generating NL explanations. However, these models have many shortcomings; they can fabricate explanations even for incorrect predictions, they are difficult to adapt to long input documents, and their training requires a large amount of labeled data. In this paper, we develop FiD-Ex, which addresses these shortcomings for seq2seq models by: 1) introducing sentence markers to eliminate explanation fabrication by encouraging extractive generation, 2) using the fusion-in-decoder architecture to handle long input contexts, and 3) intermediate fine-tuning on re-structured open domain QA datasets to improve few-shot performance. FiD-Ex significantly improves over prior work in terms of explanation metrics and task accuracy on five tasks from the ERASER explainability benchmark in both fully supervised and few-shot settings.</abstract>
      <url hash="93756ecd">2021.emnlp-main.301</url>
      <bibkey>lakhotia-etal-2021-fid</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.301</doi>
      <video href="2021.emnlp-main.301.mp4"/>
    </paper>
    <paper id="302">
      <title><fixed-case>R</fixed-case>ock<fixed-case>NER</fixed-case>: A Simple Method to Create Adversarial Examples for Evaluating the Robustness of Named Entity Recognition Models</title>
      <author><first>Bill Yuchen</first><last>Lin</last></author>
      <author><first>Wenyang</first><last>Gao</last></author>
      <author><first>Jun</first><last>Yan</last></author>
      <author><first>Ryan</first><last>Moreno</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <pages>3728–3737</pages>
      <abstract>To audit the robustness of named entity recognition (NER) models, we propose RockNER, a simple yet effective method to create natural adversarial examples. Specifically, at the entity level, we replace target entities with other entities of the same semantic class in Wikidata; at the context level, we use pre-trained language models (e.g., BERT) to generate word substitutions. Together, the two levels of at- tack produce natural adversarial examples that result in a shifted distribution from the training data on which our target models have been trained. We apply the proposed method to the OntoNotes dataset and create a new benchmark named OntoRock for evaluating the robustness of existing NER models via a systematic evaluation protocol. Our experiments and analysis reveal that even the best model has a significant performance drop, and these models seem to memorize in-domain entity patterns instead of reasoning from the context. Our work also studies the effects of a few simple data augmentation methods to improve the robustness of NER models.</abstract>
      <url hash="74dd8e81">2021.emnlp-main.302</url>
      <bibkey>lin-etal-2021-rockner</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.302</doi>
      <video href="2021.emnlp-main.302.mp4"/>
      <pwccode url="https://github.com/INK-USC/RockNER" additional="false">INK-USC/RockNER</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ontorock">OntoRock</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ontonotes-5-0">OntoNotes 5.0</pwcdataset>
    </paper>
    <paper id="303">
      <title>Diagnosing the First-Order Logical Reasoning Ability Through <fixed-case>L</fixed-case>ogic<fixed-case>NLI</fixed-case></title>
      <author><first>Jidong</first><last>Tian</last></author>
      <author><first>Yitian</first><last>Li</last></author>
      <author><first>Wenqing</first><last>Chen</last></author>
      <author><first>Liqiang</first><last>Xiao</last></author>
      <author><first>Hao</first><last>He</last></author>
      <author><first>Yaohui</first><last>Jin</last></author>
      <pages>3738–3747</pages>
      <abstract>Recently, language models (LMs) have achieved significant performance on many NLU tasks, which has spurred widespread interest for their possible applications in the scientific and social area. However, LMs have faced much criticism of whether they are truly capable of reasoning in NLU. In this work, we propose a diagnostic method for first-order logic (FOL) reasoning with a new proposed benchmark, LogicNLI. LogicNLI is an NLI-style dataset that effectively disentangles the target FOL reasoning from commonsense inference and can be used to diagnose LMs from four perspectives: accuracy, robustness, generalization, and interpretability. Experiments on BERT, RoBERTa, and XLNet, have uncovered the weaknesses of these LMs on FOL reasoning, which motivates future exploration to enhance the reasoning ability.</abstract>
      <url hash="07c8e797">2021.emnlp-main.303</url>
      <bibkey>tian-etal-2021-diagnosing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.303</doi>
      <video href="2021.emnlp-main.303.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/logiqa">LogiQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reclor">ReClor</pwcdataset>
    </paper>
    <paper id="304">
      <title>Constructing a Psychometric Testbed for Fair Natural Language Processing</title>
      <author><first>Ahmed</first><last>Abbasi</last></author>
      <author><first>David</first><last>Dobolyi</last></author>
      <author><first>John P.</first><last>Lalor</last></author>
      <author><first>Richard G.</first><last>Netemeyer</last></author>
      <author><first>Kendall</first><last>Smith</last></author>
      <author><first>Yi</first><last>Yang</last></author>
      <pages>3748–3758</pages>
      <abstract>Psychometric measures of ability, attitudes, perceptions, and beliefs are crucial for understanding user behavior in various contexts including health, security, e-commerce, and finance. Traditionally, psychometric dimensions have been measured and collected using survey-based methods. Inferring such constructs from user-generated text could allow timely, unobtrusive collection and analysis. In this paper we describe our efforts to construct a corpus for psychometric natural language processing (NLP) related to important dimensions such as trust, anxiety, numeracy, and literacy, in the health domain. We discuss our multi-step process to align user text with their survey-based response items and provide an overview of the resulting testbed which encompasses survey-based psychometric measures and accompanying user-generated text from 8,502 respondents. Our testbed also encompasses self-reported demographic information, including race, sex, age, income, and education - thereby affording opportunities for measuring bias and benchmarking fairness of text classification methods. We report preliminary results on use of the text to predict/categorize users’ survey response labels - and on the fairness of these models. We also discuss the important implications of our work and resulting testbed for future NLP research on psychometrics and fairness.</abstract>
      <url hash="1c61ed10">2021.emnlp-main.304</url>
      <bibkey>abbasi-etal-2021-constructing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.304</doi>
      <video href="2021.emnlp-main.304.mp4"/>
      <pwccode url="https://github.com/nd-hal/fair-psych-nlp" additional="false">nd-hal/fair-psych-nlp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/psychometric-nlp">Psychometric NLP</pwcdataset>
    </paper>
    <paper id="305">
      <title><fixed-case>COUGH</fixed-case>: A Challenge Dataset and Models for <fixed-case>COVID</fixed-case>-19 <fixed-case>FAQ</fixed-case> Retrieval</title>
      <author><first>Xinliang Frederick</first><last>Zhang</last></author>
      <author><first>Heming</first><last>Sun</last></author>
      <author><first>Xiang</first><last>Yue</last></author>
      <author><first>Simon</first><last>Lin</last></author>
      <author><first>Huan</first><last>Sun</last></author>
      <pages>3759–3769</pages>
      <abstract>We present a large, challenging dataset, COUGH, for COVID-19 FAQ retrieval. Similar to a standard FAQ dataset, COUGH consists of three parts: FAQ Bank, Query Bank and Relevance Set. The FAQ Bank contains ~16K FAQ items scraped from 55 credible websites (e.g., CDC and WHO). For evaluation, we introduce Query Bank and Relevance Set, where the former contains 1,236 human-paraphrased queries while the latter contains ~32 human-annotated FAQ items for each query. We analyze COUGH by testing different FAQ retrieval models built on top of BM25 and BERT, among which the best model achieves 48.8 under P@5, indicating a great challenge presented by COUGH and encouraging future research for further improvement. Our COUGH dataset is available at https://github.com/sunlab-osu/covid-faq.</abstract>
      <url hash="d2a403c3">2021.emnlp-main.305</url>
      <bibkey>zhang-etal-2021-cough</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.305</doi>
      <video href="2021.emnlp-main.305.mp4"/>
      <pwccode url="https://github.com/sunlab-osu/covid-faq" additional="false">sunlab-osu/covid-faq</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cough">COUGH</pwcdataset>
    </paper>
    <paper id="306">
      <title><fixed-case>C</fixed-case>hinese <fixed-case>WPLC</fixed-case>: A <fixed-case>C</fixed-case>hinese Dataset for Evaluating Pretrained Language Models on Word Prediction Given Long-Range Context</title>
      <author><first>Huibin</first><last>Ge</last></author>
      <author><first>Chenxi</first><last>Sun</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>3770–3778</pages>
      <abstract>This paper presents a Chinese dataset for evaluating pretrained language models on Word Prediction given Long-term Context (Chinese WPLC). We propose both automatic and manual selection strategies tailored to Chinese to guarantee that target words in passages collected from over 69K novels can only be predicted with long-term context beyond the scope of sentences containing the target words. Dataset analysis reveals that the types of target words range from common nouns to Chinese 4-character idioms. We also observe that linguistic relations between target words and long-range context exhibit diversity, including lexical match, synonym, summary and reasoning. Experiment results show that the Chinese pretrained language model PanGu-<tex-math>\alpha</tex-math> is 45 points behind human in terms of top-1 word prediction accuracy, indicating that Chinese WPLC is a challenging dataset. The dataset is publicly available at https://git.openi.org.cn/PCL-Platform.Intelligence/Chinese_WPLC.</abstract>
      <url hash="53771b29">2021.emnlp-main.306</url>
      <bibkey>ge-etal-2021-chinese</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.306</doi>
      <video href="2021.emnlp-main.306.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cbt">CBT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lambada">LAMBADA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winogrande">WinoGrande</pwcdataset>
    </paper>
    <paper id="307">
      <title><fixed-case>W</fixed-case>ino<fixed-case>L</fixed-case>ogic: <fixed-case>A</fixed-case> Zero-Shot Logic-based Diagnostic Dataset for <fixed-case>W</fixed-case>inograd <fixed-case>S</fixed-case>chema <fixed-case>C</fixed-case>hallenge</title>
      <author><first>Weinan</first><last>He</last></author>
      <author><first>Canming</first><last>Huang</last></author>
      <author><first>Yongmei</first><last>Liu</last></author>
      <author><first>Xiaodan</first><last>Zhu</last></author>
      <pages>3779–3789</pages>
      <abstract>The recent success of neural language models (NLMs) on the Winograd Schema Challenge has called for further investigation of the commonsense reasoning ability of these models. Previous diagnostic datasets rely on crowd-sourcing which fails to provide coherent commonsense crucial for solving WSC problems. To better evaluate NLMs, we propose a logic-based framework that focuses on high-quality commonsense knowledge. Specifically, we identify and collect formal knowledge formulas verified by theorem provers and translate such formulas into natural language sentences. Based on these true knowledge sentences, adversarial false ones are generated. We propose a new dataset named WinoLogic with these sentences. Given a problem in WinoLogic, NLMs need to decide whether the plausible knowledge sentences could correctly solve the corresponding WSC problems in a zero-shot setting. We also ask human annotators to validate WinoLogic to ensure it is human-agreeable. Experiments show that NLMs still struggle to comprehend commonsense knowledge as humans do, indicating that their reasoning ability could have been overestimated.</abstract>
      <url hash="6e11a1ce">2021.emnlp-main.307</url>
      <attachment type="Software" hash="af7b7a6e">2021.emnlp-main.307.Software.zip</attachment>
      <bibkey>he-etal-2021-winologic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.307</doi>
      <video href="2021.emnlp-main.307.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winogrande">WinoGrande</pwcdataset>
    </paper>
    <paper id="308">
      <title>Pseudo Zero Pronoun Resolution Improves Zero Anaphora Resolution</title>
      <author><first>Ryuto</first><last>Konno</last></author>
      <author><first>Shun</first><last>Kiyono</last></author>
      <author><first>Yuichiroh</first><last>Matsubayashi</last></author>
      <author><first>Hiroki</first><last>Ouchi</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>3790–3806</pages>
      <abstract>Masked language models (MLMs) have contributed to drastic performance improvements with regard to zero anaphora resolution (ZAR). To further improve this approach, in this study, we made two proposals. The first is a new pretraining task that trains MLMs on anaphoric relations with explicit supervision, and the second proposal is a new finetuning method that remedies a notorious issue, the pretrain-finetune discrepancy. Our experiments on Japanese ZAR demonstrated that our two proposals boost the state-of-the-art performance, and our detailed analysis provides new insights on the remaining challenges.</abstract>
      <url hash="f9a643c6">2021.emnlp-main.308</url>
      <bibkey>konno-etal-2021-pseudo</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.308</doi>
      <video href="2021.emnlp-main.308.mp4"/>
      <pwccode url="https://github.com/ryuto10/pzero-improves-zar" additional="false">ryuto10/pzero-improves-zar</pwccode>
    </paper>
    <paper id="309">
      <title>Aligning Cross-lingual Sentence Representations with Dual Momentum Contrast</title>
      <author><first>Liang</first><last>Wang</last></author>
      <author><first>Wei</first><last>Zhao</last></author>
      <author><first>Jingming</first><last>Liu</last></author>
      <pages>3807–3815</pages>
      <abstract>In this paper, we propose to align sentence representations from different languages into a unified embedding space, where semantic similarities (both cross-lingual and monolingual) can be computed with a simple dot product. Pre-trained language models are fine-tuned with the translation ranking task. Existing work (Feng et al., 2020) uses sentences within the same batch as negatives, which can suffer from the issue of easy negatives. We adapt MoCo (He et al., 2020) to further improve the quality of alignment. As the experimental results show, the sentence representations produced by our model achieve the new state-of-the-art on several tasks, including Tatoeba en-zh similarity search (Artetxe andSchwenk, 2019b), BUCC en-zh bitext mining, and semantic textual similarity on 7 datasets.</abstract>
      <url hash="b5197fa8">2021.emnlp-main.309</url>
      <bibkey>wang-etal-2021-aligning</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.309</doi>
      <video href="2021.emnlp-main.309.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="310">
      <title>Total Recall: a Customized Continual Learning Method for Neural Semantic Parsers</title>
      <author><first>Zhuang</first><last>Li</last></author>
      <author><first>Lizhen</first><last>Qu</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <pages>3816–3831</pages>
      <abstract>This paper investigates continual learning for semantic parsing. In this setting, a neural semantic parser learns tasks sequentially without accessing full training data from previous tasks. Direct application of the SOTA continual learning algorithms to this problem fails to achieve comparable performance with re-training models with all seen tasks because they have not considered the special properties of structured outputs yielded by semantic parsers. Therefore, we propose TotalRecall, a continual learning method designed for neural semantic parsers from two aspects: i) a sampling method for memory replay that diversifies logical form templates and balances distributions of parse actions in a memory; ii) a two-stage training method that significantly improves generalization capability of the parsers across tasks. We conduct extensive experiments to study the research problems involved in continual semantic parsing and demonstrate that a neural semantic parser trained with TotalRecall achieves superior performance than the one trained directly with the SOTA continual learning algorithms and achieve a 3-6 times speedup compared to re-training from scratch.</abstract>
      <url hash="6b0a4707">2021.emnlp-main.310</url>
      <bibkey>li-etal-2021-total</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.310</doi>
      <video href="2021.emnlp-main.310.mp4"/>
      <pwccode url="https://github.com/zhuang-li/cl_nsp" additional="false">zhuang-li/cl_nsp</pwccode>
    </paper>
    <paper id="311">
      <title>Exophoric Pronoun Resolution in Dialogues with Topic Regularization</title>
      <author><first>Xintong</first><last>Yu</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Yangqiu</first><last>Song</last></author>
      <author><first>Changshui</first><last>Zhang</last></author>
      <author><first>Kun</first><last>Xu</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <pages>3832–3845</pages>
      <abstract>Resolving pronouns to their referents has long been studied as a fundamental natural language understanding problem. Previous works on pronoun coreference resolution (PCR) mostly focus on resolving pronouns to mentions in text while ignoring the exophoric scenario. Exophoric pronouns are common in daily communications, where speakers may directly use pronouns to refer to some objects present in the environment without introducing the objects first. Although such objects are not mentioned in the dialogue text, they can often be disambiguated by the general topics of the dialogue. Motivated by this, we propose to jointly leverage the local context and global topics of dialogues to solve the out-of-text PCR problem. Extensive experiments demonstrate the effectiveness of adding topic regularization for resolving exophoric pronouns.</abstract>
      <url hash="3cd4dfc9">2021.emnlp-main.311</url>
      <bibkey>yu-etal-2021-exophoric</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.311</doi>
      <video href="2021.emnlp-main.311.mp4"/>
      <pwccode url="https://github.com/hkust-knowcomp/exo-pcr" additional="false">hkust-knowcomp/exo-pcr</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/vispro">VisPro</pwcdataset>
    </paper>
    <paper id="312">
      <title>Context-Aware Interaction Network for Question Matching</title>
      <author><first>Zhe</first><last>Hu</last></author>
      <author><first>Zuohui</first><last>Fu</last></author>
      <author><first>Yu</first><last>Yin</last></author>
      <author><first>Gerard</first><last>de Melo</last></author>
      <pages>3846–3853</pages>
      <abstract>Impressive milestones have been achieved in text matching by adopting a cross-attention mechanism to capture pertinent semantic connections between two sentence representations. However, regular cross-attention focuses on word-level links between the two input sequences, neglecting the importance of contextual information. We propose a context-aware interaction network (COIN) to properly align two sequences and infer their semantic relationship. Specifically, each interaction block includes (1) a context-aware cross-attention mechanism to effectively integrate contextual information when aligning two sequences, and (2) a gate fusion layer to flexibly interpolate aligned representations. We apply multiple stacked interaction blocks to produce alignments at different levels and gradually refine the attention results. Experiments on two question matching datasets and detailed analyses demonstrate the effectiveness of our model.</abstract>
      <url hash="88d61c2f">2021.emnlp-main.312</url>
      <bibkey>hu-etal-2021-context</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.312</doi>
      <video href="2021.emnlp-main.312.mp4"/>
    </paper>
    <paper id="313">
      <title><fixed-case>TEMP</fixed-case>: Taxonomy Expansion with Dynamic Margin Loss through Taxonomy-Paths</title>
      <author><first>Zichen</first><last>Liu</last></author>
      <author><first>Hongyuan</first><last>Xu</last></author>
      <author><first>Yanlong</first><last>Wen</last></author>
      <author><first>Ning</first><last>Jiang</last></author>
      <author><first>HaiYing</first><last>Wu</last></author>
      <author><first>Xiaojie</first><last>Yuan</last></author>
      <pages>3854–3863</pages>
      <abstract>As an essential form of knowledge representation, taxonomies are widely used in various downstream natural language processing tasks. However, with the continuously rising of new concepts, many existing taxonomies are unable to maintain coverage by manual expansion. In this paper, we propose TEMP, a self-supervised taxonomy expansion method, which predicts the position of new concepts by ranking the generated taxonomy-paths. For the first time, TEMP employs pre-trained contextual encoders in taxonomy construction and hypernym detection problems. Experiments prove that pre-trained contextual embeddings are able to capture hypernym-hyponym relations. To learn more detailed differences between taxonomy-paths, we train the model with dynamic margin loss by a novel dynamic margin function. Extensive evaluations exhibit that TEMP outperforms prior state-of-the-art taxonomy expansion approaches by 14.3% in accuracy and 15.8% in mean reciprocal rank on three public benchmarks.</abstract>
      <url hash="56d6c4f0">2021.emnlp-main.313</url>
      <bibkey>liu-etal-2021-temp</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.313</doi>
      <video href="2021.emnlp-main.313.mp4"/>
      <pwccode url="https://github.com/liu-zichen/TEMP" additional="false">liu-zichen/TEMP</pwccode>
    </paper>
    <paper id="314">
      <title>A Graph-Based Neural Model for End-to-End Frame Semantic Parsing</title>
      <author><first>ZhiChao</first><last>Lin</last></author>
      <author><first>Yueheng</first><last>Sun</last></author>
      <author><first>Meishan</first><last>Zhang</last></author>
      <pages>3864–3874</pages>
      <abstract>Frame semantic parsing is a semantic analysis task based on FrameNet which has received great attention recently. The task usually involves three subtasks sequentially: (1) target identification, (2) frame classification and (3) semantic role labeling. The three subtasks are closely related while previous studies model them individually, which ignores their intern connections and meanwhile induces error propagation problem. In this work, we propose an end-to-end neural model to tackle the task jointly. Concretely, we exploit a graph-based method, regarding frame semantic parsing as a graph construction problem. All predicates and roles are treated as graph nodes, and their relations are taken as graph edges. Experiment results on two benchmark datasets of frame semantic parsing show that our method is highly competitive, resulting in better performance than pipeline models.</abstract>
      <url hash="c845a9eb">2021.emnlp-main.314</url>
      <bibkey>lin-etal-2021-graph</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.314</doi>
      <video href="2021.emnlp-main.314.mp4"/>
      <pwccode url="https://github.com/ch4osmy7h/framenetparser" additional="false">ch4osmy7h/framenetparser</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="315">
      <title>Virtual Data Augmentation: A Robust and General Framework for Fine-tuning Pre-trained Models</title>
      <author><first>Kun</first><last>Zhou</last></author>
      <author><first>Wayne Xin</first><last>Zhao</last></author>
      <author><first>Sirui</first><last>Wang</last></author>
      <author><first>Fuzheng</first><last>Zhang</last></author>
      <author><first>Wei</first><last>Wu</last></author>
      <author><first>Ji-Rong</first><last>Wen</last></author>
      <pages>3875–3887</pages>
      <abstract>Recent works have shown that powerful pre-trained language models (PLM) can be fooled by small perturbations or intentional attacks. To solve this issue, various data augmentation techniques are proposed to improve the robustness of PLMs. However, it is still challenging to augment semantically relevant examples with sufficient diversity. In this work, we present Virtual Data Augmentation (VDA), a general framework for robustly fine-tuning PLMs. Based on the original token embeddings, we construct a multinomial mixture for augmenting virtual data embeddings, where a masked language model guarantees the semantic relevance and the Gaussian noise provides the augmentation diversity. Furthermore, a regularized training strategy is proposed to balance the two aspects. Extensive experiments on six datasets show that our approach is able to improve the robustness of PLMs and alleviate the performance degradation under adversarial attacks. Our codes and data are publicly available at blue<url>https://github.com/RUCAIBox/VDA</url>.</abstract>
      <url hash="3eab498f">2021.emnlp-main.315</url>
      <bibkey>zhou-etal-2021-virtual</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.315</doi>
      <video href="2021.emnlp-main.315.mp4"/>
      <pwccode url="https://github.com/rucaibox/vda" additional="false">rucaibox/vda</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="316">
      <title><fixed-case>CATE</fixed-case>: A Contrastive Pre-trained Model for Metaphor Detection with Semi-supervised Learning</title>
      <author><first>Zhenxi</first><last>Lin</last></author>
      <author><first>Qianli</first><last>Ma</last></author>
      <author><first>Jiangyue</first><last>Yan</last></author>
      <author><first>Jieyu</first><last>Chen</last></author>
      <pages>3888–3898</pages>
      <abstract>Metaphors are ubiquitous in natural language, and detecting them requires contextual reasoning about whether a semantic incongruence actually exists. Most existing work addresses this problem using pre-trained contextualized models. Despite their success, these models require a large amount of labeled data and are not linguistically-based. In this paper, we proposed a ContrAstive pre-Trained modEl (CATE) for metaphor detection with semi-supervised learning. Our model first uses a pre-trained model to obtain a contextual representation of target words and employs a contrastive objective to promote an increased distance between target words’ literal and metaphorical senses based on linguistic theories. Furthermore, we propose a simple strategy to collect large-scale candidate instances from the general corpus and generalize the model via self-training. Extensive experiments show that CATE achieves better performance against state-of-the-art baselines on several benchmark datasets.</abstract>
      <url hash="78659a3c">2021.emnlp-main.316</url>
      <bibkey>lin-etal-2021-cate</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.316</doi>
      <video href="2021.emnlp-main.316.mp4"/>
    </paper>
    <paper id="317">
      <title>To be Closer: Learning to Link up Aspects with Opinions</title>
      <author><first>Yuxiang</first><last>Zhou</last></author>
      <author><first>Lejian</first><last>Liao</last></author>
      <author><first>Yang</first><last>Gao</last></author>
      <author><first>Zhanming</first><last>Jie</last></author>
      <author><first>Wei</first><last>Lu</last></author>
      <pages>3899–3909</pages>
      <abstract>Dependency parse trees are helpful for discovering the opinion words in aspect-based sentiment analysis (ABSA) (CITATION). However, the trees obtained from off-the-shelf dependency parsers are static, and could be sub-optimal in ABSA. This is because the syntactic trees are not designed for capturing the interactions between opinion words and aspect words. In this work, we aim to shorten the distance between aspects and corresponding opinion words by learning an aspect-centric tree structure. The aspect and opinion words are expected to be closer along such tree structure compared to the standard dependency parse tree. The learning process allows the tree structure to adaptively correlate the aspect and opinion words, enabling us to better identify the polarity in the ABSA task. We conduct experiments on five aspect-based sentiment datasets, and the proposed model significantly outperforms recent strong baselines. Furthermore, our thorough analysis demonstrates the average distance between aspect and opinion words are shortened by at least 19% on the standard SemEval Restaurant14 (CITATION) dataset.</abstract>
      <url hash="d4c66633">2021.emnlp-main.317</url>
      <attachment type="Software" hash="7213fab1">2021.emnlp-main.317.Software.zip</attachment>
      <bibkey>zhou-etal-2021-closer</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.317</doi>
      <video href="2021.emnlp-main.317.mp4"/>
      <pwccode url="https://github.com/zyxnlp/aclt" additional="false">zyxnlp/aclt</pwccode>
    </paper>
    <paper id="318">
      <title>Seeking Common but Distinguishing Difference, A Joint Aspect-based Sentiment Analysis Model</title>
      <author><first>Hongjiang</first><last>Jing</last></author>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <author><first>Shu</first><last>Jiang</last></author>
      <pages>3910–3922</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) task consists of three typical subtasks: aspect term extraction, opinion term extraction, and sentiment polarity classification. These three subtasks are usually performed jointly to save resources and reduce the error propagation in the pipeline. However, most of the existing joint models only focus on the benefits of encoder sharing between subtasks but ignore the difference. Therefore, we propose a joint ABSA model, which not only enjoys the benefits of encoder sharing but also focuses on the difference to improve the effectiveness of the model. In detail, we introduce a dual-encoder design, in which a pair encoder especially focuses on candidate aspect-opinion pair classification, and the original encoder keeps attention on sequence labeling. Empirical results show that our proposed model shows robustness and significantly outperforms the previous state-of-the-art on four benchmark datasets.</abstract>
      <url hash="e0b30006">2021.emnlp-main.318</url>
      <bibkey>jing-etal-2021-seeking</bibkey>
      <revision id="1" href="2021.emnlp-main.318v1" hash="981bdc4c"/>
      <revision id="2" href="2021.emnlp-main.318v2" hash="1b80efdd" date="2021-11-18">Deleted funding sources in acknowledgment.</revision>
      <doi>10.18653/v1/2021.emnlp-main.318</doi>
      <revision id="3" href="2021.emnlp-main.318v3" hash="e0b30006" date="2022-02-09">Updated Sections 2.2, 2.3, 3.1, Figure 2, Table 1, and Table 8.</revision>
      <video href="2021.emnlp-main.318.mp4"/>
      <pwccode url="https://github.com/betahj/pairabsa" additional="false">betahj/pairabsa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/aste-data-v2">ASTE-Data-V2</pwcdataset>
    </paper>
    <paper id="319">
      <title>Argument Pair Extraction with Mutual Guidance and Inter-sentence Relation Graph</title>
      <author><first>Jianzhu</first><last>Bao</last></author>
      <author><first>Bin</first><last>Liang</last></author>
      <author><first>Jingyi</first><last>Sun</last></author>
      <author><first>Yice</first><last>Zhang</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <pages>3923–3934</pages>
      <abstract>Argument pair extraction (APE) aims to extract interactive argument pairs from two passages of a discussion. Previous work studied this task in the context of peer review and rebuttal, and decomposed it into a sequence labeling task and a sentence relation classification task. However, despite the promising performance, such an approach obtains the argument pairs implicitly by the two decomposed tasks, lacking explicitly modeling of the argument-level interactions between argument pairs. In this paper, we tackle the APE task by a mutual guidance framework, which could utilize the information of an argument in one passage to guide the identification of arguments that can form pairs with it in another passage. In this manner, two passages can mutually guide each other in the process of APE. Furthermore, we propose an inter-sentence relation graph to effectively model the inter-relations between two sentences and thus facilitates the extraction of argument pairs. Our proposed method can better represent the holistic argument-level semantics and thus explicitly capture the complex correlations between argument pairs. Experimental results show that our approach significantly outperforms the current state-of-the-art model.</abstract>
      <url hash="50f124ad">2021.emnlp-main.319</url>
      <bibkey>bao-etal-2021-argument</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.319</doi>
      <video href="2021.emnlp-main.319.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/rr">RR</pwcdataset>
    </paper>
    <paper id="320">
      <title>Emotion Inference in Multi-Turn Conversations with Addressee-Aware Module and Ensemble Strategy</title>
      <author><first>Dayu</first><last>Li</last></author>
      <author><first>Xiaodan</first><last>Zhu</last></author>
      <author><first>Yang</first><last>Li</last></author>
      <author><first>Suge</first><last>Wang</last></author>
      <author><first>Deyu</first><last>Li</last></author>
      <author><first>Jian</first><last>Liao</last></author>
      <author><first>Jianxing</first><last>Zheng</last></author>
      <pages>3935–3941</pages>
      <abstract>Emotion inference in multi-turn conversations aims to predict the participant’s emotion in the next upcoming turn without knowing the participant’s response yet, and is a necessary step for applications such as dialogue planning. However, it is a severe challenge to perceive and reason about the future feelings of participants, due to the lack of utterance information from the future. Moreover, it is crucial for emotion inference to capture the characteristics of emotional propagation in conversations, such as persistence and contagiousness. In this study, we focus on investigating the task of emotion inference in multi-turn conversations by modeling the propagation of emotional states among participants in the conversation history, and propose an addressee-aware module to automatically learn whether the participant keeps the historical emotional state or is affected by others in the next upcoming turn. In addition, we propose an ensemble strategy to further enhance the model performance. Empirical studies on three different benchmark conversation datasets demonstrate the effectiveness of the proposed model over several strong baselines.</abstract>
      <url hash="b35a3a56">2021.emnlp-main.320</url>
      <bibkey>li-etal-2021-emotion</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.320</doi>
      <video href="2021.emnlp-main.320.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/emorynlp">EmoryNLP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
    </paper>
    <paper id="321">
      <title>Improving Federated Learning for Aspect-based Sentiment Analysis via Topic Memories</title>
      <author><first>Han</first><last>Qin</last></author>
      <author><first>Guimin</first><last>Chen</last></author>
      <author><first>Yuanhe</first><last>Tian</last></author>
      <author><first>Yan</first><last>Song</last></author>
      <pages>3942–3954</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) predicts the sentiment polarity towards a particular aspect term in a sentence, which is an important task in real-world applications. To perform ABSA, the trained model is required to have a good understanding of the contextual information, especially the particular patterns that suggest the sentiment polarity. However, these patterns typically vary in different sentences, especially when the sentences come from different sources (domains), which makes ABSA still very challenging. Although combining labeled data across different sources (domains) is a promising solution to address the challenge, in practical applications, these labeled data are usually stored at different locations and might be inaccessible to each other due to privacy or legal concerns (e.g., the data are owned by different companies). To address this issue and make the best use of all labeled data, we propose a novel ABSA model with federated learning (FL) adopted to overcome the data isolation limitations and incorporate topic memory (TM) proposed to take the cases of data from diverse sources (domains) into consideration. Particularly, TM aims to identify different isolated data sources due to data inaccessibility by providing useful categorical information for localized predictions. Experimental results on a simulated environment for FL with three nodes demonstrate the effectiveness of our approach, where TM-FL outperforms different baselines including some well-designed FL frameworks.</abstract>
      <url hash="7975515f">2021.emnlp-main.321</url>
      <bibkey>qin-etal-2021-improving-federated</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.321</doi>
      <video href="2021.emnlp-main.321.mp4"/>
    </paper>
    <paper id="322">
      <title>Comparative Opinion Quintuple Extraction from Product Reviews</title>
      <author><first>Ziheng</first><last>Liu</last></author>
      <author><first>Rui</first><last>Xia</last></author>
      <author><first>Jianfei</first><last>Yu</last></author>
      <pages>3955–3965</pages>
      <abstract>As an important task in opinion mining, comparative opinion mining aims to identify comparative sentences from product reviews, extract the comparative elements, and obtain the corresponding comparative opinion tuples. However, most previous studies simply regarded comparative tuple extraction as comparative element extraction, but ignored the fact that many comparative sentences may contain multiple comparisons. The comparative opinion tuples defined in these studies also failed to explicitly provide comparative preferences. To address these limitations, in this work we first introduce a new Comparative Opinion Quintuple Extraction (COQE) task, to identify comparative sentences from product reviews and extract all comparative opinion quintuples (Subject, Object, Comparative Aspect, Comparative Opinion, Comparative Preference). Secondly, based on the existing comparative opinion mining corpora, we make supplementary annotations and construct three datasets for the COQE task. Finally, we benchmark the COQE task by proposing a new BERT-based multi-stage approach as well as three baseline systems extended from previous methods. %The new approach significantly outperforms three baseline systems on three datasets and represents a strong benchmark for COQE. Experimental results show that the new approach significantly outperforms three baseline systems on three datasets for the COQE task.</abstract>
      <url hash="86a9c3ec">2021.emnlp-main.322</url>
      <bibkey>liu-etal-2021-comparative</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.322</doi>
      <video href="2021.emnlp-main.322.mp4"/>
      <pwccode url="https://github.com/nustm/coqe" additional="false">nustm/coqe</pwccode>
    </paper>
    <paper id="323">
      <title><fixed-case>CTAL</fixed-case>: Pre-training Cross-modal Transformer for Audio-and-Language Representations</title>
      <author><first>Hang</first><last>Li</last></author>
      <author><first>Wenbiao</first><last>Ding</last></author>
      <author><first>Yu</first><last>Kang</last></author>
      <author><first>Tianqiao</first><last>Liu</last></author>
      <author><first>Zhongqin</first><last>Wu</last></author>
      <author><first>Zitao</first><last>Liu</last></author>
      <pages>3966–3977</pages>
      <abstract>Existing audio-language task-specific predictive approaches focus on building complicated late-fusion mechanisms. However, these models are facing challenges of overfitting with limited labels and low model generalization abilities. In this paper, we present a Cross-modal Transformer for Audio-and-Language, i.e., CTAL, which aims to learn the intra-modality and inter-modality connections between audio and language through two proxy tasks on a large amount of audio-and-language pairs: masked language modeling and masked cross-modal acoustic modeling. After fine-tuning our pre-trained model on multiple downstream audio-and-language tasks, we observe significant improvements across various tasks, such as, emotion classification, sentiment analysis, and speaker verification. On this basis, we further propose a specially-designed fusion mechanism that can be used in fine-tuning phase, which allows our pre-trained model to achieve better performance. Lastly, we demonstrate detailed ablation studies to prove that both our novel cross-modality fusion component and audio-language pre-training methods significantly contribute to the promising results. The code and pre-trained models are available at https://github.com/tal-ai/CTAL_EMNLP2021.</abstract>
      <url hash="c4cd7d64">2021.emnlp-main.323</url>
      <bibkey>li-etal-2021-ctal</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.323</doi>
      <video href="2021.emnlp-main.323.mp4"/>
      <pwccode url="https://github.com/ydkwim/ctal" additional="false">ydkwim/ctal</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/iemocap">IEMOCAP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
    </paper>
    <paper id="324">
      <title>Relation-aware Video Reading Comprehension for Temporal Language Grounding</title>
      <author><first>Jialin</first><last>Gao</last></author>
      <author><first>Xin</first><last>Sun</last></author>
      <author><first>Mengmeng</first><last>Xu</last></author>
      <author><first>Xi</first><last>Zhou</last></author>
      <author><first>Bernard</first><last>Ghanem</last></author>
      <pages>3978–3988</pages>
      <abstract>Temporal language grounding in videos aims to localize the temporal span relevant to the given query sentence. Previous methods treat it either as a boundary regression task or a span extraction task. This paper will formulate temporal language grounding into video reading comprehension and propose a Relation-aware Network (RaNet) to address it. This framework aims to select a video moment choice from the predefined answer set with the aid of coarse-and-fine choice-query interaction and choice-choice relation construction. A choice-query interactor is proposed to match the visual and textual information simultaneously in sentence-moment and token-moment levels, leading to a coarse-and-fine cross-modal interaction. Moreover, a novel multi-choice relation constructor is introduced by leveraging graph convolution to capture the dependencies among video moment choices for the best choice selection. Extensive experiments on ActivityNet-Captions, TACoS, and Charades-STA demonstrate the effectiveness of our solution. Codes will be available at https://github.com/Huntersxsx/RaNet.</abstract>
      <url hash="bcdfc274">2021.emnlp-main.324</url>
      <bibkey>gao-etal-2021-relation</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.324</doi>
      <video href="2021.emnlp-main.324.mp4"/>
      <pwccode url="https://github.com/Huntersxsx/RaNet" additional="false">Huntersxsx/RaNet</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/activitynet-captions">ActivityNet Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/charades">Charades</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/charades-sta">Charades-STA</pwcdataset>
    </paper>
    <paper id="325">
      <title>Mutual-Learning Improves End-to-End Speech Translation</title>
      <author><first>Jiawei</first><last>Zhao</last></author>
      <author><first>Wei</first><last>Luo</last></author>
      <author><first>Boxing</first><last>Chen</last></author>
      <author><first>Andrew</first><last>Gilman</last></author>
      <pages>3989–3994</pages>
      <abstract>A currently popular research area in end-to-end speech translation is the use of knowledge distillation from a machine translation (MT) task to improve the speech translation (ST) task. However, such scenario obviously only allows one way transfer, which is limited by the performance of the teacher model. Therefore, We hypothesis that the knowledge distillation-based approaches are sub-optimal. In this paper, we propose an alternative–a trainable mutual-learning scenario, where the MT and the ST models are collaboratively trained and are considered as peers, rather than teacher/student. This allows us to improve the performance of end-to-end ST more effectively than with a teacher-student paradigm. As a side benefit, performance of the MT model also improves. Experimental results show that in our mutual-learning scenario, models can effectively utilise the auxiliary information from peer models and achieve compelling results on Must-C dataset.</abstract>
      <url hash="29196ee5">2021.emnlp-main.325</url>
      <bibkey>zhao-etal-2021-mutual</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.325</doi>
      <video href="2021.emnlp-main.325.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/must-c">MuST-C</pwcdataset>
    </paper>
    <paper id="326">
      <title>Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization</title>
      <author><first>Tiezheng</first><last>Yu</last></author>
      <author><first>Wenliang</first><last>Dai</last></author>
      <author><first>Zihan</first><last>Liu</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>3995–4007</pages>
      <abstract>Multimodal abstractive summarization (MAS) models that summarize videos (vision modality) and their corresponding transcripts (text modality) are able to extract the essential information from massive multimodal data on the Internet. Recently, large-scale generative pre-trained language models (GPLMs) have been shown to be effective in text generation tasks. However, existing MAS models cannot leverage GPLMs’ powerful generation ability. To fill this research gap, we aim to study two research questions: 1) how to inject visual information into GPLMs without hurting their generation ability; and 2) where is the optimal place in GPLMs to inject the visual information? In this paper, we present a simple yet effective method to construct vision guided (VG) GPLMs for the MAS task using attention-based add-on layers to incorporate visual information while maintaining their original text generation ability. Results show that our best model significantly surpasses the prior state-of-the-art model by 5.7 ROUGE-1, 5.3 ROUGE-2, and 5.1 ROUGE-L scores on the How2 dataset, and our vision guidance method contributes 83.6% of the overall improvement. Furthermore, we conduct thorough ablation studies to analyze the effectiveness of various modality fusion methods and fusion locations.</abstract>
      <url hash="582297d0">2021.emnlp-main.326</url>
      <attachment type="Software" hash="cf748908">2021.emnlp-main.326.Software.zip</attachment>
      <bibkey>yu-etal-2021-vision</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.326</doi>
      <video href="2021.emnlp-main.326.mp4"/>
      <pwccode url="https://github.com/hltchkust/vg-gplms" additional="false">hltchkust/vg-gplms</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/how2">How2</pwcdataset>
    </paper>
    <paper id="327">
      <title>Natural Language Video Localization with Learnable Moment Proposals</title>
      <author><first>Shaoning</first><last>Xiao</last></author>
      <author><first>Long</first><last>Chen</last></author>
      <author><first>Jian</first><last>Shao</last></author>
      <author><first>Yueting</first><last>Zhuang</last></author>
      <author><first>Jun</first><last>Xiao</last></author>
      <pages>4008–4017</pages>
      <abstract>Given an untrimmed video and a natural language query, Natural Language Video Localization (NLVL) aims to identify the video moment described by query. To address this task, existing methods can be roughly grouped into two groups: 1) propose-and-rank models first define a set of hand-designed moment candidates and then find out the best-matching one. 2) proposal-free models directly predict two temporal boundaries of the referential moment from frames. Currently, almost all the propose-and-rank methods have inferior performance than proposal-free counterparts. In this paper, we argue that the performance of propose-and-rank models are underestimated due to the predefined manners: 1) Hand-designed rules are hard to guarantee the complete coverage of targeted segments. 2) Densely sampled candidate moments cause redundant computation and degrade the performance of ranking process. To this end, we propose a novel model termed LPNet (Learnable Proposal Network for NLVL) with a fixed set of learnable moment proposals. The position and length of these proposals are dynamically adjusted during training process. Moreover, a boundary-aware loss has been proposed to leverage frame-level information and further improve performance. Extensive ablations on two challenging NLVL benchmarks have demonstrated the effectiveness of LPNet over existing state-of-the-art methods.</abstract>
      <url hash="e06416f4">2021.emnlp-main.327</url>
      <bibkey>xiao-etal-2021-natural</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.327</doi>
      <video href="2021.emnlp-main.327.mp4"/>
      <pwccode url="https://github.com/xiaoneil/lpnet" additional="false">xiaoneil/lpnet</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/activitynet-captions">ActivityNet Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/charades-sta">Charades-STA</pwcdataset>
    </paper>
    <paper id="328">
      <title>Language-Aligned Waypoint (<fixed-case>LAW</fixed-case>) Supervision for Vision-and-Language Navigation in Continuous Environments</title>
      <author><first>Sonia</first><last>Raychaudhuri</last></author>
      <author><first>Saim</first><last>Wani</last></author>
      <author><first>Shivansh</first><last>Patel</last></author>
      <author><first>Unnat</first><last>Jain</last></author>
      <author><first>Angel</first><last>Chang</last></author>
      <pages>4018–4028</pages>
      <abstract>In the Vision-and-Language Navigation (VLN) task an embodied agent navigates a 3D environment, following natural language instructions. A challenge in this task is how to handle ‘off the path’ scenarios where an agent veers from a reference path. Prior work supervises the agent with actions based on the shortest path from the agent’s location to the goal, but such goal-oriented supervision is often not in alignment with the instruction. Furthermore, the evaluation metrics employed by prior work do not measure how much of a language instruction the agent is able to follow. In this work, we propose a simple and effective language-aligned supervision scheme, and a new metric that measures the number of sub-instructions the agent has completed during navigation.</abstract>
      <url hash="342014f2">2021.emnlp-main.328</url>
      <bibkey>raychaudhuri-etal-2021-language</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.328</doi>
      <video href="2021.emnlp-main.328.mp4"/>
    </paper>
    <paper id="329">
      <title>How to leverage the multimodal <fixed-case>EHR</fixed-case> data for better medical prediction?</title>
      <author><first>Bo</first><last>Yang</last></author>
      <author><first>Lijun</first><last>Wu</last></author>
      <pages>4029–4038</pages>
      <abstract>Healthcare is becoming a more and more important research topic recently. With the growing data in the healthcare domain, it offers a great opportunity for deep learning to improve the quality of service and reduce costs. However, the complexity of electronic health records (EHR) data is a challenge for the application of deep learning. Specifically, the data produced in the hospital admissions are monitored by the EHR system, which includes structured data like daily body temperature and unstructured data like free text and laboratory measurements. Although there are some preprocessing frameworks proposed for specific EHR data, the clinical notes that contain significant clinical value are beyond the realm of their consideration. Besides, whether these different data from various views are all beneficial to the medical tasks and how to best utilize these data remain unclear. Therefore, in this paper, we first extract the accompanying clinical notes from EHR and propose a method to integrate these data, we also comprehensively study the different models and the data leverage methods for better medical task prediction performance. The results on two prediction tasks show that our fused model with different data outperforms the state-of-the-art method without clinical notes, which illustrates the importance of our fusion method and the clinical note features.</abstract>
      <url hash="e3eea29c">2021.emnlp-main.329</url>
      <bibkey>yang-wu-2021-leverage</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.329</doi>
      <video href="2021.emnlp-main.329.mp4"/>
    </paper>
    <paper id="330">
      <title>Considering Nested Tree Structure in Sentence Extractive Summarization with Pre-trained Transformer</title>
      <author><first>Jingun</first><last>Kwon</last></author>
      <author><first>Naoki</first><last>Kobayashi</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last></author>
      <author><first>Manabu</first><last>Okumura</last></author>
      <pages>4039–4044</pages>
      <abstract>Sentence extractive summarization shortens a document by selecting sentences for a summary while preserving its important contents. However, constructing a coherent and informative summary is difficult using a pre-trained BERT-based encoder since it is not explicitly trained for representing the information of sentences in a document. We propose a nested tree-based extractive summarization model on RoBERTa (NeRoBERTa), where nested tree structures consist of syntactic and discourse trees in a given document. Experimental results on the CNN/DailyMail dataset showed that NeRoBERTa outperforms baseline models in ROUGE. Human evaluation results also showed that NeRoBERTa achieves significantly better scores than the baselines in terms of coherence and yields comparable scores to the state-of-the-art models.</abstract>
      <url hash="463fd16e">2021.emnlp-main.330</url>
      <bibkey>kwon-etal-2021-considering</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.330</doi>
      <video href="2021.emnlp-main.330.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="331">
      <title>Frame Semantic-Enhanced Sentence Modeling for Sentence-level Extractive Text Summarization</title>
      <author><first>Yong</first><last>Guan</last></author>
      <author><first>Shaoru</first><last>Guo</last></author>
      <author><first>Ru</first><last>Li</last></author>
      <author><first>Xiaoli</first><last>Li</last></author>
      <author><first>Hongye</first><last>Tan</last></author>
      <pages>4045–4052</pages>
      <abstract>Sentence-level extractive text summarization aims to select important sentences from a given document. However, it is very challenging to model the importance of sentences. In this paper, we propose a novel Frame Semantic-Enhanced Sentence Modeling for Extractive Summarization, which leverages Frame semantics to model sentences from both intra-sentence level and inter-sentence level, facilitating the text summarization task. In particular, intra-sentence level semantics leverage Frames and Frame Elements to model internal semantic structure within a sentence, while inter-sentence level semantics leverage Frame-to-Frame relations to model relationships among sentences. Extensive experiments on two benchmark corpus CNN/DM and NYT demonstrate that our model outperforms six state-of-the-art methods significantly.</abstract>
      <url hash="a1101768">2021.emnlp-main.331</url>
      <bibkey>guan-etal-2021-frame</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.331</doi>
      <video href="2021.emnlp-main.331.mp4"/>
    </paper>
    <paper id="332">
      <title><fixed-case>CAST</fixed-case>: Enhancing Code Summarization with Hierarchical Splitting and Reconstruction of Abstract Syntax Trees</title>
      <author><first>Ensheng</first><last>Shi</last></author>
      <author><first>Yanlin</first><last>Wang</last></author>
      <author><first>Lun</first><last>Du</last></author>
      <author><first>Hongyu</first><last>Zhang</last></author>
      <author><first>Shi</first><last>Han</last></author>
      <author><first>Dongmei</first><last>Zhang</last></author>
      <author><first>Hongbin</first><last>Sun</last></author>
      <pages>4053–4062</pages>
      <abstract>Code summarization aims to generate concise natural language descriptions of source code, which can help improve program comprehension and maintenance. Recent studies show that syntactic and structural information extracted from abstract syntax trees (ASTs) is conducive to summary generation. However, existing approaches fail to fully capture the rich information in ASTs because of the large size/depth of ASTs. In this paper, we propose a novel model CAST that hierarchically splits and reconstructs ASTs. First, we hierarchically split a large AST into a set of subtrees and utilize a recursive neural network to encode the subtrees. Then, we aggregate the embeddings of subtrees by reconstructing the split ASTs to get the representation of the complete AST. Finally, AST representation, together with source code embedding obtained by a vanilla code token encoder, is used for code summarization. Extensive experiments, including the ablation study and the human evaluation, on benchmarks have demonstrated the power of CAST. To facilitate reproducibility, our code and data are available at https://github.com/DeepSoftwareAnalytics/CAST.</abstract>
      <url hash="99a6e237">2021.emnlp-main.332</url>
      <bibkey>shi-etal-2021-cast</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.332</doi>
      <video href="2021.emnlp-main.332.mp4"/>
      <pwccode url="https://github.com/DeepSoftwareAnalytics/CAST" additional="false">DeepSoftwareAnalytics/CAST</pwccode>
    </paper>
    <paper id="333">
      <title><fixed-case>S</fixed-case>g<fixed-case>S</fixed-case>um:Transforming Multi-document Summarization into Sub-graph Selection</title>
      <author><first>Moye</first><last>Chen</last></author>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Jiachen</first><last>Liu</last></author>
      <author><first>Xinyan</first><last>Xiao</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <pages>4063–4074</pages>
      <abstract>Most of existing extractive multi-document summarization (MDS) methods score each sentence individually and extract salient sentences one by one to compose a summary, which have two main drawbacks: (1) neglecting both the intra and cross-document relations between sentences; (2) neglecting the coherence and conciseness of the whole summary. In this paper, we propose a novel MDS framework (SgSum) to formulate the MDS task as a sub-graph selection problem, in which source documents are regarded as a relation graph of sentences (e.g., similarity graph or discourse graph) and the candidate summaries are its sub-graphs. Instead of selecting salient sentences, SgSum selects a salient sub-graph from the relation graph as the summary. Comparing with traditional methods, our method has two main advantages: (1) the relations between sentences are captured by modeling both the graph structure of the whole document set and the candidate sub-graphs; (2) directly outputs an integrate summary in the form of sub-graph which is more informative and coherent. Extensive experiments on MultiNews and DUC datasets show that our proposed method brings substantial improvements over several strong baselines. Human evaluation results also demonstrate that our model can produce significantly more coherent and informative summaries compared with traditional MDS methods. Moreover, the proposed architecture has strong transfer ability from single to multi-document input, which can reduce the resource bottleneck in MDS tasks.</abstract>
      <url hash="ac499aee">2021.emnlp-main.333</url>
      <bibkey>chen-etal-2021-sgsum</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.333</doi>
      <video href="2021.emnlp-main.333.mp4"/>
      <pwccode url="https://github.com/PaddlePaddle/Research/tree/master/NLP/EMNLP2021-SgSum" additional="false">PaddlePaddle/Research</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multi-news">Multi-News</pwcdataset>
    </paper>
    <paper id="334">
      <title>Event Graph based Sentence Fusion</title>
      <author><first>Ruifeng</first><last>Yuan</last></author>
      <author><first>Zili</first><last>Wang</last></author>
      <author><first>Wenjie</first><last>Li</last></author>
      <pages>4075–4084</pages>
      <abstract>Sentence fusion is a conditional generation task that merges several related sentences into a coherent one, which can be deemed as a summary sentence. The importance of sentence fusion has long been recognized by communities in natural language generation, especially in text summarization. It remains challenging for a state-of-the-art neural abstractive summarization model to generate a well-integrated summary sentence. In this paper, we explore the effective sentence fusion method in the context of text summarization. We propose to build an event graph from the input sentences to effectively capture and organize related events in a structured way and use the constructed event graph to guide sentence fusion. In addition to make use of the attention over the content of sentences and graph nodes, we further develop a graph flow attention mechanism to control the fusion process via the graph structure. When evaluated on sentence fusion data built from two summarization datasets, CNN/DaliyMail and Multi-News, our model shows to achieve state-of-the-art performance in terms of Rouge and other metrics like fusion rate and faithfulness.</abstract>
      <url hash="b60700f1">2021.emnlp-main.334</url>
      <attachment type="Software" hash="8ecc98de">2021.emnlp-main.334.Software.zip</attachment>
      <bibkey>yuan-etal-2021-event</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.334</doi>
      <video href="2021.emnlp-main.334.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/multi-news">Multi-News</pwcdataset>
    </paper>
    <paper id="335">
      <title>Transformer-based Lexically Constrained Headline Generation</title>
      <author><first>Kosuke</first><last>Yamada</last></author>
      <author><first>Yuta</first><last>Hitomi</last></author>
      <author><first>Hideaki</first><last>Tamori</last></author>
      <author><first>Ryohei</first><last>Sasano</last></author>
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <author><first>Koichi</first><last>Takeda</last></author>
      <pages>4085–4090</pages>
      <abstract>This paper explores a variant of automatic headline generation methods, where a generated headline is required to include a given phrase such as a company or a product name. Previous methods using Transformer-based models generate a headline including a given phrase by providing the encoder with additional information corresponding to the given phrase. However, these methods cannot always include the phrase in the generated headline. Inspired by previous RNN-based methods generating token sequences in backward and forward directions from the given phrase, we propose a simple Transformer-based method that guarantees to include the given phrase in the high-quality generated headline. We also consider a new headline generation strategy that takes advantage of the controllable generation order of Transformer. Our experiments with the Japanese News Corpus demonstrate that our methods, which are guaranteed to include the phrase in the generated headline, achieve ROUGE scores comparable to previous Transformer-based methods. We also show that our generation strategy performs better than previous strategies.</abstract>
      <url hash="07446807">2021.emnlp-main.335</url>
      <bibkey>yamada-etal-2021-transformer</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.335</doi>
      <video href="2021.emnlp-main.335.mp4"/>
      <pwccode url="https://github.com/asahi-research/script-for-transformer-based-seq2bf" additional="false">asahi-research/script-for-transformer-based-seq2bf</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/jnc">JNC</pwcdataset>
    </paper>
    <paper id="336">
      <title>Learn to Copy from the Copying History: Correlational Copy Network for Abstractive Summarization</title>
      <author><first>Haoran</first><last>Li</last></author>
      <author><first>Song</first><last>Xu</last></author>
      <author><first>Peng</first><last>Yuan</last></author>
      <author><first>Yujia</first><last>Wang</last></author>
      <author><first>Youzheng</first><last>Wu</last></author>
      <author><first>Xiaodong</first><last>He</last></author>
      <author><first>Bowen</first><last>Zhou</last></author>
      <pages>4091–4101</pages>
      <abstract>The copying mechanism has had considerable success in abstractive summarization, facilitating models to directly copy words from the input text to the output summary. Existing works mostly employ encoder-decoder attention, which applies copying at each time step independently of the former ones. However, this may sometimes lead to incomplete copying. In this paper, we propose a novel copying scheme named Correlational Copying Network (CoCoNet) that enhances the standard copying mechanism by keeping track of the copying history. It thereby takes advantage of prior copying distributions and, at each time step, explicitly encourages the model to copy the input word that is relevant to the previously copied one. In addition, we strengthen CoCoNet through pre-training with suitable corpora that simulate the copying behaviors. Experimental results show that CoCoNet can copy more accurately and achieves new state-of-the-art performances on summarization benchmarks, including CNN/DailyMail for news summarization and SAMSum for dialogue summarization. The code and checkpoint will be publicly available.</abstract>
      <url hash="58a606c5">2021.emnlp-main.336</url>
      <bibkey>li-etal-2021-learn</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.336</doi>
      <video href="2021.emnlp-main.336.mp4"/>
      <pwccode url="https://github.com/hrlinlp/coconet" additional="false">hrlinlp/coconet</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/samsum-corpus">SAMSum Corpus</pwcdataset>
    </paper>
    <paper id="337">
      <title>Gradient-Based Adversarial Factual Consistency Evaluation for Abstractive Summarization</title>
      <author><first>Zhiyuan</first><last>Zeng</last></author>
      <author><first>Jiaze</first><last>Chen</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>4102–4108</pages>
      <abstract>Neural abstractive summarization systems have gained significant progress in recent years. However, abstractive summarization often produce inconsisitent statements or false facts. How to automatically generate highly abstract yet factually correct summaries? In this paper, we proposed an efficient weak-supervised adversarial data augmentation approach to form the factual consistency dataset. Based on the artificial dataset, we train an evaluation model that can not only make accurate and robust factual consistency discrimination but is also capable of making interpretable factual errors tracing by backpropagated gradient distribution on token embeddings. Experiments and analysis conduct on public annotated summarization and factual consistency datasets demonstrate our approach effective and reasonable.</abstract>
      <url hash="fef4caa7">2021.emnlp-main.337</url>
      <bibkey>zeng-etal-2021-gradient</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.337</doi>
      <video href="2021.emnlp-main.337.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="338">
      <title>Word Reordering for Zero-shot Cross-lingual Structured Prediction</title>
      <author><first>Tao</first><last>Ji</last></author>
      <author><first>Yong</first><last>Jiang</last></author>
      <author><first>Tao</first><last>Wang</last></author>
      <author><first>Zhongqiang</first><last>Huang</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Yuanbin</first><last>Wu</last></author>
      <author><first>Xiaoling</first><last>Wang</last></author>
      <pages>4109–4120</pages>
      <abstract>Adapting word order from one language to another is a key problem in cross-lingual structured prediction. Current sentence encoders (e.g., RNN, Transformer with position embeddings) are usually word order sensitive. Even with uniform word form representations (MUSE, mBERT), word order discrepancies may hurt the adaptation of models. In this paper, we build structured prediction models with bag-of-words inputs, and introduce a new reordering module to organizing words following the source language order, which learns task-specific reordering strategies from a general-purpose order predictor model. Experiments on zero-shot cross-lingual dependency parsing, POS tagging, and morphological tagging show that our model can significantly improve target language performances, especially for languages that are distant from the source language.</abstract>
      <url hash="bf851f2b">2021.emnlp-main.338</url>
      <attachment type="Software" hash="b1bde183">2021.emnlp-main.338.Software.zip</attachment>
      <bibkey>ji-etal-2021-word</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.338</doi>
    </paper>
    <paper id="339">
      <title>A Unified Encoding of Structures in Transition Systems</title>
      <author><first>Tao</first><last>Ji</last></author>
      <author><first>Yong</first><last>Jiang</last></author>
      <author><first>Tao</first><last>Wang</last></author>
      <author><first>Zhongqiang</first><last>Huang</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Yuanbin</first><last>Wu</last></author>
      <author><first>Xiaoling</first><last>Wang</last></author>
      <pages>4121–4133</pages>
      <abstract>Transition systems usually contain various dynamic structures (e.g., stacks, buffers). An ideal transition-based model should encode these structures completely and efficiently. Previous works relying on templates or neural network structures either only encode partial structure information or suffer from computation efficiency. In this paper, we propose a novel attention-based encoder unifying representation of all structures in a transition system. Specifically, we separate two views of items on structures, namely structure-invariant view and structure-dependent view. With the help of parallel-friendly attention network, we are able to encoding transition states with O(1) additional complexity (with respect to basic feature extractors). Experiments on the PTB and UD show that our proposed method significantly improves the test speed and achieves the best transition-based model, and is comparable to state-of-the-art methods.</abstract>
      <url hash="0198ae65">2021.emnlp-main.339</url>
      <attachment type="Software" hash="2b236b7f">2021.emnlp-main.339.Software.zip</attachment>
      <bibkey>ji-etal-2021-unified</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.339</doi>
    </paper>
    <paper id="340">
      <title>Improving Unsupervised Question Answering via Summarization-Informed Question Generation</title>
      <author><first>Chenyang</first><last>Lyu</last></author>
      <author><first>Lifeng</first><last>Shang</last></author>
      <author><first>Yvette</first><last>Graham</last></author>
      <author><first>Jennifer</first><last>Foster</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>4134–4148</pages>
      <abstract>Question Generation (QG) is the task of generating a plausible question for a given &lt;passage, answer&gt; pair. Template-based QG uses linguistically-informed heuristics to transform declarative sentences into interrogatives, whereas supervised QG uses existing Question Answering (QA) datasets to train a system to generate a question given a passage and an answer. A disadvantage of the heuristic approach is that the generated questions are heavily tied to their declarative counterparts. A disadvantage of the supervised approach is that they are heavily tied to the domain/language of the QA dataset used as training data. In order to overcome these shortcomings, we propose a distantly-supervised QG method which uses questions generated heuristically from summaries as a source of training data for a QG system. We make use of freely available news summary data, transforming declarative summary sentences into appropriate questions using heuristics informed by dependency parsing, named entity recognition and semantic role labeling. The resulting questions are then combined with the original news articles to train an end-to-end neural QG model. We extrinsically evaluate our approach using unsupervised QA: our QG model is used to generate synthetic QA pairs for training a QA model. Experimental results show that, trained with only 20k English Wikipedia-based synthetic QA pairs, the QA model substantially outperforms previous unsupervised models on three in-domain datasets (SQuAD1.1, Natural Questions, TriviaQA) and three out-of-domain datasets (NewsQA, BioASQ, DuoRC), demonstrating the transferability of the approach.</abstract>
      <url hash="6a59894c">2021.emnlp-main.340</url>
      <bibkey>lyu-etal-2021-improving</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.340</doi>
      <video href="2021.emnlp-main.340.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/duorc">DuoRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="341">
      <title><fixed-case>T</fixed-case>ransfer<fixed-case>N</fixed-case>et: An Effective and Transparent Framework for Multi-hop Question Answering over Relation Graph</title>
      <author><first>Jiaxin</first><last>Shi</last></author>
      <author><first>Shulin</first><last>Cao</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Hanwang</first><last>Zhang</last></author>
      <pages>4149–4158</pages>
      <abstract>Multi-hop Question Answering (QA) is a challenging task because it requires precise reasoning with entity relations at every step towards the answer. The relations can be represented in terms of labels in knowledge graph (e.g., spouse) or text in text corpus (e.g., they have been married for 26 years). Existing models usually infer the answer by predicting the sequential relation path or aggregating the hidden graph features. The former is hard to optimize, and the latter lacks interpretability. In this paper, we propose TransferNet, an effective and transparent model for multi-hop QA, which supports both label and text relations in a unified framework. TransferNet jumps across entities at multiple steps. At each step, it attends to different parts of the question, computes activated scores for relations, and then transfer the previous entity scores along activated relations in a differentiable way. We carry out extensive experiments on three datasets and demonstrate that TransferNet surpasses the state-of-the-art models by a large margin. In particular, on MetaQA, it achieves 100% accuracy in 2-hop and 3-hop questions. By qualitative analysis, we show that TransferNet has transparent and interpretable intermediate results.</abstract>
      <url hash="1c9332f4">2021.emnlp-main.341</url>
      <bibkey>shi-etal-2021-transfernet</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.341</doi>
      <video href="2021.emnlp-main.341.mp4"/>
      <pwccode url="https://github.com/shijx12/TransferNet" additional="false">shijx12/TransferNet</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/metaqa">MetaQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/simplequestions">SimpleQuestions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimovies">WikiMovies</pwcdataset>
    </paper>
    <paper id="342">
      <title>Topic Transferable Table Question Answering</title>
      <author><first>Saneem</first><last>Chemmengath</last></author>
      <author><first>Vishwajeet</first><last>Kumar</last></author>
      <author><first>Samarth</first><last>Bharadwaj</last></author>
      <author><first>Jaydeep</first><last>Sen</last></author>
      <author><first>Mustafa</first><last>Canim</last></author>
      <author><first>Soumen</first><last>Chakrabarti</last></author>
      <author><first>Alfio</first><last>Gliozzo</last></author>
      <author><first>Karthik</first><last>Sankaranarayanan</last></author>
      <pages>4159–4172</pages>
      <abstract>Weakly-supervised table question-answering (TableQA) models have achieved state-of-art performance by using pre-trained BERT transformer to jointly encoding a question and a table to produce structured query for the question. However, in practical settings TableQA systems are deployed over table corpora having topic and word distributions quite distinct from BERT’s pretraining corpus. In this work we simulate the practical topic shift scenario by designing novel challenge benchmarks WikiSQL-TS and WikiTable-TS, consisting of train-dev-test splits in five distinct topic groups, based on the popular WikiSQL and WikiTable-Questions datasets. We empirically show that, despite pre-training on large open-domain text, performance of models degrades significantly when they are evaluated on unseen topics. In response, we propose T3QA (Topic Transferable Table Question Answering) a pragmatic adaptation framework for TableQA comprising of: (1) topic-specific vocabulary injection into BERT, (2) a novel text-to-text transformer generator (such as T5, GPT2) based natural language question generation pipeline focused on generating topic-specific training data, and (3) a logical form re-ranker. We show that T3QA provides a reasonably good baseline for our topic shift benchmarks. We believe our topic split benchmarks will lead to robust TableQA solutions that are better suited for practical deployment</abstract>
      <url hash="2c1cf0d2">2021.emnlp-main.342</url>
      <bibkey>chemmengath-etal-2021-topic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.342</doi>
      <video href="2021.emnlp-main.342.mp4"/>
      <pwccode url="https://github.com/ibm/t3qa" additional="false">ibm/t3qa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisql">WikiSQL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitablequestions">WikiTableQuestions</pwcdataset>
    </paper>
    <paper id="343">
      <title><fixed-case>W</fixed-case>eb<fixed-case>SRC</fixed-case>: A Dataset for Web-Based Structural Reading Comprehension</title>
      <author><first>Xingyu</first><last>Chen</last></author>
      <author><first>Zihan</first><last>Zhao</last></author>
      <author><first>Lu</first><last>Chen</last></author>
      <author><first>JiaBao</first><last>Ji</last></author>
      <author><first>Danyang</first><last>Zhang</last></author>
      <author><first>Ao</first><last>Luo</last></author>
      <author><first>Yuxuan</first><last>Xiong</last></author>
      <author><first>Kai</first><last>Yu</last></author>
      <pages>4173–4185</pages>
      <abstract>Web search is an essential way for humans to obtain information, but it’s still a great challenge for machines to understand the contents of web pages. In this paper, we introduce the task of web-based structural reading comprehension. Given a web page and a question about it, the task is to find an answer from the web page. This task requires a system not only to understand the semantics of texts but also the structure of the web page. Moreover, we proposed WebSRC, a novel Web-based Structural Reading Comprehension dataset. WebSRC consists of 400K question-answer pairs, which are collected from 6.4K web pages with corresponding HTML source code, screenshots, and metadata. Each question in WebSRC requires a certain structural understanding of a web page to answer, and the answer is either a text span on the web page or yes/no. We evaluate various strong baselines on our dataset to show the difficulty of our task. We also investigate the usefulness of structural information and visual features. Our dataset and baselines have been publicly available.</abstract>
      <url hash="12e79825">2021.emnlp-main.343</url>
      <bibkey>chen-etal-2021-websrc</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.343</doi>
      <video href="2021.emnlp-main.343.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="344">
      <title>Cryptonite: A Cryptic Crossword Benchmark for Extreme Ambiguity in Language</title>
      <author><first>Avia</first><last>Efrat</last></author>
      <author><first>Uri</first><last>Shaham</last></author>
      <author><first>Dan</first><last>Kilman</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <pages>4186–4192</pages>
      <abstract>Current NLP datasets targeting ambiguity can be solved by a native speaker with relative ease. We present Cryptonite, a large-scale dataset based on cryptic crosswords, which is both linguistically complex and naturally sourced. Each example in Cryptonite is a cryptic clue, a short phrase or sentence with a misleading surface reading, whose solving requires disambiguating semantic, syntactic, and phonetic wordplays, as well as world knowledge. Cryptic clues pose a challenge even for experienced solvers, though top-tier experts can solve them with almost 100% accuracy. Cryptonite is a challenging task for current models; fine-tuning T5-Large on 470k cryptic clues achieves only 7.6% accuracy, on par with the accuracy of a rule-based clue solver (8.6%).</abstract>
      <url hash="79aef513">2021.emnlp-main.344</url>
      <bibkey>efrat-etal-2021-cryptonite</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.344</doi>
      <video href="2021.emnlp-main.344.mp4"/>
      <pwccode url="https://github.com/aviaefrat/cryptonite" additional="false">aviaefrat/cryptonite</pwccode>
    </paper>
    <paper id="345">
      <title>End-to-End Entity Resolution and Question Answering Using Differentiable Knowledge Graphs</title>
      <author><first>Amir</first><last>Saffari</last></author>
      <author><first>Armin</first><last>Oliya</last></author>
      <author><first>Priyanka</first><last>Sen</last></author>
      <author><first>Tom</first><last>Ayoola</last></author>
      <pages>4193–4200</pages>
      <abstract>Recently, end-to-end (E2E) trained models for question answering over knowledge graphs (KGQA) have delivered promising results using only a weakly supervised dataset. However, these models are trained and evaluated in a setting where hand-annotated question entities are supplied to the model, leaving the important and non-trivial task of entity resolution (ER) outside the scope of E2E learning. In this work, we extend the boundaries of E2E learning for KGQA to include the training of an ER component. Our model only needs the question text and the answer entities to train, and delivers a stand-alone QA model that does not require an additional ER component to be supplied during runtime. Our approach is fully differentiable, thanks to its reliance on a recent method for building differentiable KGs (Cohen et al., 2020). We evaluate our E2E trained model on two public datasets and show that it comes close to baseline models that use hand-annotated entities.</abstract>
      <url hash="43007347">2021.emnlp-main.345</url>
      <bibkey>saffari-etal-2021-end</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.345</doi>
      <video href="2021.emnlp-main.345.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/simplequestions">SimpleQuestions</pwcdataset>
    </paper>
    <paper id="346">
      <title>Improving Query Graph Generation for Complex Question Answering over Knowledge Base</title>
      <author><first>Kechen</first><last>Qin</last></author>
      <author><first>Cheng</first><last>Li</last></author>
      <author><first>Virgil</first><last>Pavlu</last></author>
      <author><first>Javed</first><last>Aslam</last></author>
      <pages>4201–4207</pages>
      <abstract>Most of the existing Knowledge-based Question Answering (KBQA) methods first learn to map the given question to a query graph, and then convert the graph to an executable query to find the answer. The query graph is typically expanded progressively from the topic entity based on a sequence prediction model. In this paper, we propose a new solution to query graph generation that works in the opposite manner: we start with the entire knowledge base and gradually shrink it to the desired query graph. This approach improves both the efficiency and the accuracy of query graph generation, especially for complex multi-hop questions. Experimental results show that our method achieves state-of-the-art performance on ComplexWebQuestion (CWQ) dataset.</abstract>
      <url hash="54ebb3cd">2021.emnlp-main.346</url>
      <bibkey>qin-etal-2021-improving-query</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.346</doi>
      <video href="2021.emnlp-main.346.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/simplequestions">SimpleQuestions</pwcdataset>
    </paper>
    <paper id="347">
      <title><fixed-case>D</fixed-case>isco<fixed-case>DVT</fixed-case>: <fixed-case>G</fixed-case>enerating Long Text with Discourse-Aware Discrete Variational Transformer</title>
      <author><first>Haozhe</first><last>Ji</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>4208–4224</pages>
      <abstract>Despite the recent advances in applying pre-trained language models to generate high-quality texts, generating long passages that maintain long-range coherence is yet challenging for these models. In this paper, we propose DiscoDVT, a discourse-aware discrete variational Transformer to tackle the incoherence issue. DiscoDVT learns a discrete variable sequence that summarizes the global structure of the text and then applies it to guide the generation process at each decoding step. To further embed discourse-aware information into the discrete latent representations, we introduce an auxiliary objective to model the discourse relations within the text. We conduct extensive experiments on two open story generation datasets and demonstrate that the latent codes learn meaningful correspondence to the discourse structures that guide the model to generate long texts with better long-range coherence.</abstract>
      <url hash="758ae64b">2021.emnlp-main.347</url>
      <bibkey>ji-huang-2021-discodvt</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.347</doi>
      <video href="2021.emnlp-main.347.mp4"/>
      <pwccode url="https://github.com/cdjhz/discodvt" additional="false">cdjhz/discodvt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/writingprompts">WritingPrompts</pwcdataset>
    </paper>
    <paper id="348">
      <title>Mathematical Word Problem Generation from Commonsense Knowledge Graph and Equations</title>
      <author><first>Tianqiao</first><last>Liu</last></author>
      <author><first>Qiang</first><last>Fang</last></author>
      <author><first>Wenbiao</first><last>Ding</last></author>
      <author><first>Hang</first><last>Li</last></author>
      <author><first>Zhongqin</first><last>Wu</last></author>
      <author><first>Zitao</first><last>Liu</last></author>
      <pages>4225–4240</pages>
      <abstract>There is an increasing interest in the use of mathematical word problem (MWP) generation in educational assessment. Different from standard natural question generation, MWP generation needs to maintain the underlying mathematical operations between quantities and variables, while at the same time ensuring the relevance between the output and the given topic. To address above problem, we develop an end-to-end neural model to generate diverse MWPs in real-world scenarios from commonsense knowledge graph and equations. The proposed model (1) learns both representations from edge-enhanced Levi graphs of symbolic equations and commonsense knowledge; (2) automatically fuses equation and commonsense knowledge information via a self-planning module when generating the MWPs. Experiments on an educational gold-standard set and a large-scale generated MWP set show that our approach is superior on the MWP generation task, and it outperforms the SOTA models in terms of both automatic evaluation metrics, i.e., BLEU-4, ROUGE-L, Self-BLEU, and human evaluation metrics, i.e., equation relevance, topic relevance, and language coherence. To encourage reproducible results, we make our code and MWP dataset public available at <url>https://github.com/tal-ai/MaKE_EMNLP2021</url>.</abstract>
      <url hash="170b9494">2021.emnlp-main.348</url>
      <bibkey>liu-etal-2021-mathematical</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.348</doi>
      <video href="2021.emnlp-main.348.mp4"/>
      <pwccode url="https://github.com/tal-ai/make_emnlp2021" additional="false">tal-ai/make_emnlp2021</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mawps">MAWPS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mathqa">MathQA</pwcdataset>
    </paper>
    <paper id="349">
      <title>Generic resources are what you need: Style transfer tasks without task-specific parallel training data</title>
      <author><first>Huiyuan</first><last>Lai</last></author>
      <author><first>Antonio</first><last>Toral</last></author>
      <author><first>Malvina</first><last>Nissim</last></author>
      <pages>4241–4254</pages>
      <abstract>Style transfer aims to rewrite a source text in a different target style while preserving its content. We propose a novel approach to this task that leverages generic resources, and without using any task-specific parallel (source–target) data outperforms existing unsupervised approaches on the two most popular style transfer tasks: formality transfer and polarity swap. In practice, we adopt a multi-step procedure which builds on a generic pre-trained sequence-to-sequence model (BART). First, we strengthen the model’s ability to rewrite by further pre-training BART on both an existing collection of generic paraphrases, as well as on synthetic pairs created using a general-purpose lexical resource. Second, through an iterative back-translation approach, we train two models, each in a transfer direction, so that they can provide each other with synthetically generated pairs, dynamically in the training process. Lastly, we let our best resulting model generate static synthetic pairs to be used in a supervised training regime. Besides methodology and state-of-the-art results, a core contribution of this work is a reflection on the nature of the two tasks we address, and how their differences are highlighted by their response to our approach.</abstract>
      <url hash="4b8c3c7b">2021.emnlp-main.349</url>
      <bibkey>lai-etal-2021-generic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.349</doi>
      <video href="2021.emnlp-main.349.mp4"/>
      <pwccode url="https://github.com/laihuiyuan/generic-resources-for-tst" additional="false">laihuiyuan/generic-resources-for-tst</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
    </paper>
    <paper id="350">
      <title>Revisiting Pivot-Based Paraphrase Generation: Language Is Not the Only Optional Pivot</title>
      <author><first>Yitao</first><last>Cai</last></author>
      <author><first>Yue</first><last>Cao</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>4255–4268</pages>
      <abstract>Paraphrases refer to texts that convey the same meaning with different expression forms. Pivot-based methods, also known as the round-trip translation, have shown promising results in generating high-quality paraphrases. However, existing pivot-based methods all rely on language as the pivot, where large-scale, high-quality parallel bilingual texts are required. In this paper, we explore the feasibility of using semantic and syntactic representations as the pivot for paraphrase generation. Concretely, we transform a sentence into a variety of different semantic or syntactic representations (including AMR, UD, and latent semantic representation), and then decode the sentence back from the semantic representations. We further explore a pretraining-based approach to compress the pipeline process into an end-to-end framework. We conduct experiments comparing different approaches with different kinds of pivots. Experimental results show that taking AMR as pivot can obtain paraphrases with better quality than taking language as the pivot. The end-to-end framework can reduce semantic shift when language is used as the pivot. Besides, several unsupervised pivot-based methods can generate paraphrases with similar quality as the supervised sequence-to-sequence model, which indicates that parallel data of paraphrases may not be necessary for paraphrase generation.</abstract>
      <url hash="4f481e59">2021.emnlp-main.350</url>
      <bibkey>cai-etal-2021-revisiting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.350</doi>
      <video href="2021.emnlp-main.350.mp4"/>
    </paper>
    <paper id="351">
      <title>Structural Adapters in Pretrained Language Models for <fixed-case>AMR</fixed-case>-to-<fixed-case>T</fixed-case>ext Generation</title>
      <author><first>Leonardo F. R.</first><last>Ribeiro</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>4269–4282</pages>
      <abstract>Pretrained language models (PLM) have recently advanced graph-to-text generation, where the input graph is linearized into a sequence and fed into the PLM to obtain its representation. However, efficiently encoding the graph structure in PLMs is challenging because such models were pretrained on natural language, and modeling structured data may lead to catastrophic forgetting of distributional knowledge. In this paper, we propose StructAdapt, an adapter method to encode graph structure into PLMs. Contrary to prior work, StructAdapt effectively models interactions among the nodes based on the graph connectivity, only training graph structure-aware adapter parameters. In this way, we incorporate task-specific knowledge while maintaining the topological structure of the graph. We empirically show the benefits of explicitly encoding graph structure into PLMs using StructAdapt, outperforming the state of the art on two AMR-to-text datasets, training only 5.1% of the PLM parameters.</abstract>
      <url hash="0888b04a">2021.emnlp-main.351</url>
      <bibkey>ribeiro-etal-2021-structural</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.351</doi>
      <video href="2021.emnlp-main.351.mp4"/>
      <pwccode url="https://github.com/ukplab/structadapt" additional="false">ukplab/structadapt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/amr3-0">AMR3.0</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ldc2020t02">LDC2020T02</pwcdataset>
    </paper>
    <paper id="352">
      <title>Data-to-text Generation by Splicing Together Nearest Neighbors</title>
      <author><first>Sam</first><last>Wiseman</last></author>
      <author><first>Arturs</first><last>Backurs</last></author>
      <author><first>Karl</first><last>Stratos</last></author>
      <pages>4283–4299</pages>
      <abstract>We propose to tackle data-to-text generation tasks by directly splicing together retrieved segments of text from “neighbor” source-target pairs. Unlike recent work that conditions on retrieved neighbors but generates text token-by-token, left-to-right, we learn a policy that directly manipulates segments of neighbor text, by inserting or replacing them in partially constructed generations. Standard techniques for training such a policy require an oracle derivation for each generation, and we prove that finding the shortest such derivation can be reduced to parsing under a particular weighted context-free grammar. We find that policies learned in this way perform on par with strong baselines in terms of automatic and human evaluation, but allow for more interpretable and controllable generation.</abstract>
      <url hash="5aa404e4">2021.emnlp-main.352</url>
      <bibkey>wiseman-etal-2021-data</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.352</doi>
      <video href="2021.emnlp-main.352.mp4"/>
      <pwccode url="https://github.com/swiseman/neighbor-splicing" additional="false">swiseman/neighbor-splicing</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikibio">WikiBio</pwcdataset>
    </paper>
    <paper id="353">
      <title>Contextualize Knowledge Bases with Transformer for End-to-end Task-Oriented Dialogue Systems</title>
      <author><first>Yanjie</first><last>Gou</last></author>
      <author><first>Yinjie</first><last>Lei</last></author>
      <author><first>Lingqiao</first><last>Liu</last></author>
      <author><first>Yong</first><last>Dai</last></author>
      <author><first>Chunxu</first><last>Shen</last></author>
      <pages>4300–4310</pages>
      <abstract>Incorporating knowledge bases (KB) into end-to-end task-oriented dialogue systems is challenging, since it requires to properly represent the entity of KB, which is associated with its KB context and dialogue context. The existing works represent the entity with only perceiving a part of its KB context, which can lead to the less effective representation due to the information loss, and adversely favor KB reasoning and response generation. To tackle this issue, we explore to fully contextualize the entity representation by dynamically perceiving all the relevant entities and dialogue history. To achieve this, we propose a COntext-aware Memory Enhanced Transformer framework (COMET), which treats the KB as a sequence and leverages a novel Memory Mask to enforce the entity to only focus on its relevant entities and dialogue history, while avoiding the distraction from the irrelevant entities. Through extensive experiments, we show that our COMET framework can achieve superior performance over the state of the arts.</abstract>
      <url hash="ba4403de">2021.emnlp-main.353</url>
      <bibkey>gou-etal-2021-contextualize</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.353</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
    </paper>
    <paper id="354">
      <title>Efficient Dialogue Complementary Policy Learning via Deep <fixed-case>Q</fixed-case>-network Policy and Episodic Memory Policy</title>
      <author><first>Yangyang</first><last>Zhao</last></author>
      <author><first>Zhenyu</first><last>Wang</last></author>
      <author><first>Changxi</first><last>Zhu</last></author>
      <author><first>Shihan</first><last>Wang</last></author>
      <pages>4311–4323</pages>
      <abstract>Deep reinforcement learning has shown great potential in training dialogue policies. However, its favorable performance comes at the cost of many rounds of interaction. Most of the existing dialogue policy methods rely on a single learning system, while the human brain has two specialized learning and memory systems, supporting to find good solutions without requiring copious examples. Inspired by the human brain, this paper proposes a novel complementary policy learning (CPL) framework, which exploits the complementary advantages of the episodic memory (EM) policy and the deep Q-network (DQN) policy to achieve fast and effective dialogue policy learning. In order to coordinate between the two policies, we proposed a confidence controller to control the complementary time according to their relative efficacy at different stages. Furthermore, memory connectivity and time pruning are proposed to guarantee the flexible and adaptive generalization of the EM policy in dialog tasks. Experimental results on three dialogue datasets show that our method significantly outperforms existing methods relying on a single learning system.</abstract>
      <url hash="8b6f283a">2021.emnlp-main.354</url>
      <bibkey>zhao-etal-2021-efficient</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.354</doi>
    </paper>
    <paper id="355">
      <title><fixed-case>CRFR</fixed-case>: Improving Conversational Recommender Systems via Flexible Fragments Reasoning on Knowledge Graphs</title>
      <author><first>Jinfeng</first><last>Zhou</last></author>
      <author><first>Bo</first><last>Wang</last></author>
      <author><first>Ruifang</first><last>He</last></author>
      <author><first>Yuexian</first><last>Hou</last></author>
      <pages>4324–4334</pages>
      <abstract>Although paths of user interests shift in knowledge graphs (KGs) can benefit conversational recommender systems (CRS), explicit reasoning on KGs has not been well considered in CRS, due to the complex of high-order and incomplete paths. We propose CRFR, which effectively does explicit multi-hop reasoning on KGs with a conversational context-based reinforcement learning model. Considering the incompleteness of KGs, instead of learning single complete reasoning path, CRFR flexibly learns multiple reasoning fragments which are likely contained in the complete paths of interests shift. A fragments-aware unified model is then designed to fuse the fragments information from item-oriented and concept-oriented KGs to enhance the CRS response with entities and words from the fragments. Extensive experiments demonstrate CRFR’s SOTA performance on recommendation, conversation and conversation interpretability.</abstract>
      <url hash="df1167d3">2021.emnlp-main.355</url>
      <bibkey>zhou-etal-2021-crfr</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.355</doi>
      <video href="2021.emnlp-main.355.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/redial">ReDial</pwcdataset>
    </paper>
    <paper id="356">
      <title><fixed-case>D</fixed-case>u<fixed-case>R</fixed-case>ec<fixed-case>D</fixed-case>ial 2.0: A Bilingual Parallel Corpus for Conversational Recommendation</title>
      <author><first>Zeming</first><last>Liu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <author><first>Zheng-Yu</first><last>Niu</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Wanxiang</first><last>Che</last></author>
      <pages>4335–4347</pages>
      <abstract>In this paper, we provide a bilingual parallel human-to-human recommendation dialog dataset (DuRecDial 2.0) to enable researchers to explore a challenging task of multilingual and cross-lingual conversational recommendation. The difference between DuRecDial 2.0 and existing conversational recommendation datasets is that the data item (Profile, Goal, Knowledge, Context, Response) in DuRecDial 2.0 is annotated in two languages, both English and Chinese, while other datasets are built with the setting of a single language. We collect 8.2k dialogs aligned across English and Chinese languages (16.5k dialogs and 255k utterances in total) that are annotated by crowdsourced workers with strict quality control procedure. We then build monolingual, multilingual, and cross-lingual conversational recommendation baselines on DuRecDial 2.0. Experiment results show that the use of additional English data can bring performance improvement for Chinese conversational recommendation, indicating the benefits of DuRecDial 2.0. Finally, this dataset provides a challenging testbed for future studies of monolingual, multilingual, and cross-lingual conversational recommendation.</abstract>
      <url hash="d8bb4476">2021.emnlp-main.356</url>
      <bibkey>liu-etal-2021-durecdial</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.356</doi>
      <video href="2021.emnlp-main.356.mp4"/>
      <pwccode url="https://github.com/liuzeming01/durecdial" additional="false">liuzeming01/durecdial</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/durecdial">DuRecDial</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/redial">ReDial</pwcdataset>
    </paper>
    <paper id="357">
      <title>End-to-End Learning of Flowchart Grounded Task-Oriented Dialogs</title>
      <author><first>Dinesh</first><last>Raghu</last></author>
      <author><first>Shantanu</first><last>Agarwal</last></author>
      <author><first>Sachindra</first><last>Joshi</last></author>
      <author><first/><last>Mausam</last></author>
      <pages>4348–4366</pages>
      <abstract>We propose a novel problem within end-to-end learning of task oriented dialogs (TOD), in which the dialog system mimics a troubleshooting agent who helps a user by diagnosing their problem (e.g., car not starting). Such dialogs are grounded in domain-specific flowcharts, which the agent is supposed to follow during the conversation. Our task exposes novel technical challenges for neural TOD, such as grounding an utterance to the flowchart without explicit annotation, referring to additional manual pages when user asks a clarification question, and ability to follow unseen flowcharts at test time. We release a dataset (FLODIAL) consisting of 2,738 dialogs grounded on 12 different troubleshooting flowcharts. We also design a neural model, FLONET, which uses a retrieval-augmented generation architecture to train the dialog agent. Our experiments find that FLONET can do zero-shot transfer to unseen flowcharts, and sets a strong baseline for future research.</abstract>
      <url hash="9857fbd2">2021.emnlp-main.357</url>
      <bibkey>raghu-etal-2021-end</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.357</doi>
      <video href="2021.emnlp-main.357.mp4"/>
      <pwccode url="https://github.com/dair-iitd/flonet" additional="false">dair-iitd/flonet</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/flodial">FloDial</pwcdataset>
    </paper>
    <paper id="358">
      <title>Dimensional Emotion Detection from Categorical Emotion</title>
      <author><first>Sungjoon</first><last>Park</last></author>
      <author><first>Jiseon</first><last>Kim</last></author>
      <author><first>Seonghyeon</first><last>Ye</last></author>
      <author><first>Jaeyeol</first><last>Jeon</last></author>
      <author><first>Hee Young</first><last>Park</last></author>
      <author><first>Alice</first><last>Oh</last></author>
      <pages>4367–4380</pages>
      <abstract>We present a model to predict fine-grained emotions along the continuous dimensions of valence, arousal, and dominance (VAD) with a corpus with categorical emotion annotations. Our model is trained by minimizing the EMD (Earth Mover’s Distance) loss between the predicted VAD score distribution and the categorical emotion distributions sorted along VAD, and it can simultaneously classify the emotion categories and predict the VAD scores for a given sentence. We use pre-trained RoBERTa-Large and fine-tune on three different corpora with categorical labels and evaluate on EmoBank corpus with VAD scores. We show that our approach reaches comparable performance to that of the state-of-the-art classifiers in categorical emotion classification and shows significant positive correlations with the ground truth VAD scores. Also, further training with supervision of VAD labels leads to improved performance especially when dataset is small. We also present examples of predictions of appropriate emotion words that are not part of the original annotations.</abstract>
      <url hash="b0fec8db">2021.emnlp-main.358</url>
      <bibkey>park-etal-2021-dimensional</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.358</doi>
      <video href="2021.emnlp-main.358.mp4"/>
      <pwccode url="https://github.com/sungjoonpark/emotiondetection" additional="false">sungjoonpark/emotiondetection</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/emobank">EmoBank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/isear">ISEAR</pwcdataset>
    </paper>
    <paper id="359">
      <title>Not All Negatives are Equal: <fixed-case>L</fixed-case>abel-Aware Contrastive Loss for Fine-grained Text Classification</title>
      <author><first>Varsha</first><last>Suresh</last></author>
      <author><first>Desmond</first><last>Ong</last></author>
      <pages>4381–4394</pages>
      <abstract>Fine-grained classification involves dealing with datasets with larger number of classes with subtle differences between them. Guiding the model to focus on differentiating dimensions between these commonly confusable classes is key to improving performance on fine-grained tasks. In this work, we analyse the contrastive fine-tuning of pre-trained language models on two fine-grained text classification tasks, emotion classification and sentiment analysis. We adaptively embed class relationships into a contrastive objective function to help differently weigh the positives and negatives, and in particular, weighting closely confusable negatives more than less similar negative examples. We find that Label-aware Contrastive Loss outperforms previous contrastive methods, in the presence of larger number and/or more confusable classes, and helps models to produce output distributions that are more differentiated.</abstract>
      <url hash="dbdabe96">2021.emnlp-main.359</url>
      <bibkey>suresh-ong-2021-negatives</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.359</doi>
      <video href="2021.emnlp-main.359.mp4"/>
      <pwccode url="https://github.com/varsha33/lcl_loss" additional="false">varsha33/lcl_loss</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/goemotions">GoEmotions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/isear">ISEAR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="360">
      <title>Joint Multi-modal Aspect-Sentiment Analysis with Auxiliary Cross-modal Relation Detection</title>
      <author><first>Xincheng</first><last>Ju</last></author>
      <author><first>Dong</first><last>Zhang</last></author>
      <author><first>Rong</first><last>Xiao</last></author>
      <author><first>Junhui</first><last>Li</last></author>
      <author><first>Shoushan</first><last>Li</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Guodong</first><last>Zhou</last></author>
      <pages>4395–4405</pages>
      <abstract>Aspect terms extraction (ATE) and aspect sentiment classification (ASC) are two fundamental and fine-grained sub-tasks in aspect-level sentiment analysis (ALSA). In the textual analysis, joint extracting both aspect terms and sentiment polarities has been drawn much attention due to the better applications than individual sub-task. However, in the multi-modal scenario, the existing studies are limited to handle each sub-task independently, which fails to model the innate connection between the above two objectives and ignores the better applications. Therefore, in this paper, we are the first to jointly perform multi-modal ATE (MATE) and multi-modal ASC (MASC), and we propose a multi-modal joint learning approach with auxiliary cross-modal relation detection for multi-modal aspect-level sentiment analysis (MALSA). Specifically, we first build an auxiliary text-image relation detection module to control the proper exploitation of visual information. Second, we adopt the hierarchical framework to bridge the multi-modal connection between MATE and MASC, as well as separately visual guiding for each sub module. Finally, we can obtain all aspect-level sentiment polarities dependent on the jointly extracted specific aspects. Extensive experiments show the effectiveness of our approach against the joint textual approaches, pipeline and collapsed multi-modal approaches.</abstract>
      <url hash="84931f70">2021.emnlp-main.360</url>
      <bibkey>ju-etal-2021-joint</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.360</doi>
      <video href="2021.emnlp-main.360.mp4"/>
      <pwccode url="https://github.com/manlp-suda/jml" additional="false">manlp-suda/jml</pwccode>
    </paper>
    <paper id="361">
      <title>Solving Aspect Category Sentiment Analysis as a Text Generation Task</title>
      <author><first>Jian</first><last>Liu</last></author>
      <author><first>Zhiyang</first><last>Teng</last></author>
      <author><first>Leyang</first><last>Cui</last></author>
      <author><first>Hanmeng</first><last>Liu</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>4406–4416</pages>
      <abstract>Aspect category sentiment analysis has attracted increasing research attention. The dominant methods make use of pre-trained language models by learning effective aspect category-specific representations, and adding specific output layers to its pre-trained representation. We consider a more direct way of making use of pre-trained language models, by casting the ACSA tasks into natural language generation tasks, using natural language sentences to represent the output. Our method allows more direct use of pre-trained knowledge in seq2seq language models by directly following the task setting during pre-training. Experiments on several benchmarks show that our method gives the best reported results, having large advantages in few-shot and zero-shot settings.</abstract>
      <url hash="8f2fe667">2021.emnlp-main.361</url>
      <bibkey>liu-etal-2021-solving</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.361</doi>
      <video href="2021.emnlp-main.361.mp4"/>
      <pwccode url="https://github.com/lgw863/acsa-generation" additional="false">lgw863/acsa-generation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mams">MAMS</pwcdataset>
    </paper>
    <paper id="362">
      <title>Semantics-Preserved Data Augmentation for Aspect-Based Sentiment Analysis</title>
      <author><first>Ting-Wei</first><last>Hsu</last></author>
      <author><first>Chung-Chi</first><last>Chen</last></author>
      <author><first>Hen-Hsen</first><last>Huang</last></author>
      <author><first>Hsin-Hsi</first><last>Chen</last></author>
      <pages>4417–4422</pages>
      <abstract>Both the issues of data deficiencies and semantic consistency are important for data augmentation. Most of previous methods address the first issue, but ignore the second one. In the cases of aspect-based sentiment analysis, violation of the above issues may change the aspect and sentiment polarity. In this paper, we propose a semantics-preservation data augmentation approach by considering the importance of each word in a textual sequence according to the related aspects and sentiments. We then substitute the unimportant tokens with two replacement strategies without altering the aspect-level polarity. Our approach is evaluated on several publicly available sentiment analysis datasets and the real-world stock price/risk movement prediction scenarios. Experimental results show that our methodology achieves better performances in all datasets.</abstract>
      <url hash="bc36f227">2021.emnlp-main.362</url>
      <attachment type="Software" hash="67ccb15b">2021.emnlp-main.362.Software.zip</attachment>
      <bibkey>hsu-etal-2021-semantics</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.362</doi>
      <video href="2021.emnlp-main.362.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mams">MAMS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/stocknet-1">StockNet</pwcdataset>
    </paper>
    <paper id="363">
      <title>The Effect of Round-Trip Translation on Fairness in Sentiment Analysis</title>
      <author><first>Jonathan Gabel</first><last>Christiansen</last></author>
      <author><first>Mathias</first><last>Gammelgaard</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>4423–4428</pages>
      <abstract>Sentiment analysis systems have been shown to exhibit sensitivity to protected attributes. Round-trip translation, on the other hand, has been shown to normalize text. We explore the impact of round-trip translation on the demographic parity of sentiment classifiers and show how round-trip translation consistently improves classification fairness at test time (reducing up to 47% of between-group gaps). We also explore the idea of retraining sentiment classifiers on round-trip-translated data.</abstract>
      <url hash="be1a6c3e">2021.emnlp-main.363</url>
      <bibkey>christiansen-etal-2021-effect</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.363</doi>
      <video href="2021.emnlp-main.363.mp4"/>
    </paper>
    <paper id="364">
      <title><fixed-case>CH</fixed-case>o<fixed-case>R</fixed-case>a<fixed-case>L</fixed-case>: Collecting Humor Reaction Labels from Millions of Social Media Users</title>
      <author><first>Zixiaofan</first><last>Yang</last></author>
      <author><first>Shayan</first><last>Hooshmand</last></author>
      <author><first>Julia</first><last>Hirschberg</last></author>
      <pages>4429–4435</pages>
      <abstract>Humor detection has gained attention in recent years due to the desire to understand user-generated content with figurative language. However, substantial individual and cultural differences in humor perception make it very difficult to collect a large-scale humor dataset with reliable humor labels. We propose CHoRaL, a framework to generate perceived humor labels on Facebook posts, using the naturally available user reactions to these posts with no manual annotation needed. CHoRaL provides both binary labels and continuous scores of humor and non-humor. We present the largest dataset to date with labeled humor on 785K posts related to COVID-19. Additionally, we analyze the expression of COVID-related humor in social media by extracting lexico-semantic and affective features from the posts, and build humor detection models with performance similar to humans. CHoRaL enables the development of large-scale humor detection models on any topic and opens a new path to the study of humor on social media.</abstract>
      <url hash="05b67585">2021.emnlp-main.364</url>
      <bibkey>yang-etal-2021-choral</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.364</doi>
      <video href="2021.emnlp-main.364.mp4"/>
    </paper>
    <paper id="365">
      <title><fixed-case>CSDS</fixed-case>: A Fine-Grained <fixed-case>C</fixed-case>hinese Dataset for Customer Service Dialogue Summarization</title>
      <author><first>Haitao</first><last>Lin</last></author>
      <author><first>Liqun</first><last>Ma</last></author>
      <author><first>Junnan</first><last>Zhu</last></author>
      <author><first>Lu</first><last>Xiang</last></author>
      <author><first>Yu</first><last>Zhou</last></author>
      <author><first>Jiajun</first><last>Zhang</last></author>
      <author><first>Chengqing</first><last>Zong</last></author>
      <pages>4436–4451</pages>
      <abstract>Dialogue summarization has drawn much attention recently. Especially in the customer service domain, agents could use dialogue summaries to help boost their works by quickly knowing customer’s issues and service progress. These applications require summaries to contain the perspective of a single speaker and have a clear topic flow structure, while neither are available in existing datasets. Therefore, in this paper, we introduce a novel Chinese dataset for Customer Service Dialogue Summarization (CSDS). CSDS improves the abstractive summaries in two aspects: (1) In addition to the overall summary for the whole dialogue, role-oriented summaries are also provided to acquire different speakers’ viewpoints. (2) All the summaries sum up each topic separately, thus containing the topic-level structure of the dialogue. We define tasks in CSDS as generating the overall summary and different role-oriented summaries for a given dialogue. Next, we compare various summarization methods on CSDS, and experiment results show that existing methods are prone to generate redundant and incoherent summaries. Besides, the performance becomes much worse when analyzing the performance on role-oriented summaries and topic structures. We hope that this study could benchmark Chinese dialogue summarization and benefit further studies.</abstract>
      <url hash="b0fd9833">2021.emnlp-main.365</url>
      <bibkey>lin-etal-2021-csds</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.365</doi>
      <video href="2021.emnlp-main.365.mp4"/>
      <pwccode url="https://github.com/xiaolinAndy/CSDS" additional="false">xiaolinAndy/CSDS</pwccode>
    </paper>
    <paper id="366">
      <title><fixed-case>C</fixed-case>od<fixed-case>RED</fixed-case>: A Cross-Document Relation Extraction Dataset for Acquiring Knowledge in the Wild</title>
      <author><first>Yuan</first><last>Yao</last></author>
      <author><first>Jiaju</first><last>Du</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>4452–4472</pages>
      <abstract>Existing relation extraction (RE) methods typically focus on extracting relational facts between entity pairs within single sentences or documents. However, a large quantity of relational facts in knowledge bases can only be inferred across documents in practice. In this work, we present the problem of cross-document RE, making an initial step towards knowledge acquisition in the wild. To facilitate the research, we construct the first human-annotated cross-document RE dataset CodRED. Compared to existing RE datasets, CodRED presents two key challenges: Given two entities, (1) it requires finding the relevant documents that can provide clues for identifying their relations; (2) it requires reasoning over multiple documents to extract the relational facts. We conduct comprehensive experiments to show that CodRED is challenging to existing RE methods including strong BERT-based models.</abstract>
      <url hash="e524d1d5">2021.emnlp-main.366</url>
      <bibkey>yao-etal-2021-codred</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.366</doi>
      <video href="2021.emnlp-main.366.mp4"/>
      <pwccode url="https://github.com/thunlp/codred" additional="false">thunlp/codred</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bc5cdr">BC5CDR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/knowledgenet">KnowledgeNet</pwcdataset>
    </paper>
    <paper id="367">
      <title>Building and Evaluating Open-Domain Dialogue Corpora with Clarifying Questions</title>
      <author><first>Mohammad</first><last>Aliannejadi</last></author>
      <author><first>Julia</first><last>Kiseleva</last></author>
      <author><first>Aleksandr</first><last>Chuklin</last></author>
      <author><first>Jeff</first><last>Dalton</last></author>
      <author><first>Mikhail</first><last>Burtsev</last></author>
      <pages>4473–4484</pages>
      <abstract>Enabling open-domain dialogue systems to ask clarifying questions when appropriate is an important direction for improving the quality of the system response. Namely, for cases when a user request is not specific enough for a conversation system to provide an answer right away, it is desirable to ask a clarifying question to increase the chances of retrieving a satisfying answer. To address the problem of ‘asking clarifying questions in open-domain dialogues’: (1) we collect and release a new dataset focused on open-domain single- and multi-turn conversations, (2) we benchmark several state-of-the-art neural baselines, and (3) we propose a pipeline consisting of offline and online steps for evaluating the quality of clarifying questions in various dialogues. These contributions are suitable as a foundation for further research.</abstract>
      <url hash="6ff0caec">2021.emnlp-main.367</url>
      <bibkey>aliannejadi-etal-2021-building</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.367</doi>
      <video href="2021.emnlp-main.367.mp4"/>
      <pwccode url="https://github.com/aliannejadi/ClariQ" additional="false">aliannejadi/ClariQ</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/clariq">ClariQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qulac">Qulac</pwcdataset>
    </paper>
    <paper id="368">
      <title>We Need to Talk About train-dev-test Splits</title>
      <author><first>Rob</first><last>van der Goot</last></author>
      <pages>4485–4494</pages>
      <abstract>Standard train-dev-test splits used to benchmark multiple models against each other are ubiquitously used in Natural Language Processing (NLP). In this setup, the train data is used for training the model, the development set for evaluating different versions of the proposed model(s) during development, and the test set to confirm the answers to the main research question(s). However, the introduction of neural networks in NLP has led to a different use of these standard splits; the development set is now often used for model selection during the training procedure. Because of this, comparing multiple versions of the same model during development leads to overestimation on the development data. As an effect, people have started to compare an increasing amount of models on the test data, leading to faster overfitting and “expiration” of our test sets. We propose to use a tune-set when developing neural network methods, which can be used for model picking so that comparing the different versions of a new model can safely be done on the development data.</abstract>
      <url hash="8f826e85">2021.emnlp-main.368</url>
      <attachment type="Software" hash="8ea0c706">2021.emnlp-main.368.Software.tgz</attachment>
      <bibkey>van-der-goot-2021-need</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.368</doi>
      <video href="2021.emnlp-main.368.mp4"/>
      <pwccode url="https://bitbucket.org/robvanderg/tuneset" additional="false">robvanderg/tuneset</pwccode>
    </paper>
    <paper id="369">
      <title><fixed-case>P</fixed-case>ho<fixed-case>MT</fixed-case>: A High-Quality and Large-Scale Benchmark Dataset for <fixed-case>V</fixed-case>ietnamese-<fixed-case>E</fixed-case>nglish Machine Translation</title>
      <author><first>Long</first><last>Doan</last></author>
      <author><first>Linh The</first><last>Nguyen</last></author>
      <author><first>Nguyen Luong</first><last>Tran</last></author>
      <author><first>Thai</first><last>Hoang</last></author>
      <author><first>Dat Quoc</first><last>Nguyen</last></author>
      <pages>4495–4503</pages>
      <abstract>We introduce a high-quality and large-scale Vietnamese-English parallel dataset of 3.02M sentence pairs, which is 2.9M pairs larger than the benchmark Vietnamese-English machine translation corpus IWSLT15. We conduct experiments comparing strong neural baselines and well-known automatic translation engines on our dataset and find that in both automatic and human evaluations: the best performance is obtained by fine-tuning the pre-trained sequence-to-sequence denoising auto-encoder mBART. To our best knowledge, this is the first large-scale Vietnamese-English machine translation study. We hope our publicly available dataset and study can serve as a starting point for future research and applications on Vietnamese-English machine translation. We release our dataset at: https://github.com/VinAIResearch/PhoMT</abstract>
      <url hash="587dc8f9">2021.emnlp-main.369</url>
      <bibkey>doan-etal-2021-phomt</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.369</doi>
      <video href="2021.emnlp-main.369.mp4"/>
      <pwccode url="https://github.com/vinairesearch/phomt" additional="false">vinairesearch/phomt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/phomt">PhoMT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimatrix">WikiMatrix</pwcdataset>
    </paper>
    <paper id="370">
      <title>Lying Through One’s Teeth: A Study on Verbal Leakage Cues</title>
      <author><first>Min-Hsuan</first><last>Yeh</last></author>
      <author><first>Lun-Wei</first><last>Ku</last></author>
      <pages>4504–4510</pages>
      <abstract>Although many studies use the LIWC lexicon to show the existence of verbal leakage cues in lie detection datasets, none mention how verbal leakage cues are influenced by means of data collection, or the impact thereof on the performance of models. In this paper, we study verbal leakage cues to understand the effect of the data construction method on their significance, and examine the relationship between such cues and models’ validity. The LIWC word-category dominance scores of seven lie detection datasets are used to show that audio statements and lie-based annotations indicate a greater number of strong verbal leakage cue categories. Moreover, we evaluate the validity of state-of-the-art lie detection models with cross- and in-dataset testing. Results show that in both types of testing, models trained on a dataset with more strong verbal leakage cue categories—as opposed to only a greater number of strong cues—yield superior results, suggesting that verbal leakage cues are a key factor for selecting lie detection datasets.</abstract>
      <url hash="3c455fef">2021.emnlp-main.370</url>
      <bibkey>yeh-ku-2021-lying</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.370</doi>
      <video href="2021.emnlp-main.370.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/liar">LIAR</pwcdataset>
    </paper>
    <paper id="371">
      <title>Multi-granularity Textual Adversarial Attack with Behavior Cloning</title>
      <author><first>Yangyi</first><last>Chen</last></author>
      <author><first>Jin</first><last>Su</last></author>
      <author><first>Wei</first><last>Wei</last></author>
      <pages>4511–4526</pages>
      <abstract>Recently, the textual adversarial attack models become increasingly popular due to their successful in estimating the robustness of NLP models. However, existing works have obvious deficiencies. (1)They usually consider only a single granularity of modification strategies (e.g. word-level or sentence-level), which is insufficient to explore the holistic textual space for generation; (2) They need to query victim models hundreds of times to make a successful attack, which is highly inefficient in practice. To address such problems, in this paper we propose MAYA, a Multi-grAnularitY Attack model to effectively generate high-quality adversarial samples with fewer queries to victim models. Furthermore, we propose a reinforcement-learning based method to train a multi-granularity attack agent through behavior cloning with the expert knowledge from our MAYA algorithm to further reduce the query times. Additionally, we also adapt the agent to attack black-box models that only output labels without confidence scores. We conduct comprehensive experiments to evaluate our attack models by attacking BiLSTM, BERT and RoBERTa in two different black-box attack settings and three benchmark datasets. Experimental results show that our models achieve overall better attacking performance and produce more fluent and grammatical adversarial samples compared to baseline models. Besides, our adversarial attack agent significantly reduces the query times in both attack settings. Our codes are released at https://github.com/Yangyi-Chen/MAYA.</abstract>
      <url hash="d50154be">2021.emnlp-main.371</url>
      <bibkey>chen-etal-2021-multi</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.371</doi>
      <video href="2021.emnlp-main.371.mp4"/>
      <pwccode url="https://github.com/yangyi-chen/maya" additional="false">yangyi-chen/maya</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="372">
      <title>All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality</title>
      <author><first>William</first><last>Timkey</last></author>
      <author><first>Marten</first><last>van Schijndel</last></author>
      <pages>4527–4546</pages>
      <abstract>Similarity measures are a vital tool for understanding how language models represent and process language. Standard representational similarity measures such as cosine similarity and Euclidean distance have been successfully used in static word embedding models to understand how words cluster in semantic space. Recently, these measures have been applied to embeddings from contextualized models such as BERT and GPT-2. In this work, we call into question the informativity of such measures for contextualized language models. We find that a small number of rogue dimensions, often just 1-3, dominate these measures. Moreover, we find a striking mismatch between the dimensions that dominate similarity measures and those which are important to the behavior of the model. We show that simple postprocessing techniques such as standardization are able to correct for rogue dimensions and reveal underlying representational quality. We argue that accounting for rogue dimensions is essential for any similarity-based analysis of contextual language models.</abstract>
      <url hash="a3c7ad7b">2021.emnlp-main.372</url>
      <bibkey>timkey-van-schijndel-2021-bark</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.372</doi>
      <video href="2021.emnlp-main.372.mp4"/>
      <pwccode url="https://github.com/wtimkey/rogue-dimensions" additional="false">wtimkey/rogue-dimensions</pwccode>
    </paper>
    <paper id="373">
      <title><fixed-case>I</fixed-case>ncorporating <fixed-case>R</fixed-case>esidual and <fixed-case>N</fixed-case>ormalization <fixed-case>L</fixed-case>ayers into <fixed-case>A</fixed-case>nalysis of <fixed-case>M</fixed-case>asked <fixed-case>L</fixed-case>anguage <fixed-case>M</fixed-case>odels</title>
      <author><first>Goro</first><last>Kobayashi</last></author>
      <author><first>Tatsuki</first><last>Kuribayashi</last></author>
      <author><first>Sho</first><last>Yokoi</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>4547–4568</pages>
      <abstract>Transformer architecture has become ubiquitous in the natural language processing field. To interpret the Transformer-based models, their attention patterns have been extensively analyzed. However, the Transformer architecture is not only composed of the multi-head attention; other components can also contribute to Transformers’ progressive performance. In this study, we extended the scope of the analysis of Transformers from solely the attention patterns to the whole attention block, i.e., multi-head attention, residual connection, and layer normalization. Our analysis of Transformer-based masked language models shows that the token-to-token interaction performed via attention has less impact on the intermediate representations than previously assumed. These results provide new intuitive explanations of existing reports; for example, discarding the learned attention patterns tends not to adversely affect the performance. The codes of our experiments are publicly available.</abstract>
      <url hash="5af48518">2021.emnlp-main.373</url>
      <bibkey>kobayashi-etal-2021-incorporating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.373</doi>
      <video href="2021.emnlp-main.373.mp4"/>
      <pwccode url="https://github.com/gorokoba560/norm-analysis-of-transformer" additional="true">gorokoba560/norm-analysis-of-transformer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="374">
      <title>Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer</title>
      <author><first>Fanchao</first><last>Qi</last></author>
      <author><first>Yangyi</first><last>Chen</last></author>
      <author><first>Xurui</first><last>Zhang</last></author>
      <author><first>Mukai</first><last>Li</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>4569–4580</pages>
      <abstract>Adversarial attacks and backdoor attacks are two common security threats that hang over deep learning. Both of them harness task-irrelevant features of data in their implementation. Text style is a feature that is naturally irrelevant to most NLP tasks, and thus suitable for adversarial and backdoor attacks. In this paper, we make the first attempt to conduct adversarial and backdoor attacks based on text style transfer, which is aimed at altering the style of a sentence while preserving its meaning. We design an adversarial attack method and a backdoor attack method, and conduct extensive experiments to evaluate them. Experimental results show that popular NLP models are vulnerable to both adversarial and backdoor attacks based on text style transfer—the attack success rates can exceed 90% without much effort. It reflects the limited ability of NLP models to handle the feature of text style that has not been widely realized. In addition, the style transfer-based adversarial and backdoor attack methods show superiority to baselines in many aspects. All the code and data of this paper can be obtained at https://github.com/thunlp/StyleAttack.</abstract>
      <url hash="dd7a40e6">2021.emnlp-main.374</url>
      <bibkey>qi-etal-2021-mind</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.374</doi>
      <video href="2021.emnlp-main.374.mp4"/>
      <pwccode url="https://github.com/thunlp/styleattack" additional="false">thunlp/styleattack</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hate-speech">Hate Speech</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="375">
      <title>Sociolectal Analysis of Pretrained Language Models</title>
      <author><first>Sheng</first><last>Zhang</last></author>
      <author><first>Xin</first><last>Zhang</last></author>
      <author><first>Weiming</first><last>Zhang</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>4581–4588</pages>
      <abstract>Using data from English cloze tests, in which subjects also self-reported their gender, age, education, and race, we examine performance differences of pretrained language models across demographic groups, defined by these (protected) attributes. We demonstrate wide performance gaps across demographic groups and show that pretrained language models systematically disfavor young non-white male speakers; i.e., not only do pretrained language models learn social biases (stereotypical associations) – pretrained language models also learn sociolectal biases, learning to speak more like some than like others. We show, however, that, with the exception of BERT models, larger pretrained language models reduce some the performance gaps between majority and minority groups.</abstract>
      <url hash="81262ab0">2021.emnlp-main.375</url>
      <bibkey>zhang-etal-2021-sociolectal</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.375</doi>
      <video href="2021.emnlp-main.375.mp4"/>
    </paper>
    <paper id="376">
      <title>Examining Cross-lingual Contextual Embeddings with Orthogonal Structural Probes</title>
      <author><first>Tomasz</first><last>Limisiewicz</last></author>
      <author><first>David</first><last>Mareček</last></author>
      <pages>4589–4598</pages>
      <abstract>State-of-the-art contextual embeddings are obtained from large language models available only for a few languages. For others, we need to learn representations using a multilingual model. There is an ongoing debate on whether multilingual embeddings can be aligned in a space shared across many languages. The novel Orthogonal Structural Probe (Limisiewicz and Mareček, 2021) allows us to answer this question for specific linguistic features and learn a projection based only on mono-lingual annotated datasets. We evaluate syntactic (UD) and lexical (WordNet) structural information encoded inmBERT’s contextual representations for nine diverse languages. We observe that for languages closely related to English, no transformation is needed. The evaluated information is encoded in a shared cross-lingual embedding space. For other languages, it is beneficial to apply orthogonal transformation learned separately for each language. We successfully apply our findings to zero-shot and few-shot cross-lingual parsing.</abstract>
      <url hash="029bce0d">2021.emnlp-main.376</url>
      <bibkey>limisiewicz-marecek-2021-examining</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.376</doi>
      <video href="2021.emnlp-main.376.mp4"/>
    </paper>
    <paper id="377">
      <title>Are <fixed-case>T</fixed-case>ransformers a Modern Version of <fixed-case>ELIZA</fixed-case>? <fixed-case>O</fixed-case>bservations on <fixed-case>F</fixed-case>rench Object Verb Agreement</title>
      <author><first>Bingzhi</first><last>Li</last></author>
      <author><first>Guillaume</first><last>Wisniewski</last></author>
      <author><first>Benoit</first><last>Crabbé</last></author>
      <pages>4599–4610</pages>
      <abstract>Many recent works have demonstrated that unsupervised sentence representations of neural networks encode syntactic information by observing that neural language models are able to predict the agreement between a verb and its subject. We take a critical look at this line of research by showing that it is possible to achieve high accuracy on this agreement task with simple surface heuristics, indicating a possible flaw in our assessment of neural networks’ syntactic ability. Our fine-grained analyses of results on the long-range French object-verb agreement show that contrary to LSTMs, Transformers are able to capture a non-trivial amount of grammatical structure.</abstract>
      <url hash="5e9da7db">2021.emnlp-main.377</url>
      <bibkey>li-etal-2021-transformers</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.377</doi>
      <video href="2021.emnlp-main.377.mp4"/>
    </paper>
    <paper id="378">
      <title>Fine-grained Entity Typing via Label Reasoning</title>
      <author><first>Qing</first><last>Liu</last></author>
      <author><first>Hongyu</first><last>Lin</last></author>
      <author><first>Xinyan</first><last>Xiao</last></author>
      <author><first>Xianpei</first><last>Han</last></author>
      <author><first>Le</first><last>Sun</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <pages>4611–4622</pages>
      <abstract>Conventional entity typing approaches are based on independent classification paradigms, which make them difficult to recognize inter-dependent, long-tailed and fine-grained entity types. In this paper, we argue that the implicitly entailed extrinsic and intrinsic dependencies between labels can provide critical knowledge to tackle the above challenges. To this end, we propose Label Reasoning Network(LRN), which sequentially reasons fine-grained entity labels by discovering and exploiting label dependencies knowledge entailed in the data. Specifically, LRN utilizes an auto-regressive network to conduct deductive reasoning and a bipartite attribute graph to conduct inductive reasoning between labels, which can effectively model, learn and reason complex label dependencies in a sequence-to-set, end-to-end manner. Experiments show that LRN achieves the state-of-the-art performance on standard ultra fine-grained entity typing benchmarks, and can also resolve the long tail label problem effectively.</abstract>
      <url hash="6264b717">2021.emnlp-main.378</url>
      <bibkey>liu-etal-2021-fine</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.378</doi>
      <video href="2021.emnlp-main.378.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/open-entity-1">Open Entity</pwcdataset>
    </paper>
    <paper id="379">
      <title>Enhanced Language Representation with Label Knowledge for Span Extraction</title>
      <author><first>Pan</first><last>Yang</last></author>
      <author><first>Xin</first><last>Cong</last></author>
      <author><first>Zhenyu</first><last>Sun</last></author>
      <author><first>Xingwu</first><last>Liu</last></author>
      <pages>4623–4635</pages>
      <abstract>Span extraction, aiming to extract text spans (such as words or phrases) from plain text, is a fundamental process in Information Extraction. Recent works introduce the label knowledge to enhance the text representation by formalizing the span extraction task into a question answering problem (QA Formalization), which achieves state-of-the-art performance. However, such a QA Formalization does not fully exploit the label knowledge and causes a dramatic decrease in efficiency of training/inference. To address those problems, we introduce a fresh paradigm to integrate label knowledge and further propose a novel model to explicitly and efficiently integrate label knowledge into text representations. Specifically, it encodes texts and label annotations independently and then integrates label knowledge into text representation with an elaborate-designed semantics fusion module. We conduct extensive experiments on three typical span extraction tasks: flat NER, nested NER, and event detection. The empirical results show that 1) our model achieves a new state-of-the-art performance on four benchmarks, and 2) reduces training time and inference time by 76% and 77% on average, respectively, compared with the QA Formalization paradigm.</abstract>
      <url hash="9c0a36cf">2021.emnlp-main.379</url>
      <bibkey>yang-etal-2021-enhanced</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.379</doi>
      <video href="2021.emnlp-main.379.mp4"/>
      <pwccode url="https://github.com/akeepers/lear" additional="false">akeepers/lear</pwccode>
    </paper>
    <paper id="380">
      <title><fixed-case>PRIDE</fixed-case>: <fixed-case>P</fixed-case>redicting <fixed-case>R</fixed-case>elationships in <fixed-case>C</fixed-case>onversations</title>
      <author><first>Anna</first><last>Tigunova</last></author>
      <author><first>Paramita</first><last>Mirza</last></author>
      <author><first>Andrew</first><last>Yates</last></author>
      <author><first>Gerhard</first><last>Weikum</last></author>
      <pages>4636–4650</pages>
      <abstract>Automatically extracting interpersonal relationships of conversation interlocutors can enrich personal knowledge bases to enhance personalized search, recommenders and chatbots. To infer speakers’ relationships from dialogues we propose PRIDE, a neural multi-label classifier, based on BERT and Transformer for creating a conversation representation. PRIDE utilizes dialogue structure and augments it with external knowledge about speaker features and conversation style.Unlike prior works, we address multi-label prediction of fine-grained relationships. We release large-scale datasets, based on screenplays of movies and TV shows, with directed relationships of conversation participants. Extensive experiments on both datasets show superior performance of PRIDE compared to the state-of-the-art baselines.</abstract>
      <url hash="b11b038d">2021.emnlp-main.380</url>
      <bibkey>tigunova-etal-2021-pride</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.380</doi>
      <video href="2021.emnlp-main.380.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ddrel">DDRel</pwcdataset>
    </paper>
    <paper id="381">
      <title>Extracting Fine-Grained Knowledge Graphs of Scientific Claims: Dataset and Transformer-Based Results</title>
      <author><first>Ian</first><last>Magnusson</last></author>
      <author><first>Scott</first><last>Friedman</last></author>
      <pages>4651–4658</pages>
      <abstract>Recent transformer-based approaches demonstrate promising results on relational scientific information extraction. Existing datasets focus on high-level description of how research is carried out. Instead we focus on the subtleties of how experimental associations are presented by building SciClaim, a dataset of scientific claims drawn from Social and Behavior Science (SBS), PubMed, and CORD-19 papers. Our novel graph annotation schema incorporates not only coarse-grained entity spans as nodes and relations as edges between them, but also fine-grained attributes that modify entities and their relations, for a total of 12,738 labels in the corpus. By including more label types and more than twice the label density of previous datasets, SciClaim captures causal, comparative, predictive, statistical, and proportional associations over experimental variables along with their qualifications, subtypes, and evidence. We extend work in transformer-based joint entity and relation extraction to effectively infer our schema, showing the promise of fine-grained knowledge graphs in scientific claims and beyond.</abstract>
      <url hash="34b1bdff">2021.emnlp-main.381</url>
      <bibkey>magnusson-friedman-2021-extracting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.381</doi>
      <video href="2021.emnlp-main.381.mp4"/>
      <pwccode url="https://github.com/siftech/sciclaim" additional="false">siftech/sciclaim</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cord-19">CORD-19</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scierc">SciERC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scirex">SciREX</pwcdataset>
    </paper>
    <paper id="382">
      <title>Sequential Cross-Document Coreference Resolution</title>
      <author><first>Emily</first><last>Allaway</last></author>
      <author><first>Shuai</first><last>Wang</last></author>
      <author><first>Miguel</first><last>Ballesteros</last></author>
      <pages>4659–4671</pages>
      <abstract>Relating entities and events in text is a key component of natural language understanding. Cross-document coreference resolution, in particular, is important for the growing interest in multi-document analysis tasks. In this work we propose a new model that extends the efficient sequential prediction paradigm for coreference resolution to cross-document settings and achieves competitive results for both entity and event coreference while providing strong evidence of the efficacy of both sequential models and higher-order inference in cross-document settings. Our model incrementally composes mentions into cluster representations and predicts links between a mention and the already constructed clusters, approximating a higher-order model. In addition, we conduct extensive ablation studies that provide new insights into the importance of various inputs and representation types in coreference.</abstract>
      <url hash="0e3d612c">2021.emnlp-main.382</url>
      <bibkey>allaway-etal-2021-sequential</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.382</doi>
      <video href="2021.emnlp-main.382.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ecb">ECB+</pwcdataset>
    </paper>
    <paper id="383">
      <title>Mixture-of-Partitions: Infusing Large Biomedical Knowledge Graphs into <fixed-case>BERT</fixed-case></title>
      <author><first>Zaiqiao</first><last>Meng</last></author>
      <author><first>Fangyu</first><last>Liu</last></author>
      <author><first>Thomas</first><last>Clark</last></author>
      <author><first>Ehsan</first><last>Shareghi</last></author>
      <author><first>Nigel</first><last>Collier</last></author>
      <pages>4672–4681</pages>
      <abstract>Infusing factual knowledge into pre-trained models is fundamental for many knowledge-intensive tasks. In this paper, we proposed Mixture-of-Partitions (MoP), an infusion approach that can handle a very large knowledge graph (KG) by partitioning it into smaller sub-graphs and infusing their specific knowledge into various BERT models using lightweight adapters. To leverage the overall factual knowledge for a target task, these sub-graph adapters are further fine-tuned along with the underlying BERT through a mixture layer. We evaluate our MoP with three biomedical BERTs (SciBERT, BioBERT, PubmedBERT) on six downstream tasks (inc. NLI, QA, Classification), and the results show that our MoP consistently enhances the underlying BERTs in task performance, and achieves new SOTA performances on five evaluated datasets.</abstract>
      <url hash="e8e89df6">2021.emnlp-main.383</url>
      <bibkey>meng-etal-2021-mixture</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.383</doi>
      <video href="2021.emnlp-main.383.mp4"/>
      <pwccode url="https://github.com/cambridgeltl/mop" additional="false">cambridgeltl/mop</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/pubmedqa">PubMedQA</pwcdataset>
    </paper>
    <paper id="384">
      <title>Filling the Gaps in <fixed-case>A</fixed-case>ncient <fixed-case>A</fixed-case>kkadian Texts: A Masked Language Modelling Approach</title>
      <author><first>Koren</first><last>Lazar</last></author>
      <author><first>Benny</first><last>Saret</last></author>
      <author><first>Asaf</first><last>Yehudai</last></author>
      <author><first>Wayne</first><last>Horowitz</last></author>
      <author><first>Nathan</first><last>Wasserman</last></author>
      <author><first>Gabriel</first><last>Stanovsky</last></author>
      <pages>4682–4691</pages>
      <abstract>We present models which complete missing text given transliterations of ancient Mesopotamian documents, originally written on cuneiform clay tablets (2500 BCE - 100 CE). Due to the tablets’ deterioration, scholars often rely on contextual cues to manually fill in missing parts in the text in a subjective and time-consuming process. We identify that this challenge can be formulated as a masked language modelling task, used mostly as a pretraining objective for contextualized language models. Following, we develop several architectures focusing on the Akkadian language, the lingua franca of the time. We find that despite data scarcity (1M tokens) we can achieve state of the art performance on missing tokens prediction (89% hit@5) using a greedy decoding scheme and pretraining on data from other languages and different time periods. Finally, we conduct human evaluations showing the applicability of our models in assisting experts to transcribe texts in extinct languages.</abstract>
      <url hash="f27776de">2021.emnlp-main.384</url>
      <bibkey>lazar-etal-2021-filling</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.384</doi>
      <video href="2021.emnlp-main.384.mp4"/>
      <pwccode url="https://github.com/slab-nlp/akk" additional="false">slab-nlp/akk</pwccode>
    </paper>
    <paper id="385">
      <title><fixed-case>AV</fixed-case>oca<fixed-case>D</fixed-case>o: Strategy for Adapting Vocabulary to Downstream Domain</title>
      <author><first>Jimin</first><last>Hong</last></author>
      <author><first>TaeHee</first><last>Kim</last></author>
      <author><first>Hyesu</first><last>Lim</last></author>
      <author><first>Jaegul</first><last>Choo</last></author>
      <pages>4692–4700</pages>
      <abstract>During the fine-tuning phase of transfer learning, the pretrained vocabulary remains unchanged, while model parameters are updated. The vocabulary generated based on the pretrained data is suboptimal for downstream data when domain discrepancy exists. We propose to consider the vocabulary as an optimizable parameter, allowing us to update the vocabulary by expanding it with domain specific vocabulary based on a tokenization statistic. Furthermore, we preserve the embeddings of the added words from overfitting to downstream data by utilizing knowledge learned from a pretrained language model with a regularization term. Our method achieved consistent performance improvements on diverse domains (i.e., biomedical, computer science, news, and reviews).</abstract>
      <url hash="d7984f5f">2021.emnlp-main.385</url>
      <bibkey>hong-etal-2021-avocado</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.385</doi>
      <video href="2021.emnlp-main.385.mp4"/>
      <pwccode url="https://github.com/Jimin9401/avocado" additional="false">Jimin9401/avocado</pwccode>
    </paper>
    <paper id="386">
      <title>Can We Improve Model Robustness through Secondary Attribute Counterfactuals?</title>
      <author><first>Ananth</first><last>Balashankar</last></author>
      <author><first>Xuezhi</first><last>Wang</last></author>
      <author><first>Ben</first><last>Packer</last></author>
      <author><first>Nithum</first><last>Thain</last></author>
      <author><first>Ed</first><last>Chi</last></author>
      <author><first>Alex</first><last>Beutel</last></author>
      <pages>4701–4712</pages>
      <abstract>Developing robust NLP models that perform well on many, even small, slices of data is a significant but important challenge, with implications from fairness to general reliability. To this end, recent research has explored how models rely on spurious correlations, and how counterfactual data augmentation (CDA) can mitigate such issues. In this paper we study how and why modeling counterfactuals over multiple attributes can go significantly further in improving model performance. We propose RDI, a context-aware methodology which takes into account the impact of secondary attributes on the model’s predictions and increases sensitivity for secondary attributes over reweighted counterfactually augmented data. By implementing RDI in the context of toxicity detection, we find that accounting for secondary attributes can significantly improve robustness, with improvements in sliced accuracy on the original dataset up to 7% compared to existing robustness methods. We also demonstrate that RDI generalizes to the coreference resolution task and provide guidelines to extend this to other tasks.</abstract>
      <url hash="84016528">2021.emnlp-main.386</url>
      <bibkey>balashankar-etal-2021-improve</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.386</doi>
      <video href="2021.emnlp-main.386.mp4"/>
    </paper>
    <paper id="387">
      <title>Long-Range Modeling of Source Code Files with e<fixed-case>WASH</fixed-case>: Extended Window Access by Syntax Hierarchy</title>
      <author><first>Colin</first><last>Clement</last></author>
      <author><first>Shuai</first><last>Lu</last></author>
      <author><first>Xiaoyu</first><last>Liu</last></author>
      <author><first>Michele</first><last>Tufano</last></author>
      <author><first>Dawn</first><last>Drain</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <author><first>Neel</first><last>Sundaresan</last></author>
      <author><first>Alexey</first><last>Svyatkovskiy</last></author>
      <pages>4713–4722</pages>
      <abstract>Statistical language modeling and translation with transformers have found many successful applications in program understanding and generation tasks, setting high benchmarks for tools in modern software development environments. The finite context window of these neural models means, however, that they will be unable to leverage the entire relevant context of large files and packages for any given task. While there are many efforts to extend the context window, we introduce an architecture-independent approach for leveraging the syntactic hierarchies of source code for incorporating entire file-level context into a fixed-length window. Using concrete syntax trees of each source file we extract syntactic hierarchies and integrate them into context window by selectively removing from view more specific, less relevant scopes for a given task. We evaluate this approach on code generation tasks and joint translation of natural language and source code in Python programming language, achieving a new state-of-the-art in code completion and summarization for Python in the CodeXGLUE benchmark. We also introduce new CodeXGLUE benchmarks for user-experience-motivated tasks: code completion with normalized literals, method body completion/code summarization conditioned on file-level context.</abstract>
      <url hash="d6f47389">2021.emnlp-main.387</url>
      <bibkey>clement-etal-2021-long</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.387</doi>
      <video href="2021.emnlp-main.387.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/codesearchnet">CodeSearchNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/codexglue">CodeXGLUE</pwcdataset>
    </paper>
    <paper id="388">
      <title>Can Language Models be Biomedical Knowledge Bases?</title>
      <author><first>Mujeen</first><last>Sung</last></author>
      <author><first>Jinhyuk</first><last>Lee</last></author>
      <author><first>Sean</first><last>Yi</last></author>
      <author><first>Minji</first><last>Jeon</last></author>
      <author><first>Sungdong</first><last>Kim</last></author>
      <author><first>Jaewoo</first><last>Kang</last></author>
      <pages>4723–4734</pages>
      <abstract>Pre-trained language models (LMs) have become ubiquitous in solving various natural language processing (NLP) tasks. There has been increasing interest in what knowledge these LMs contain and how we can extract that knowledge, treating LMs as knowledge bases (KBs). While there has been much work on probing LMs in the general domain, there has been little attention to whether these powerful LMs can be used as domain-specific KBs. To this end, we create the BioLAMA benchmark, which is comprised of 49K biomedical factual knowledge triples for probing biomedical LMs. We find that biomedical LMs with recently proposed probing methods can achieve up to 18.51% Acc@5 on retrieving biomedical knowledge. Although this seems promising given the task difficulty, our detailed analyses reveal that most predictions are highly correlated with prompt templates without any subjects, hence producing similar results on each relation and hindering their capabilities to be used as domain-specific KBs. We hope that BioLAMA can serve as a challenging benchmark for biomedical factual probing.</abstract>
      <url hash="497056c4">2021.emnlp-main.388</url>
      <bibkey>sung-etal-2021-language</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.388</doi>
      <video href="2021.emnlp-main.388.mp4"/>
      <pwccode url="https://github.com/dmis-lab/biolama" additional="false">dmis-lab/biolama</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/biolama">BioLAMA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lama">LAMA</pwcdataset>
    </paper>
    <paper id="389">
      <title><fixed-case>L</fixed-case>ayout<fixed-case>R</fixed-case>eader: Pre-training of Text and Layout for Reading Order Detection</title>
      <author><first>Zilong</first><last>Wang</last></author>
      <author><first>Yiheng</first><last>Xu</last></author>
      <author><first>Lei</first><last>Cui</last></author>
      <author><first>Jingbo</first><last>Shang</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <pages>4735–4744</pages>
      <abstract>Reading order detection is the cornerstone to understanding visually-rich documents (e.g., receipts and forms). Unfortunately, no existing work took advantage of advanced deep learning models because it is too laborious to annotate a large enough dataset. We observe that the reading order of WORD documents is embedded in their XML metadata; meanwhile, it is easy to convert WORD documents to PDFs or images. Therefore, in an automated manner, we construct ReadingBank, a benchmark dataset that contains reading order, text, and layout information for 500,000 document images covering a wide spectrum of document types. This first-ever large-scale dataset unleashes the power of deep neural networks for reading order detection. Specifically, our proposed LayoutReader captures the text and layout information for reading order prediction using the seq2seq model. It performs almost perfectly in reading order detection and significantly improves both open-source and commercial OCR engines in ordering text lines in their results in our experiments. The dataset and models are publicly available at https://aka.ms/layoutreader.</abstract>
      <url hash="ad71e00b">2021.emnlp-main.389</url>
      <bibkey>wang-etal-2021-layoutreader</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.389</doi>
      <video href="2021.emnlp-main.389.mp4"/>
      <pwccode url="https://github.com/microsoft/unilm" additional="false">microsoft/unilm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/readingbank">ReadingBank</pwcdataset>
    </paper>
    <paper id="390">
      <title>Region under <fixed-case>D</fixed-case>iscussion for visual dialog</title>
      <author><first>Mauricio</first><last>Mazuecos</last></author>
      <author><first>Franco M.</first><last>Luque</last></author>
      <author><first>Jorge</first><last>Sánchez</last></author>
      <author><first>Hernán</first><last>Maina</last></author>
      <author><first>Thomas</first><last>Vadora</last></author>
      <author><first>Luciana</first><last>Benotti</last></author>
      <pages>4745–4759</pages>
      <abstract>Visual Dialog is assumed to require the dialog history to generate correct responses during a dialog. However, it is not clear from previous work how dialog history is needed for visual dialog. In this paper we define what it means for a visual question to require dialog history and we release a subset of the Guesswhat?! questions for which their dialog history completely changes their responses. We propose a novel interpretable representation that visually grounds dialog history: the Region under Discussion. It constrains the image’s spatial features according to a semantic representation of the history inspired by the information structure notion of Question under Discussion.We evaluate the architecture on task-specific multimodal models and the visual transformer model LXMERT.</abstract>
      <url hash="55fdb197">2021.emnlp-main.390</url>
      <attachment type="Software" hash="a12b6638">2021.emnlp-main.390.Software.zip</attachment>
      <bibkey>mazuecos-etal-2021-region</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.390</doi>
      <video href="2021.emnlp-main.390.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/guesswhat">GuessWhat?!</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visdial">VisDial</pwcdataset>
    </paper>
    <paper id="391">
      <title>Learning grounded word meaning representations on similarity graphs</title>
      <author><first>Mariella</first><last>Dimiccoli</last></author>
      <author><first>Herwig</first><last>Wendt</last></author>
      <author><first>Pau</first><last>Batlle Franch</last></author>
      <pages>4760–4769</pages>
      <abstract>This paper introduces a novel approach to learn visually grounded meaning representations of words as low-dimensional node embeddings on an underlying graph hierarchy. The lower level of the hierarchy models modality-specific word representations, conditioned to another modality, through dedicated but communicating graphs, while the higher level puts these representations together on a single graph to learn a representation jointly from both modalities. The topology of each graph models similarity relations among words, and is estimated jointly with the graph embedding. The assumption underlying this model is that words sharing similar meaning correspond to communities in an underlying graph in a low-dimensional space. We named this model Hierarchical Multi-Modal Similarity Graph Embedding (HM-SGE). Experimental results validate the ability of HM-SGE to simulate human similarity judgments and concept categorization, outperforming the state of the art.</abstract>
      <url hash="e5d05ba7">2021.emnlp-main.391</url>
      <bibkey>dimiccoli-etal-2021-learning</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.391</doi>
      <video href="2021.emnlp-main.391.mp4"/>
      <pwccode url="https://github.com/mdimiccoli/hm-sge" additional="false">mdimiccoli/hm-sge</pwccode>
    </paper>
    <paper id="392">
      <title><fixed-case>W</fixed-case>hy<fixed-case>A</fixed-case>ct: Identifying Action Reasons in Lifestyle Vlogs</title>
      <author><first>Oana</first><last>Ignat</last></author>
      <author><first>Santiago</first><last>Castro</last></author>
      <author><first>Hanwen</first><last>Miao</last></author>
      <author><first>Weiji</first><last>Li</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <pages>4770–4785</pages>
      <abstract>We aim to automatically identify human action reasons in online videos. We focus on the widespread genre of lifestyle vlogs, in which people perform actions while verbally describing them. We introduce and make publicly available the WhyAct dataset, consisting of 1,077 visual actions manually annotated with their reasons. We describe a multimodal model that leverages visual and textual information to automatically infer the reasons corresponding to an action presented in the video.</abstract>
      <url hash="da0ec43a">2021.emnlp-main.392</url>
      <bibkey>ignat-etal-2021-whyact</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.392</doi>
      <video href="2021.emnlp-main.392.mp4"/>
      <pwccode url="https://github.com/michigannlp/vlog_action_reason" additional="false">michigannlp/vlog_action_reason</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/whyact">WhyAct</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vcr">VCR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vlogs">Vlogs</pwcdataset>
    </paper>
    <paper id="393">
      <title>Genre as Weak Supervision for Cross-lingual Dependency Parsing</title>
      <author><first>Max</first><last>Müller-Eberstein</last></author>
      <author><first>Rob</first><last>van der Goot</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>4786–4802</pages>
      <abstract>Recent work has shown that monolingual masked language models learn to represent data-driven notions of language variation which can be used for domain-targeted training data selection. Dataset genre labels are already frequently available, yet remain largely unexplored in cross-lingual setups. We harness this genre metadata as a weak supervision signal for targeted data selection in zero-shot dependency parsing. Specifically, we project treebank-level genre information to the finer-grained sentence level, with the goal to amplify information implicitly stored in unsupervised contextualized representations. We demonstrate that genre is recoverable from multilingual contextual embeddings and that it provides an effective signal for training data selection in cross-lingual, zero-shot scenarios. For 12 low-resource language treebanks, six of which are test-only, our genre-specific methods significantly outperform competitive baselines as well as recent embedding-based methods for data selection. Moreover, genre-based data selection provides new state-of-the-art results for three of these target languages.</abstract>
      <url hash="71deb6c1">2021.emnlp-main.393</url>
      <bibkey>muller-eberstein-etal-2021-genre</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.393</doi>
      <video href="2021.emnlp-main.393.mp4"/>
      <pwccode url="https://github.com/personads/ud-selection" additional="false">personads/ud-selection</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="394">
      <title>On the Relation between Syntactic Divergence and Zero-Shot Performance</title>
      <author><first>Ofir</first><last>Arviv</last></author>
      <author><first>Dmitry</first><last>Nikolaev</last></author>
      <author><first>Taelin</first><last>Karidi</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <pages>4803–4817</pages>
      <abstract>We explore the link between the extent to which syntactic relations are preserved in translation and the ease of correctly constructing a parse tree in a zero-shot setting. While previous work suggests such a relation, it tends to focus on the macro level and not on the level of individual edges—a gap we aim to address. As a test case, we take the transfer of Universal Dependencies (UD) parsing from English to a diverse set of languages and conduct two sets of experiments. In one, we analyze zero-shot performance based on the extent to which English source edges are preserved in translation. In another, we apply three linguistically motivated transformations to UD, creating more cross-lingually stable versions of it, and assess their zero-shot parsability. In order to compare parsing performance across different schemes, we perform extrinsic evaluation on the downstream task of cross-lingual relation extraction (RE) using a subset of a standard English RE benchmark translated to Russian and Korean. In both sets of experiments, our results suggest a strong relation between cross-lingual stability and zero-shot parsing performance.</abstract>
      <url hash="29f721bf">2021.emnlp-main.394</url>
      <bibkey>arviv-etal-2021-relation</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.394</doi>
      <video href="2021.emnlp-main.394.mp4"/>
      <pwccode url="https://github.com/ofirarviv/improving-ud" additional="false">ofirarviv/improving-ud</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/translated-tacred">Translated TACRED</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="395">
      <title>Improved Latent Tree Induction with Distant Supervision via Span Constraints</title>
      <author><first>Zhiyang</first><last>Xu</last></author>
      <author><first>Andrew</first><last>Drozdov</last></author>
      <author><first>Jay Yoon</first><last>Lee</last></author>
      <author><first>Tim</first><last>O’Gorman</last></author>
      <author><first>Subendhu</first><last>Rongali</last></author>
      <author><first>Dylan</first><last>Finkbeiner</last></author>
      <author><first>Shilpa</first><last>Suresh</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>4818–4831</pages>
      <abstract>For over thirty years, researchers have developed and analyzed methods for latent tree induction as an approach for unsupervised syntactic parsing. Nonetheless, modern systems still do not perform well enough compared to their supervised counterparts to have any practical use as structural annotation of text. In this work, we present a technique that uses distant supervision in the form of span constraints (i.e. phrase bracketing) to improve performance in unsupervised constituency parsing. Using a relatively small number of span constraints we can substantially improve the output from DIORA, an already competitive unsupervised parsing system. Compared with full parse tree annotation, span constraints can be acquired with minimal effort, such as with a lexicon derived from Wikipedia, to find exact text matches. Our experiments show span constraints based on entities improves constituency parsing on English WSJ Penn Treebank by more than 5 F1. Furthermore, our method extends to any domain where span constraints are easily attainable, and as a case study we demonstrate its effectiveness by parsing biomedical text from the CRAFT dataset.</abstract>
      <url hash="02716d62">2021.emnlp-main.395</url>
      <bibkey>xu-etal-2021-improved</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.395</doi>
      <video href="2021.emnlp-main.395.mp4"/>
      <pwccode url="https://github.com/iesl/distantly-supervised-diora" additional="false">iesl/distantly-supervised-diora</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/medmentions">MedMentions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="396">
      <title>Aligning Multidimensional Worldviews and Discovering Ideological Differences</title>
      <author><first>Jeremiah</first><last>Milbauer</last></author>
      <author><first>Adarsh</first><last>Mathew</last></author>
      <author><first>James</first><last>Evans</last></author>
      <pages>4832–4845</pages>
      <abstract>The Internet is home to thousands of communities, each with their own unique worldview and associated ideological differences. With new communities constantly emerging and serving as ideological birthplaces, battlegrounds, and bunkers, it is critical to develop a framework for understanding worldviews and ideological distinction. Most existing work, however, takes a predetermined view based on political polarization: the “right vs. left” dichotomy of U.S. politics. In reality, both political polarization – and worldviews more broadly – transcend one-dimensional difference, and deserve a more complete analysis. Extending the ability of word embedding models to capture the semantic and cultural characteristics of their training corpora, we propose a novel method for discovering the multifaceted ideological and worldview characteristics of communities. Using over 1B comments collected from the largest communities on Reddit.com representing ~40% of Reddit activity, we demonstrate the efficacy of this approach to uncover complex ideological differences across multiple axes of polarization.</abstract>
      <url hash="d7217dc0">2021.emnlp-main.396</url>
      <bibkey>milbauer-etal-2021-aligning</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.396</doi>
      <video href="2021.emnlp-main.396.mp4"/>
    </paper>
    <paper id="397">
      <title>Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts</title>
      <author><first>Ashutosh</first><last>Baheti</last></author>
      <author><first>Maarten</first><last>Sap</last></author>
      <author><first>Alan</first><last>Ritter</last></author>
      <author><first>Mark</first><last>Riedl</last></author>
      <pages>4846–4862</pages>
      <abstract>Dialogue models trained on human conversations inadvertently learn to generate toxic responses. In addition to producing explicitly offensive utterances, these models can also implicitly insult a group or individual by aligning themselves with an offensive statement. To better understand the dynamics of contextually offensive language, we investigate the stance of dialogue model responses in offensive Reddit conversations. Specifically, we create ToxiChat, a crowd-annotated dataset of 2,000 Reddit threads and model responses labeled with offensive language and stance. Our analysis reveals that 42% of human responses agree with toxic comments, whereas only 13% agree with safe comments. This undesirable behavior is learned by neural dialogue models, such as DialoGPT, which we show are two times more likely to agree with offensive comments. To enable automatic detection of offensive language, we fine-tuned transformer-based classifiers on ToxiChat that achieve 0.71 F1 for offensive labels and 0.53 Macro-F1 for stance labels. Finally, we quantify the effectiveness of controllable text generation (CTG) methods to mitigate the tendency of neural dialogue models to agree with offensive comments. Compared to the baseline, our best CTG model achieves a 19% reduction in agreement with offensive comments and produces 29% fewer offensive replies. Our work highlights the need for further efforts to characterize and analyze inappropriate behavior in dialogue models, in order to help make them safer.</abstract>
      <url hash="f240bf88">2021.emnlp-main.397</url>
      <bibkey>baheti-etal-2021-just</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.397</doi>
      <video href="2021.emnlp-main.397.mp4"/>
      <pwccode url="https://github.com/abaheti95/toxichat" additional="false">abaheti95/toxichat</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sbic">SBIC</pwcdataset>
    </paper>
    <paper id="398">
      <title>Multi-Modal Open-Domain Dialogue</title>
      <author><first>Kurt</first><last>Shuster</last></author>
      <author><first>Eric Michael</first><last>Smith</last></author>
      <author><first>Da</first><last>Ju</last></author>
      <author><first>Jason</first><last>Weston</last></author>
      <pages>4863–4883</pages>
      <abstract>Recent work in open-domain conversational agents has demonstrated that significant improvements in humanness and user preference can be achieved via massive scaling in both pre-training data and model size (Adiwardana et al., 2020; Roller et al., 2020). However, if we want to build agents with human-like abilities, we must expand beyond handling just text. A particularly important topic is the ability to see images and communicate about what is perceived. With the goal of getting humans to engage in multi-modal dialogue, we investigate combining components from state-of-the-art open-domain dialogue agents with those from state-of-the-art vision models. We study incorporating different image fusion schemes and domain-adaptive pre-training and fine-tuning strategies, and show that our best resulting model outperforms strong existing models in multi-modal dialogue while simultaneously performing as well as its predecessor (text-only) BlenderBot (Roller et al., 2020) in text-based conversation. We additionally investigate and incorporate safety components in our final model, and show that such efforts do not diminish model performance with respect to human preference.</abstract>
      <url hash="9bfa83f0">2021.emnlp-main.398</url>
      <bibkey>shuster-etal-2021-multi</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.398</doi>
      <video href="2021.emnlp-main.398.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/blended-skill-talk">Blended Skill Talk</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coco-captions">COCO Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/convai2">ConvAI2</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/empatheticdialogues">EmpatheticDialogues</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="399">
      <title>A Label-Aware <fixed-case>BERT</fixed-case> Attention Network for Zero-Shot Multi-Intent Detection in Spoken Language Understanding</title>
      <author><first>Ting-Wei</first><last>Wu</last></author>
      <author><first>Ruolin</first><last>Su</last></author>
      <author><first>Biing</first><last>Juang</last></author>
      <pages>4884–4896</pages>
      <abstract>With the early success of query-answer assistants such as Alexa and Siri, research attempts to expand system capabilities of handling service automation are now abundant. However, preliminary systems have quickly found the inadequacy in relying on simple classification techniques to effectively accomplish the automation task. The main challenge is that the dialogue often involves complexity in user’s intents (or purposes) which are multiproned, subject to spontaneous change, and difficult to track. Furthermore, public datasets have not considered these complications and the general semantic annotations are lacking which may result in zero-shot problem. Motivated by the above, we propose a Label-Aware BERT Attention Network (LABAN) for zero-shot multi-intent detection. We first encode input utterances with BERT and construct a label embedded space by considering embedded semantics in intent labels. An input utterance is then classified based on its projection weights on each intent embedding in this embedded space. We show that it successfully extends to few/zero-shot setting where part of intent labels are unseen in training data, by also taking account of semantics in these unseen intent labels. Experimental results show that our approach is capable of detecting many unseen intent labels correctly. It also achieves the state-of-the-art performance on five multi-intent datasets in normal cases.</abstract>
      <url hash="07f4a1fb">2021.emnlp-main.399</url>
      <bibkey>wu-etal-2021-label</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.399</doi>
      <video href="2021.emnlp-main.399.mp4"/>
      <pwccode url="https://github.com/waynewu6250/laban" additional="false">waynewu6250/laban</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mixatis">MixATIS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mixsnips-1">MixSNIPs</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
    </paper>
    <paper id="400">
      <title>Zero-Shot Dialogue Disentanglement by Self-Supervised Entangled Response Selection</title>
      <author><first>Ta-Chung</first><last>Chi</last></author>
      <author><first>Alexander</first><last>Rudnicky</last></author>
      <pages>4897–4902</pages>
      <abstract>Dialogue disentanglement aims to group utterances in a long and multi-participant dialogue into threads. This is useful for discourse analysis and downstream applications such as dialogue response selection, where it can be the first step to construct a clean context/response set. Unfortunately, labeling all <i>reply-to</i> links takes quadratic effort w.r.t the number of utterances: an annotator must check all preceding utterances to identify the one to which the current utterance is a reply. In this paper, we are the first to propose a <b>zero-shot</b> dialogue disentanglement solution. Firstly, we train a model on a multi-participant response selection dataset harvested from the web which is not annotated; we then apply the trained model to perform zero-shot dialogue disentanglement. Without any labeled data, our model can achieve a cluster F1 score of 25. We also fine-tune the model using various amounts of labeled data. Experiments show that with only 10% of the data, we achieve nearly the same performance of using the full dataset.</abstract>
      <url hash="3cd15251">2021.emnlp-main.400</url>
      <bibkey>chi-rudnicky-2021-zero</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.400</doi>
      <video href="2021.emnlp-main.400.mp4"/>
      <pwccode url="https://github.com/chijames/zero_shot_dialogue_disentanglement" additional="false">chijames/zero_shot_dialogue_disentanglement</pwccode>
    </paper>
    <paper id="401">
      <title><fixed-case>SIMMC</fixed-case> 2.0: A Task-oriented Dialog Dataset for Immersive Multimodal Conversations</title>
      <author><first>Satwik</first><last>Kottur</last></author>
      <author><first>Seungwhan</first><last>Moon</last></author>
      <author><first>Alborz</first><last>Geramifard</last></author>
      <author><first>Babak</first><last>Damavandi</last></author>
      <pages>4903–4912</pages>
      <abstract>Next generation task-oriented dialog systems need to understand conversational contexts with their perceived surroundings, to effectively help users in the real-world multimodal environment. Existing task-oriented dialog datasets aimed towards virtual assistance fall short and do not situate the dialog in the user’s multimodal context. To overcome, we present a new dataset for Situated and Interactive Multimodal Conversations, SIMMC 2.0, which includes 11K task-oriented user&lt;-&gt;assistant dialogs (117K utterances) in the shopping domain, grounded in immersive and photo-realistic scenes. The dialogs are collection using a two-phase pipeline: (1) A novel multimodal dialog simulator generates simulated dialog flows, with an emphasis on diversity and richness of interactions, (2) Manual paraphrasing of generating utterances to draw from natural language distribution. We provide an in-depth analysis of the collected dataset, and describe in detail the four main benchmark tasks we propose for SIMMC 2.0. Our baseline model, powered by the state-of-the-art language model, shows promising results, and highlights new challenges and directions for the community to study.</abstract>
      <url hash="eb0a418a">2021.emnlp-main.401</url>
      <attachment type="Software" hash="f0f9c662">2021.emnlp-main.401.Software.zip</attachment>
      <bibkey>kottur-etal-2021-simmc</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.401</doi>
      <video href="2021.emnlp-main.401.mp4"/>
      <pwccode url="https://github.com/facebookresearch/simmc2" additional="false">facebookresearch/simmc2</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/simmc">SIMMC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="402">
      <title><fixed-case>RAST</fixed-case>: Domain-Robust Dialogue Rewriting as Sequence Tagging</title>
      <author><first>Jie</first><last>Hao</last></author>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Liwei</first><last>Wang</last></author>
      <author><first>Kun</first><last>Xu</last></author>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <pages>4913–4924</pages>
      <abstract>The task of dialogue rewriting aims to reconstruct the latest dialogue utterance by copying the missing content from the dialogue context. Until now, the existing models for this task suffer from the robustness issue, i.e., performances drop dramatically when testing on a different dataset. We address this robustness issue by proposing a novel sequence-tagging-based model so that the search space is significantly reduced, yet the core of this task is still well covered. As a common issue of most tagging models for text generation, the model’s outputs may lack fluency. To alleviate this issue, we inject the loss signal from BLEU or GPT-2 under a REINFORCE framework. Experiments show huge improvements of our model over the current state-of-the-art systems when transferring to another dataset.</abstract>
      <url hash="8ff8e055">2021.emnlp-main.402</url>
      <bibkey>hao-etal-2021-rast</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.402</doi>
      <video href="2021.emnlp-main.402.mp4"/>
    </paper>
    <paper id="403">
      <title><fixed-case>MRF</fixed-case>-Chat: Improving Dialogue with <fixed-case>M</fixed-case>arkov Random Fields</title>
      <author><first>Ishaan</first><last>Grover</last></author>
      <author><first>Matthew</first><last>Huggins</last></author>
      <author><first>Cynthia</first><last>Breazeal</last></author>
      <author><first>Hae Won</first><last>Park</last></author>
      <pages>4925–4936</pages>
      <abstract>Recent state-of-the-art approaches in open-domain dialogue include training end-to-end deep-learning models to learn various conversational features like emotional content of response, symbolic transitions of dialogue contexts in a knowledge graph and persona of the agent and the user, among others. While neural models have shown reasonable results, modelling the cognitive processes that humans use when conversing with each other may improve the agent’s quality of responses. A key element of natural conversation is to tailor one’s response such that it accounts for concepts that the speaker and listener may or may not know and the contextual relevance of all prior concepts used in conversation. We show that a rich representation and explicit modeling of these psychological processes can improve predictions made by existing neural network models. In this work, we propose a novel probabilistic approach using Markov Random Fields (MRF) to augment existing deep-learning methods for improved next utterance prediction. Using human and automatic evaluations, we show that our augmentation approach significantly improves the performance of existing state-of-the-art retrieval models for open-domain conversational agents.</abstract>
      <url hash="b120ce70">2021.emnlp-main.403</url>
      <bibkey>grover-etal-2021-mrf</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.403</doi>
      <video href="2021.emnlp-main.403.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/blended-skill-talk">Blended Skill Talk</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/convai2">ConvAI2</pwcdataset>
    </paper>
    <paper id="404">
      <title>Dialogue State Tracking with a Language Model using Schema-Driven Prompting</title>
      <author><first>Chia-Hsuan</first><last>Lee</last></author>
      <author><first>Hao</first><last>Cheng</last></author>
      <author><first>Mari</first><last>Ostendorf</last></author>
      <pages>4937–4949</pages>
      <abstract>Task-oriented conversational systems often use dialogue state tracking to represent the user’s intentions, which involves filling in values of pre-defined slots. Many approaches have been proposed, often using task-specific architectures with special-purpose classifiers. Recently, good results have been obtained using more general architectures based on pretrained language models. Here, we introduce a new variation of the language modeling approach that uses schema-driven prompting to provide task-aware history encoding that is used for both categorical and non-categorical slots. We further improve performance by augmenting the prompting with schema descriptions, a naturally occurring source of in-domain knowledge. Our purely generative system achieves state-of-the-art performance on MultiWOZ 2.2 and achieves competitive performance on two other benchmarks: MultiWOZ 2.1 and M2M. The data and code will be available at https://github.com/chiahsuan156/DST-as-Prompting.</abstract>
      <url hash="8c222adc">2021.emnlp-main.404</url>
      <bibkey>lee-etal-2021-dialogue</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.404</doi>
      <video href="2021.emnlp-main.404.mp4"/>
      <pwccode url="https://github.com/chiahsuan156/dst-as-prompting" additional="false">chiahsuan156/dst-as-prompting</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
    </paper>
    <paper id="405">
      <title>Signed Coreference Resolution</title>
      <author><first>Kayo</first><last>Yin</last></author>
      <author><first>Kenneth</first><last>DeHaan</last></author>
      <author><first>Malihe</first><last>Alikhani</last></author>
      <pages>4950–4961</pages>
      <abstract>Coreference resolution is key to many natural language processing tasks and yet has been relatively unexplored in Sign Language Processing. In signed languages, space is primarily used to establish reference. Solving coreference resolution for signed languages would not only enable higher-level Sign Language Processing systems, but also enhance our understanding of language in different modalities and of situated references, which are key problems in studying grounded language. In this paper, we: (1) introduce Signed Coreference Resolution (SCR), a new challenge for coreference modeling and Sign Language Processing; (2) collect an annotated corpus of German Sign Language with gold labels for coreference together with an annotation software for the task; (3) explore features of hand gesture, iconicity, and spatial situated properties and move forward to propose a set of linguistically informed heuristics and unsupervised models for the task; (4) put forward several proposals about ways to address the complexities of this challenge effectively.</abstract>
      <url hash="d911a424">2021.emnlp-main.405</url>
      <bibkey>yin-etal-2021-signed</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.405</doi>
      <video href="2021.emnlp-main.405.mp4"/>
      <pwccode url="https://github.com/kayoyin/scr" additional="false">kayoyin/scr</pwccode>
    </paper>
    <paper id="406">
      <title>Consistent Accelerated Inference via Confident Adaptive Transformers</title>
      <author><first>Tal</first><last>Schuster</last></author>
      <author><first>Adam</first><last>Fisch</last></author>
      <author><first>Tommi</first><last>Jaakkola</last></author>
      <author><first>Regina</first><last>Barzilay</last></author>
      <pages>4962–4979</pages>
      <abstract>We develop a novel approach for confidently accelerating inference in the large and expensive multilayer Transformers that are now ubiquitous in natural language processing (NLP). Amortized or approximate computational methods increase efficiency, but can come with unpredictable performance costs. In this work, we present CATs – Confident Adaptive Transformers – in which we simultaneously increase computational efficiency, while guaranteeing a specifiable degree of consistency with the original model with high confidence. Our method trains additional prediction heads on top of intermediate layers, and dynamically decides when to stop allocating computational effort to each input using a meta consistency classifier. To calibrate our early prediction stopping rule, we formulate a unique extension of conformal prediction. We demonstrate the effectiveness of this approach on four classification and regression tasks.</abstract>
      <url hash="cd2ad2bf">2021.emnlp-main.406</url>
      <bibkey>schuster-etal-2021-consistent</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.406</doi>
      <video href="2021.emnlp-main.406.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vitaminc">VitaminC</pwcdataset>
    </paper>
    <paper id="407">
      <title>Improving and Simplifying Pattern Exploiting Training</title>
      <author><first>Derek</first><last>Tam</last></author>
      <author><first>Rakesh</first><last>R. Menon</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <author><first>Shashank</first><last>Srivastava</last></author>
      <author><first>Colin</first><last>Raffel</last></author>
      <pages>4980–4991</pages>
      <abstract>Recently, pre-trained language models (LMs) have achieved strong performance when fine-tuned on difficult benchmarks like SuperGLUE. However, performance can suffer when there are very few labeled examples available for fine-tuning. Pattern Exploiting Training (PET) is a recent approach that leverages patterns for few-shot learning. However, PET uses task-specific unlabeled data. In this paper, we focus on few-shot learning without any unlabeled data and introduce ADAPET, which modifies PET’s objective to provide denser supervision during fine-tuning. As a result, ADAPET outperforms PET on SuperGLUE without any task-specific unlabeled data.</abstract>
      <url hash="306af24d">2021.emnlp-main.407</url>
      <attachment type="Software" hash="329004aa">2021.emnlp-main.407.Software.zip</attachment>
      <bibkey>tam-etal-2021-improving</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.407</doi>
      <video href="2021.emnlp-main.407.mp4"/>
      <pwccode url="https://github.com/rrmenon10/ADAPET" additional="true">rrmenon10/ADAPET</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multirc">MultiRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/record">ReCoRD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wic">WiC</pwcdataset>
    </paper>
    <paper id="408">
      <title>Unsupervised Data Augmentation with Naive Augmentation and without Unlabeled Data</title>
      <author><first>David</first><last>Lowell</last></author>
      <author><first>Brian</first><last>Howard</last></author>
      <author><first>Zachary C.</first><last>Lipton</last></author>
      <author><first>Byron</first><last>Wallace</last></author>
      <pages>4992–5001</pages>
      <abstract>Unsupervised Data Augmentation (UDA) is a semisupervised technique that applies a consistency loss to penalize differences between a model’s predictions on (a) observed (unlabeled) examples; and (b) corresponding ‘noised’ examples produced via data augmentation. While UDA has gained popularity for text classification, open questions linger over which design decisions are necessary and how to extend the method to sequence labeling tasks. In this paper, we re-examine UDA and demonstrate its efficacy on several sequential tasks. Our main contribution is an empirical study of UDA to establish which components of the algorithm confer benefits in NLP. Notably, although prior work has emphasized the use of clever augmentation techniques including back-translation, we find that enforcing consistency between predictions assigned to observed and randomly substituted words often yields comparable (or greater) benefits compared to these more complex perturbation models. Furthermore, we find that applying UDA’s consistency loss affords meaningful gains without any unlabeled data at all, i.e., in a standard supervised setting. In short, UDA need not be unsupervised to realize much of its noted benefits, and does not require complex data augmentation to be effective.</abstract>
      <url hash="ec528452">2021.emnlp-main.408</url>
      <bibkey>lowell-etal-2021-unsupervised</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.408</doi>
      <video href="2021.emnlp-main.408.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ebm-nlp">EBM-NLP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/evidence-inference">Evidence Inference</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="409">
      <title>Pre-train or Annotate? Domain Adaptation with a Constrained Budget</title>
      <author><first>Fan</first><last>Bai</last></author>
      <author><first>Alan</first><last>Ritter</last></author>
      <author><first>Wei</first><last>Xu</last></author>
      <pages>5002–5015</pages>
      <abstract>Recent work has demonstrated that pre-training in-domain language models can boost performance when adapting to a new domain. However, the costs associated with pre-training raise an important question: given a fixed budget, what steps should an NLP practitioner take to maximize performance? In this paper, we study domain adaptation under budget constraints, and approach it as a customer choice problem between data annotation and pre-training. Specifically, we measure the annotation cost of three procedural text datasets and the pre-training cost of three in-domain language models. Then we evaluate the utility of different combinations of pre-training and data annotation under varying budget constraints to assess which combination strategy works best. We find that, for small budgets, spending all funds on annotation leads to the best performance; once the budget becomes large enough, a combination of data annotation and in-domain pre-training works more optimally. We therefore suggest that task-specific data annotation should be part of an economical strategy when adapting an NLP model to a new domain.</abstract>
      <url hash="7276f144">2021.emnlp-main.409</url>
      <bibkey>bai-etal-2021-pre</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.409</doi>
      <video href="2021.emnlp-main.409.mp4"/>
      <pwccode url="https://github.com/bflashcp3f/procbert" additional="false">bflashcp3f/procbert</pwccode>
    </paper>
    <paper id="410">
      <title>Lawyers are Dishonest? Quantifying Representational Harms in Commonsense Knowledge Resources</title>
      <author><first>Ninareh</first><last>Mehrabi</last></author>
      <author><first>Pei</first><last>Zhou</last></author>
      <author><first>Fred</first><last>Morstatter</last></author>
      <author><first>Jay</first><last>Pujara</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <author><first>Aram</first><last>Galstyan</last></author>
      <pages>5016–5033</pages>
      <abstract>Warning: this paper contains content that may be offensive or upsetting. Commonsense knowledge bases (CSKB) are increasingly used for various natural language processing tasks. Since CSKBs are mostly human-generated and may reflect societal biases, it is important to ensure that such biases are not conflated with the notion of commonsense. Here we focus on two widely used CSKBs, ConceptNet and GenericsKB, and establish the presence of bias in the form of two types of representational harms, overgeneralization of polarized perceptions and representation disparity across different demographic groups in both CSKBs. Next, we find similar representational harms for downstream models that use ConceptNet. Finally, we propose a filtering-based approach for mitigating such harms, and observe that our filtered-based approach can reduce the issues in both resources and models but leads to a performance drop, leaving room for future work to build fairer and stronger commonsense models.</abstract>
      <url hash="a97613c9">2021.emnlp-main.410</url>
      <bibkey>mehrabi-etal-2021-lawyers</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.410</doi>
      <video href="2021.emnlp-main.410.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/genericskb">GenericsKB</pwcdataset>
    </paper>
    <paper id="411">
      <title><fixed-case>OSC</fixed-case>a<fixed-case>R</fixed-case>: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings</title>
      <author><first>Sunipa</first><last>Dev</last></author>
      <author><first>Tao</first><last>Li</last></author>
      <author><first>Jeff M</first><last>Phillips</last></author>
      <author><first>Vivek</first><last>Srikumar</last></author>
      <pages>5034–5050</pages>
      <abstract>Language representations are known to carry stereotypical biases and, as a result, lead to biased predictions in downstream tasks. While existing methods are effective at mitigating biases by linear projection, such methods are too aggressive: they not only remove bias, but also erase valuable information from word embeddings. We develop new measures for evaluating specific information retention that demonstrate the tradeoff between bias removal and information retention. To address this challenge, we propose OSCaR (Orthogonal Subspace Correction and Rectification), a bias-mitigating method that focuses on disentangling biased associations between concepts instead of removing concepts wholesale. Our experiments on gender biases show that OSCaR is a well-balanced approach that ensures that semantic information is retained in the embeddings and bias is also effectively mitigated.</abstract>
      <url hash="5bd5e3a3">2021.emnlp-main.411</url>
      <bibkey>dev-etal-2021-oscar</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.411</doi>
      <video href="2021.emnlp-main.411.mp4"/>
      <pwccode url="https://github.com/Shaul1321/nullspace_projection" additional="false">Shaul1321/nullspace_projection</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="412">
      <title>Sentence-Permuted Paragraph Generation</title>
      <author><first>Wenhao</first><last>Yu</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Tong</first><last>Zhao</last></author>
      <author><first>Zhichun</first><last>Guo</last></author>
      <author><first>Meng</first><last>Jiang</last></author>
      <pages>5051–5062</pages>
      <abstract>Generating paragraphs of diverse contents is important in many applications. Existing generation models produce similar contents from homogenized contexts due to the fixed left-to-right sentence order. Our idea is permuting the sentence orders to improve the content diversity of multi-sentence paragraph. We propose a novel framework PermGen whose objective is to maximize the expected log-likelihood of output paragraph distributions with respect to all possible sentence orders. PermGen uses hierarchical positional embedding and designs new procedures for training, and decoding in the sentence-permuted generation. Experiments on three paragraph generation benchmarks demonstrate PermGen generates more diverse outputs with a higher quality than existing models.</abstract>
      <url hash="0346f9b4">2021.emnlp-main.412</url>
      <bibkey>yu-etal-2021-sentence</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.412</doi>
      <video href="2021.emnlp-main.412.mp4"/>
      <pwccode url="https://github.com/wyu97/permgen" additional="false">wyu97/permgen</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/agenda">AGENDA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rocstories">ROCStories</pwcdataset>
    </paper>
    <paper id="413">
      <title>Extract, Denoise and Enforce: Evaluating and Improving Concept Preservation for Text-to-Text Generation</title>
      <author><first>Yuning</first><last>Mao</last></author>
      <author><first>Wenchang</first><last>Ma</last></author>
      <author><first>Deren</first><last>Lei</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <pages>5063–5074</pages>
      <abstract>Prior studies on text-to-text generation typically assume that the model could figure out what to attend to in the input and what to include in the output via seq2seq learning, with only the parallel training data and no additional guidance. However, it remains unclear whether current models can preserve important concepts in the source input, as seq2seq learning does not have explicit focus on the concepts and commonly used evaluation metrics also treat them equally important as other tokens. In this paper, we present a systematic analysis that studies whether current seq2seq models, especially pre-trained language models, are good enough for preserving important input concepts and to what extent explicitly guiding generation with the concepts as lexical constraints is beneficial. We answer the above questions by conducting extensive analytical experiments on four representative text-to-text generation tasks. Based on the observations, we then propose a simple yet effective framework to automatically extract, denoise, and enforce important input concepts as lexical constraints. This new method performs comparably or better than its unconstrained counterpart on automatic metrics, demonstrates higher coverage for concept preservation, and receives better ratings in the human evaluation. Our code is available at https://github.com/morningmoni/EDE.</abstract>
      <url hash="d6884d47">2021.emnlp-main.413</url>
      <bibkey>mao-etal-2021-extract</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.413</doi>
      <video href="2021.emnlp-main.413.mp4"/>
      <pwccode url="https://github.com/morningmoni/ede" additional="true">morningmoni/ede</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="414">
      <title>Paraphrase Generation: A Survey of the State of the Art</title>
      <author><first>Jianing</first><last>Zhou</last></author>
      <author><first>Suma</first><last>Bhat</last></author>
      <pages>5075–5086</pages>
      <abstract>This paper focuses on paraphrase generation,which is a widely studied natural language generation task in NLP. With the development of neural models, paraphrase generation research has exhibited a gradual shift to neural methods in the recent years. This has provided architectures for contextualized representation of an input text and generating fluent, diverseand human-like paraphrases. This paper surveys various approaches to paraphrase generation with a main focus on neural methods.</abstract>
      <url hash="fc8044b8">2021.emnlp-main.414</url>
      <bibkey>zhou-bhat-2021-paraphrase</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.414</doi>
      <video href="2021.emnlp-main.414.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="415">
      <title>Exposure Bias versus Self-Recovery: Are Distortions Really Incremental for Autoregressive Text Generation?</title>
      <author><first>Tianxing</first><last>He</last></author>
      <author><first>Jingzhao</first><last>Zhang</last></author>
      <author><first>Zhiming</first><last>Zhou</last></author>
      <author><first>James</first><last>Glass</last></author>
      <pages>5087–5102</pages>
      <abstract>Exposure bias has been regarded as a central problem for auto-regressive language models (LM). It claims that teacher forcing would cause the test-time generation to be incrementally distorted due to the training-generation discrepancy. Although a lot of algorithms have been proposed to avoid teacher forcing and therefore alleviate exposure bias, there is little work showing how serious the exposure bias problem actually is. In this work, we focus on the task of open-ended language generation, propose metrics to quantify the impact of exposure bias in the aspects of quality, diversity, and consistency. Our key intuition is that if we feed ground-truth data prefixes (instead of prefixes generated by the model itself) into the model and ask it to continue the generation, the performance should become much better because the training-generation discrepancy in the prefix is removed. Both automatic and human evaluations are conducted in our experiments. On the contrary to the popular belief in exposure bias, we find that the the distortion induced by the prefix discrepancy is limited, and does not seem to be incremental during the generation. Moreover, our analysis reveals an interesting self-recovery ability of the LM, which we hypothesize to be countering the harmful effects from exposure bias.</abstract>
      <url hash="512311c2">2021.emnlp-main.415</url>
      <bibkey>he-etal-2021-exposure</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.415</doi>
      <video href="2021.emnlp-main.415.mp4"/>
    </paper>
    <paper id="416">
      <title>Generating Self-Contained and Summary-Centric Question Answer Pairs via Differentiable Reward Imitation Learning</title>
      <author><first>Li</first><last>Zhou</last></author>
      <author><first>Kevin</first><last>Small</last></author>
      <author><first>Yong</first><last>Zhang</last></author>
      <author><first>Sandeep</first><last>Atluri</last></author>
      <pages>5103–5135</pages>
      <abstract>Motivated by suggested question generation in conversational news recommendation systems, we propose a model for generating question-answer pairs (QA pairs) with self-contained, summary-centric questions and length-constrained, article-summarizing answers. We begin by collecting a new dataset of news articles with questions as titles and pairing them with summaries of varying length. This dataset is used to learn a QA pair generation model producing summaries as answers that balance brevity with sufficiency jointly with their corresponding questions. We then reinforce the QA pair generation process with a differentiable reward function to mitigate exposure bias, a common problem in natural language generation. Both automatic metrics and human evaluation demonstrate these QA pairs successfully capture the central gists of the articles and achieve high answer accuracy.</abstract>
      <url hash="69962c6b">2021.emnlp-main.416</url>
      <bibkey>zhou-etal-2021-generating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.416</doi>
      <video href="2021.emnlp-main.416.mp4"/>
      <pwccode url="https://github.com/amazon-research/sc2qa-dril" additional="false">amazon-research/sc2qa-dril</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="417">
      <title>Unsupervised Paraphrasing with Pretrained Language Models</title>
      <author><first>Tong</first><last>Niu</last></author>
      <author><first>Semih</first><last>Yavuz</last></author>
      <author><first>Yingbo</first><last>Zhou</last></author>
      <author><first>Nitish Shirish</first><last>Keskar</last></author>
      <author><first>Huan</first><last>Wang</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <pages>5136–5150</pages>
      <abstract>Paraphrase generation has benefited extensively from recent progress in the designing of training objectives and model architectures. However, previous explorations have largely focused on supervised methods, which require a large amount of labeled data that is costly to collect. To address this drawback, we adopt a transfer learning approach and propose a training pipeline that enables pre-trained language models to generate high-quality paraphrases in an unsupervised setting. Our recipe consists of task-adaptation, self-supervision, and a novel decoding algorithm named Dynamic Blocking (DB). To enforce a surface form dissimilar from the input, whenever the language model emits a token contained in the source sequence, DB prevents the model from outputting the subsequent source token for the next generation step. We show with automatic and human evaluations that our approach achieves state-of-the-art performance on both the Quora Question Pair (QQP) and the ParaNMT datasets and is robust to domain shift between the two datasets of distinct distributions. We also demonstrate that our model transfers to paraphrasing in other languages without any additional finetuning.</abstract>
      <url hash="41742221">2021.emnlp-main.417</url>
      <bibkey>niu-etal-2021-unsupervised</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.417</doi>
      <video href="2021.emnlp-main.417.mp4"/>
    </paper>
    <paper id="418">
      <title>Profanity-Avoiding Training Framework for Seq2seq Models with Certified Robustness</title>
      <author><first>Hengtong</first><last>Zhang</last></author>
      <author><first>Tianhang</first><last>Zheng</last></author>
      <author><first>Yaliang</first><last>Li</last></author>
      <author><first>Jing</first><last>Gao</last></author>
      <author><first>Lu</first><last>Su</last></author>
      <author id="bo-li-vanderbilt"><first>Bo</first><last>Li</last></author>
      <pages>5151–5161</pages>
      <abstract>Seq2seq models have demonstrated their incredible effectiveness in a large variety of applications. However, recent research has shown that inappropriate language in training samples and well-designed testing cases can induce seq2seq models to output profanity. These outputs may potentially hurt the usability of seq2seq models and make the end-users feel offended. To address this problem, we propose a training framework with certified robustness to eliminate the causes that trigger the generation of profanity. The proposed training framework leverages merely a short list of profanity examples to prevent seq2seq models from generating a broader spectrum of profanity. The framework is composed of a pattern-eliminating training component to suppress the impact of language patterns with profanity in the training set, and a trigger-resisting training component to provide certified robustness for seq2seq models against intentionally injected profanity-triggering expressions in test samples. In the experiments, we consider two representative NLP tasks that seq2seq can be applied to, i.e., style transfer and dialogue generation. Extensive experimental results show that the proposed training framework can successfully prevent the NLP models from generating profanity.</abstract>
      <url hash="1063d181">2021.emnlp-main.418</url>
      <bibkey>zhang-etal-2021-profanity</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.418</doi>
    </paper>
    <paper id="419">
      <title>Journalistic Guidelines Aware News Image Captioning</title>
      <author><first>Xuewen</first><last>Yang</last></author>
      <author><first>Svebor</first><last>Karaman</last></author>
      <author><first>Joel</first><last>Tetreault</last></author>
      <author><first>Alejandro</first><last>Jaimes</last></author>
      <pages>5162–5175</pages>
      <abstract>The task of news article image captioning aims to generate descriptive and informative captions for news article images. Unlike conventional image captions that simply describe the content of the image in general terms, news image captions follow journalistic guidelines and rely heavily on named entities to describe the image content, often drawing context from the whole article they are associated with. In this work, we propose a new approach to this task, motivated by caption guidelines that journalists follow. Our approach, Journalistic Guidelines Aware News Image Captioning (JoGANIC), leverages the structure of captions to improve the generation quality and guide our representation design. Experimental results, including detailed ablation studies, on two large-scale publicly available datasets show that JoGANIC substantially outperforms state-of-the-art methods both on caption generation and named entity related metrics.</abstract>
      <url hash="75b8ffee">2021.emnlp-main.419</url>
      <bibkey>yang-etal-2021-journalistic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.419</doi>
      <video href="2021.emnlp-main.419.mp4"/>
      <pwccode url="https://github.com/dataminr-ai/joganic" additional="false">dataminr-ai/joganic</pwccode>
    </paper>
    <paper id="420">
      <title><fixed-case>AESOP</fixed-case>: Paraphrase Generation with Adaptive Syntactic Control</title>
      <author><first>Jiao</first><last>Sun</last></author>
      <author><first>Xuezhe</first><last>Ma</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>5176–5189</pages>
      <abstract>We propose to control paraphrase generation through carefully chosen target syntactic structures to generate more proper and higher quality paraphrases. Our model, AESOP, leverages a pretrained language model and adds deliberately chosen syntactical control via a retrieval-based selection module to generate fluent paraphrases. Experiments show that AESOP achieves state-of-the-art performances on semantic preservation and syntactic conformation on two benchmark datasets with ground-truth syntactic control from human-annotated exemplars. Moreover, with the retrieval-based target syntax selection module, AESOP generates paraphrases with even better qualities than the current best model using human-annotated target syntactic parses according to human evaluation. We further demonstrate the effectiveness of AESOP to improve classification models’ robustness to syntactic perturbation by data augmentation on two GLUE tasks.</abstract>
      <url hash="9026394d">2021.emnlp-main.420</url>
      <bibkey>sun-etal-2021-aesop</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.420</doi>
      <video href="2021.emnlp-main.420.mp4"/>
      <pwccode url="https://github.com/pluslabnlp/aesop" additional="false">pluslabnlp/aesop</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="421">
      <title>Refocusing on Relevance: Personalization in <fixed-case>NLG</fixed-case></title>
      <author><first>Shiran</first><last>Dudy</last></author>
      <author><first>Steven</first><last>Bedrick</last></author>
      <author><first>Bonnie</first><last>Webber</last></author>
      <pages>5190–5202</pages>
      <abstract>Many NLG tasks such as summarization, dialogue response, or open domain question answering, focus primarily on a source text in order to generate a target response. This standard approach falls short, however, when a user’s intent or context of work is not easily recoverable based solely on that source text– a scenario that we argue is more of the rule than the exception. In this work, we argue that NLG systems in general should place a much higher level of emphasis on making use of additional context, and suggest that relevance (as used in Information Retrieval) be thought of as a crucial tool for designing user-oriented text-generating tasks. We further discuss possible harms and hazards around such personalization, and argue that value-sensitive design represents a crucial path forward through these challenges.</abstract>
      <url hash="6ba7f724">2021.emnlp-main.421</url>
      <bibkey>dudy-etal-2021-refocusing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.421</doi>
      <video href="2021.emnlp-main.421.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
    </paper>
    <paper id="422">
      <title>The Future is not One-dimensional: Complex Event Schema Induction by Graph Modeling for Event Prediction</title>
      <author><first>Manling</first><last>Li</last></author>
      <author><first>Sha</first><last>Li</last></author>
      <author><first>Zhenhailong</first><last>Wang</last></author>
      <author><first>Lifu</first><last>Huang</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <author><first>Clare</first><last>Voss</last></author>
      <pages>5203–5215</pages>
      <abstract>Event schemas encode knowledge of stereotypical structures of events and their connections. As events unfold, schemas are crucial to act as a scaffolding. Previous work on event schema induction focuses either on atomic events or linear temporal event sequences, ignoring the interplay between events via arguments and argument relations. We introduce a new concept of Temporal Complex Event Schema: a graph-based schema representation that encompasses events, arguments, temporal connections and argument relations. In addition, we propose a Temporal Event Graph Model that predicts event instances following the temporal complex event schema. To build and evaluate such schemas, we release a new schema learning corpus containing 6,399 documents accompanied with event graphs, and we have manually constructed gold-standard schemas. Intrinsic evaluations by schema matching and instance graph perplexity, prove the superior quality of our probabilistic graph schema library compared to linear representations. Extrinsic evaluation on schema-guided future event prediction further demonstrates the predictive power of our event graph model, significantly outperforming human schemas and baselines by more than 17.8% on HITS@1.</abstract>
      <url hash="c55fb007">2021.emnlp-main.422</url>
      <bibkey>li-etal-2021-future</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.422</doi>
      <video href="2021.emnlp-main.422.mp4"/>
      <pwccode url="https://github.com/limanling/temporal-graph-schema" additional="false">limanling/temporal-graph-schema</pwccode>
    </paper>
    <paper id="423">
      <title>Learning Constraints and Descriptive Segmentation for Subevent Detection</title>
      <author><first>Haoyu</first><last>Wang</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>5216–5226</pages>
      <abstract>Event mentions in text correspond to real-world events of varying degrees of granularity. The task of subevent detection aims to resolve this granularity issue, recognizing the membership of multi-granular events in event complexes. Since knowing the span of descriptive contexts of event complexes helps infer the membership of events, we propose the task of event-based text segmentation (EventSeg) as an auxiliary task to improve the learning for subevent detection. To bridge the two tasks together, we propose an approach to learning and enforcing constraints that capture dependencies between subevent detection and EventSeg prediction, as well as guiding the model to make globally consistent inference. Specifically, we adopt Rectifier Networks for constraint learning and then convert the learned constraints to a regularization term in the loss function of the neural model. Experimental results show that the proposed method outperforms baseline methods by 2.3% and 2.5% on benchmark datasets for subevent detection, HiEve and IC, respectively, while achieving a decent performance on EventSeg prediction.</abstract>
      <url hash="969d0bbe">2021.emnlp-main.423</url>
      <bibkey>wang-etal-2021-learning-constraints</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.423</doi>
    </paper>
    <paper id="424">
      <title><fixed-case>C</fixed-case>hem<fixed-case>NER</fixed-case>: Fine-Grained Chemistry Named Entity Recognition with Ontology-Guided Distant Supervision</title>
      <author><first>Xuan</first><last>Wang</last></author>
      <author><first>Vivian</first><last>Hu</last></author>
      <author><first>Xiangchen</first><last>Song</last></author>
      <author><first>Shweta</first><last>Garg</last></author>
      <author><first>Jinfeng</first><last>Xiao</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>5227–5240</pages>
      <abstract>Scientific literature analysis needs fine-grained named entity recognition (NER) to provide a wide range of information for scientific discovery. For example, chemistry research needs to study dozens to hundreds of distinct, fine-grained entity types, making consistent and accurate annotation difficult even for crowds of domain experts. On the other hand, domain-specific ontologies and knowledge bases (KBs) can be easily accessed, constructed, or integrated, which makes distant supervision realistic for fine-grained chemistry NER. In distant supervision, training labels are generated by matching mentions in a document with the concepts in the knowledge bases (KBs). However, this kind of KB-matching suffers from two major challenges: incomplete annotation and noisy annotation. We propose ChemNER, an ontology-guided, distantly-supervised method for fine-grained chemistry NER to tackle these challenges. It leverages the chemistry type ontology structure to generate distant labels with novel methods of flexible KB-matching and ontology-guided multi-type disambiguation. It significantly improves the distant label generation for the subsequent sequence labeling model training. We also provide an expert-labeled, chemistry NER dataset with 62 fine-grained chemistry types (e.g., chemical compounds and chemical reactions). Experimental results show that ChemNER is highly effective, outperforming substantially the state-of-the-art NER methods (with .25 absolute F1 score improvement).</abstract>
      <url hash="c7ad2f30">2021.emnlp-main.424</url>
      <bibkey>wang-etal-2021-chemner</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.424</doi>
      <video href="2021.emnlp-main.424.mp4"/>
    </paper>
    <paper id="425">
      <title>Moving on from <fixed-case>O</fixed-case>nto<fixed-case>N</fixed-case>otes: Coreference Resolution Model Transfer</title>
      <author><first>Patrick</first><last>Xia</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>5241–5256</pages>
      <abstract>Academic neural models for coreference resolution (coref) are typically trained on a single dataset, OntoNotes, and model improvements are benchmarked on that same dataset. However, real-world applications of coref depend on the annotation guidelines and the domain of the target dataset, which often differ from those of OntoNotes. We aim to quantify transferability of coref models based on the number of annotated documents available in the target dataset. We examine eleven target datasets and find that continued training is consistently effective and especially beneficial when there are few target documents. We establish new benchmarks across several datasets, including state-of-the-art results on PreCo.</abstract>
      <url hash="c81b861b">2021.emnlp-main.425</url>
      <bibkey>xia-van-durme-2021-moving</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.425</doi>
      <video href="2021.emnlp-main.425.mp4"/>
      <pwccode url="" additional="true"/>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/preco">PreCo</pwcdataset>
    </paper>
    <paper id="426">
      <title>Document-level Entity-based Extraction as Template Generation</title>
      <author><first>Kung-Hsiang</first><last>Huang</last></author>
      <author><first>Sam</first><last>Tang</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>5257–5269</pages>
      <abstract>Document-level entity-based extraction (EE), aiming at extracting entity-centric information such as entity roles and entity relations, is key to automatic knowledge acquisition from text corpora for various domains. Most document-level EE systems build extractive models, which struggle to model long-term dependencies among entities at the document level. To address this issue, we propose a generative framework for two document-level EE tasks: role-filler entity extraction (REE) and relation extraction (RE). We first formulate them as a template generation problem, allowing models to efficiently capture cross-entity dependencies, exploit label semantics, and avoid the exponential computation complexity of identifying N-ary relations. A novel cross-attention guided copy mechanism, TopK Copy, is incorporated into a pre-trained sequence-to-sequence model to enhance the capabilities of identifying key information in the input document. Experiments done on the MUC-4 and SciREX dataset show new state-of-the-art results on REE (+3.26%), binary RE (+4.8%), and 4-ary RE (+2.7%) in F1 score.</abstract>
      <url hash="ed2611c2">2021.emnlp-main.426</url>
      <bibkey>huang-etal-2021-document</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.426</doi>
      <video href="2021.emnlp-main.426.mp4"/>
      <pwccode url="https://github.com/PlusLabNLP/TempGen" additional="false">PlusLabNLP/TempGen</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/muc-4">MUC-4</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scirex">SciREX</pwcdataset>
    </paper>
    <paper id="427">
      <title>Learning Prototype Representations Across Few-Shot Tasks for Event Detection</title>
      <author><first>Viet</first><last>Lai</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Thien Huu</first><last>Nguyen</last></author>
      <pages>5270–5277</pages>
      <abstract>We address the sampling bias and outlier issues in few-shot learning for event detection, a subtask of information extraction. We propose to model the relations between training tasks in episodic few-shot learning by introducing cross-task prototypes. We further propose to enforce prediction consistency among classifiers across tasks to make the model more robust to outliers. Our extensive experiment shows a consistent improvement on three few-shot learning datasets. The findings suggest that our model is more robust when labeled data of novel event types is limited. The source code is available at http://github.com/laiviet/fsl-proact.</abstract>
      <url hash="f5bce283">2021.emnlp-main.427</url>
      <bibkey>lai-etal-2021-learning</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.427</doi>
      <video href="2021.emnlp-main.427.mp4"/>
      <pwccode url="https://github.com/laiviet/fsl-proact" additional="false">laiviet/fsl-proact</pwccode>
    </paper>
    <paper id="428">
      <title>Lifelong Event Detection with Knowledge Transfer</title>
      <author><first>Pengfei</first><last>Yu</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Prem</first><last>Natarajan</last></author>
      <pages>5278–5290</pages>
      <abstract>Traditional supervised Information Extraction (IE) methods can extract structured knowledge elements from unstructured data, but it is limited to a pre-defined target ontology. In reality, the ontology of interest may change over time, adding emergent new types or more fine-grained subtypes. We propose a new lifelong learning framework to address this challenge. We focus on lifelong event detection as an exemplar case and propose a new problem formulation that is also generalizable to other IE tasks. In event detection and more general IE tasks, rich correlations or semantic relatedness exist among hierarchical knowledge element types. In our proposed framework, knowledge is being transferred between learned old event types and new event types. Specifically, we update old knowledge with new event types’ mentions using a self-training loss. In addition, we aggregate old event types’ representations based on their similarities with new event types to initialize the new event types’ representations. Experimental results show that our framework outperforms competitive baselines with a 5.1% absolute gain in the F1 score. Moreover, our proposed framework can boost the F1 score for over 30% absolute gain on some new long-tail rare event types with few training instances. Our knowledge transfer module improves performance on both learned event types and new event types under the lifelong learning setting, showing that it helps consolidate old knowledge and improve novel knowledge acquisition.</abstract>
      <url hash="a6ec3397">2021.emnlp-main.428</url>
      <bibkey>yu-etal-2021-lifelong</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.428</doi>
      <video href="2021.emnlp-main.428.mp4"/>
      <pwccode url="https://github.com/perfec-yu/lifelong-ed" additional="false">perfec-yu/lifelong-ed</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/maven">MAVEN</pwcdataset>
    </paper>
    <paper id="429">
      <title>Modular Self-Supervision for Document-Level Relation Extraction</title>
      <author><first>Sheng</first><last>Zhang</last></author>
      <author><first>Cliff</first><last>Wong</last></author>
      <author><first>Naoto</first><last>Usuyama</last></author>
      <author><first>Sarthak</first><last>Jain</last></author>
      <author><first>Tristan</first><last>Naumann</last></author>
      <author><first>Hoifung</first><last>Poon</last></author>
      <pages>5291–5302</pages>
      <abstract>Extracting relations across large text spans has been relatively underexplored in NLP, but it is particularly important for high-value domains such as biomedicine, where obtaining high recall of the latest findings is crucial for practical applications. Compared to conventional information extraction confined to short text spans, document-level relation extraction faces additional challenges in both inference and learning. Given longer text spans, state-of-the-art neural architectures are less effective and task-specific self-supervision such as distant supervision becomes very noisy. In this paper, we propose decomposing document-level relation extraction into relation detection and argument resolution, taking inspiration from Davidsonian semantics. This enables us to incorporate explicit discourse modeling and leverage modular self-supervision for each sub-problem, which is less noise-prone and can be further refined end-to-end via variational EM. We conduct a thorough evaluation in biomedical machine reading for precision oncology, where cross-paragraph relation mentions are prevalent. Our method outperforms prior state of the art, such as multi-scale learning and graph neural networks, by over 20 absolute F1 points. The gain is particularly pronounced among the most challenging relation instances whose arguments never co-occur in a paragraph.</abstract>
      <url hash="5c979ffa">2021.emnlp-main.429</url>
      <bibkey>zhang-etal-2021-modular</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.429</doi>
      <video href="2021.emnlp-main.429.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/docred">DocRED</pwcdataset>
    </paper>
    <paper id="430">
      <title>Unsupervised Paraphrasing Consistency Training for Low Resource Named Entity Recognition</title>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Ricardo</first><last>Henao</last></author>
      <pages>5303–5308</pages>
      <abstract>Unsupervised consistency training is a way of semi-supervised learning that encourages consistency in model predictions between the original and augmented data. For Named Entity Recognition (NER), existing approaches augment the input sequence with token replacement, assuming annotations on the replaced positions unchanged. In this paper, we explore the use of paraphrasing as a more principled data augmentation scheme for NER unsupervised consistency training. Specifically, we convert Conditional Random Field (CRF) into a multi-label classification module and encourage consistency on the entity appearance between the original and paraphrased sequences. Experiments show that our method is especially effective when annotations are limited.</abstract>
      <url hash="d4db2894">2021.emnlp-main.430</url>
      <bibkey>wang-henao-2021-unsupervised</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.430</doi>
    </paper>
    <paper id="431">
      <title>Fine-grained Entity Typing without Knowledge Base</title>
      <author><first>Jing</first><last>Qian</last></author>
      <author><first>Yibin</first><last>Liu</last></author>
      <author><first>Lemao</first><last>Liu</last></author>
      <author><first>Yangming</first><last>Li</last></author>
      <author><first>Haiyun</first><last>Jiang</last></author>
      <author><first>Haisong</first><last>Zhang</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <pages>5309–5319</pages>
      <abstract>Existing work on Fine-grained Entity Typing (FET) typically trains automatic models on the datasets obtained by using Knowledge Bases (KB) as distant supervision. However, the reliance on KB means this training setting can be hampered by the lack of or the incompleteness of the KB. To alleviate this limitation, we propose a novel setting for training FET models: FET without accessing any knowledge base. Under this setting, we propose a two-step framework to train FET models. In the first step, we automatically create pseudo data with fine-grained labels from a large unlabeled dataset. Then a neural network model is trained based on the pseudo data, either in an unsupervised way or using self-training under the weak guidance from a coarse-grained Named Entity Recognition (NER) model. Experimental results show that our method achieves competitive performance with respect to the models trained on the original KB-supervised datasets.</abstract>
      <url hash="4ab86250">2021.emnlp-main.431</url>
      <bibkey>qian-etal-2021-fine</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.431</doi>
      <video href="2021.emnlp-main.431.mp4"/>
      <pwccode url="https://github.com/lemaoliu/fet-data" additional="false">lemaoliu/fet-data</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/figer">FIGER</pwcdataset>
    </paper>
    <paper id="432">
      <title>Adversarial Attack against Cross-lingual Knowledge Graph Alignment</title>
      <author><first>Zeru</first><last>Zhang</last></author>
      <author><first>Zijie</first><last>Zhang</last></author>
      <author><first>Yang</first><last>Zhou</last></author>
      <author><first>Lingfei</first><last>Wu</last></author>
      <author><first>Sixing</first><last>Wu</last></author>
      <author><first>Xiaoying</first><last>Han</last></author>
      <author><first>Dejing</first><last>Dou</last></author>
      <author><first>Tianshi</first><last>Che</last></author>
      <author><first>Da</first><last>Yan</last></author>
      <pages>5320–5337</pages>
      <abstract>Recent literatures have shown that knowledge graph (KG) learning models are highly vulnerable to adversarial attacks. However, there is still a paucity of vulnerability analyses of cross-lingual entity alignment under adversarial attacks. This paper proposes an adversarial attack model with two novel attack techniques to perturb the KG structure and degrade the quality of deep cross-lingual entity alignment. First, an entity density maximization method is employed to hide the attacked entities in dense regions in two KGs, such that the derived perturbations are unnoticeable. Second, an attack signal amplification method is developed to reduce the gradient vanishing issues in the process of adversarial attacks for further improving the attack effectiveness.</abstract>
      <url hash="9385a31d">2021.emnlp-main.432</url>
      <bibkey>zhang-etal-2021-adversarial-attack</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.432</doi>
      <video href="2021.emnlp-main.432.mp4"/>
    </paper>
    <paper id="433">
      <title>Towards Realistic Few-Shot Relation Extraction</title>
      <author><first>Sam</first><last>Brody</last></author>
      <author><first>Sichao</first><last>Wu</last></author>
      <author><first>Adrian</first><last>Benton</last></author>
      <pages>5338–5345</pages>
      <abstract>In recent years, few-shot models have been applied successfully to a variety of NLP tasks. Han et al. (2018) introduced a few-shot learning framework for relation classification, and since then, several models have surpassed human performance on this task, leading to the impression that few-shot relation classification is solved. In this paper we take a deeper look at the efficacy of strong few-shot classification models in the more common relation extraction setting, and show that typical few-shot evaluation metrics obscure a wide variability in performance across relations. In particular, we find that state of the art few-shot relation classification models overly rely on entity type information, and propose modifications to the training routine to encourage models to better discriminate between relations involving similar entity types.</abstract>
      <url hash="a4479588">2021.emnlp-main.433</url>
      <bibkey>brody-etal-2021-towards</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.433</doi>
      <video href="2021.emnlp-main.433.mp4"/>
      <pwccode url="https://github.com/bloomberg/emnlp21_fewrel" additional="false">bloomberg/emnlp21_fewrel</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel-2-0">FewRel 2.0</pwcdataset>
    </paper>
    <paper id="434">
      <title>Data Augmentation for Cross-Domain Named Entity Recognition</title>
      <author><first>Shuguang</first><last>Chen</last></author>
      <author><first>Gustavo</first><last>Aguilar</last></author>
      <author><first>Leonardo</first><last>Neves</last></author>
      <author><first>Thamar</first><last>Solorio</last></author>
      <pages>5346–5356</pages>
      <abstract>Current work in named entity recognition (NER) shows that data augmentation techniques can produce more robust models. However, most existing techniques focus on augmenting in-domain data in low-resource scenarios where annotated data is quite limited. In this work, we take this research direction to the opposite and study cross-domain data augmentation for the NER task. We investigate the possibility of leveraging data from high-resource domains by projecting it into the low-resource domains. Specifically, we propose a novel neural architecture to transform the data representation from a high-resource to a low-resource domain by learning the patterns (e.g. style, noise, abbreviations, etc.) in the text that differentiate them and a shared feature space where both domains are aligned. We experiment with diverse datasets and show that transforming the data to the low-resource domain representation achieves significant improvements over only using data from high-resource domains.</abstract>
      <url hash="b067fba8">2021.emnlp-main.434</url>
      <bibkey>chen-etal-2021-data</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.434</doi>
      <video href="2021.emnlp-main.434.mp4"/>
      <pwccode url="https://github.com/ritual-uh/style_ner" additional="false">ritual-uh/style_ner</pwccode>
    </paper>
    <paper id="435">
      <title>Incorporating medical knowledge in <fixed-case>BERT</fixed-case> for clinical relation extraction</title>
      <author><first>Arpita</first><last>Roy</last></author>
      <author><first>Shimei</first><last>Pan</last></author>
      <pages>5357–5366</pages>
      <abstract>In recent years pre-trained language models (PLM) such as BERT have proven to be very effective in diverse NLP tasks such as Information Extraction, Sentiment Analysis and Question Answering. Trained with massive general-domain text, these pre-trained language models capture rich syntactic, semantic and discourse information in the text. However, due to the differences between general and specific domain text (e.g., Wikipedia versus clinic notes), these models may not be ideal for domain-specific tasks (e.g., extracting clinical relations). Furthermore, it may require additional medical knowledge to understand clinical text properly. To solve these issues, in this research, we conduct a comprehensive examination of different techniques to add medical knowledge into a pre-trained BERT model for clinical relation extraction. Our best model outperforms the state-of-the-art systems on the benchmark i2b2/VA 2010 clinical relation extraction dataset.</abstract>
      <url hash="f7c80bb4">2021.emnlp-main.435</url>
      <bibkey>roy-pan-2021-incorporating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.435</doi>
      <video href="2021.emnlp-main.435.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/blue">BLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/umls">UMLS</pwcdataset>
    </paper>
    <paper id="436">
      <title><fixed-case>ECONET</fixed-case>: Effective Continual Pretraining of Language Models for Event Temporal Reasoning</title>
      <author><first>Rujun</first><last>Han</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>5367–5380</pages>
      <abstract>While pre-trained language models (PTLMs) have achieved noticeable success on many NLP tasks, they still struggle for tasks that require event temporal reasoning, which is essential for event-centric applications. We present a continual pre-training approach that equips PTLMs with targeted knowledge about event temporal relations. We design self-supervised learning objectives to recover masked-out event and temporal indicators and to discriminate sentences from their corrupted counterparts (where event or temporal indicators got replaced). By further pre-training a PTLM with these objectives jointly, we reinforce its attention to event and temporal information, yielding enhanced capability on event temporal reasoning. This **E**ffective **CON**tinual pre-training framework for **E**vent **T**emporal reasoning (ECONET) improves the PTLMs’ fine-tuning performances across five relation extraction and question answering tasks and achieves new or on-par state-of-the-art performances in most of our downstream tasks.</abstract>
      <url hash="a82f1c78">2021.emnlp-main.436</url>
      <bibkey>han-etal-2021-econet</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.436</doi>
      <video href="2021.emnlp-main.436.mp4"/>
      <pwccode url="https://github.com/pluslabnlp/econet" additional="true">pluslabnlp/econet</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mc-taco">MC-TACO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/torque">Torque</pwcdataset>
    </paper>
    <paper id="437">
      <title>Learning from Noisy Labels for Entity-Centric Information Extraction</title>
      <author><first>Wenxuan</first><last>Zhou</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <pages>5381–5392</pages>
      <abstract>Recent information extraction approaches have relied on training deep neural models. However, such models can easily overfit noisy labels and suffer from performance degradation. While it is very costly to filter noisy labels in large learning resources, recent studies show that such labels take more training steps to be memorized and are more frequently forgotten than clean labels, therefore are identifiable in training. Motivated by such properties, we propose a simple co-regularization framework for entity-centric information extraction, which consists of several neural models with identical structures but different parameter initialization. These models are jointly optimized with the task-specific losses and are regularized to generate similar predictions based on an agreement loss, which prevents overfitting on noisy labels. Extensive experiments on two widely used but noisy benchmarks for information extraction, TACRED and CoNLL03, demonstrate the effectiveness of our framework. We release our code to the community for future research.</abstract>
      <url hash="0bcb5d2b">2021.emnlp-main.437</url>
      <bibkey>zhou-chen-2021-learning</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.437</doi>
      <video href="2021.emnlp-main.437.mp4"/>
      <pwccode url="https://github.com/wzhouad/NLL-IE" additional="false">wzhouad/NLL-IE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll">CoNLL++</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tacred">TACRED</pwcdataset>
    </paper>
    <paper id="438">
      <title>Extracting Material Property Measurement Data from Scientific Articles</title>
      <author><first>Gihan</first><last>Panapitiya</last></author>
      <author><first>Fred</first><last>Parks</last></author>
      <author><first>Jonathan</first><last>Sepulveda</last></author>
      <author><first>Emily</first><last>Saldanha</last></author>
      <pages>5393–5402</pages>
      <abstract>Machine learning-based prediction of material properties is often hampered by the lack of sufficiently large training data sets. The majority of such measurement data is embedded in scientific literature and the ability to automatically extract these data is essential to support the development of reliable property prediction methods. In this work, we describe a methodology for developing an automatic property extraction framework using material solubility as the target property. We create a training and evaluation data set containing tags for solubility-related entities using a combination of regular expressions and manual tagging. We then compare five entity recognition models leveraging both token-level and span-level architectures on the task of classifying solute names, solubility values, and solubility units. Additionally, we explore a novel pretraining approach that leverages automated chemical name and quantity extraction tools to generate large datasets that do not rely on intensive manual tagging. Finally, we perform an analysis to identify the causes of classification errors.</abstract>
      <url hash="ecb1e4a0">2021.emnlp-main.438</url>
      <bibkey>panapitiya-etal-2021-extracting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.438</doi>
      <video href="2021.emnlp-main.438.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/s2orc">S2ORC</pwcdataset>
    </paper>
    <paper id="439">
      <title>Modeling Document-Level Context for Event Detection via Important Context Selection</title>
      <author><first>Amir</first><last>Pouran Ben Veyseh</last></author>
      <author><first>Minh Van</first><last>Nguyen</last></author>
      <author><first>Nghia</first><last>Ngo Trung</last></author>
      <author><first>Bonan</first><last>Min</last></author>
      <author><first>Thien Huu</first><last>Nguyen</last></author>
      <pages>5403–5413</pages>
      <abstract>The task of Event Detection (ED) in Information Extraction aims to recognize and classify trigger words of events in text. The recent progress has featured advanced transformer-based language models (e.g., BERT) as a critical component in state-of-the-art models for ED. However, the length limit for input texts is a barrier for such ED models as they cannot encode long-range document-level context that has been shown to be beneficial for ED. To address this issue, we propose a novel method to model document-level context for ED that dynamically selects relevant sentences in the document for the event prediction of the target sentence. The target sentence will be then augmented with the selected sentences and consumed entirely by transformer-based language models for improved representation learning for ED. To this end, the REINFORCE algorithm is employed to train the relevant sentence selection for ED. Several information types are then introduced to form the reward function for the training process, including ED performance, sentence similarity, and discourse relations. Our extensive experiments on multiple benchmark datasets reveal the effectiveness of the proposed model, leading to new state-of-the-art performance.</abstract>
      <url hash="3f9aef73">2021.emnlp-main.439</url>
      <bibkey>pouran-ben-veyseh-etal-2021-modeling</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.439</doi>
      <video href="2021.emnlp-main.439.mp4"/>
    </paper>
    <paper id="440">
      <title>Crosslingual Transfer Learning for Relation and Event Extraction via Word Category and Class Alignments</title>
      <author><first>Minh Van</first><last>Nguyen</last></author>
      <author><first>Tuan Ngo</first><last>Nguyen</last></author>
      <author><first>Bonan</first><last>Min</last></author>
      <author><first>Thien Huu</first><last>Nguyen</last></author>
      <pages>5414–5426</pages>
      <abstract>Previous work on crosslingual Relation and Event Extraction (REE) suffers from the monolingual bias issue due to the training of models on only the source language data. An approach to overcome this issue is to use unlabeled data in the target language to aid the alignment of crosslingual representations, i.e., via fooling a language discriminator. However, as this approach does not condition on class information, a target language example of a class could be incorrectly aligned to a source language example of a different class. To address this issue, we propose a novel crosslingual alignment method that leverages class information of REE tasks for representation learning. In particular, we propose to learn two versions of representation vectors for each class in an REE task based on either source or target language examples. Representation vectors for corresponding classes will then be aligned to achieve class-aware alignment for crosslingual representations. In addition, we propose to further align representation vectors for language-universal word categories (i.e., parts of speech and dependency relations). As such, a novel filtering mechanism is presented to facilitate the learning of word category representations from contextualized representations on input texts based on adversarial learning. We conduct extensive crosslingual experiments with English, Chinese, and Arabic over REE tasks. The results demonstrate the benefits of the proposed method that significantly advances the state-of-the-art performance in these settings.</abstract>
      <url hash="47ae30a8">2021.emnlp-main.440</url>
      <bibkey>nguyen-etal-2021-crosslingual</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.440</doi>
      <video href="2021.emnlp-main.440.mp4"/>
    </paper>
    <paper id="441">
      <title>Corpus-based Open-Domain Event Type Induction</title>
      <author><first>Jiaming</first><last>Shen</last></author>
      <author><first>Yunyi</first><last>Zhang</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>5427–5440</pages>
      <abstract>Traditional event extraction methods require predefined event types and their corresponding annotations to learn event extractors. These prerequisites are often hard to be satisfied in real-world applications. This work presents a corpus-based open-domain event type induction method that automatically discovers a set of event types from a given corpus. As events of the same type could be expressed in multiple ways, we propose to represent each event type as a cluster of &lt;predicate sense, object head&gt; pairs. Specifically, our method (1) selects salient predicates and object heads, (2) disambiguates predicate senses using only a verb sense dictionary, and (3) obtains event types by jointly embedding and clustering &lt;predicate sense, object head&gt; pairs in a latent spherical space. Our experiments, on three datasets from different domains, show our method can discover salient and high-quality event types, according to both automatic and human evaluations.</abstract>
      <url hash="08ef28ef">2021.emnlp-main.441</url>
      <bibkey>shen-etal-2021-corpus</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.441</doi>
      <video href="2021.emnlp-main.441.mp4"/>
      <pwccode url="https://github.com/mickeystroller/etypeclus" additional="false">mickeystroller/etypeclus</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="442">
      <title><fixed-case>PDALN</fixed-case>: Progressive Domain Adaptation over a Pre-trained Model for Low-Resource Cross-Domain Named Entity Recognition</title>
      <author><first>Tao</first><last>Zhang</last></author>
      <author><first>Congying</first><last>Xia</last></author>
      <author><first>Philip S.</first><last>Yu</last></author>
      <author><first>Zhiwei</first><last>Liu</last></author>
      <author><first>Shu</first><last>Zhao</last></author>
      <pages>5441–5451</pages>
      <abstract>Cross-domain Named Entity Recognition (NER) transfers the NER knowledge from high-resource domains to the low-resource target domain. Due to limited labeled resources and domain shift, cross-domain NER is a challenging task. To address these challenges, we propose a progressive domain adaptation Knowledge Distillation (KD) approach – PDALN. It achieves superior domain adaptability by employing three components: (1) Adaptive data augmentation techniques, which alleviate cross-domain gap and label sparsity simultaneously; (2) Multi-level Domain invariant features, derived from a multi-grained MMD (Maximum Mean Discrepancy) approach, to enable knowledge transfer across domains; (3) Advanced KD schema, which progressively enables powerful pre-trained language models to perform domain adaptation. Extensive experiments on four benchmarks show that PDALN can effectively adapt high-resource domains to low-resource target domains, even if they are diverse in terms and writing styles. Comparison with other baselines indicates the state-of-the-art performance of PDALN.</abstract>
      <url hash="47507954">2021.emnlp-main.442</url>
      <bibkey>zhang-etal-2021-pdaln</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.442</doi>
      <video href="2021.emnlp-main.442.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2016-ner">WNUT 2016 NER</pwcdataset>
    </paper>
    <paper id="443">
      <title>Multi-Vector Attention Models for Deep Re-ranking</title>
      <author><first>Giulio</first><last>Zhou</last></author>
      <author><first>Jacob</first><last>Devlin</last></author>
      <pages>5452–5456</pages>
      <abstract>Large-scale document retrieval systems often utilize two styles of neural network models which live at two different ends of the joint computation vs. accuracy spectrum. The first style is dual encoder (or two-tower) models, where the query and document representations are computed completely independently and combined with a simple dot product operation. The second style is cross-attention models, where the query and document features are concatenated in the input layer and all computation is based on the joint query-document representation. Dual encoder models are typically used for retrieval and deep re-ranking, while cross-attention models are typically used for shallow re-ranking. In this paper, we present a lightweight architecture that explores this joint cost vs. accuracy trade-off based on multi-vector attention (MVA). We thoroughly evaluate our method on the MS-MARCO passage retrieval dataset and show how to efficiently trade off retrieval accuracy with joint computation and offline document storage cost. We show that a highly compressed document representation and inexpensive joint computation can be achieved through a combination of learned pooling tokens and aggressive downprojection. Our code and model checkpoints are open-source and available on GitHub.</abstract>
      <url hash="2ad032fa">2021.emnlp-main.443</url>
      <bibkey>zhou-devlin-2021-multi</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.443</doi>
      <video href="2021.emnlp-main.443.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
    </paper>
    <paper id="444">
      <title>Toward Deconfounding the Effect of Entity Demographics for Question Answering Accuracy</title>
      <author><first>Maharshi</first><last>Gor</last></author>
      <author><first>Kellie</first><last>Webster</last></author>
      <author><first>Jordan</first><last>Boyd-Graber</last></author>
      <pages>5457–5473</pages>
      <abstract>The goal of question answering (QA) is to answer _any_ question. However, major QA datasets have skewed distributions over gender, profession, and nationality. Despite that skew, an analysis of model accuracy reveals little evidence that accuracy is lower for people based on gender or nationality; instead, there is more variation on professions (question topic) and question ambiguity. But QA’s lack of representation could itself hide evidence of bias, necessitating QA datasets that better represent global diversity.</abstract>
      <url hash="fc063721">2021.emnlp-main.444</url>
      <bibkey>gor-etal-2021-toward</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.444</doi>
      <video href="2021.emnlp-main.444.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="445">
      <title>Exploring Strategies for Generalizable Commonsense Reasoning with Pre-trained Models</title>
      <author><first>Kaixin</first><last>Ma</last></author>
      <author><first>Filip</first><last>Ilievski</last></author>
      <author><first>Jonathan</first><last>Francis</last></author>
      <author><first>Satoru</first><last>Ozaki</last></author>
      <author><first>Eric</first><last>Nyberg</last></author>
      <author><first>Alessandro</first><last>Oltramari</last></author>
      <pages>5474–5483</pages>
      <abstract>Commonsense reasoning benchmarks have been largely solved by fine-tuning language models. The downside is that fine-tuning may cause models to overfit to task-specific data and thereby forget their knowledge gained during pre-training. Recent works only propose lightweight model updates as models may already possess useful knowledge from past experience, but a challenge remains in understanding what parts and to what extent models should be refined for a given task. In this paper, we investigate what models learn from commonsense reasoning datasets. We measure the impact of three different adaptation methods on the generalization and accuracy of models. Our experiments with two models show that fine-tuning performs best, by learning both the content and the structure of the task, but suffers from overfitting and limited generalization to novel answers. We observe that alternative adaptation methods like prefix-tuning have comparable accuracy, but generalize better to unseen answers and are more robust to adversarial splits.</abstract>
      <url hash="96ae37b4">2021.emnlp-main.445</url>
      <bibkey>ma-etal-2021-exploring</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.445</doi>
      <video href="2021.emnlp-main.445.mp4"/>
      <pwccode url="https://github.com/mayer123/cs_model_adaptation" additional="false">mayer123/cs_model_adaptation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/commongen">CommonGen</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="446">
      <title>Transformer Feed-Forward Layers Are Key-Value Memories</title>
      <author><first>Mor</first><last>Geva</last></author>
      <author><first>Roei</first><last>Schuster</last></author>
      <author><first>Jonathan</first><last>Berant</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <pages>5484–5495</pages>
      <abstract>Feed-forward layers constitute two-thirds of a transformer model’s parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys’ input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model’s layers via residual connections to produce the final output distribution.</abstract>
      <url hash="d2a3c43d">2021.emnlp-main.446</url>
      <bibkey>geva-etal-2021-transformer</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.446</doi>
      <video href="2021.emnlp-main.446.mp4"/>
      <pwccode url="https://github.com/mega002/ff-layers" additional="false">mega002/ff-layers</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="447">
      <title>Connecting Attributions and <fixed-case>QA</fixed-case> Model Behavior on Realistic Counterfactuals</title>
      <author><first>Xi</first><last>Ye</last></author>
      <author><first>Rohan</first><last>Nair</last></author>
      <author><first>Greg</first><last>Durrett</last></author>
      <pages>5496–5512</pages>
      <abstract>When a model attribution technique highlights a particular part of the input, a user might understand this highlight as making a statement about counterfactuals (Miller, 2019): if that part of the input were to change, the model’s prediction might change as well. This paper investigates how well different attribution techniques align with this assumption on realistic counterfactuals in the case of reading comprehension (RC). RC is a particularly challenging test case, as token-level attributions that have been extensively studied in other NLP tasks such as sentiment analysis are less suitable to represent the reasoning that RC models perform. We construct counterfactual sets for three different RC settings, and through heuristics that can connect attribution methods’ outputs to high-level model behavior, we can evaluate how useful different attribution methods and even different formats are for understanding counterfactuals. We find that pairwise attributions are better suited to RC than token-level attributions across these different RC settings, with our best performance coming from a modification that we propose to an existing pairwise attribution method.</abstract>
      <url hash="3e8b1385">2021.emnlp-main.447</url>
      <bibkey>ye-etal-2021-connecting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.447</doi>
      <video href="2021.emnlp-main.447.mp4"/>
      <pwccode url="https://github.com/xiye17/EvalQAExpl" additional="false">xiye17/EvalQAExpl</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="448">
      <title>How Do Neural Sequence Models Generalize? Local and Global Cues for Out-of-Distribution Prediction</title>
      <author><first>D. Anthony</first><last>Bau</last></author>
      <author><first>Jacob</first><last>Andreas</last></author>
      <pages>5513–5526</pages>
      <abstract>After a neural sequence model encounters an unexpected token, can its behavior be predicted? We show that RNN and transformer language models exhibit structured, consistent generalization in out-of-distribution contexts. We begin by introducing two idealized models of generalization in next-word prediction: a lexical context model in which generalization is consistent with the last word observed, and a syntactic context model in which generalization is consistent with the global structure of the input. In experiments in English, Finnish, Mandarin, and random regular languages, we demonstrate that neural language models interpolate between these two forms of generalization: their predictions are well-approximated by a log-linear combination of lexical and syntactic predictive distributions. We then show that, in some languages, noise mediates the two forms of generalization: noise applied to input tokens encourages syntactic generalization, while noise in history representations encourages lexical generalization. Finally, we offer a preliminary theoretical explanation of these results by proving that the observed interpolation behavior is expected in log-linear models with a particular feature correlation structure. These results help explain the effectiveness of two popular regularization schemes and show that aspects of sequence model generalization can be understood and controlled.</abstract>
      <url hash="39e82f9f">2021.emnlp-main.448</url>
      <bibkey>bau-andreas-2021-neural</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.448</doi>
      <video href="2021.emnlp-main.448.mp4"/>
    </paper>
    <paper id="449">
      <title>Comparing Text Representations: <fixed-case>A</fixed-case> Theory-Driven Approach</title>
      <author><first>Gregory</first><last>Yauney</last></author>
      <author><first>David</first><last>Mimno</last></author>
      <pages>5527–5539</pages>
      <abstract>Much of the progress in contemporary NLP has come from learning representations, such as masked language model (MLM) contextual embeddings, that turn challenging problems into simple classification tasks. But how do we quantify and explain this effect? We adapt general tools from computational learning theory to fit the specific characteristics of text datasets and present a method to evaluate the compatibility between representations and tasks. Even though many tasks can be easily solved with simple bag-of-words (BOW) representations, BOW does poorly on hard natural language inference tasks. For one such task we find that BOW cannot distinguish between real and randomized labelings, while pre-trained MLM representations show 72x greater distinction between real and random labelings than BOW. This method provides a calibrated, quantitative measure of the difficulty of a classification-based NLP task, enabling comparisons between representations without requiring empirical evaluations that may be sensitive to initializations and hyperparameters. The method provides a fresh perspective on the patterns in a dataset and the alignment of those patterns with specific labels.</abstract>
      <url hash="6fe4fed0">2021.emnlp-main.449</url>
      <bibkey>yauney-mimno-2021-comparing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.449</doi>
      <video href="2021.emnlp-main.449.mp4"/>
      <pwccode url="https://github.com/gyauney/data-label-alignment" additional="false">gyauney/data-label-alignment</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="450">
      <title>Human Rationales as Attribution Priors for Explainable Stance Detection</title>
      <author><first>Sahil</first><last>Jayaram</last></author>
      <author><first>Emily</first><last>Allaway</last></author>
      <pages>5540–5554</pages>
      <abstract>As NLP systems become better at detecting opinions and beliefs from text, it is important to ensure not only that models are accurate but also that they arrive at their predictions in ways that align with human reasoning. In this work, we present a method for imparting human-like rationalization to a stance detection model using crowdsourced annotations on a small fraction of the training data. We show that in a data-scarce setting, our approach can improve the reasoning of a state-of-the-art classifier—particularly for inputs containing challenging phenomena such as sarcasm—at no cost in predictive performance. Furthermore, we demonstrate that attention weights surpass a leading attribution method in providing faithful explanations of our model’s predictions, thus serving as a computationally cheap and reliable source of attributions for our model.</abstract>
      <url hash="dfcee043">2021.emnlp-main.450</url>
      <bibkey>jayaram-allaway-2021-human</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.450</doi>
      <video href="2021.emnlp-main.450.mp4"/>
      <pwccode url="https://github.com/sahilj97/explainable-stance-detection" additional="false">sahilj97/explainable-stance-detection</pwccode>
    </paper>
    <paper id="451">
      <title>The Stem Cell Hypothesis: Dilemma behind Multi-Task Learning with Transformer Encoders</title>
      <author><first>Han</first><last>He</last></author>
      <author><first>Jinho D.</first><last>Choi</last></author>
      <pages>5555–5577</pages>
      <abstract>Multi-task learning with transformer encoders (MTL) has emerged as a powerful technique to improve performance on closely-related tasks for both accuracy and efficiency while a question still remains whether or not it would perform as well on tasks that are distinct in nature. We first present MTL results on five NLP tasks, POS, NER, DEP, CON, and SRL, and depict its deficiency over single-task learning. We then conduct an extensive pruning analysis to show that a certain set of attention heads get claimed by most tasks during MTL, who interfere with one another to fine-tune those heads for their own objectives. Based on this finding, we propose the Stem Cell Hypothesis to reveal the existence of attention heads naturally talented for many tasks that cannot be jointly trained to create adequate embeddings for all of those tasks. Finally, we design novel parameter-free probes to justify our hypothesis and demonstrate how attention heads are transformed across the five tasks during MTL through label analysis.</abstract>
      <url hash="d74d25cc">2021.emnlp-main.451</url>
      <bibkey>he-choi-2021-stem</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.451</doi>
      <video href="2021.emnlp-main.451.mp4"/>
      <pwccode url="https://github.com/emorynlp/stem-cell-hypothesis" additional="false">emorynlp/stem-cell-hypothesis</pwccode>
    </paper>
    <paper id="452">
      <title>Text Counterfactuals via Latent Optimization and <fixed-case>S</fixed-case>hapley-Guided Search</title>
      <author><first>Xiaoli</first><last>Fern</last></author>
      <author><first>Quintin</first><last>Pope</last></author>
      <pages>5578–5593</pages>
      <abstract>We study the problem of generating counterfactual text for a classifier as a means for understanding and debugging classification. Given a textual input and a classification model, we aim to minimally alter the text to change the model’s prediction. White-box approaches have been successfully applied to similar problems in vision where one can directly optimize the continuous input. Optimization-based approaches become difficult in the language domain due to the discrete nature of text. We bypass this issue by directly optimizing in the latent space and leveraging a language model to generate candidate modifications from optimized latent representations. We additionally use Shapley values to estimate the combinatoric effect of multiple changes. We then use these estimates to guide a beam search for the final counterfactual text. We achieve favorable performance compared to recent white-box and black-box baselines using human and automatic evaluations. Ablation studies show that both latent optimization and the use of Shapley values improve success rate and the quality of the generated counterfactuals.</abstract>
      <url hash="b725ebe1">2021.emnlp-main.452</url>
      <attachment type="Software" hash="afb5d814">2021.emnlp-main.452.Software.zip</attachment>
      <bibkey>fern-pope-2021-text</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.452</doi>
      <video href="2021.emnlp-main.452.mp4"/>
      <pwccode url="https://github.com/QuintinPope/CLOSS" additional="false">QuintinPope/CLOSS</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="453">
      <title>“Average” Approximates “First Principal Component”? An Empirical Analysis on Representations from Neural Language Models</title>
      <author><first>Zihan</first><last>Wang</last></author>
      <author><first>Chengyu</first><last>Dong</last></author>
      <author><first>Jingbo</first><last>Shang</last></author>
      <pages>5594–5603</pages>
      <abstract>Contextualized representations based on neural language models have furthered the state of the art in various NLP tasks. Despite its great success, the nature of such representations remains a mystery. In this paper, we present an empirical property of these representations—”average” approximates “first principal component”. Specifically, experiments show that the average of these representations shares almost the same direction as the first principal component of the matrix whose columns are these representations. We believe this explains why the average representation is always a simple yet strong baseline. Our further examinations show that this property also holds in more challenging scenarios, for example, when the representations are from a model right after its random initialization. Therefore, we conjecture that this property is intrinsic to the distribution of representations and not necessarily related to the input structure. We realize that these representations empirically follow a normal distribution for each dimension, and by assuming this is true, we demonstrate that the empirical property can be in fact derived mathematically.</abstract>
      <url hash="ffa0cd89">2021.emnlp-main.453</url>
      <bibkey>wang-etal-2021-average</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.453</doi>
      <video href="2021.emnlp-main.453.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/kp20k">KP20k</pwcdataset>
    </paper>
    <paper id="454">
      <title>Controlled Evaluation of Grammatical Knowledge in <fixed-case>M</fixed-case>andarin <fixed-case>C</fixed-case>hinese Language Models</title>
      <author><first>Yiwen</first><last>Wang</last></author>
      <author><first>Jennifer</first><last>Hu</last></author>
      <author><first>Roger</first><last>Levy</last></author>
      <author><first>Peng</first><last>Qian</last></author>
      <pages>5604–5620</pages>
      <abstract>Prior work has shown that structural supervision helps English language models learn generalizations about syntactic phenomena such as subject-verb agreement. However, it remains unclear if such an inductive bias would also improve language models’ ability to learn grammatical dependencies in typologically different languages. Here we investigate this question in Mandarin Chinese, which has a logographic, largely syllable-based writing system; different word order; and sparser morphology than English. We train LSTMs, Recurrent Neural Network Grammars, Transformer language models, and Transformer-parameterized generative parsing models on two Mandarin Chinese datasets of different sizes. We evaluate the models’ ability to learn different aspects of Mandarin grammar that assess syntactic and semantic relationships. We find suggestive evidence that structural supervision helps with representing syntactic state across intervening content and improves performance in low-data settings, suggesting that the benefits of hierarchical inductive biases in acquiring dependency relationships may extend beyond English.</abstract>
      <url hash="f059d76d">2021.emnlp-main.454</url>
      <bibkey>wang-etal-2021-controlled</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.454</doi>
      <video href="2021.emnlp-main.454.mp4"/>
      <pwccode url="https://github.com/yiwenwang03/syntactic-generalization-mandarin" additional="false">yiwenwang03/syntactic-generalization-mandarin</pwccode>
    </paper>
    <paper id="455">
      <title><fixed-case>G</fixed-case>rad<fixed-case>TS</fixed-case>: A Gradient-Based Automatic Auxiliary Task Selection Method Based on Transformer Networks</title>
      <author><first>Weicheng</first><last>Ma</last></author>
      <author><first>Renze</first><last>Lou</last></author>
      <author><first>Kai</first><last>Zhang</last></author>
      <author><first>Lili</first><last>Wang</last></author>
      <author><first>Soroush</first><last>Vosoughi</last></author>
      <pages>5621–5632</pages>
      <abstract>A key problem in multi-task learning (MTL) research is how to select high-quality auxiliary tasks automatically. This paper presents GradTS, an automatic auxiliary task selection method based on gradient calculation in Transformer-based models. Compared to AUTOSEM, a strong baseline method, GradTS improves the performance of MT-DNN with a bert-base-cased backend model, from 0.33% to 17.93% on 8 natural language understanding (NLU) tasks in the GLUE benchmarks. GradTS is also time-saving since (1) its gradient calculations are based on single-task experiments and (2) the gradients are re-used without additional experiments when the candidate task set changes. On the 8 GLUE classification tasks, for example, GradTS costs on average 21.32% less time than AUTOSEM with comparable GPU consumption. Further, we show the robustness of GradTS across various task settings and model selections, e.g. mixed objectives among candidate tasks. The efficiency and efficacy of GradTS in these case studies illustrate its general applicability in MTL research without requiring manual task filtering or costly parameter tuning.</abstract>
      <url hash="42b49de4">2021.emnlp-main.455</url>
      <bibkey>ma-etal-2021-gradts</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.455</doi>
      <video href="2021.emnlp-main.455.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/meld">MELD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="456">
      <title><fixed-case>N</fixed-case>egat<fixed-case>ER</fixed-case>: <fixed-case>U</fixed-case>nsupervised <fixed-case>D</fixed-case>iscovery of <fixed-case>N</fixed-case>egatives in <fixed-case>C</fixed-case>ommonsense <fixed-case>K</fixed-case>nowledge <fixed-case>B</fixed-case>ases</title>
      <author><first>Tara</first><last>Safavi</last></author>
      <author><first>Jing</first><last>Zhu</last></author>
      <author><first>Danai</first><last>Koutra</last></author>
      <pages>5633–5646</pages>
      <abstract>Codifying commonsense knowledge in machines is a longstanding goal of artificial intelligence. Recently, much progress toward this goal has been made with automatic knowledge base (KB) construction techniques. However, such techniques focus primarily on the acquisition of positive (true) KB statements, even though negative (false) statements are often also important for discriminative reasoning over commonsense KBs. As a first step toward the latter, this paper proposes NegatER, a framework that ranks potential negatives in commonsense KBs using a contextual language model (LM). Importantly, as most KBs do not contain negatives, NegatER relies only on the positive knowledge in the LM and does not require ground-truth negative examples. Experiments demonstrate that, compared to multiple contrastive data augmentation approaches, NegatER yields negatives that are more grammatical, coherent, and informative—leading to statistically significant accuracy improvements in a challenging KB completion task and confirming that the positive knowledge in LMs can be “re-purposed” to generate negative knowledge.</abstract>
      <url hash="0d09cf6c">2021.emnlp-main.456</url>
      <bibkey>safavi-etal-2021-negater</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.456</doi>
      <video href="2021.emnlp-main.456.mp4"/>
      <pwccode url="https://github.com/tsafavi/negater" additional="false">tsafavi/negater</pwccode>
    </paper>
    <paper id="457">
      <title>Instance-adaptive training with noise-robust losses against noisy labels</title>
      <author><first>Lifeng</first><last>Jin</last></author>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Kun</first><last>Xu</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <pages>5647–5663</pages>
      <abstract>In order to alleviate the huge demand for annotated datasets for different tasks, many recent natural language processing datasets have adopted automated pipelines for fast-tracking usable data. However, model training with such datasets poses a challenge because popular optimization objectives are not robust to label noise induced in the annotation generation process. Several noise-robust losses have been proposed and evaluated on tasks in computer vision, but they generally use a single dataset-wise hyperparamter to control the strength of noise resistance. This work proposes novel instance-adaptive training frameworks to change single dataset-wise hyperparameters of noise resistance in such losses to be instance-wise. Such instance-wise noise resistance hyperparameters are predicted by special instance-level label quality predictors, which are trained along with the main classification models. Experiments on noisy and corrupted NLP datasets show that proposed instance-adaptive training frameworks help increase the noise-robustness provided by such losses, promoting the use of the frameworks and associated losses in NLP models trained with noisy data.</abstract>
      <url hash="4d50790d">2021.emnlp-main.457</url>
      <bibkey>jin-etal-2021-instance</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.457</doi>
      <video href="2021.emnlp-main.457.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="458">
      <title>Distributionally Robust Multilingual Machine Translation</title>
      <author><first>Chunting</first><last>Zhou</last></author>
      <author><first>Daniel</first><last>Levy</last></author>
      <author><first>Xian</first><last>Li</last></author>
      <author><first>Marjan</first><last>Ghazvininejad</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>5664–5674</pages>
      <abstract>Multilingual neural machine translation (MNMT) learns to translate multiple language pairs with a single model, potentially improving both the accuracy and the memory-efficiency of deployed models. However, the heavy data imbalance between languages hinders the model from performing uniformly across language pairs. In this paper, we propose a new learning objective for MNMT based on distributionally robust optimization, which minimizes the worst-case expected loss over the set of language pairs. We further show how to practically optimize this objective for large translation corpora using an iterated best response scheme, which is both effective and incurs negligible additional computational cost compared to standard empirical risk minimization. We perform extensive experiments on three sets of languages from two datasets and show that our method consistently outperforms strong baseline methods in terms of average and per-language performance under both many-to-one and one-to-many translation settings.</abstract>
      <url hash="86b0c8df">2021.emnlp-main.458</url>
      <bibkey>zhou-etal-2021-distributionally</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.458</doi>
      <video href="2021.emnlp-main.458.mp4"/>
      <pwccode url="https://github.com/violet-zct/fairseq-dro-mnmt" additional="false">violet-zct/fairseq-dro-mnmt</pwccode>
    </paper>
    <paper id="459">
      <title>Model Selection for Cross-lingual Transfer</title>
      <author><first>Yang</first><last>Chen</last></author>
      <author><first>Alan</first><last>Ritter</last></author>
      <pages>5675–5687</pages>
      <abstract>Transformers that are pre-trained on multilingual corpora, such as, mBERT and XLM-RoBERTa, have achieved impressive cross-lingual transfer capabilities. In the zero-shot transfer setting, only English training data is used, and the fine-tuned model is evaluated on another target language. While this works surprisingly well, substantial variance has been observed in target language performance between different fine-tuning runs, and in the zero-shot setup, no target-language development data is available to select among multiple fine-tuned models. Prior work has relied on English dev data to select among models that are fine-tuned with different learning rates, number of steps and other hyperparameters, often resulting in suboptimal choices. In this paper, we show that it is possible to select consistently better models when small amounts of annotated data are available in auxiliary pivot languages. We propose a machine learning approach to model selection that uses the fine-tuned model’s own internal representations to predict its cross-lingual capabilities. In extensive experiments we find that this method consistently selects better models than English validation data across twenty five languages (including eight low-resource languages), and often achieves results that are comparable to model selection using target language development data.</abstract>
      <url hash="48886322">2021.emnlp-main.459</url>
      <bibkey>chen-ritter-2021-model</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.459</doi>
      <video href="2021.emnlp-main.459.mp4"/>
      <pwccode url="https://github.com/edchengg/model_selection" additional="false">edchengg/model_selection</pwccode>
    </paper>
    <paper id="460">
      <title>Continual Few-Shot Learning for Text Classification</title>
      <author><first>Ramakanth</first><last>Pasunuru</last></author>
      <author><first>Veselin</first><last>Stoyanov</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>5688–5702</pages>
      <abstract>Natural Language Processing (NLP) is increasingly relying on general end-to-end systems that need to handle many different linguistic phenomena and nuances. For example, a Natural Language Inference (NLI) system has to recognize sentiment, handle numbers, perform coreference, etc. Our solutions to complex problems are still far from perfect, so it is important to create systems that can learn to correct mistakes quickly, incrementally, and with little training data. In this work, we propose a continual few-shot learning (CFL) task, in which a system is challenged with a difficult phenomenon and asked to learn to correct mistakes with only a few (10 to 15) training examples. To this end, we first create benchmarks based on previously annotated data: two NLI (ANLI and SNLI) and one sentiment analysis (IMDB) datasets. Next, we present various baselines from diverse paradigms (e.g., memory-aware synapses and Prototypical networks) and compare them on few-shot learning and continual few-shot learning setups. Our contributions are in creating a benchmark suite and evaluation protocol for continual few-shot learning on the text classification tasks, and making several interesting observations on the behavior of similarity-based methods. We hope that our work serves as a useful starting point for future work on this important topic.</abstract>
      <url hash="504d4f84">2021.emnlp-main.460</url>
      <bibkey>pasunuru-etal-2021-continual</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.460</doi>
      <video href="2021.emnlp-main.460.mp4"/>
      <pwccode url="https://github.com/ramakanth-pasunuru/cfl-benchmark" additional="false">ramakanth-pasunuru/cfl-benchmark</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="461">
      <title>Efficient Nearest Neighbor Language Models</title>
      <author><first>Junxian</first><last>He</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last></author>
      <pages>5703–5714</pages>
      <abstract>Non-parametric neural language models (NLMs) learn predictive distributions of text utilizing an external datastore, which allows them to learn through explicitly memorizing the training datapoints. While effective, these models often require retrieval from a large datastore at test time, significantly increasing the inference overhead and thus limiting the deployment of non-parametric NLMs in practical applications. In this paper, we take the recently proposed k-nearest neighbors language model as an example, exploring methods to improve its efficiency along various dimensions. Experiments on the standard WikiText-103 benchmark and domain-adaptation datasets show that our methods are able to achieve up to a 6x speed-up in inference speed while retaining comparable performance. The empirical analysis we present may provide guidelines for future research seeking to develop or deploy more efficient non-parametric NLMs.</abstract>
      <url hash="bbcbf48d">2021.emnlp-main.461</url>
      <bibkey>he-etal-2021-efficient</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.461</doi>
      <video href="2021.emnlp-main.461.mp4"/>
      <pwccode url="https://github.com/jxhe/efficient-knnlm" additional="true">jxhe/efficient-knnlm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="462">
      <title><fixed-case>ST</fixed-case>ra<fixed-case>TA</fixed-case>: Self-Training with Task Augmentation for Better Few-shot Learning</title>
      <author><first>Tu</first><last>Vu</last></author>
      <author><first>Minh-Thang</first><last>Luong</last></author>
      <author><first>Quoc</first><last>Le</last></author>
      <author><first>Grady</first><last>Simon</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <pages>5715–5731</pages>
      <abstract>Despite their recent successes in tackling many NLP tasks, large-scale pre-trained language models do not perform as well in few-shot settings where only a handful of training examples are available. To address this shortcoming, we propose STraTA, which stands for Self-Training with Task Augmentation, an approach that builds on two key ideas for effective leverage of unlabeled data. First, STraTA uses task augmentation, a novel technique that synthesizes a large amount of data for auxiliary-task fine-tuning from target-task unlabeled texts. Second, STraTA performs self-training by further fine-tuning the strong base model created by task augmentation on a broad distribution of pseudo-labeled data. Our experiments demonstrate that STraTA can substantially improve sample efficiency across 12 few-shot benchmarks. Remarkably, on the SST-2 sentiment dataset, STraTA, with only 8 training examples per class, achieves comparable results to standard fine-tuning with 67K training examples. Our analyses reveal that task augmentation and self-training are both complementary and independently effective.</abstract>
      <url hash="8f8c14aa">2021.emnlp-main.462</url>
      <bibkey>vu-etal-2021-strata</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.462</doi>
      <video href="2021.emnlp-main.462.mp4"/>
      <pwccode url="https://github.com/google-research/google-research/tree/master/STraTA" additional="false">google-research/google-research</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="463">
      <title><fixed-case>TADPOLE</fixed-case>: <fixed-case>T</fixed-case>ask <fixed-case>AD</fixed-case>apted <fixed-case>P</fixed-case>re-Training via <fixed-case>A</fixed-case>n<fixed-case>O</fixed-case>ma<fixed-case>L</fixed-case>y <fixed-case>D</fixed-case><fixed-case>E</fixed-case>tection</title>
      <author><first>Vivek</first><last>Madan</last></author>
      <author><first>Ashish</first><last>Khetan</last></author>
      <author><first>Zohar</first><last>Karnin</last></author>
      <pages>5732–5746</pages>
      <abstract>The paradigm of pre-training followed by finetuning has become a standard procedure for NLP tasks, with a known problem of domain shift between the pre-training and downstream corpus. Previous works have tried to mitigate this problem with additional pre-training, either on the downstream corpus itself when it is large enough, or on a manually curated unlabeled corpus of a similar domain. In this paper, we address the problem for the case when the downstream corpus is too small for additional pre-training. We propose TADPOLE, a task adapted pre-training framework based on data selection techniques adapted from <i>Domain Adaptation</i>. We formulate the data selection as an anomaly detection problem that unlike existing methods works well when the downstream corpus is limited in size. It results in a scalable and efficient unsupervised technique that eliminates the need for any manual data curation. We evaluate our framework on eight tasks across four different domains: Biomedical, Computer Science, News, and Movie reviews, and compare its performance against competitive baseline techniques from the area of Domain Adaptation. Our framework outperforms all the baseline methods. On small datasets with less than 5K training examples, we get a gain of 1.82% in performance with additional pre-training for only 5% steps compared to the originally pre-trained models. It also compliments some of the other techniques such as data augmentation known for boosting performance when downstream corpus is small; highest performance is achieved when data augmentation is combined with task adapted pre-training.</abstract>
      <url hash="d3bcac39">2021.emnlp-main.463</url>
      <bibkey>madan-etal-2021-tadpole</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.463</doi>
      <video href="2021.emnlp-main.463.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scierc">SciERC</pwcdataset>
    </paper>
    <paper id="464">
      <title>Gradient-based Adversarial Attacks against Text Transformers</title>
      <author><first>Chuan</first><last>Guo</last></author>
      <author><first>Alexandre</first><last>Sablayrolles</last></author>
      <author><first>Hervé</first><last>Jégou</last></author>
      <author><first>Douwe</first><last>Kiela</last></author>
      <pages>5747–5757</pages>
      <abstract>We propose the first general-purpose gradient-based adversarial attack against transformer models. Instead of searching for a single adversarial example, we search for a distribution of adversarial examples parameterized by a continuous-valued matrix, hence enabling gradient-based optimization. We empirically demonstrate that our white-box attack attains state-of-the-art attack performance on a variety of natural language tasks, outperforming prior work in terms of adversarial success rate with matching imperceptibility as per automated and human evaluation. Furthermore, we show that a powerful black-box transfer attack, enabled by sampling from the adversarial distribution, matches or exceeds existing methods, while only requiring hard-label outputs.</abstract>
      <url hash="41d427f7">2021.emnlp-main.464</url>
      <bibkey>guo-etal-2021-gradient</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.464</doi>
      <video href="2021.emnlp-main.464.mp4"/>
      <pwccode url="https://github.com/facebookresearch/text-adversarial-attack" additional="true">facebookresearch/text-adversarial-attack</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="465">
      <title>Do Transformer Modifications Transfer Across Implementations and Applications?</title>
      <author><first>Sharan</first><last>Narang</last></author>
      <author><first>Hyung Won</first><last>Chung</last></author>
      <author><first>Yi</first><last>Tay</last></author>
      <author><first>Liam</first><last>Fedus</last></author>
      <author><first>Thibault</first><last>Fevry</last></author>
      <author><first>Michael</first><last>Matena</last></author>
      <author><first>Karishma</first><last>Malkan</last></author>
      <author><first>Noah</first><last>Fiedel</last></author>
      <author><first>Noam</first><last>Shazeer</last></author>
      <author><first>Zhenzhong</first><last>Lan</last></author>
      <author><first>Yanqi</first><last>Zhou</last></author>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Nan</first><last>Ding</last></author>
      <author><first>Jake</first><last>Marcus</last></author>
      <author><first>Adam</first><last>Roberts</last></author>
      <author><first>Colin</first><last>Raffel</last></author>
      <pages>5758–5773</pages>
      <abstract>The research community has proposed copious modifications to the Transformer architecture since it was introduced over three years ago, relatively few of which have seen widespread adoption. In this paper, we comprehensively evaluate many of these modifications in a shared experimental setting that covers most of the common uses of the Transformer in natural language processing. Surprisingly, we find that most modifications do not meaningfully improve performance. Furthermore, most of the Transformer variants we found beneficial were either developed in the same codebase that we used or are relatively minor changes. We conjecture that performance improvements may strongly depend on implementation details and correspondingly make some recommendations for improving the generality of experimental results.</abstract>
      <url hash="f395c653">2021.emnlp-main.465</url>
      <bibkey>narang-etal-2021-transformer</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.465</doi>
      <video href="2021.emnlp-main.465.mp4"/>
      <pwccode url="https://github.com/google-research/google-research/tree/master/transformer_modifications/" additional="false">google-research/google-research</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/c4">C4</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestions">WebQuestions</pwcdataset>
    </paper>
    <paper id="466">
      <title>Paired Examples as Indirect Supervision in Latent Decision Models</title>
      <author><first>Nitish</first><last>Gupta</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>5774–5785</pages>
      <abstract>Compositional, structured models are appealing because they explicitly decompose problems and provide interpretable intermediate outputs that give confidence that the model is not simply latching onto data artifacts. Learning these models is challenging, however, because end-task supervision only provides a weak indirect signal on what values the latent decisions should take. This often results in the model failing to learn to perform the intermediate tasks correctly. In this work, we introduce a way to leverage paired examples that provide stronger cues for learning latent decisions. When two related training examples share internal substructure, we add an additional training objective to encourage consistency between their latent decisions. Such an objective does not require external supervision for the values of the latent output, or even the end task, yet provides an additional training signal to that provided by individual training examples themselves. We apply our method to improve compositional question answering using neural module networks on the DROP dataset. We explore three ways to acquire paired questions in DROP: (a) discovering naturally occurring paired examples within the dataset, (b) constructing paired examples using templates, and (c) generating paired examples using a question generation model. We empirically demonstrate that our proposed approach improves both in- and out-of-distribution generalization and leads to correct latent decision predictions.</abstract>
      <url hash="31f73695">2021.emnlp-main.466</url>
      <bibkey>gupta-etal-2021-paired</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.466</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
    </paper>
    <paper id="467">
      <title>Pairwise Supervised Contrastive Learning of Sentence Representations</title>
      <author><first>Dejiao</first><last>Zhang</last></author>
      <author><first>Shang-Wen</first><last>Li</last></author>
      <author><first>Wei</first><last>Xiao</last></author>
      <author><first>Henghui</first><last>Zhu</last></author>
      <author><first>Ramesh</first><last>Nallapati</last></author>
      <author><first>Andrew O.</first><last>Arnold</last></author>
      <author><first>Bing</first><last>Xiang</last></author>
      <pages>5786–5798</pages>
      <abstract>Many recent successes in sentence representation learning have been achieved by simply fine-tuning on the Natural Language Inference (NLI) datasets with triplet loss or siamese loss. Nevertheless, they share a common weakness: sentences in a contradiction pair are not necessarily from different semantic categories. Therefore, optimizing the semantic entailment and contradiction reasoning objective alone is inadequate to capture the high-level semantic structure. The drawback is compounded by the fact that the vanilla siamese or triplet losses only learn from individual sentence pairs or triplets, which often suffer from bad local optima. In this paper, we propose PairSupCon, an instance discrimination based approach aiming to bridge semantic entailment and contradiction understanding with high-level categorical concept encoding. We evaluate PairSupCon on various downstream tasks that involve understanding sentence semantics at different granularities. We outperform the previous state-of-the-art method with 10%–13% averaged improvement on eight clustering tasks, and 5%–6% averaged improvement on seven semantic textual similarity (STS) tasks.</abstract>
      <url hash="0eb1a54c">2021.emnlp-main.467</url>
      <bibkey>zhang-etal-2021-pairwise</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.467</doi>
      <video href="2021.emnlp-main.467.mp4"/>
      <pwccode url="https://github.com/amazon-research/sentence-representations" additional="false">amazon-research/sentence-representations</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="468">
      <title>Muppet: Massive Multi-task Representations with Pre-Finetuning</title>
      <author><first>Armen</first><last>Aghajanyan</last></author>
      <author><first>Anchit</first><last>Gupta</last></author>
      <author><first>Akshat</first><last>Shrivastava</last></author>
      <author><first>Xilun</first><last>Chen</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <author><first>Sonal</first><last>Gupta</last></author>
      <pages>5799–5811</pages>
      <abstract>We propose pre-finetuning, an additional large-scale learning stage between language model pre-training and fine-tuning. Pre-finetuning is massively multi-task learning (around 50 datasets, over 4.8 million total labeled examples), and is designed to encourage learning of representations that generalize better to many different tasks. We show that pre-finetuning consistently improves performance for pretrained discriminators (e.g. RoBERTa) and generation models (e.g. BART) on a wide range of tasks (sentence prediction, commonsense reasoning, MRC, etc.), while also significantly improving sample efficiency during fine-tuning. We also show that large-scale multi-tasking is crucial; pre-finetuning can hurt performance when few tasks are used up until a critical point (usually above 15) after which performance improves linearly in the number of tasks.</abstract>
      <url hash="529197ff">2021.emnlp-main.468</url>
      <bibkey>aghajanyan-etal-2021-muppet</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.468</doi>
      <video href="2021.emnlp-main.468.mp4"/>
      <pwccode url="https://huggingface.co/facebook/muppet-roberta-base" additional="true">facebook/muppet-roberta-base</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hellaswag">HellaSwag</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rte">RTE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reddit">Reddit</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reddit-tifu">Reddit TIFU</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/swag">SWAG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="469">
      <title>Diverse Distributions of Self-Supervised Tasks for Meta-Learning in <fixed-case>NLP</fixed-case></title>
      <author><first>Trapit</first><last>Bansal</last></author>
      <author><first>Karthick Prasad</first><last>Gunasekaran</last></author>
      <author><first>Tong</first><last>Wang</last></author>
      <author><first>Tsendsuren</first><last>Munkhdalai</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>5812–5824</pages>
      <abstract>Meta-learning considers the problem of learning an efficient learning process that can leverage its past experience to accurately solve new tasks. However, the efficacy of meta-learning crucially depends on the distribution of tasks available for training, and this is often assumed to be known a priori or constructed from limited supervised datasets. In this work, we aim to provide task distributions for meta-learning by considering self-supervised tasks automatically proposed from unlabeled text, to enable large-scale meta-learning in NLP. We design multiple distributions of self-supervised tasks by considering important aspects of task diversity, difficulty, type, domain, and curriculum, and investigate how they affect meta-learning performance. Our analysis shows that all these factors meaningfully alter the task distribution, some inducing significant improvements in downstream few-shot accuracy of the meta-learned models. Empirically, results on 20 downstream tasks show significant improvements in few-shot learning – adding up to +4.2% absolute accuracy (on average) to the previous unsupervised meta-learning method, and perform comparably to supervised methods on the FewRel 2.0 benchmark.</abstract>
      <url hash="b816a7df">2021.emnlp-main.469</url>
      <bibkey>bansal-etal-2021-diverse</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.469</doi>
      <video href="2021.emnlp-main.469.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ccnet">CCNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel-2-0">FewRel 2.0</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="470">
      <title>A Simple and Effective Method To Eliminate the Self Language Bias in Multilingual Representations</title>
      <author><first>Ziyi</first><last>Yang</last></author>
      <author><first>Yinfei</first><last>Yang</last></author>
      <author><first>Daniel</first><last>Cer</last></author>
      <author><first>Eric</first><last>Darve</last></author>
      <pages>5825–5832</pages>
      <abstract>Language agnostic and semantic-language information isolation is an emerging research direction for multilingual representations models. We explore this problem from a novel angle of geometric algebra and semantic space. A simple but highly effective method “Language Information Removal (LIR)” factors out language identity information from semantic related components in multilingual representations pre-trained on multi-monolingual data. A post-training and model-agnostic method, LIR only uses simple linear operations, e.g. matrix factorization and orthogonal projection. LIR reveals that for weak-alignment multilingual systems, the principal components of semantic spaces primarily encodes language identity information. We first evaluate the LIR on a cross-lingual question answer retrieval task (LAReQA), which requires the strong alignment for the multilingual embedding space. Experiment shows that LIR is highly effectively on this task, yielding almost 100% relative improvement in MAP for weak-alignment models. We then evaluate the LIR on Amazon Reviews and XEVAL dataset, with the observation that removing language information is able to improve the cross-lingual transfer performance.</abstract>
      <url hash="8efe8383">2021.emnlp-main.470</url>
      <bibkey>yang-etal-2021-simple</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.470</doi>
      <video href="2021.emnlp-main.470.mp4"/>
      <pwccode url="https://github.com/ziyi-yang/lir" additional="false">ziyi-yang/lir</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad-r">LAReQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wiki-40b">Wiki-40B</pwcdataset>
    </paper>
    <paper id="471">
      <title>A Massively Multilingual Analysis of Cross-linguality in Shared Embedding Space</title>
      <author><first>Alexander</first><last>Jones</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <author><first>Kyle</first><last>Mahowald</last></author>
      <pages>5833–5847</pages>
      <abstract>In cross-lingual language models, representations for many different languages live in the same space. Here, we investigate the linguistic and non-linguistic factors affecting sentence-level alignment in cross-lingual pretrained language models for 101 languages and 5,050 language pairs. Using BERT-based LaBSE and BiLSTM-based LASER as our models, and the Bible as our corpus, we compute a task-based measure of cross-lingual alignment in the form of bitext retrieval performance, as well as four intrinsic measures of vector space alignment and isomorphism. We then examine a range of linguistic, quasi-linguistic, and training-related features as potential predictors of these alignment metrics. The results of our analyses show that word order agreement and agreement in morphological complexity are two of the strongest linguistic predictors of cross-linguality. We also note in-family training data as a stronger predictor than language-specific training data across the board. We verify some of our linguistic findings by looking at the effect of morphological segmentation on English-Inuktitut alignment, in addition to examining the effect of word order agreement on isomorphism for 66 zero-shot language pairs from a different corpus. We make the data and code for our experiments publicly available.</abstract>
      <url hash="6b7f3c59">2021.emnlp-main.471</url>
      <attachment type="Software" hash="6e5c2a8b">2021.emnlp-main.471.Software.zip</attachment>
      <bibkey>jones-etal-2021-massively</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.471</doi>
      <video href="2021.emnlp-main.471.mp4"/>
      <pwccode url="https://github.com/alexjonesnlp/xlanalysis5k" additional="false">alexjonesnlp/xlanalysis5k</pwccode>
    </paper>
    <paper id="472">
      <title>Frustratingly Simple but Surprisingly Strong: Using Language-Independent Features for Zero-shot Cross-lingual Semantic Parsing</title>
      <author><first>Jingfeng</first><last>Yang</last></author>
      <author><first>Federico</first><last>Fancellu</last></author>
      <author><first>Bonnie</first><last>Webber</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>5848–5856</pages>
      <abstract>The availability of corpora has led to significant advances in training semantic parsers in English. Unfortunately, for languages other than English, annotated data is limited and so is the performance of the developed parsers. Recently, pretrained multilingual models have been proven useful for zero-shot cross-lingual transfer in many NLP tasks. What else does it require to apply a parser trained in English to other languages for zero-shot cross-lingual semantic parsing? Will simple language-independent features help? To this end, we experiment with six Discourse Representation Structure (DRS) semantic parsers in English, and generalize them to Italian, German and Dutch, where there are only a small number of manually annotated parses available. Extensive experiments show that despite its simplicity, adding Universal Dependency (UD) relations and Universal POS tags (UPOS) as model-agnostic features achieves surprisingly strong improvement on all parsers.</abstract>
      <url hash="36ad4e94">2021.emnlp-main.472</url>
      <attachment type="Software" hash="23f7f604">2021.emnlp-main.472.Software.zip</attachment>
      <bibkey>yang-etal-2021-frustratingly</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.472</doi>
      <video href="2021.emnlp-main.472.mp4"/>
      <pwccode url="https://github.com/gt-salt/multilingual-drs-semantic-parsing" additional="false">gt-salt/multilingual-drs-semantic-parsing</pwccode>
    </paper>
    <paper id="473">
      <title>Improving Simultaneous Translation by Incorporating Pseudo-References with Fewer Reorderings</title>
      <author><first>Junkun</first><last>Chen</last></author>
      <author><first>Renjie</first><last>Zheng</last></author>
      <author><first>Atsuhito</first><last>Kita</last></author>
      <author><first>Mingbo</first><last>Ma</last></author>
      <author><first>Liang</first><last>Huang</last></author>
      <pages>5857–5864</pages>
      <abstract>Simultaneous translation is vastly different from full-sentence translation, in the sense that it starts translation before the source sentence ends, with only a few words delay. However, due to the lack of large-scale, high-quality simultaneous translation datasets, most such systems are still trained on conventional full-sentence bitexts. This is far from ideal for the simultaneous scenario due to the abundance of unnecessary long-distance reorderings in those bitexts. We propose a novel method that rewrites the target side of existing full-sentence corpora into simultaneous-style translation. Experiments on Zh<tex-math>\rightarrow</tex-math>En and Ja<tex-math>\rightarrow</tex-math>En simultaneous translation show substantial improvements (up to +2.7 BLEU) with the addition of these generated pseudo-references.</abstract>
      <url hash="55262223">2021.emnlp-main.473</url>
      <bibkey>chen-etal-2021-improving-simultaneous</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.473</doi>
      <video href="2021.emnlp-main.473.mp4"/>
    </paper>
    <paper id="474">
      <title>Classification-based Quality Estimation: Small and Efficient Models for Real-world Applications</title>
      <author><first>Shuo</first><last>Sun</last></author>
      <author><first>Ahmed</first><last>El-Kishky</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>James</first><last>Cross</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <author><first>Francisco</first><last>Guzmán</last></author>
      <pages>5865–5875</pages>
      <abstract>Sentence-level Quality estimation (QE) of machine translation is traditionally formulated as a regression task, and the performance of QE models is typically measured by Pearson correlation with human labels. Recent QE models have achieved previously-unseen levels of correlation with human judgments, but they rely on large multilingual contextualized language models that are computationally expensive and make them infeasible for real-world applications. In this work, we evaluate several model compression techniques for QE and find that, despite their popularity in other NLP tasks, they lead to poor performance in this regression setting. We observe that a full model parameterization is required to achieve SoTA results in a regression task. However, we argue that the level of expressiveness of a model in a continuous range is unnecessary given the downstream applications of QE, and show that reframing QE as a classification problem and evaluating QE models using classification metrics would better reflect their actual performance in real-world applications.</abstract>
      <url hash="db4e751d">2021.emnlp-main.474</url>
      <bibkey>sun-etal-2021-classification</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.474</doi>
      <video href="2021.emnlp-main.474.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqe-pe">MLQE-PE</pwcdataset>
    </paper>
    <paper id="475">
      <title>A Large-Scale Study of Machine Translation in <fixed-case>T</fixed-case>urkic Languages</title>
      <author><first>Jamshidbek</first><last>Mirzakhalov</last></author>
      <author><first>Anoop</first><last>Babu</last></author>
      <author><first>Duygu</first><last>Ataman</last></author>
      <author><first>Sherzod</first><last>Kariev</last></author>
      <author><first>Francis</first><last>Tyers</last></author>
      <author><first>Otabek</first><last>Abduraufov</last></author>
      <author><first>Mammad</first><last>Hajili</last></author>
      <author><first>Sardana</first><last>Ivanova</last></author>
      <author><first>Abror</first><last>Khaytbaev</last></author>
      <author><first>Antonio</first><last>Laverghetta Jr.</last></author>
      <author><first>Bekhzodbek</first><last>Moydinboyev</last></author>
      <author><first>Esra</first><last>Onal</last></author>
      <author><first>Shaxnoza</first><last>Pulatova</last></author>
      <author><first>Ahsan</first><last>Wahab</last></author>
      <author><first>Orhan</first><last>Firat</last></author>
      <author><first>Sriram</first><last>Chellappan</last></author>
      <pages>5876–5890</pages>
      <abstract>Recent advances in neural machine translation (NMT) have pushed the quality of machine translation systems to the point where they are becoming widely adopted to build competitive systems. However, there is still a large number of languages that are yet to reap the benefits of NMT. In this paper, we provide the first large-scale case study of the practical application of MT in the Turkic language family in order to realize the gains of NMT for Turkic languages under high-resource to extremely low-resource scenarios. In addition to presenting an extensive analysis that identifies the bottlenecks towards building competitive systems to ameliorate data scarcity, our study has several key contributions, including, i) a large parallel corpus covering 22 Turkic languages consisting of common public datasets in combination with new datasets of approximately 1.4 million parallel sentences, ii) bilingual baselines for 26 language pairs, iii) novel high-quality test sets in three different translation domains and iv) human evaluation scores. All models, scripts, and data will be released to the public.</abstract>
      <url hash="ac6ae7e8">2021.emnlp-main.475</url>
      <bibkey>mirzakhalov-etal-2021-large</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.475</doi>
      <video href="2021.emnlp-main.475.mp4"/>
      <pwccode url="https://github.com/turkic-interlingua/til-mt" additional="false">turkic-interlingua/til-mt</pwccode>
    </paper>
    <paper id="476">
      <title>Analyzing the Surprising Variability in Word Embedding Stability Across Languages</title>
      <author><first>Laura</first><last>Burdick</last></author>
      <author><first>Jonathan K.</first><last>Kummerfeld</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <pages>5891–5901</pages>
      <abstract>Word embeddings are powerful representations that form the foundation of many natural language processing architectures, both in English and in other languages. To gain further insight into word embeddings, we explore their stability (e.g., overlap between the nearest neighbors of a word in different embedding spaces) in diverse languages. We discuss linguistic properties that are related to stability, drawing out insights about correlations with affixing, language gender systems, and other features. This has implications for embedding use, particularly in research that uses them to study language trends.</abstract>
      <url hash="2331dae8">2021.emnlp-main.476</url>
      <bibkey>burdick-etal-2021-analyzing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.476</doi>
      <video href="2021.emnlp-main.476.mp4"/>
      <pwccode url="https://github.com/laura-burdick/multilingual-stability" additional="false">laura-burdick/multilingual-stability</pwccode>
    </paper>
    <paper id="477">
      <title>Rule-based Morphological Inflection Improves Neural Terminology Translation</title>
      <author><first>Weijia</first><last>Xu</last></author>
      <author><first>Marine</first><last>Carpuat</last></author>
      <pages>5902–5914</pages>
      <abstract>Current approaches to incorporating terminology constraints in machine translation (MT) typically assume that the constraint terms are provided in their correct morphological forms. This limits their application to real-world scenarios where constraint terms are provided as lemmas. In this paper, we introduce a modular framework for incorporating lemma constraints in neural MT (NMT) in which linguistic knowledge and diverse types of NMT models can be flexibly applied. It is based on a novel cross-lingual inflection module that inflects the target lemma constraints based on the source context. We explore linguistically motivated rule-based and data-driven neural-based inflection modules and design English-German health and English-Lithuanian news test suites to evaluate them in domain adaptation and low-resource MT settings. Results show that our rule-based inflection module helps NMT models incorporate lemma constraints more accurately than a neural module and outperforms the existing end-to-end approach with lower training costs.</abstract>
      <url hash="e31fa80c">2021.emnlp-main.477</url>
      <bibkey>xu-carpuat-2021-rule</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.477</doi>
      <video href="2021.emnlp-main.477.mp4"/>
      <pwccode url="https://github.com/izecson/terminology-translation" additional="false">izecson/terminology-translation</pwccode>
    </paper>
    <paper id="478">
      <title>Data and Parameter Scaling Laws for Neural Machine Translation</title>
      <author><first>Mitchell A</first><last>Gordon</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <author><first>Jared</first><last>Kaplan</last></author>
      <pages>5915–5922</pages>
      <abstract>We observe that the development cross-entropy loss of supervised neural machine translation models scales like a power law with the amount of training data and the number of non-embedding parameters in the model. We discuss some practical implications of these results, such as predicting BLEU achieved by large scale models and predicting the ROI of labeling data in low-resource language pairs.</abstract>
      <url hash="8f42801d">2021.emnlp-main.478</url>
      <bibkey>gordon-etal-2021-data</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.478</doi>
      <video href="2021.emnlp-main.478.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="479">
      <title>Good-Enough Example Extrapolation</title>
      <author><first>Jason</first><last>Wei</last></author>
      <pages>5923–5929</pages>
      <abstract>This paper asks whether extrapolating the hidden space distribution of text examples from one class onto another is a valid inductive bias for data augmentation. To operationalize this question, I propose a simple data augmentation protocol called “good-enough example extrapolation” (GE3). GE3 is lightweight and has no hyperparameters. Applied to three text classification datasets for various data imbalance scenarios, GE3 improves performance more than upsampling and other hidden-space data augmentation methods.</abstract>
      <url hash="42d4d4b6">2021.emnlp-main.479</url>
      <attachment type="Software" hash="31382f34">2021.emnlp-main.479.Software.zip</attachment>
      <bibkey>wei-2021-good</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.479</doi>
      <video href="2021.emnlp-main.479.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/fewrel">FewRel</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
    </paper>
    <paper id="480">
      <title>Learning to Selectively Learn for Weakly-supervised Paraphrase Generation</title>
      <author><first>Kaize</first><last>Ding</last></author>
      <author><first>Dingcheng</first><last>Li</last></author>
      <author><first>Alexander Hanbo</first><last>Li</last></author>
      <author><first>Xing</first><last>Fan</last></author>
      <author><first>Chenlei</first><last>Guo</last></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last></author>
      <author><first>Huan</first><last>Liu</last></author>
      <pages>5930–5940</pages>
      <abstract>Paraphrase generation is a longstanding NLP task that has diverse applications on downstream NLP tasks. However, the effectiveness of existing efforts predominantly relies on large amounts of golden labeled data. Though unsupervised endeavors have been proposed to alleviate this issue, they may fail to generate meaningful paraphrases due to the lack of supervision signals. In this work, we go beyond the existing paradigms and propose a novel approach to generate high-quality paraphrases with data of weak supervision. Specifically, we tackle the weakly-supervised paraphrase generation problem by: (1) obtaining abundant weakly-labeled parallel sentences via retrieval-based pseudo paraphrase expansion; and (2) developing a meta-learning framework to progressively select valuable samples for fine-tuning a pre-trained language model BART on the sentential paraphrasing task. We demonstrate that our approach achieves significant improvements over existing unsupervised approaches, and is even comparable in performance with supervised state-of-the-arts.</abstract>
      <url hash="1a9935ee">2021.emnlp-main.480</url>
      <bibkey>ding-etal-2021-learning</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.480</doi>
      <video href="2021.emnlp-main.480.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
    </paper>
    <paper id="481">
      <title>Effective Convolutional Attention Network for Multi-label Clinical Document Classification</title>
      <author id="yang-liu-3m"><first>Yang</first><last>Liu</last></author>
      <author><first>Hua</first><last>Cheng</last></author>
      <author><first>Russell</first><last>Klopfer</last></author>
      <author><first>Matthew R.</first><last>Gormley</last></author>
      <author><first>Thomas</first><last>Schaaf</last></author>
      <pages>5941–5953</pages>
      <abstract>Multi-label document classification (MLDC) problems can be challenging, especially for long documents with a large label set and a long-tail distribution over labels. In this paper, we present an effective convolutional attention network for the MLDC problem with a focus on medical code prediction from clinical documents. Our innovations are three-fold: (1) we utilize a deep convolution-based encoder with the squeeze-and-excitation networks and residual networks to aggregate the information across the document and learn meaningful document representations that cover different ranges of texts; (2) we explore multi-layer and sum-pooling attention to extract the most informative features from these multi-scale representations; (3) we combine binary cross entropy loss and focal loss to improve performance for rare labels. We focus our evaluation study on MIMIC-III, a widely used dataset in the medical domain. Our models outperform prior work on medical coding and achieve new state-of-the-art results on multiple metrics. We also demonstrate the language independent nature of our approach by applying it to two non-English datasets. Our model outperforms prior best model and a multilingual Transformer model by a substantial margin.</abstract>
      <url hash="fc96e3d0">2021.emnlp-main.481</url>
      <bibkey>liu-etal-2021-effective</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.481</doi>
      <video href="2021.emnlp-main.481.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mimic-iii">MIMIC-III</pwcdataset>
    </paper>
    <paper id="482">
      <title>Contrastive Code Representation Learning</title>
      <author><first>Paras</first><last>Jain</last></author>
      <author><first>Ajay</first><last>Jain</last></author>
      <author><first>Tianjun</first><last>Zhang</last></author>
      <author><first>Pieter</first><last>Abbeel</last></author>
      <author><first>Joseph</first><last>Gonzalez</last></author>
      <author><first>Ion</first><last>Stoica</last></author>
      <pages>5954–5971</pages>
      <abstract>Recent work learns contextual representations of source code by reconstructing tokens from their context. For downstream semantic understanding tasks like code clone detection, these representations should ideally capture program functionality. However, we show that the popular reconstruction-based RoBERTa model is sensitive to source code edits, even when the edits preserve semantics. We propose ContraCode: a contrastive pre-training task that learns code functionality, not form. ContraCode pre-trains a neural network to identify functionally similar variants of a program among many non-equivalent distractors. We scalably generate these variants using an automated source-to-source compiler as a form of data augmentation. Contrastive pre-training outperforms RoBERTa on an adversarial code clone detection benchmark by 39% AUROC. Surprisingly, improved adversarial robustness translates to better accuracy over natural code; ContraCode improves summarization and TypeScript type inference accuracy by 2 to 13 percentage points over competitive baselines. All source is available at https://github.com/parasj/contracode.</abstract>
      <url hash="3d293e81">2021.emnlp-main.482</url>
      <bibkey>jain-etal-2021-contrastive</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.482</doi>
      <video href="2021.emnlp-main.482.mp4"/>
      <pwccode url="https://github.com/parasj/contracode" additional="false">parasj/contracode</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/codesearchnet">CodeSearchNet</pwcdataset>
    </paper>
    <paper id="483">
      <title><fixed-case>IGA</fixed-case>: An Intent-Guided Authoring Assistant</title>
      <author><first>Simeng</first><last>Sun</last></author>
      <author><first>Wenlong</first><last>Zhao</last></author>
      <author><first>Varun</first><last>Manjunatha</last></author>
      <author><first>Rajiv</first><last>Jain</last></author>
      <author><first>Vlad</first><last>Morariu</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Balaji Vasan</first><last>Srinivasan</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <pages>5972–5985</pages>
      <abstract>While large-scale pretrained language models have significantly improved writing assistance functionalities such as autocomplete, more complex and controllable writing assistants have yet to be explored. We leverage advances in language modeling to build an interactive writing assistant that generates and rephrases text according to fine-grained author specifications. Users provide input to our Intent-Guided Assistant (IGA) in the form of text interspersed with tags that correspond to specific rhetorical directives (e.g., adding description or contrast, or rephrasing a particular sentence). We fine-tune a language model on a dataset heuristically-labeled with author intent, which allows IGA to fill in these tags with generated text that users can subsequently edit to their liking. A series of automatic and crowdsourced evaluations confirm the quality of IGA’s generated outputs, while a small-scale user study demonstrates author preference for IGA over baseline methods in a creative writing task. We release our dataset, code, and demo to spur further research into AI-assisted writing.</abstract>
      <url hash="bccb34f3">2021.emnlp-main.483</url>
      <bibkey>sun-etal-2021-iga</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.483</doi>
      <video href="2021.emnlp-main.483.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/pomo">PoMo</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikilarge">WikiLarge</pwcdataset>
    </paper>
    <paper id="484">
      <title>Math Word Problem Generation with Mathematical Consistency and Problem Context Constraints</title>
      <author><first>Zichao</first><last>Wang</last></author>
      <author><first>Andrew</first><last>Lan</last></author>
      <author><first>Richard</first><last>Baraniuk</last></author>
      <pages>5986–5999</pages>
      <abstract>We study the problem of generating arithmetic math word problems (MWPs) given a math equation that specifies the mathematical computation and a context that specifies the problem scenario. Existing approaches are prone to generating MWPs that are either mathematically invalid or have unsatisfactory language quality. They also either ignore the context or require manual specification of a problem template, which compromises the diversity of the generated MWPs. In this paper, we develop a novel MWP generation approach that leverages i) pre-trained language models and a context keyword selection model to improve the language quality of generated MWPs and ii) an equation consistency constraint for math equations to improve the mathematical validity of the generated MWPs. Extensive quantitative and qualitative experiments on three real-world MWP datasets demonstrate the superior performance of our approach compared to various baselines.</abstract>
      <url hash="c7ab2234">2021.emnlp-main.484</url>
      <bibkey>wang-etal-2021-math</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.484</doi>
      <video href="2021.emnlp-main.484.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mawps">MAWPS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/math23k">Math23K</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mathqa">MathQA</pwcdataset>
    </paper>
    <paper id="485">
      <title>Navigating the Kaleidoscope of <fixed-case>COVID</fixed-case>-19 Misinformation Using Deep Learning</title>
      <author><first>Yuanzhi</first><last>Chen</last></author>
      <author><first>Mohammad</first><last>Hasan</last></author>
      <pages>6000–6017</pages>
      <abstract>Irrespective of the success of the deep learning-based mixed-domain transfer learning approach for solving various Natural Language Processing tasks, it does not lend a generalizable solution for detecting misinformation from COVID-19 social media data. Due to the inherent complexity of this type of data, caused by its dynamic (context evolves rapidly), nuanced (misinformation types are often ambiguous), and diverse (skewed, fine-grained, and overlapping categories) nature, it is imperative for an effective model to capture both the local and global context of the target domain. By conducting a systematic investigation, we show that: (i) the deep Transformer-based pre-trained models, utilized via the mixed-domain transfer learning, are only good at capturing the local context, thus exhibits poor generalization, and (ii) a combination of shallow network-based domain-specific models and convolutional neural networks can efficiently extract local as well as global context directly from the target data in a hierarchical fashion, enabling it to offer a more generalizable solution.</abstract>
      <url hash="3cfc2d12">2021.emnlp-main.485</url>
      <bibkey>chen-hasan-2021-navigating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.485</doi>
      <video href="2021.emnlp-main.485.mp4"/>
    </paper>
    <paper id="486">
      <title>Detecting Health Advice in Medical Research Literature</title>
      <author><first>Yingya</first><last>Li</last></author>
      <author><first>Jun</first><last>Wang</last></author>
      <author><first>Bei</first><last>Yu</last></author>
      <pages>6018–6029</pages>
      <abstract>Health and medical researchers often give clinical and policy recommendations to inform health practice and public health policy. However, no current health information system supports the direct retrieval of health advice. This study fills the gap by developing and validating an NLP-based prediction model for identifying health advice in research publications. We annotated a corpus of 6,000 sentences extracted from structured abstracts in PubMed publications as ‘“strong advice”, “weak advice”, or “no advice”, and developed a BERT-based model that can predict, with a macro-averaged F1-score of 0.93, whether a sentence gives strong advice, weak advice, or not. The prediction model generalized well to sentences in both unstructured abstracts and discussion sections, where health advice normally appears. We also conducted a case study that applied this prediction model to retrieve specific health advice on COVID-19 treatments from LitCovid, a large COVID research literature portal, demonstrating the usefulness of retrieving health advice sentences as an advanced research literature navigation function for health researchers and the general public.</abstract>
      <url hash="0a65c1ff">2021.emnlp-main.486</url>
      <bibkey>li-etal-2021-detecting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.486</doi>
      <video href="2021.emnlp-main.486.mp4"/>
      <pwccode url="https://github.com/junwang4/detecting-health-advice" additional="false">junwang4/detecting-health-advice</pwccode>
    </paper>
    <paper id="487">
      <title>A Semantic Feature-Wise Transformation Relation Network for Automatic Short Answer Grading</title>
      <author><first>Zhaohui</first><last>Li</last></author>
      <author><first>Yajur</first><last>Tomar</last></author>
      <author><first>Rebecca J.</first><last>Passonneau</last></author>
      <pages>6030–6040</pages>
      <abstract>Automatic short answer grading (ASAG) is the task of assessing students’ short natural language responses to objective questions. It is a crucial component of new education platforms, and could support more wide-spread use of constructed response questions to replace cognitively less challenging multiple choice questions. We propose a Semantic Feature-wise transformation Relation Network (SFRN) that exploits the multiple components of ASAG datasets more effectively. SFRN captures relational knowledge among the questions (Q), reference answers or rubrics (R), and labeled student answers (A). A relation network learns vector representations for the elements of QRA triples, then combines the learned representations using learned semantic feature-wise transformations. We apply translation-based data augmentation to address the two problems of limited training data, and high data skew for multi-class ASAG tasks. Our model has up to 11% performance improvement over state-of-the-art results on the benchmark SemEval-2013 datasets, and surpasses custom approaches designed for a Kaggle challenge, demonstrating its generality.</abstract>
      <url hash="66e9b312">2021.emnlp-main.487</url>
      <bibkey>li-etal-2021-semantic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.487</doi>
      <video href="2021.emnlp-main.487.mp4"/>
    </paper>
    <paper id="488">
      <title>Evaluating Scholarly Impact: Towards Content-Aware Bibliometrics</title>
      <author><first>Saurav</first><last>Manchanda</last></author>
      <author><first>George</first><last>Karypis</last></author>
      <pages>6041–6053</pages>
      <abstract>Quantitatively measuring the impact-related aspects of scientific, engineering, and technological (SET) innovations is a fundamental problem with broad applications. Traditional citation-based measures for assessing the impact of innovations and related entities do not take into account the content of the publications. This limits their ability to provide rigorous quality-related metrics because they cannot account for the reasons that led to a citation. We present approaches to estimate content-aware bibliometrics to quantitatively measure the scholarly impact of a publication. Our approaches assess the impact of a cited publication by the extent to which the cited publication informs the citing publication. We introduce a new metric, called “Content Informed Index” (CII), that uses the content of the paper as a source of distant-supervision, to quantify how much the cited-node informs the citing-node. We evaluate the weights estimated by our approach on three manually annotated datasets, where the annotations quantify the extent of information in the citation. Particularly, we evaluate how well the ranking imposed by our approach associates with the ranking imposed by the manual annotations. CII achieves up to 103% improvement in performance as compared to the second-best performing approach.</abstract>
      <url hash="ab13dd8b">2021.emnlp-main.488</url>
      <bibkey>manchanda-karypis-2021-evaluating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.488</doi>
      <video href="2021.emnlp-main.488.mp4"/>
      <pwccode url="https://github.com/gurdaspuriya/evaluating-scholarly-impact" additional="false">gurdaspuriya/evaluating-scholarly-impact</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/s2orc">S2ORC</pwcdataset>
    </paper>
    <paper id="489">
      <title>A Scalable Framework for Learning From Implicit User Feedback to Improve Natural Language Understanding in Large-Scale Conversational <fixed-case>AI</fixed-case> Systems</title>
      <author><first>Sunghyun</first><last>Park</last></author>
      <author><first>Han</first><last>Li</last></author>
      <author><first>Ameen</first><last>Patel</last></author>
      <author><first>Sidharth</first><last>Mudgal</last></author>
      <author><first>Sungjin</first><last>Lee</last></author>
      <author><first>Young-Bum</first><last>Kim</last></author>
      <author><first>Spyros</first><last>Matsoukas</last></author>
      <author><first>Ruhi</first><last>Sarikaya</last></author>
      <pages>6054–6063</pages>
      <abstract>Natural Language Understanding (NLU) is an established component within a conversational AI or digital assistant system, and it is responsible for producing semantic understanding of a user request. We propose a scalable and automatic approach for improving NLU in a large-scale conversational AI system by leveraging implicit user feedback, with an insight that user interaction data and dialog context have rich information embedded from which user satisfaction and intention can be inferred. In particular, we propose a domain-agnostic framework for curating new supervision data for improving NLU from live production traffic. With an extensive set of experiments, we show the results of applying the framework and improving NLU for a large-scale production system across 10 domains.</abstract>
      <url hash="66d00b1e">2021.emnlp-main.489</url>
      <bibkey>park-etal-2021-scalable</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.489</doi>
      <video href="2021.emnlp-main.489.mp4"/>
    </paper>
    <paper id="490">
      <title>Summarize-then-Answer: Generating Concise Explanations for Multi-hop Reading Comprehension</title>
      <author><first>Naoya</first><last>Inoue</last></author>
      <author><first>Harsh</first><last>Trivedi</last></author>
      <author><first>Steven</first><last>Sinha</last></author>
      <author><first>Niranjan</first><last>Balasubramanian</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>6064–6080</pages>
      <abstract>How can we generate concise explanations for multi-hop Reading Comprehension (RC)? The current strategies of identifying supporting sentences can be seen as an extractive question-focused summarization of the input text. However, these extractive explanations are not necessarily concise i.e. not minimally sufficient for answering a question. Instead, we advocate for an abstractive approach, where we propose to generate a question-focused, abstractive summary of input paragraphs and then feed it to an RC system. Given a limited amount of human-annotated abstractive explanations, we train the abstractive explainer in a semi-supervised manner, where we start from the supervised model and then train it further through trial and error maximizing a conciseness-promoted reward function. Our experiments demonstrate that the proposed abstractive explainer can generate more compact explanations than an extractive explainer with limited supervision (only 2k instances) while maintaining sufficiency.</abstract>
      <url hash="6ae2e8d7">2021.emnlp-main.490</url>
      <bibkey>inoue-etal-2021-summarize</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.490</doi>
      <video href="2021.emnlp-main.490.mp4"/>
      <pwccode url="https://github.com/stonybrooknlp/suqa" additional="false">stonybrooknlp/suqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
    </paper>
    <paper id="491">
      <title><fixed-case>F</fixed-case>ewshot<fixed-case>QA</fixed-case>: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models</title>
      <author><first>Rakesh</first><last>Chada</last></author>
      <author><first>Pradeep</first><last>Natarajan</last></author>
      <pages>6081–6090</pages>
      <abstract>The task of learning from only a few examples (called a few-shot setting) is of key importance and relevance to a real-world setting. For question answering (QA), the current state-of-the-art pre-trained models typically need fine-tuning on tens of thousands of examples to obtain good results. Their performance degrades significantly in a few-shot setting (&lt; 100 examples). To address this, we propose a simple fine-tuning framework that leverages pre-trained text-to-text models and is directly aligned with their pre-training framework. Specifically, we construct the input as a concatenation of the question, a mask token representing the answer span and a context. Given this input, the model is fine-tuned using the same objective as that of its pre-training objective. Through experimental studies on various few-shot configurations, we show that this formulation leads to significant gains on multiple QA benchmarks (an absolute gain of 34.2 F1 points on average when there are only 16 training examples). The gains extend further when used with larger models (Eg:- 72.3 F1 on SQuAD using BART-large with only 32 examples) and translate well to a multilingual setting . On the multilingual TydiQA benchmark, our model outperforms the XLM-Roberta-large by an absolute margin of upto 40 F1 points and an average of 33 F1 points in a few-shot setting (&lt;= 64 training examples). We conduct detailed ablation studies to analyze factors contributing to these gains.</abstract>
      <url hash="4272c2d9">2021.emnlp-main.491</url>
      <bibkey>chada-natarajan-2021-fewshotqa</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.491</doi>
      <video href="2021.emnlp-main.491.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDi QA</pwcdataset>
    </paper>
    <paper id="492">
      <title>Multi-stage Training with Improved Negative Contrast for Neural Passage Retrieval</title>
      <author><first>Jing</first><last>Lu</last></author>
      <author><first>Gustavo</first><last>Hernandez Abrego</last></author>
      <author><first>Ji</first><last>Ma</last></author>
      <author><first>Jianmo</first><last>Ni</last></author>
      <author><first>Yinfei</first><last>Yang</last></author>
      <pages>6091–6103</pages>
      <abstract>In the context of neural passage retrieval, we study three promising techniques: synthetic data generation, negative sampling, and fusion. We systematically investigate how these techniques contribute to the performance of the retrieval system and how they complement each other. We propose a multi-stage framework comprising of pre-training with synthetic data, fine-tuning with labeled data, and negative sampling at both stages. We study six negative sampling strategies and apply them to the fine-tuning stage and, as a noteworthy novelty, to the synthetic data that we use for pre-training. Also, we explore fusion methods that combine negatives from different strategies. We evaluate our system using two passage retrieval tasks for open-domain QA and using MS MARCO. Our experiments show that augmenting the negative contrast in both stages is effective to improve passage retrieval accuracy and, importantly, they also show that synthetic data generation and negative sampling have additive benefits. Moreover, using the fusion of different kinds allows us to reach performance that establishes a new state-of-the-art level in two of the tasks we evaluated.</abstract>
      <url hash="e4443ebb">2021.emnlp-main.492</url>
      <bibkey>lu-etal-2021-multi</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.492</doi>
      <video href="2021.emnlp-main.492.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="493">
      <title>Perhaps <fixed-case>PTLM</fixed-case>s Should Go to School – A Task to Assess Open Book and Closed Book <fixed-case>QA</fixed-case></title>
      <author><first>Manuel</first><last>Ciosici</last></author>
      <author><first>Joe</first><last>Cecil</last></author>
      <author><first>Dong-Ho</first><last>Lee</last></author>
      <author><first>Alex</first><last>Hedges</last></author>
      <author><first>Marjorie</first><last>Freedman</last></author>
      <author><first>Ralph</first><last>Weischedel</last></author>
      <pages>6104–6111</pages>
      <abstract>Our goal is to deliver a new task and leaderboard to stimulate research on question answering and pre-trained language models (PTLMs) to understand a significant instructional document, e.g., an introductory college textbook or a manual. PTLMs have shown great success in many question-answering tasks, given significant supervised training, but much less so in zero-shot settings. We propose a new task that includes two college-level introductory texts in the social sciences (American Government 2e) and humanities (U.S. History), hundreds of true/false statements based on review questions written by the textbook authors, validation/development tests based on the first eight chapters of the textbooks, blind tests based on the remaining textbook chapters, and baseline results given state-of-the-art PTLMs. Since the questions are balanced, random performance should be ~50%. T5, fine-tuned with BoolQ achieves the same performance, suggesting that the textbook’s content is not pre-represented in the PTLM. Taking the exam closed book, but having read the textbook (i.e., adding the textbook to T5’s pre-training), yields at best minor improvement (56%), suggesting that the PTLM may not have “understood” the textbook (or perhaps misunderstood the questions). Performance is better (~60%) when the exam is taken open-book (i.e., allowing the machine to automatically retrieve a paragraph and use it to answer the question).</abstract>
      <url hash="586e0f3f">2021.emnlp-main.493</url>
      <bibkey>ciosici-etal-2021-perhaps</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.493</doi>
      <video href="2021.emnlp-main.493.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/arc">ARC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="494">
      <title><fixed-case>R</fixed-case>eason<fixed-case>BERT</fixed-case>: <fixed-case>P</fixed-case>re-trained to Reason with Distant Supervision</title>
      <author><first>Xiang</first><last>Deng</last></author>
      <author><first>Yu</first><last>Su</last></author>
      <author><first>Alyssa</first><last>Lees</last></author>
      <author><first>You</first><last>Wu</last></author>
      <author><first>Cong</first><last>Yu</last></author>
      <author><first>Huan</first><last>Sun</last></author>
      <pages>6112–6127</pages>
      <abstract>We present ReasonBert, a pre-training method that augments language models with the ability to reason over long-range relations and multiple, possibly hybrid contexts. Unlike existing pre-training methods that only harvest learning signals from local contexts of naturally occurring texts, we propose a generalized notion of distant supervision to automatically connect multiple pieces of text and tables to create pre-training examples that require long-range reasoning. Different types of reasoning are simulated, including intersecting multiple pieces of evidence, bridging from one piece of evidence to another, and detecting unanswerable cases. We conduct a comprehensive evaluation on a variety of extractive question answering datasets ranging from single-hop to multi-hop and from text-only to table-only to hybrid that require various reasoning capabilities and show that ReasonBert achieves remarkable improvement over an array of strong baselines. Few-shot experiments further demonstrate that our pre-training method substantially improves sample efficiency.</abstract>
      <url hash="8bdf3941">2021.emnlp-main.494</url>
      <bibkey>deng-etal-2021-reasonbert</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.494</doi>
      <video href="2021.emnlp-main.494.mp4"/>
      <pwccode url="https://github.com/sunlab-osu/reasonbert" additional="false">sunlab-osu/reasonbert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hybridqa">HybridQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/searchqa">SearchQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="495">
      <title>Single-dataset Experts for Multi-dataset Question Answering</title>
      <author><first>Dan</first><last>Friedman</last></author>
      <author><first>Ben</first><last>Dodge</last></author>
      <author><first>Danqi</first><last>Chen</last></author>
      <pages>6128–6137</pages>
      <abstract>Many datasets have been created for training reading comprehension models, and a natural question is whether we can combine them to build models that (1) perform better on all of the training datasets and (2) generalize and transfer better to new datasets. Prior work has addressed this goal by training one network simultaneously on multiple datasets, which works well on average but is prone to over- or under-fitting different sub- distributions and might transfer worse compared to source models with more overlap with the target dataset. Our approach is to model multi-dataset question answering with an ensemble of single-dataset experts, by training a collection of lightweight, dataset-specific adapter modules (Houlsby et al., 2019) that share an underlying Transformer model. We find that these Multi-Adapter Dataset Experts (MADE) outperform all our baselines in terms of in-distribution accuracy, and simple methods based on parameter-averaging lead to better zero-shot generalization and few-shot transfer performance, offering a strong and versatile starting point for building new reading comprehension systems.</abstract>
      <url hash="dc747808">2021.emnlp-main.495</url>
      <bibkey>friedman-etal-2021-single</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.495</doi>
      <video href="2021.emnlp-main.495.mp4"/>
      <pwccode url="https://github.com/princeton-nlp/made" additional="false">princeton-nlp/made</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/duorc">DuoRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="496">
      <title>Simple Entity-Centric Questions Challenge Dense Retrievers</title>
      <author><first>Christopher</first><last>Sciavolino</last></author>
      <author><first>Zexuan</first><last>Zhong</last></author>
      <author><first>Jinhyuk</first><last>Lee</last></author>
      <author><first>Danqi</first><last>Chen</last></author>
      <pages>6138–6148</pages>
      <abstract>Open-domain question answering has exploded in popularity recently due to the success of dense retrieval models, which have surpassed sparse models using only a few supervised training examples. However, in this paper, we demonstrate current dense models are not yet the holy grail of retrieval. We first construct EntityQuestions, a set of simple, entity-rich questions based on facts from Wikidata (e.g., “Where was Arve Furset born?”), and observe that dense retrievers drastically under-perform sparse methods. We investigate this issue and uncover that dense retrievers can only generalize to common entities unless the question pattern is explicitly observed during training. We discuss two simple solutions towards addressing this critical problem. First, we demonstrate that data augmentation is unable to fix the generalization problem. Second, we argue a more robust passage encoder helps facilitate better question adaptation using specialized question encoders. We hope our work can shed light on the challenges in creating a robust, universal dense retriever that works well across different input distributions.</abstract>
      <url hash="3a787a5a">2021.emnlp-main.496</url>
      <bibkey>sciavolino-etal-2021-simple</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.496</doi>
      <video href="2021.emnlp-main.496.mp4"/>
      <pwccode url="https://github.com/princeton-nlp/entityquestions" additional="false">princeton-nlp/entityquestions</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/entityquestions">EntityQuestions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paq">PAQ</pwcdataset>
    </paper>
    <paper id="497">
      <title>Mitigating False-Negative Contexts in Multi-document Question Answering with Retrieval Marginalization</title>
      <author><first>Ansong</first><last>Ni</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <author><first>Pradeep</first><last>Dasigi</last></author>
      <pages>6149–6161</pages>
      <abstract>Question Answering (QA) tasks requiring information from multiple documents often rely on a retrieval model to identify relevant information for reasoning. The retrieval model is typically trained to maximize the likelihood of the labeled supporting evidence. However, when retrieving from large text corpora such as Wikipedia, the correct answer can often be obtained from multiple evidence candidates. Moreover, not all such candidates are labeled as positive during annotation, rendering the training signal weak and noisy. This problem is exacerbated when the questions are unanswerable or when the answers are Boolean, since the model cannot rely on lexical overlap to make a connection between the answer and supporting evidence. We develop a new parameterization of set-valued retrieval that handles unanswerable queries, and we show that marginalizing over this set during training allows a model to mitigate false negatives in supporting evidence annotations. We test our method on two multi-document QA datasets, IIRC and HotpotQA. On IIRC, we show that joint modeling with marginalization improves model performance by 5.5 F1 points and achieves a new state-of-the-art performance of 50.5 F1. We also show that retrieval marginalization results in 4.1 QA F1 improvement over a non-marginalized baseline on HotpotQA in the fullwiki setting.</abstract>
      <url hash="c6f18453">2021.emnlp-main.497</url>
      <bibkey>ni-etal-2021-mitigating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.497</doi>
      <video href="2021.emnlp-main.497.mp4"/>
      <pwccode url="https://github.com/niansong1996/retrieval_marginalization" additional="false">niansong1996/retrieval_marginalization</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/iirc">IIRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="498">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>D</fixed-case>oc2<fixed-case>D</fixed-case>ial: Modeling Dialogues Grounded in Multiple Documents</title>
      <author><first>Song</first><last>Feng</last></author>
      <author><first>Siva Sankalp</first><last>Patel</last></author>
      <author><first>Hui</first><last>Wan</last></author>
      <author><first>Sachindra</first><last>Joshi</last></author>
      <pages>6162–6176</pages>
      <abstract>We propose MultiDoc2Dial, a new task and dataset on modeling goal-oriented dialogues grounded in multiple documents. Most previous works treat document-grounded dialogue modeling as machine reading comprehension task based on a single given document or passage. In this work, we aim to address more realistic scenarios where a goal-oriented information-seeking conversation involves multiple topics, and hence is grounded on different documents. To facilitate such task, we introduce a new dataset that contains dialogues grounded in multiple documents from four different domains. We also explore modeling the dialogue-based and document-based contexts in the dataset. We present strong baseline approaches and various experimental results, aiming to support further research efforts on such a task.</abstract>
      <url hash="24beff46">2021.emnlp-main.498</url>
      <bibkey>feng-etal-2021-multidoc2dial</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.498</doi>
      <video href="2021.emnlp-main.498.mp4"/>
      <pwccode url="https://github.com/IBM/multidoc2dial" additional="false">IBM/multidoc2dial</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multidoc2dial">MultiDoc2Dial</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/doqa">DoQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/doc2dial-1">Doc2Dial</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quac">QuAC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sharc">ShARC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/doc2dial">doc2dial</pwcdataset>
    </paper>
    <paper id="499">
      <title><fixed-case>G</fixed-case>up<fixed-case>S</fixed-case>hup: Summarizing Open-Domain Code-Switched Conversations</title>
      <author><first>Laiba</first><last>Mehnaz</last></author>
      <author><first>Debanjan</first><last>Mahata</last></author>
      <author><first>Rakesh</first><last>Gosangi</last></author>
      <author><first>Uma Sushmitha</first><last>Gunturi</last></author>
      <author><first>Riya</first><last>Jain</last></author>
      <author><first>Gauri</first><last>Gupta</last></author>
      <author><first>Amardeep</first><last>Kumar</last></author>
      <author><first>Isabelle G.</first><last>Lee</last></author>
      <author><first>Anish</first><last>Acharya</last></author>
      <author><first>Rajiv Ratn</first><last>Shah</last></author>
      <pages>6177–6192</pages>
      <abstract>Code-switching is the communication phenomenon where the speakers switch between different languages during a conversation. With the widespread adoption of conversational agents and chat platforms, code-switching has become an integral part of written conversations in many multi-lingual communities worldwide. Therefore, it is essential to develop techniques for understanding and summarizing these conversations. Towards this objective, we introduce the task of abstractive summarization of Hindi-English (Hi-En) code-switched conversations. We also develop the first code-switched conversation summarization dataset - <i>GupShup</i>, which contains over 6,800 Hi-En conversations and their corresponding human-annotated summaries in English (En) and Hi-En. We present a detailed account of the entire data collection and annotation process. We analyze the dataset using various code-switching statistics. We train state-of-the-art abstractive summarization models and report their performances using both automated metrics and human evaluation. Our results show that multi-lingual mBART and multi-view seq2seq models obtain the best performances on this new dataset. We also conduct an extensive qualitative analysis to provide insight into the models and some of their shortcomings.</abstract>
      <url hash="712d3676">2021.emnlp-main.499</url>
      <bibkey>mehnaz-etal-2021-gupshup</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.499</doi>
      <video href="2021.emnlp-main.499.mp4"/>
      <pwccode url="https://github.com/midas-research/gupshup" additional="false">midas-research/gupshup</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/samsum-corpus">SAMSum Corpus</pwcdataset>
    </paper>
    <paper id="500">
      <title><fixed-case>B</fixed-case>i<fixed-case>SECT</fixed-case>: Learning to Split and Rephrase Sentences with Bitexts</title>
      <author><first>Joongwon</first><last>Kim</last></author>
      <author><first>Mounica</first><last>Maddela</last></author>
      <author><first>Reno</first><last>Kriz</last></author>
      <author><first>Wei</first><last>Xu</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <pages>6193–6209</pages>
      <abstract>An important task in NLP applications such as sentence simplification is the ability to take a long, complex sentence and split it into shorter sentences, rephrasing as necessary. We introduce a novel dataset and a new model for this ‘split and rephrase’ task. Our BiSECT training data consists of 1 million long English sentences paired with shorter, meaning-equivalent English sentences. We obtain these by extracting 1-2 sentence alignments in bilingual parallel corpora and then using machine translation to convert both sides of the corpus into the same language. BiSECT contains higher quality training examples than the previous Split and Rephrase corpora, with sentence splits that require more significant modifications. We categorize examples in our corpus and use these categories in a novel model that allows us to target specific regions of the input sentence to be split and edited. Moreover, we show that models trained on BiSECT can perform a wider variety of split operations and improve upon previous state-of-the-art approaches in automatic and human evaluations.</abstract>
      <url hash="6affb1a8">2021.emnlp-main.500</url>
      <bibkey>kim-etal-2021-bisect</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.500</doi>
      <video href="2021.emnlp-main.500.mp4"/>
      <pwccode url="https://github.com/mounicam/bisect" additional="false">mounicam/bisect</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bisect">BiSECT</pwcdataset>
    </paper>
    <paper id="501">
      <title>Data Collection vs. Knowledge Graph Completion: What is Needed to Improve Coverage?</title>
      <author><first>Kenneth</first><last>Church</last></author>
      <author><first>Yuchen</first><last>Bian</last></author>
      <pages>6210–6215</pages>
      <abstract>This survey/position paper discusses ways to improve coverage of resources such as WordNet. Rapp estimated correlations, rho, between corpus statistics and pyscholinguistic norms. rho improves with quantity (corpus size) and quality (balance). 1M words is enough for simple estimates (unigram frequencies), but at least 100x more is required for good estimates of word associations and embeddings. Given such estimates, WordNet’s coverage is remarkable. WordNet was developed on SemCor, a small sample (200k words) from the Brown Corpus. Knowledge Graph Completion (KGC) attempts to learn missing links from subsets. But Rapp’s estimates of sizes suggest it would be more profitable to collect more data than to infer missing information that is not there.</abstract>
      <url hash="70928075">2021.emnlp-main.501</url>
      <bibkey>church-bian-2021-data</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.501</doi>
      <video href="2021.emnlp-main.501.mp4"/>
    </paper>
    <paper id="502">
      <title>Universal Sentence Representation Learning with Conditional Masked Language Model</title>
      <author><first>Ziyi</first><last>Yang</last></author>
      <author><first>Yinfei</first><last>Yang</last></author>
      <author><first>Daniel</first><last>Cer</last></author>
      <author><first>Jax</first><last>Law</last></author>
      <author><first>Eric</first><last>Darve</last></author>
      <pages>6216–6228</pages>
      <abstract>This paper presents a novel training method, Conditional Masked Language Modeling (CMLM), to effectively learn sentence representations on large scale unlabeled corpora. CMLM integrates sentence representation learning into MLM training by conditioning on the encoded vectors of adjacent sentences. Our English CMLM model achieves state-of-the-art performance on SentEval, even outperforming models learned using supervised signals. As a fully unsupervised learning method, CMLM can be conveniently extended to a broad range of languages and domains. We find that a multilingual CMLM model co-trained with bitext retrieval (BR) and natural language inference (NLI) tasks outperforms the previous state-of-the-art multilingual models by a large margin, e.g. 10% improvement upon baseline models on cross-lingual semantic search. We explore the same language bias of the learned representations, and propose a simple, post-training and model agnostic approach to remove the language identifying information from the representation while still retaining sentence semantics.</abstract>
      <url hash="706ceda5">2021.emnlp-main.502</url>
      <bibkey>yang-etal-2021-universal</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.502</doi>
      <video href="2021.emnlp-main.502.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/paws-x">PAWS-X</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="503">
      <title>On the Benefit of Syntactic Supervision for Cross-lingual Transfer in Semantic Role Labeling</title>
      <author><first>Zhisong</first><last>Zhang</last></author>
      <author><first>Emma</first><last>Strubell</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <pages>6229–6246</pages>
      <abstract>Although recent developments in neural architectures and pre-trained representations have greatly increased state-of-the-art model performance on fully-supervised semantic role labeling (SRL), the task remains challenging for languages where supervised SRL training data are not abundant. Cross-lingual learning can improve performance in this setting by transferring knowledge from high-resource languages to low-resource ones. Moreover, we hypothesize that annotations of syntactic dependencies can be leveraged to further facilitate cross-lingual transfer. In this work, we perform an empirical exploration of the helpfulness of syntactic supervision for crosslingual SRL within a simple multitask learning scheme. With comprehensive evaluations across ten languages (in addition to English) and three SRL benchmark datasets, including both dependency- and span-based SRL, we show the effectiveness of syntactic supervision in low-resource scenarios.</abstract>
      <url hash="b4124564">2021.emnlp-main.503</url>
      <bibkey>zhang-etal-2021-benefit</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.503</doi>
      <video href="2021.emnlp-main.503.mp4"/>
      <pwccode url="https://github.com/zzsfornlp/zmsp" additional="false">zzsfornlp/zmsp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="504">
      <title>Implicit Premise Generation with Discourse-aware Commonsense Knowledge Models</title>
      <author><first>Tuhin</first><last>Chakrabarty</last></author>
      <author><first>Aadit</first><last>Trivedi</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <pages>6247–6252</pages>
      <abstract>Enthymemes are defined as arguments where a premise or conclusion is left implicit. We tackle the task of generating the <i>implicit premise in an enthymeme</i>, which requires not only an understanding of the stated conclusion and premise but also additional inferences that could depend on commonsense knowledge. The largest available dataset for enthymemes (Habernal et al., 2018) consists of 1.7k samples, which is not large enough to train a neural text generation model. To address this issue, we take advantage of a similar task and dataset: Abductive reasoning in narrative text (Bhagavatula et al., 2020). However, we show that simply using a state-of-the-art seq2seq model fine-tuned on this data might not generate meaningful implicit premises associated with the given enthymemes. We demonstrate that encoding discourse-aware commonsense during fine-tuning improves the quality of the generated implicit premises and outperforms all other baselines both in automatic and human evaluations on three different datasets.</abstract>
      <url hash="5dbdab40">2021.emnlp-main.504</url>
      <bibkey>chakrabarty-etal-2021-implicit</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.504</doi>
      <video href="2021.emnlp-main.504.mp4"/>
      <pwccode url="https://github.com/tuhinjubcse/enthymemesemnlp2021" additional="false">tuhinjubcse/enthymemesemnlp2021</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/art-dataset">ART Dataset</pwcdataset>
    </paper>
    <paper id="505">
      <title>Inducing Transformer’s Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks</title>
      <author><first>Yichen</first><last>Jiang</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>6253–6265</pages>
      <abstract>Systematic compositionality is an essential mechanism in human language, allowing the recombination of known parts to create novel expressions. However, existing neural models have been shown to lack this basic ability in learning symbolic structures. Motivated by the failure of a Transformer model on the SCAN compositionality challenge (Lake and Baroni, 2018), which requires parsing a command into actions, we propose two auxiliary sequence prediction tasks as additional training supervision. These automatically-generated sequences are more representative of the underlying compositional symbolic structures of the input data. During inference, the model jointly predicts the next action and the next tokens in the auxiliary sequences at each step. Experiments on the SCAN dataset show that our method encourages the Transformer to understand compositional structures of the command, improving its accuracy on multiple challenging splits from ≤ 10% to 100%. With only 418 (5%) training instances, our approach still achieves 97.8% accuracy on the MCD1 split. Therefore, we argue that compositionality can be induced in Transformers given minimal but proper guidance. We also show that a better result is achieved using less contextualized vectors as the attention’s query, providing insights into architecture choices in achieving systematic compositionality. Finally, we show positive generalization results on the grounded-SCAN task (Ruis et al., 2020).</abstract>
      <url hash="eb65d8de">2021.emnlp-main.505</url>
      <bibkey>jiang-bansal-2021-inducing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.505</doi>
      <video href="2021.emnlp-main.505.mp4"/>
      <pwccode url="https://github.com/jiangyctarheel/compositional-auxseq" additional="false">jiangyctarheel/compositional-auxseq</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cfq">CFQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scan">SCAN</pwcdataset>
    </paper>
    <paper id="506">
      <title>Flexible Generation of Natural Language Deductions</title>
      <author><first>Kaj</first><last>Bostrom</last></author>
      <author><first>Xinyu</first><last>Zhao</last></author>
      <author><first>Swarat</first><last>Chaudhuri</last></author>
      <author><first>Greg</first><last>Durrett</last></author>
      <pages>6266–6278</pages>
      <abstract>An interpretable system for open-domain reasoning needs to express its reasoning process in a transparent form. Natural language is an attractive representation for this purpose — it is both highly expressive and easy for humans to understand. However, manipulating natural language statements in logically consistent ways is hard: models must cope with variation in how meaning is expressed while remaining precise. In this paper, we describe ParaPattern, a method for building models to generate deductive inferences from diverse natural language inputs without direct human supervision. We train BART-based models (Lewis et al., 2020) to generate the result of applying a particular logical operation to one or more premise statements. Crucially, we develop a largely automated pipeline for constructing suitable training examples from Wikipedia. We evaluate our models using out-of-domain sentence compositions from the QASC (Khot et al., 2020) and EntailmentBank (Dalvi et al., 2021) datasets as well as targeted perturbation sets. Our results show that our models are substantially more accurate and flexible than baseline systems. ParaPattern achieves 85% validity on examples of the ‘substitution’ operation from EntailmentBank without the use of any in-domain training data, matching the performance of a model fine-tuned for EntailmentBank. The full source code for our method is publicly available.</abstract>
      <url hash="918230f2">2021.emnlp-main.506</url>
      <attachment type="Software" hash="05f29bbe">2021.emnlp-main.506.Software.zip</attachment>
      <bibkey>bostrom-etal-2021-flexible</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.506</doi>
      <video href="2021.emnlp-main.506.mp4"/>
      <pwccode url="https://github.com/alephic/ParaPattern" additional="false">alephic/ParaPattern</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/entailmentbank">EntailmentBank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/proofwriter">ProofWriter</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qasc">QASC</pwcdataset>
    </paper>
    <paper id="507">
      <title>Structure-aware Fine-tuning of Sequence-to-sequence Transformers for Transition-based <fixed-case>AMR</fixed-case> Parsing</title>
      <author><first>Jiawei</first><last>Zhou</last></author>
      <author><first>Tahira</first><last>Naseem</last></author>
      <author><first>Ramón</first><last>Fernandez Astudillo</last></author>
      <author><first>Young-Suk</first><last>Lee</last></author>
      <author><first>Radu</first><last>Florian</last></author>
      <author><first>Salim</first><last>Roukos</last></author>
      <pages>6279–6290</pages>
      <abstract>Predicting linearized Abstract Meaning Representation (AMR) graphs using pre-trained sequence-to-sequence Transformer models has recently led to large improvements on AMR parsing benchmarks. These parsers are simple and avoid explicit modeling of structure but lack desirable properties such as graph well-formedness guarantees or built-in graph-sentence alignments. In this work we explore the integration of general pre-trained sequence-to-sequence language models and a structure-aware transition-based approach. We depart from a pointer-based transition system and propose a simplified transition set, designed to better exploit pre-trained language models for structured fine-tuning. We also explore modeling the parser state within the pre-trained encoder-decoder architecture and different vocabulary strategies for the same purpose. We provide a detailed comparison with recent progress in AMR parsing and show that the proposed parser retains the desirable properties of previous transition-based approaches, while being simpler and reaching the new parsing state of the art for AMR 2.0, without the need for graph re-categorization.</abstract>
      <url hash="0cfd512f">2021.emnlp-main.507</url>
      <bibkey>zhou-etal-2021-structure</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.507</doi>
      <video href="2021.emnlp-main.507.mp4"/>
      <pwccode url="https://github.com/IBM/transition-amr-parser" additional="false">IBM/transition-amr-parser</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ldc2017t10">LDC2017T10</pwcdataset>
    </paper>
    <paper id="508">
      <title>Think about it! Improving defeasible reasoning by first modeling the question scenario.</title>
      <author><first>Aman</first><last>Madaan</last></author>
      <author><first>Niket</first><last>Tandon</last></author>
      <author><first>Dheeraj</first><last>Rajagopal</last></author>
      <author><first>Peter</first><last>Clark</last></author>
      <author><first>Yiming</first><last>Yang</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <pages>6291–6310</pages>
      <abstract>Defeasible reasoning is the mode of reasoning where conclusions can be overturned by taking into account new evidence. Existing cognitive science literature on defeasible reasoning suggests that a person forms a “mental model” of the problem scenario before answering questions. Our research goal asks whether neural models can similarly benefit from envisioning the question scenario before answering a defeasible query. Our approach is, given a question, to have a model first create a graph of relevant influences, and then leverage that graph as an additional input when answering the question. Our system, CURIOUS, achieves a new state-of-the-art on three different defeasible reasoning datasets. This result is significant as it illustrates that performance can be improved by guiding a system to “think about” a question and explicitly model the scenario, rather than answering reflexively.</abstract>
      <url hash="9b25d655">2021.emnlp-main.508</url>
      <bibkey>madaan-etal-2021-think</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.508</doi>
      <video href="2021.emnlp-main.508.mp4"/>
      <pwccode url="https://github.com/madaan/thinkaboutit" additional="false">madaan/thinkaboutit</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/atomic">ATOMIC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wiqa">WIQA</pwcdataset>
    </paper>
    <paper id="509">
      <title>Open Aspect Target Sentiment Classification with Natural Language Prompts</title>
      <author><first>Ronald</first><last>Seoh</last></author>
      <author><first>Ian</first><last>Birle</last></author>
      <author><first>Mrinal</first><last>Tak</last></author>
      <author><first>Haw-Shiuan</first><last>Chang</last></author>
      <author><first>Brian</first><last>Pinette</last></author>
      <author><first>Alfred</first><last>Hough</last></author>
      <pages>6311–6322</pages>
      <abstract>For many business applications, we often seek to analyze sentiments associated with any arbitrary aspects of commercial products, despite having a very limited amount of labels or even without any labels at all. However, existing aspect target sentiment classification (ATSC) models are not trainable if annotated datasets are not available. Even with labeled data, they fall short of reaching satisfactory performance. To address this, we propose simple approaches that better solve ATSC with natural language prompts, enabling the task under zero-shot cases and enhancing supervised settings, especially for few-shot cases. Under the few-shot setting for SemEval 2014 Task 4 laptop domain, our method of reformulating ATSC as an NLI task outperforms supervised SOTA approaches by up to 24.13 accuracy points and 33.14 macro F1 points. Moreover, we demonstrate that our prompts could handle implicitly stated aspects as well: our models reach about 77% accuracy on detecting sentiments for aspect categories (e.g., food), which do not necessarily appear within the text, even though we trained the models only with explicitly mentioned aspect terms (e.g., fajitas) from just 16 reviews - while the accuracy of the no-prompt baseline is only around 65%.</abstract>
      <url hash="925372c6">2021.emnlp-main.509</url>
      <bibkey>seoh-etal-2021-open</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.509</doi>
      <video href="2021.emnlp-main.509.mp4"/>
      <pwccode url="https://github.com/ronaldseoh/atsc_prompts" additional="false">ronaldseoh/atsc_prompts</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2014-task-4-sub-task-2">SemEval 2014 Task 4 Sub Task 2</pwcdataset>
    </paper>
    <paper id="510">
      <title>Does <fixed-case>BERT</fixed-case> Learn as Humans Perceive? Understanding Linguistic Styles through Lexica</title>
      <author><first>Shirley Anugrah</first><last>Hayati</last></author>
      <author><first>Dongyeop</first><last>Kang</last></author>
      <author><first>Lyle</first><last>Ungar</last></author>
      <pages>6323–6331</pages>
      <abstract>People convey their intention and attitude through linguistic styles of the text that they write. In this study, we investigate lexicon usages across styles throughout two lenses: human perception and machine word importance, since words differ in the strength of the stylistic cues that they provide. To collect labels of human perception, we curate a new dataset, Hummingbird, on top of benchmarking style datasets. We have crowd workers highlight the representative words in the text that makes them think the text has the following styles: politeness, sentiment, offensiveness, and five emotion types. We then compare these human word labels with word importance derived from a popular fine-tuned style classifier like BERT. Our results show that the BERT often finds content words not relevant to the target style as important words used in style prediction, but humans do not perceive the same way even though for some styles (e.g., positive sentiment and joy) human- and machine-identified words share significant overlap for some styles.</abstract>
      <url hash="068a99d5">2021.emnlp-main.510</url>
      <bibkey>hayati-etal-2021-bert</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.510</doi>
      <video href="2021.emnlp-main.510.mp4"/>
      <pwccode url="https://github.com/sweetpeach/hummingbird" additional="false">sweetpeach/hummingbird</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hummingbird">Hummingbird</pwcdataset>
    </paper>
    <paper id="511">
      <title>Improving Stance Detection with Multi-Dataset Learning and Knowledge Distillation</title>
      <author><first>Yingjie</first><last>Li</last></author>
      <author><first>Chenye</first><last>Zhao</last></author>
      <author><first>Cornelia</first><last>Caragea</last></author>
      <pages>6332–6345</pages>
      <abstract>Stance detection determines whether the author of a text is in favor of, against or neutral to a specific target and provides valuable insights into important events such as legalization of abortion. Despite significant progress on this task, one of the remaining challenges is the scarcity of annotations. Besides, most previous works focused on a hard-label training in which meaningful similarities among categories are discarded during training. To address these challenges, first, we evaluate a multi-target and a multi-dataset training settings by training one model on each dataset and datasets of different domains, respectively. We show that models can learn more universal representations with respect to targets in these settings. Second, we investigate the knowledge distillation in stance detection and observe that transferring knowledge from a teacher model to a student model can be beneficial in our proposed training settings. Moreover, we propose an Adaptive Knowledge Distillation (AKD) method that applies instance-specific temperature scaling to the teacher and student predictions. Results show that the multi-dataset model performs best on all datasets and it can be further improved by the proposed AKD, outperforming the state-of-the-art by a large margin. We publicly release our code.</abstract>
      <url hash="0bb31b69">2021.emnlp-main.511</url>
      <bibkey>li-etal-2021-improving-stance</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.511</doi>
      <video href="2021.emnlp-main.511.mp4"/>
      <pwccode url="https://github.com/chuchun8/mdl-stance-distillation" additional="false">chuchun8/mdl-stance-distillation</pwccode>
    </paper>
    <paper id="512">
      <title>Discovering the Unknown Knowns: Turning Implicit Knowledge in the Dataset into Explicit Training Examples for Visual Question Answering</title>
      <author><first>Jihyung</first><last>Kil</last></author>
      <author><first>Cheng</first><last>Zhang</last></author>
      <author><first>Dong</first><last>Xuan</last></author>
      <author><first>Wei-Lun</first><last>Chao</last></author>
      <pages>6346–6361</pages>
      <abstract>Visual question answering (VQA) is challenging not only because the model has to handle multi-modal information, but also because it is just so hard to collect sufficient training examples — there are too many questions one can ask about an image. As a result, a VQA model trained solely on human-annotated examples could easily over-fit specific question styles or image contents that are being asked, leaving the model largely ignorant about the sheer diversity of questions. Existing methods address this issue primarily by introducing an auxiliary task such as visual grounding, cycle consistency, or debiasing. In this paper, we take a drastically different approach. We found that many of the “unknowns” to the learned VQA model are indeed “known” in the dataset implicitly. For instance, questions asking about the same object in different images are likely paraphrases; the number of detected or annotated objects in an image already provides the answer to the “how many” question, even if the question has not been annotated for that image. Building upon these insights, we present a simple data augmentation pipeline SimpleAug to turn this “known” knowledge into training examples for VQA. We show that these augmented examples can notably improve the learned VQA models’ performance, not only on the VQA-CP dataset with language prior shifts but also on the VQA v2 dataset without such shifts. Our method further opens up the door to leverage weakly-labeled or unlabeled images in a principled way to enhance VQA models. Our code and data are publicly available at https://github.com/heendung/simpleAUG.</abstract>
      <url hash="63c28a40">2021.emnlp-main.512</url>
      <bibkey>kil-etal-2021-discovering</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.512</doi>
      <video href="2021.emnlp-main.512.mp4"/>
      <pwccode url="https://github.com/heendung/simpleaug" additional="false">heendung/simpleaug</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering-v2-0">Visual Question Answering v2.0</pwcdataset>
    </paper>
    <paper id="513">
      <title>Improving Pre-trained Vision-and-Language Embeddings for Phrase Grounding</title>
      <author><first>Zi-Yi</first><last>Dou</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>6362–6371</pages>
      <abstract>Phrase grounding aims to map textual phrases to their associated image regions, which can be a prerequisite for multimodal reasoning and can benefit tasks requiring identifying objects based on language. With pre-trained vision-and-language models achieving impressive performance across tasks, it remains unclear if we can directly utilize their learned embeddings for phrase grounding without fine-tuning. To this end, we propose a method to extract matched phrase-region pairs from pre-trained vision-and-language embeddings and propose four fine-tuning objectives to improve the model phrase grounding ability using image-caption data without any supervised grounding signals. Experiments on two representative datasets demonstrate the effectiveness of our objectives, outperforming baseline models in both weakly-supervised and supervised phrase grounding settings. In addition, we evaluate the aligned embeddings on several other downstream tasks and show that we can achieve better phrase grounding without sacrificing representation generality.</abstract>
      <url hash="f851e2f4">2021.emnlp-main.513</url>
      <bibkey>dou-peng-2021-improving</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.513</doi>
      <video href="2021.emnlp-main.513.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="514">
      <title>Sequential Randomized Smoothing for Adversarially Robust Speech Recognition</title>
      <author><first>Raphael</first><last>Olivier</last></author>
      <author><first>Bhiksha</first><last>Raj</last></author>
      <pages>6372–6386</pages>
      <abstract>While Automatic Speech Recognition has been shown to be vulnerable to adversarial attacks, defenses against these attacks are still lagging. Existing, naive defenses can be partially broken with an adaptive attack. In classification tasks, the Randomized Smoothing paradigm has been shown to be effective at defending models. However, it is difficult to apply this paradigm to ASR tasks, due to their complexity and the sequential nature of their outputs. Our paper overcomes some of these challenges by leveraging speech-specific tools like enhancement and ROVER voting to design an ASR model that is robust to perturbations. We apply adaptive versions of state-of-the-art attacks, such as the Imperceptible ASR attack, to our model, and show that our strongest defense is robust to all attacks that use inaudible noise, and can only be broken with very high distortion.</abstract>
      <url hash="c4007561">2021.emnlp-main.514</url>
      <bibkey>olivier-raj-2021-sequential</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.514</doi>
      <video href="2021.emnlp-main.514.mp4"/>
      <pwccode url="https://github.com/raphaelolivier/smoothingasr" additional="false">raphaelolivier/smoothingasr</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
    </paper>
    <paper id="515">
      <title>Hitting your <fixed-case>MARQ</fixed-case>: Multimodal <fixed-case>AR</fixed-case>gument Quality Assessment in Long Debate Video</title>
      <author><first>Md Kamrul</first><last>Hasan</last></author>
      <author><first>James</first><last>Spann</last></author>
      <author><first>Masum</first><last>Hasan</last></author>
      <author><first>Md Saiful</first><last>Islam</last></author>
      <author><first>Kurtis</first><last>Haut</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <author><first>Ehsan</first><last>Hoque</last></author>
      <pages>6387–6397</pages>
      <abstract>The combination of gestures, intonations, and textual content plays a key role in argument delivery. However, the current literature mostly considers textual content while assessing the quality of an argument, and it is limited to datasets containing short sequences (18-48 words). In this paper, we study argument quality assessment in a multimodal context, and experiment on DBATES, a publicly available dataset of long debate videos. First, we propose a set of interpretable debate centric features such as clarity, content variation, body movement cues, and pauses, inspired by theories of argumentation quality. Second, we design the Multimodal ARgument Quality assessor (MARQ) – a hierarchical neural network model that summarizes the multimodal signals on long sequences and enriches the multimodal embedding with debate centric features. Our proposed MARQ model achieves an accuracy of 81.91% on the argument quality prediction task and outperforms established baseline models with an error rate reduction of 22.7%. Through ablation studies, we demonstrate the importance of multimodal cues in modeling argument quality.</abstract>
      <url hash="e8e5cbb2">2021.emnlp-main.515</url>
      <bibkey>hasan-etal-2021-hitting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.515</doi>
      <video href="2021.emnlp-main.515.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/dbates">DBATES</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ibm-rank-30k">IBM-Rank-30k</pwcdataset>
    </paper>
    <paper id="516">
      <title>Mind the Context: The Impact of Contextualization in Neural Module Networks for Grounding Visual Referring Expressions</title>
      <author><first>Arjun</first><last>Akula</last></author>
      <author><first>Spandana</first><last>Gella</last></author>
      <author><first>Keze</first><last>Wang</last></author>
      <author><first>Song-Chun</first><last>Zhu</last></author>
      <author><first>Siva</first><last>Reddy</last></author>
      <pages>6398–6416</pages>
      <abstract>Neural module networks (NMN) are a popular approach for grounding visual referring expressions. Prior implementations of NMN use pre-defined and fixed textual inputs in their module instantiation. This necessitates a large number of modules as they lack the ability to share weights and exploit associations between similar textual contexts (e.g. “dark cube on the left” vs. “black cube on the left”). In this work, we address these limitations and evaluate the impact of contextual clues in improving the performance of NMN models. First, we address the problem of fixed textual inputs by parameterizing the module arguments. This substantially reduce the number of modules in NMN by up to 75% without any loss in performance. Next we propose a method to contextualize our parameterized model to enhance the module’s capacity in exploiting the visiolinguistic associations. Our model outperforms the state-of-the-art NMN model on CLEVR-Ref+ dataset with +8.1% improvement in accuracy on the single-referent test set and +4.3% on the full test set. Additionally, we demonstrate that contextualization provides +11.2% and +1.7% improvements in accuracy over prior NMN models on CLOSURE and NLVR2. We further evaluate the impact of our contextualization by constructing a contrast set for CLEVR-Ref+, which we call CC-Ref+. We significantly outperform the baselines by as much as +10.4% absolute accuracy on CC-Ref+, illustrating the generalization skills of our approach.</abstract>
      <url hash="8f7d835d">2021.emnlp-main.516</url>
      <bibkey>akula-etal-2021-mind</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.516</doi>
      <video href="2021.emnlp-main.516.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/clevr">CLEVR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/clevr-ref">CLEVR-Ref+</pwcdataset>
    </paper>
    <paper id="517">
      <title>Weakly-Supervised Visual-Retriever-Reader for Knowledge-based Question Answering</title>
      <author><first>Man</first><last>Luo</last></author>
      <author><first>Yankai</first><last>Zeng</last></author>
      <author><first>Pratyay</first><last>Banerjee</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <pages>6417–6431</pages>
      <abstract>Knowledge-based visual question answering (VQA) requires answering questions with external knowledge in addition to the content of images. One dataset that is mostly used in evaluating knowledge-based VQA is OK-VQA, but it lacks a gold standard knowledge corpus for retrieval. Existing work leverage different knowledge bases (e.g., ConceptNet and Wikipedia) to obtain external knowledge. Because of varying knowledge bases, it is hard to fairly compare models’ performance. To address this issue, we collect a natural language knowledge base that can be used for any VQA system. Moreover, we propose a Visual Retriever-Reader pipeline to approach knowledge-based VQA. The visual retriever aims to retrieve relevant knowledge, and the visual reader seeks to predict answers based on given knowledge. We introduce various ways to retrieve knowledge using text and images and two reader styles: classification and extraction. Both the retriever and reader are trained with weak supervision. Our experimental results show that a good retriever can significantly improve the reader’s performance on the OK-VQA challenge. The code and corpus are provided in https://github.com/luomancs/retriever_reader_for_okvqa.git.</abstract>
      <url hash="f0515533">2021.emnlp-main.517</url>
      <bibkey>luo-etal-2021-weakly</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.517</doi>
      <video href="2021.emnlp-main.517.mp4"/>
      <pwccode url="https://github.com/luomancs/retriever_reader_for_okvqa" additional="false">luomancs/retriever_reader_for_okvqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ok-vqa">OK-VQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="518">
      <title><fixed-case>NDH</fixed-case>-Full: Learning and Evaluating Navigational Agents on Full-Length Dialogue</title>
      <author><first>Hyounghun</first><last>Kim</last></author>
      <author><first>Jialu</first><last>Li</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>6432–6442</pages>
      <abstract>Communication between human and mobile agents is getting increasingly important as such agents are widely deployed in our daily lives. Vision-and-Dialogue Navigation is one of the tasks that evaluate the agent’s ability to interact with humans for assistance and navigate based on natural language responses. In this paper, we explore the Navigation from Dialogue History (NDH) task, which is based on the Cooperative Vision-and-Dialogue Navigation (CVDN) dataset, and present a state-of-the-art model which is built upon Vision-Language transformers. However, despite achieving competitive performance, we find that the agent in the NDH task is not evaluated appropriately by the primary metric – Goal Progress. By analyzing the performance mismatch between Goal Progress and other metrics (e.g., normalized Dynamic Time Warping) from our state-of-the-art model, we show that NDH’s sub-path based task setup (i.e., navigating partial trajectory based on its correspondent subset of the full dialogue) does not provide the agent with enough supervision signal towards the goal region. Therefore, we propose a new task setup called NDH-Full which takes the full dialogue and the whole navigation path as one instance. We present a strong baseline model and show initial results on this new task. We further describe several approaches that we try, in order to improve the model performance (based on curriculum learning, pre-training, and data-augmentation), suggesting potential useful training methods on this new NDH-Full task.</abstract>
      <url hash="55ecd0ef">2021.emnlp-main.518</url>
      <bibkey>kim-etal-2021-ndh</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.518</doi>
      <video href="2021.emnlp-main.518.mp4"/>
      <pwccode url="https://github.com/hyounghk/ndh-full" additional="false">hyounghk/ndh-full</pwccode>
    </paper>
    <paper id="519">
      <title>Timeline Summarization based on Event Graph Compression via Time-Aware Optimal Transport</title>
      <author><first>Manling</first><last>Li</last></author>
      <author><first>Tengfei</first><last>Ma</last></author>
      <author><first>Mo</first><last>Yu</last></author>
      <author><first>Lingfei</first><last>Wu</last></author>
      <author><first>Tian</first><last>Gao</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <pages>6443–6456</pages>
      <abstract>Timeline Summarization identifies major events from a news collection and describes them following temporal order, with key dates tagged. Previous methods generally generate summaries separately for each date after they determine the key dates of events. These methods overlook the events’ intra-structures (arguments) and inter-structures (event-event connections). Following a different route, we propose to represent the news articles as an event-graph, thus the summarization becomes compressing the whole graph to its salient sub-graph. The key hypothesis is that the events connected through shared arguments and temporal order depict the skeleton of a timeline, containing events that are semantically related, temporally coherent and structurally salient in the global event graph. A time-aware optimal transport distance is then introduced for learning the compression model in an unsupervised manner. We show that our approach significantly improves on the state of the art on three real-world datasets, including two public standard benchmarks and our newly collected Timeline100 dataset.</abstract>
      <url hash="22b6177e">2021.emnlp-main.519</url>
      <bibkey>li-etal-2021-timeline</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.519</doi>
      <video href="2021.emnlp-main.519.mp4"/>
      <pwccode url="https://github.com/limanling/event-graph-summarization" additional="false">limanling/event-graph-summarization</pwccode>
    </paper>
    <paper id="520">
      <title><fixed-case>S</fixed-case>tream<fixed-case>H</fixed-case>over: Livestream Transcript Summarization and Annotation</title>
      <author><first>Sangwoo</first><last>Cho</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Tim</first><last>Ganter</last></author>
      <author><first>Trung</first><last>Bui</last></author>
      <author><first>Nedim</first><last>Lipka</last></author>
      <author><first>Walter</first><last>Chang</last></author>
      <author><first>Hailin</first><last>Jin</last></author>
      <author><first>Jonathan</first><last>Brandt</last></author>
      <author><first>Hassan</first><last>Foroosh</last></author>
      <author id="fei-liu-utdallas"><first>Fei</first><last>Liu</last></author>
      <pages>6457–6474</pages>
      <abstract>With the explosive growth of livestream broadcasting, there is an urgent need for new summarization technology that enables us to create a preview of streamed content and tap into this wealth of knowledge. However, the problem is nontrivial due to the informal nature of spoken language. Further, there has been a shortage of annotated datasets that are necessary for transcript summarization. In this paper, we present StreamHover, a framework for annotating and summarizing livestream transcripts. With a total of over 500 hours of videos annotated with both extractive and abstractive summaries, our benchmark dataset is significantly larger than currently existing annotated corpora. We explore a neural extractive summarization model that leverages vector-quantized variational autoencoder to learn latent vector representations of spoken utterances and identify salient utterances from the transcripts to form summaries. We show that our model generalizes better and improves performance over strong baselines. The results of this study provide an avenue for future research to improve summarization solutions for efficient browsing of livestreams.</abstract>
      <url hash="d9e4e71f">2021.emnlp-main.520</url>
      <bibkey>cho-etal-2021-streamhover</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.520</doi>
      <video href="2021.emnlp-main.520.mp4"/>
      <pwccode url="https://github.com/ucfnlp/streamhover" additional="false">ucfnlp/streamhover</pwccode>
    </paper>
    <paper id="521">
      <title>Cross-Register Projection for Headline Part of Speech Tagging</title>
      <author><first>Adrian</first><last>Benton</last></author>
      <author><first>Hanyang</first><last>Li</last></author>
      <author><first>Igor</first><last>Malioutov</last></author>
      <pages>6475–6490</pages>
      <abstract>Part of speech (POS) tagging is a familiar NLP task. State of the art taggers routinely achieve token-level accuracies of over 97% on news body text, evidence that the problem is well understood. However, the register of English news headlines, “headlinese”, is very different from the register of long-form text, causing POS tagging models to underperform on headlines. In this work, we automatically annotate news headlines with POS tags by projecting predicted tags from corresponding sentences in news bodies. We train a multi-domain POS tagger on both long-form and headline text and show that joint training on both registers improves over training on just one or naïvely concatenating training sets. We evaluate on a newly-annotated corpus of over 5,248 English news headlines from the Google sentence compression corpus, and show that our model yields a 23% relative error reduction per token and 19% per headline. In addition, we demonstrate that better headline POS tags can improve the performance of a syntax-based open information extraction system. We make POSH, the POS-tagged Headline corpus, available to encourage research in improved NLP models for news headlines.</abstract>
      <url hash="3210a2d1">2021.emnlp-main.521</url>
      <bibkey>benton-etal-2021-cross</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.521</doi>
      <video href="2021.emnlp-main.521.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/english-web-treebank">English Web Treebank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sentence-compression">Sentence Compression</pwcdataset>
    </paper>
    <paper id="522">
      <title>Editing Factual Knowledge in Language Models</title>
      <author><first>Nicola</first><last>De Cao</last></author>
      <author><first>Wilker</first><last>Aziz</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <pages>6491–6506</pages>
      <abstract>The factual knowledge acquired during pre-training and stored in the parameters of Language Models (LMs) can be useful in downstream tasks (e.g., question answering or textual inference). However, some facts can be incorrectly induced or become obsolete over time. We present KnowledgeEditor, a method which can be used to edit this knowledge and, thus, fix ‘bugs’ or unexpected predictions without the need for expensive re-training or fine-tuning. Besides being computationally efficient, KnowledgeEditordoes not require any modifications in LM pre-training (e.g., the use of meta-learning). In our approach, we train a hyper-network with constrained optimization to modify a fact without affecting the rest of the knowledge; the trained hyper-network is then used to predict the weight update at test time. We show KnowledgeEditor’s efficacy with two popular architectures and knowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and ii) a sequence-to-sequence BART model for question answering. With our method, changing a prediction on the specific wording of a query tends to result in a consistent change in predictions also for its paraphrases. We show that this can be further encouraged by exploiting (e.g., automatically-generated) paraphrases during training. Interestingly, our hyper-network can be regarded as a ‘probe’ revealing which components need to be changed to manipulate factual knowledge; our analysis shows that the updates tend to be concentrated on a small subset of components. Source code available at https://github.com/nicola-decao/KnowledgeEditor</abstract>
      <url hash="d82198d4">2021.emnlp-main.522</url>
      <bibkey>de-cao-etal-2021-editing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.522</doi>
      <pwccode url="https://github.com/nicola-decao/KnowledgeEditor" additional="true">nicola-decao/KnowledgeEditor</pwccode>
    </paper>
    <paper id="523">
      <title>Sparse Attention with Linear Units</title>
      <author><first>Biao</first><last>Zhang</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <pages>6507–6520</pages>
      <abstract>Recently, it has been argued that encoder-decoder models can be made more interpretable by replacing the softmax function in the attention with its sparse variants. In this work, we introduce a novel, simple method for achieving sparsity in attention: we replace the softmax activation with a ReLU, and show that sparsity naturally emerges from such a formulation. Training stability is achieved with layer normalization with either a specialized initialization or an additional gating function. Our model, which we call Rectified Linear Attention (ReLA), is easy to implement and more efficient than previously proposed sparse attention mechanisms. We apply ReLA to the Transformer and conduct experiments on five machine translation tasks. ReLA achieves translation performance comparable to several strong baselines, with training and decoding speed similar to that of the vanilla attention. Our analysis shows that ReLA delivers high sparsity rate and head diversity, and the induced cross attention achieves better accuracy with respect to source-target word alignment than recent sparsified softmax-based models. Intriguingly, ReLA heads also learn to attend to nothing (i.e. ‘switch off’) for some queries, which is not possible with sparsified softmax alternatives.</abstract>
      <url hash="543a4f27">2021.emnlp-main.523</url>
      <bibkey>zhang-etal-2021-sparse</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.523</doi>
      <video href="2021.emnlp-main.523.mp4"/>
      <pwccode url="https://github.com/bzhangGo/zero" additional="true">bzhangGo/zero</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
    </paper>
    <paper id="524">
      <title>Knowledge Base Completion Meets Transfer Learning</title>
      <author><first>Vid</first><last>Kocijan</last></author>
      <author><first>Thomas</first><last>Lukasiewicz</last></author>
      <pages>6521–6533</pages>
      <abstract>The aim of knowledge base completion is to predict unseen facts from existing facts in knowledge bases. In this work, we introduce the first approach for transfer of knowledge from one collection of facts to another without the need for entity or relation matching. The method works for both canonicalized knowledge bases and uncanonicalized or open knowledge bases, i.e., knowledge bases where more than one copy of a real-world entity or relation may exist. Such knowledge bases are a natural output of automated information extraction tools that extract structured data from unstructured text. Our main contribution is a method that can make use of a large-scale pretraining on facts, collected from unstructured text, to improve predictions on structured data from a specific domain. The introduced method is the most impactful on small datasets such as ReVerb20K, where we obtained a 6% absolute increase of mean reciprocal rank and 65% relative decrease of mean rank over the previously best method, despite not relying on large pre-trained models like BERT.</abstract>
      <url hash="9a2550f8">2021.emnlp-main.524</url>
      <bibkey>kocijan-lukasiewicz-2021-knowledge</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.524</doi>
      <video href="2021.emnlp-main.524.mp4"/>
      <pwccode url="https://github.com/vid-koci/kbctransferlearning" additional="false">vid-koci/kbctransferlearning</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/olpbench">OLPBENCH</pwcdataset>
    </paper>
    <paper id="525">
      <title><fixed-case>SPECTRA</fixed-case>: Sparse Structured Text Rationalization</title>
      <author><first>Nuno M.</first><last>Guerreiro</last></author>
      <author><first>André F. T.</first><last>Martins</last></author>
      <pages>6534–6550</pages>
      <abstract>Selective rationalization aims to produce decisions along with rationales (e.g., text highlights or word alignments between two sentences). Commonly, rationales are modeled as stochastic binary masks, requiring sampling-based gradient estimators, which complicates training and requires careful hyperparameter tuning. Sparse attention mechanisms are a deterministic alternative, but they lack a way to regularize the rationale extraction (e.g., to control the sparsity of a text highlight or the number of alignments). In this paper, we present a unified framework for deterministic extraction of structured explanations via constrained inference on a factor graph, forming a differentiable layer. Our approach greatly eases training and rationale regularization, generally outperforming previous work on what comes to performance and plausibility of the extracted rationales. We further provide a comparative study of stochastic and deterministic methods for rationale extraction for classification and natural language inference tasks, jointly assessing their predictive power, quality of the explanations, and model variability.</abstract>
      <url hash="e0ef9dc5">2021.emnlp-main.525</url>
      <bibkey>guerreiro-martins-2021-spectra</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.525</doi>
      <video href="2021.emnlp-main.525.mp4"/>
      <pwccode url="https://github.com/deep-spin/spectra-rationalization" additional="true">deep-spin/spectra-rationalization</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="526">
      <title>Towards Zero-Shot Knowledge Distillation for Natural Language Processing</title>
      <author><first>Ahmad</first><last>Rashid</last></author>
      <author><first>Vasileios</first><last>Lioutas</last></author>
      <author><first>Abbas</first><last>Ghaddar</last></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last></author>
      <pages>6551–6561</pages>
      <abstract>Knowledge distillation (KD) is a common knowledge transfer algorithm used for model compression across a variety of deep learning based natural language processing (NLP) solutions. In its regular manifestations, KD requires access to the teacher’s training data for knowledge transfer to the student network. However, privacy concerns, data regulations and proprietary reasons may prevent access to such data. We present, to the best of our knowledge, the first work on Zero-shot Knowledge Distillation for NLP, where the student learns from the much larger teacher without any task specific data. Our solution combines out-of-domain data and adversarial training to learn the teacher’s output distribution. We investigate six tasks from the GLUE benchmark and demonstrate that we can achieve between 75% and 92% of the teacher’s classification score (accuracy or F1) while compressing the model 30 times.</abstract>
      <url hash="01c17615">2021.emnlp-main.526</url>
      <bibkey>rashid-etal-2021-towards</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.526</doi>
      <video href="2021.emnlp-main.526.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="527">
      <title>Adversarial Regularization as Stackelberg Game: An Unrolled Optimization Approach</title>
      <author><first>Simiao</first><last>Zuo</last></author>
      <author><first>Chen</first><last>Liang</last></author>
      <author><first>Haoming</first><last>Jiang</last></author>
      <author><first>Xiaodong</first><last>Liu</last></author>
      <author><first>Pengcheng</first><last>He</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <author><first>Weizhu</first><last>Chen</last></author>
      <author><first>Tuo</first><last>Zhao</last></author>
      <pages>6562–6577</pages>
      <abstract>Adversarial regularization has been shown to improve the generalization performance of deep learning models in various natural language processing tasks. Existing works usually formulate the method as a zero-sum game, which is solved by alternating gradient descent/ascent algorithms. Such a formulation treats the adversarial and the defending players equally, which is undesirable because only the defending player contributes to the generalization performance. To address this issue, we propose Stackelberg Adversarial Regularization (SALT), which formulates adversarial regularization as a Stackelberg game. This formulation induces a competition between a leader and a follower, where the follower generates perturbations, and the leader trains the model subject to the perturbations. Different from conventional approaches, in SALT, the leader is in an advantageous position. When the leader moves, it recognizes the strategy of the follower and takes the anticipated follower’s outcomes into consideration. Such a leader’s advantage enables us to improve the model fitting to the unperturbed data. The leader’s strategic information is captured by the Stackelberg gradient, which is obtained using an unrolling algorithm. Our experimental results on a set of machine translation and natural language understanding tasks show that SALT outperforms existing adversarial regularization baselines across all tasks. Our code is publicly available.</abstract>
      <url hash="13bdb63c">2021.emnlp-main.527</url>
      <bibkey>zuo-etal-2021-adversarial</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.527</doi>
      <video href="2021.emnlp-main.527.mp4"/>
      <pwccode url="https://github.com/SimiaoZuo/Stackelberg-Adv" additional="false">SimiaoZuo/Stackelberg-Adv</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="528">
      <title>Aspect-Controllable Opinion Summarization</title>
      <author><first>Reinald Kim</first><last>Amplayo</last></author>
      <author><first>Stefanos</first><last>Angelidis</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <pages>6578–6593</pages>
      <abstract>Recent work on opinion summarization produces general summaries based on a set of input reviews and the popularity of opinions expressed in them. In this paper, we propose an approach that allows the generation of customized summaries based on aspect queries (e.g., describing the location and room of a hotel). Using a review corpus, we create a synthetic training dataset of (review, summary) pairs enriched with aspect controllers which are induced by a multi-instance learning model that predicts the aspects of a document at different levels of granularity. We fine-tune a pretrained model using our synthetic dataset and generate aspect-specific summaries by modifying the aspect controllers. Experiments on two benchmarks show that our model outperforms the previous state of the art and generates personalized summaries by controlling the number of aspects discussed in them.</abstract>
      <url hash="e65dc561">2021.emnlp-main.528</url>
      <bibkey>amplayo-etal-2021-aspect</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.528</doi>
      <video href="2021.emnlp-main.528.mp4"/>
      <pwccode url="https://github.com/rktamplayo/acesum" additional="false">rktamplayo/acesum</pwccode>
    </paper>
    <paper id="529">
      <title><fixed-case>Q</fixed-case>uest<fixed-case>E</fixed-case>val: Summarization Asks for Fact-based Evaluation</title>
      <author><first>Thomas</first><last>Scialom</last></author>
      <author><first>Paul-Alexis</first><last>Dray</last></author>
      <author><first>Sylvain</first><last>Lamprier</last></author>
      <author><first>Benjamin</first><last>Piwowarski</last></author>
      <author><first>Jacopo</first><last>Staiano</last></author>
      <author><first>Alex</first><last>Wang</last></author>
      <author><first>Patrick</first><last>Gallinari</last></author>
      <pages>6594–6604</pages>
      <abstract>Summarization evaluation remains an open research problem: current metrics such as ROUGE are known to be limited and to correlate poorly with human judgments. To alleviate this issue, recent work has proposed evaluation metrics which rely on question answering models to assess whether a summary contains all the relevant information in its source document. Though promising, the proposed approaches have so far failed to correlate better than ROUGE with human judgments. In this paper, we extend previous approaches and propose a unified framework, named QuestEval. In contrast to established metrics such as ROUGE or BERTScore, QuestEval does not require any ground-truth reference. Nonetheless, QuestEval substantially improves the correlation with human judgments over four evaluation dimensions (consistency, coherence, fluency, and relevance), as shown in extensive experiments.</abstract>
      <url hash="cf58cfa9">2021.emnlp-main.529</url>
      <bibkey>scialom-etal-2021-questeval</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.529</doi>
      <video href="2021.emnlp-main.529.mp4"/>
      <pwccode url="https://github.com/recitalAI/QuestEval" additional="true">recitalAI/QuestEval</pwccode>
    </paper>
    <paper id="530">
      <title>Simple Conversational Data Augmentation for Semi-supervised Abstractive Dialogue Summarization</title>
      <author><first>Jiaao</first><last>Chen</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>6605–6616</pages>
      <abstract>Abstractive conversation summarization has received growing attention while most current state-of-the-art summarization models heavily rely on human-annotated summaries. To reduce the dependence on labeled summaries, in this work, we present a simple yet effective set of Conversational Data Augmentation (CODA) methods for semi-supervised abstractive conversation summarization, such as random swapping/deletion to perturb the discourse relations inside conversations, dialogue-acts-guided insertion to interrupt the development of conversations, and conditional-generation-based substitution to substitute utterances with their paraphrases generated based on the conversation context. To further utilize unlabeled conversations, we combine CODA with two-stage noisy self-training where we first pre-train the summarization model on unlabeled conversations with pseudo summaries and then fine-tune it on labeled conversations. Experiments conducted on the recent conversation summarization datasets demonstrate the effectiveness of our methods over several state-of-the-art data augmentation baselines.</abstract>
      <url hash="62be795e">2021.emnlp-main.530</url>
      <bibkey>chen-yang-2021-simple</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.530</doi>
      <video href="2021.emnlp-main.530.mp4"/>
      <pwccode url="https://github.com/gt-salt/coda" additional="false">gt-salt/coda</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/samsum-corpus">SAMSum Corpus</pwcdataset>
    </paper>
    <paper id="531">
      <title>Finding a Balanced Degree of Automation for Summary Evaluation</title>
      <author><first>Shiyue</first><last>Zhang</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>6617–6632</pages>
      <abstract>Human evaluation for summarization tasks is reliable but brings in issues of reproducibility and high costs. Automatic metrics are cheap and reproducible but sometimes poorly correlated with human judgment. In this work, we propose flexible semiautomatic to automatic summary evaluation metrics, following the Pyramid human evaluation method. Semi-automatic Lite2Pyramid retains the reusable human-labeled Summary Content Units (SCUs) for reference(s) but replaces the manual work of judging SCUs’ presence in system summaries with a natural language inference (NLI) model. Fully automatic Lite3Pyramid further substitutes SCUs with automatically extracted Semantic Triplet Units (STUs) via a semantic role labeling (SRL) model. Finally, we propose in-between metrics, Lite2.xPyramid, where we use a simple regressor to predict how well the STUs can simulate SCUs and retain SCUs that are more difficult to simulate, which provides a smooth transition and balance between automation and manual evaluation. Comparing to 15 existing metrics, we evaluate human-metric correlations on 3 existing meta-evaluation datasets and our newly collected PyrXSum (with 100/10 XSum examples/systems). It shows that Lite2Pyramid consistently has the best summary-level correlations; Lite3Pyramid works better than or comparable to other automatic metrics; Lite2.xPyramid trades off small correlation drops for larger manual effort reduction, which can reduce costs for future data collection.</abstract>
      <url hash="071dc2a3">2021.emnlp-main.531</url>
      <bibkey>zhang-bansal-2021-finding</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.531</doi>
      <video href="2021.emnlp-main.531.mp4"/>
      <pwccode url="https://github.com/zhangshiyue/lite2-3pyramid" additional="false">zhangshiyue/lite2-3pyramid</pwccode>
    </paper>
    <paper id="532">
      <title><fixed-case>CLIFF</fixed-case>: Contrastive Learning for Improving Faithfulness and Factuality in Abstractive Summarization</title>
      <author><first>Shuyang</first><last>Cao</last></author>
      <author><first>Lu</first><last>Wang</last></author>
      <pages>6633–6649</pages>
      <abstract>We study generating abstractive summaries that are faithful and factually consistent with the given articles. A novel contrastive learning formulation is presented, which leverages both reference summaries, as positive training data, and automatically generated erroneous summaries, as negative training data, to train summarization systems that are better at distinguishing between them. We further design four types of strategies for creating negative samples, to resemble errors made commonly by two state-of-the-art models, BART and PEGASUS, found in our new human annotations of summary errors. Experiments on XSum and CNN/Daily Mail show that our contrastive learning framework is robust across datasets and models. It consistently produces more factual summaries than strong comparisons with post error correction, entailment-based reranking, and unlikelihood training, according to QA-based factuality evaluation. Human judges echo the observation and find that our model summaries correct more errors.</abstract>
      <url hash="7337a237">2021.emnlp-main.532</url>
      <bibkey>cao-wang-2021-cliff</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.532</doi>
      <video href="2021.emnlp-main.532.mp4"/>
      <pwccode url="https://github.com/makcedward/nlpaug" additional="false">makcedward/nlpaug</pwccode>
    </paper>
    <paper id="533">
      <title>Multilingual Unsupervised Neural Machine Translation with Denoising Adapters</title>
      <author><first>Ahmet</first><last>Üstün</last></author>
      <author><first>Alexandre</first><last>Berard</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <author><first>Matthias</first><last>Gallé</last></author>
      <pages>6650–6662</pages>
      <abstract>We consider the problem of multilingual unsupervised machine translation, translating to and from languages that only have monolingual data by using auxiliary parallel language pairs. For this problem the standard procedure so far to leverage the monolingual data is _back-translation_, which is computationally costly and hard to tune. In this paper we propose instead to use _denoising adapters_, adapter layers with a denoising objective, on top of pre-trained mBART-50. In addition to the modularity and flexibility of such an approach we show that the resulting translations are on-par with back-translating as measured by BLEU, and furthermore it allows adding unseen languages incrementally.</abstract>
      <url hash="d983a648">2021.emnlp-main.533</url>
      <bibkey>ustun-etal-2021-multilingual</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.533</doi>
      <video href="2021.emnlp-main.533.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/flores">FLoRes</pwcdataset>
    </paper>
    <paper id="534">
      <title><fixed-case>BERT</fixed-case>, m<fixed-case>BERT</fixed-case>, or <fixed-case>B</fixed-case>i<fixed-case>BERT</fixed-case>? A Study on Contextualized Embeddings for Neural Machine Translation</title>
      <author><first>Haoran</first><last>Xu</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <author><first>Kenton</first><last>Murray</last></author>
      <pages>6663–6675</pages>
      <abstract>The success of bidirectional encoders using masked language models, such as BERT, on numerous natural language processing tasks has prompted researchers to attempt to incorporate these pre-trained models into neural machine translation (NMT) systems. However, proposed methods for incorporating pre-trained models are non-trivial and mainly focus on BERT, which lacks a comparison of the impact that other pre-trained models may have on translation performance. In this paper, we demonstrate that simply using the output (contextualized embeddings) of a tailored and suitable bilingual pre-trained language model (dubbed BiBERT) as the input of the NMT encoder achieves state-of-the-art translation performance. Moreover, we also propose a stochastic layer selection approach and a concept of a dual-directional translation model to ensure the sufficient utilization of contextualized embeddings. In the case of without using back translation, our best models achieve BLEU scores of 30.45 for En→De and 38.61 for De→En on the IWSLT’14 dataset, and 31.26 for En→De and 34.94 for De→En on the WMT’14 dataset, which exceeds all published numbers.</abstract>
      <url hash="8d079ab2">2021.emnlp-main.534</url>
      <bibkey>xu-etal-2021-bert</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.534</doi>
      <video href="2021.emnlp-main.534.mp4"/>
      <pwccode url="https://github.com/fe1ixxu/BiBERT" additional="true">fe1ixxu/BiBERT</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/oscar">OSCAR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="535">
      <title>Controlling Machine Translation for Multiple Attributes with Additive Interventions</title>
      <author><first>Andrea</first><last>Schioppa</last></author>
      <author><first>David</first><last>Vilar</last></author>
      <author><first>Artem</first><last>Sokolov</last></author>
      <author><first>Katja</first><last>Filippova</last></author>
      <pages>6676–6696</pages>
      <abstract>Fine-grained control of machine translation (MT) outputs along multiple attributes is critical for many modern MT applications and is a requirement for gaining users’ trust. A standard approach for exerting control in MT is to prepend the input with a special tag to signal the desired output attribute. Despite its simplicity, attribute tagging has several drawbacks: continuous values must be binned into discrete categories, which is unnatural for certain applications; interference between multiple tags is poorly understood. We address these problems by introducing vector-valued interventions which allow for fine-grained control over multiple attributes simultaneously via a weighted linear combination of the corresponding vectors. For some attributes, our approach even allows for fine-tuning a model trained without annotations to support such interventions. In experiments with three attributes (length, politeness and monotonicity) and two language pairs (English to German and Japanese) our models achieve better control over a wider range of tasks compared to tagging, and translation quality does not degrade when no control is requested. Finally, we demonstrate how to enable control in an already trained model after a relatively cheap fine-tuning stage.</abstract>
      <url hash="9e005586">2021.emnlp-main.535</url>
      <bibkey>schioppa-etal-2021-controlling</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.535</doi>
      <video href="2021.emnlp-main.535.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/jesc">JESC</pwcdataset>
    </paper>
    <paper id="536">
      <title>A Generative Framework for Simultaneous Machine Translation</title>
      <author><first>Yishu</first><last>Miao</last></author>
      <author><first>Phil</first><last>Blunsom</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>6697–6706</pages>
      <abstract>We propose a generative framework for simultaneous machine translation. Conventional approaches use a fixed number of source words to translate or learn dynamic policies for the number of source words by reinforcement learning. Here we formulate simultaneous translation as a structural sequence-to-sequence learning problem. A latent variable is introduced to model read or translate actions at every time step, which is then integrated out to consider all the possible translation policies. A re-parameterised Poisson prior is used to regularise the policies which allows the model to explicitly balance translation quality and latency. The experiments demonstrate the effectiveness and robustness of the generative framework, which achieves the best BLEU scores given different average translation latencies on benchmark datasets.</abstract>
      <url hash="764afb8e">2021.emnlp-main.536</url>
      <bibkey>miao-etal-2021-generative</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.536</doi>
      <video href="2021.emnlp-main.536.mp4"/>
    </paper>
    <paper id="537">
      <title>It Is Not As Good As You Think! Evaluating Simultaneous Machine Translation on Interpretation Data</title>
      <author><first>Jinming</first><last>Zhao</last></author>
      <author><first>Philip</first><last>Arthur</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <author><first>Ehsan</first><last>Shareghi</last></author>
      <pages>6707–6715</pages>
      <abstract>Most existing simultaneous machine translation (SiMT) systems are trained and evaluated on offline translation corpora. We argue that SiMT systems should be trained and tested on real interpretation data. To illustrate this argument, we propose an interpretation test set and conduct a realistic evaluation of SiMT trained on offline translations. Our results, on our test set along with 3 existing smaller scale language pairs, highlight the difference of up-to 13.83 BLEU score when SiMT models are evaluated on translation vs interpretation data. In the absence of interpretation training data, we propose a translation-to-interpretation (T2I) style transfer method which allows converting existing offline translations into interpretation-style data, leading to up-to 2.8 BLEU improvement. However, the evaluation gap remains notable, calling for constructing large-scale interpretation corpora better suited for evaluating and developing SiMT systems.</abstract>
      <url hash="a21c618c">2021.emnlp-main.537</url>
      <bibkey>zhao-etal-2021-good</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.537</doi>
      <video href="2021.emnlp-main.537.mp4"/>
      <pwccode url="https://github.com/mingzi151/interpretationdata" additional="false">mingzi151/interpretationdata</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/europarl">Europarl</pwcdataset>
    </paper>
    <paper id="538">
      <title>Boosting Cross-Lingual Transfer via Self-Learning with Uncertainty Estimation</title>
      <author><first>Liyan</first><last>Xu</last></author>
      <author><first>Xuchao</first><last>Zhang</last></author>
      <author><first>Xujiang</first><last>Zhao</last></author>
      <author><first>Haifeng</first><last>Chen</last></author>
      <author><first>Feng</first><last>Chen</last></author>
      <author><first>Jinho D.</first><last>Choi</last></author>
      <pages>6716–6723</pages>
      <abstract>Recent multilingual pre-trained language models have achieved remarkable zero-shot performance, where the model is only finetuned on one source language and directly evaluated on target languages. In this work, we propose a self-learning framework that further utilizes unlabeled data of target languages, combined with uncertainty estimation in the process to select high-quality silver labels. Three different uncertainties are adapted and analyzed specifically for the cross lingual transfer: Language Heteroscedastic/Homoscedastic Uncertainty (LEU/LOU), Evidential Uncertainty (EVI). We evaluate our framework with uncertainties on two cross-lingual tasks including Named Entity Recognition (NER) and Natural Language Inference (NLI) covering 40 languages in total, which outperforms the baselines significantly by 10 F1 for NER on average and 2.5 accuracy for NLI.</abstract>
      <url hash="9dd570d4">2021.emnlp-main.538</url>
      <bibkey>xu-etal-2021-boosting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.538</doi>
      <video href="2021.emnlp-main.538.mp4"/>
      <pwccode url="https://github.com/lxucs/multilingual-sl" additional="false">lxucs/multilingual-sl</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="539">
      <title><fixed-case>L</fixed-case>evenshtein Training for Word-level Quality Estimation</title>
      <author><first>Shuoyang</first><last>Ding</last></author>
      <author><first>Marcin</first><last>Junczys-Dowmunt</last></author>
      <author><first>Matt</first><last>Post</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>6724–6733</pages>
      <abstract>We propose a novel scheme to use the Levenshtein Transformer to perform the task of word-level quality estimation. A Levenshtein Transformer is a natural fit for this task: trained to perform decoding in an iterative manner, a Levenshtein Transformer can learn to post-edit without explicit supervision. To further minimize the mismatch between the translation task and the word-level QE task, we propose a two-stage transfer learning procedure on both augmented data and human post-editing data. We also propose heuristics to construct reference labels that are compatible with subword-level finetuning and inference. Results on WMT 2020 QE shared task dataset show that our proposed method has superior data efficiency under the data-constrained setting and competitive performance under the unconstrained setting.</abstract>
      <url hash="a889fd4e">2021.emnlp-main.539</url>
      <bibkey>ding-etal-2021-levenshtein</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.539</doi>
      <video href="2021.emnlp-main.539.mp4"/>
      <pwccode url="https://github.com/shuoyangd/stenella" additional="false">shuoyangd/stenella</pwccode>
    </paper>
    <paper id="540">
      <title>Interactive Machine Comprehension with Dynamic Knowledge Graphs</title>
      <author><first>Xingdi</first><last>Yuan</last></author>
      <pages>6734–6750</pages>
      <abstract>Interactive machine reading comprehension (iMRC) is machine comprehension tasks where knowledge sources are partially observable. An agent must interact with an environment sequentially to gather necessary knowledge in order to answer a question. We hypothesize that graph representations are good inductive biases, which can serve as an agent’s memory mechanism in iMRC tasks. We explore four different categories of graphs that can capture text information at various levels. We describe methods that dynamically build and update these graphs during information gathering, as well as neural models to encode graph representations in RL agents. Extensive experiments on iSQuAD suggest that graph representations can result in significant performance improvements for RL agents.</abstract>
      <url hash="0a7f6862">2021.emnlp-main.540</url>
      <bibkey>yuan-2021-interactive</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.540</doi>
      <video href="2021.emnlp-main.540.mp4"/>
      <pwccode url="https://github.com/xingdi-eric-yuan/imrc_graph_public" additional="false">xingdi-eric-yuan/imrc_graph_public</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="541">
      <title>Residual Adapters for Parameter-Efficient <fixed-case>ASR</fixed-case> Adaptation to Atypical and Accented Speech</title>
      <author><first>Katrin</first><last>Tomanek</last></author>
      <author><first>Vicky</first><last>Zayats</last></author>
      <author><first>Dirk</first><last>Padfield</last></author>
      <author><first>Kara</first><last>Vaillancourt</last></author>
      <author><first>Fadi</first><last>Biadsy</last></author>
      <pages>6751–6760</pages>
      <abstract>Automatic Speech Recognition (ASR) systems are often optimized to work best for speakers with canonical speech patterns. Unfortunately, these systems perform poorly when tested on atypical speech and heavily accented speech. It has previously been shown that personalization through model fine-tuning substantially improves performance. However, maintaining such large models per speaker is costly and difficult to scale. We show that by adding a relatively small number of extra parameters to the encoder layers via so-called residual adapter, we can achieve similar adaptation gains compared to model fine-tuning, while only updating a tiny fraction (less than 0.5%) of the model parameters. We demonstrate this on two speech adaptation tasks (atypical and accented speech) and for two state-of-the-art ASR architectures.</abstract>
      <url hash="8fcee1f2">2021.emnlp-main.541</url>
      <bibkey>tomanek-etal-2021-residual</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.541</doi>
      <video href="2021.emnlp-main.541.mp4"/>
    </paper>
    <paper id="542">
      <title>Visual News: Benchmark and Challenges in News Image Captioning</title>
      <author><first>Fuxiao</first><last>Liu</last></author>
      <author><first>Yinghan</first><last>Wang</last></author>
      <author><first>Tianlu</first><last>Wang</last></author>
      <author><first>Vicente</first><last>Ordonez</last></author>
      <pages>6761–6771</pages>
      <abstract>We propose Visual News Captioner, an entity-aware model for the task of news image captioning. We also introduce Visual News, a large-scale benchmark consisting of more than one million news images along with associated news articles, image captions, author information, and other metadata. Unlike the standard image captioning task, news images depict situations where people, locations, and events are of paramount importance. Our proposed method can effectively combine visual and textual features to generate captions with richer information such as events and entities. More specifically, built upon the Transformer architecture, our model is further equipped with novel multi-modal feature fusion techniques and attention mechanisms, which are designed to generate named entities more accurately. Our method utilizes much fewer parameters while achieving slightly better prediction results than competing methods. Our larger and more diverse Visual News dataset further highlights the remaining challenges in captioning news images.</abstract>
      <url hash="fea3b545">2021.emnlp-main.542</url>
      <bibkey>liu-etal-2021-visual</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.542</doi>
      <video href="2021.emnlp-main.542.mp4"/>
      <pwccode url="https://github.com/FuxiaoLiu/VisualNews-Repository" additional="false">FuxiaoLiu/VisualNews-Repository</pwccode>
    </paper>
    <paper id="543">
      <title>Integrating Visuospatial, Linguistic, and Commonsense Structure into Story Visualization</title>
      <author><first>Adyasha</first><last>Maharana</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>6772–6786</pages>
      <abstract>While much research has been done in text-to-image synthesis, little work has been done to explore the usage of linguistic structure of the input text. Such information is even more important for story visualization since its inputs have an explicit narrative structure that needs to be translated into an image sequence (or visual story). Prior work in this domain has shown that there is ample room for improvement in the generated image sequence in terms of visual quality, consistency and relevance. In this paper, we first explore the use of constituency parse trees using a Transformer-based recurrent architecture for encoding structured input. Second, we augment the structured input with commonsense information and study the impact of this external knowledge on the generation of visual story. Third, we also incorporate visual structure via bounding boxes and dense captioning to provide feedback about the characters/objects in generated images within a dual learning setup. We show that off-the-shelf dense-captioning models trained on Visual Genome can improve the spatial structure of images from a different target domain without needing fine-tuning. We train the model end-to-end using intra-story contrastive loss (between words and image sub-regions) and show significant improvements in visual quality. Finally, we provide an analysis of the linguistic and visuo-spatial information.</abstract>
      <url hash="7e74d4b0">2021.emnlp-main.543</url>
      <bibkey>maharana-bansal-2021-integrating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.543</doi>
      <video href="2021.emnlp-main.543.mp4"/>
      <pwccode url="https://github.com/adymaharana/vlcstorygan" additional="false">adymaharana/vlcstorygan</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
    <paper id="544">
      <title><fixed-case>V</fixed-case>ideo<fixed-case>CLIP</fixed-case>: Contrastive Pre-training for Zero-shot Video-Text Understanding</title>
      <author><first>Hu</first><last>Xu</last></author>
      <author><first>Gargi</first><last>Ghosh</last></author>
      <author><first>Po-Yao</first><last>Huang</last></author>
      <author><first>Dmytro</first><last>Okhonko</last></author>
      <author><first>Armen</first><last>Aghajanyan</last></author>
      <author><first>Florian</first><last>Metze</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <author><first>Christoph</first><last>Feichtenhofer</last></author>
      <pages>6787–6800</pages>
      <abstract>We present VideoCLIP, a contrastive approach to pre-train a unified model for zero-shot video and text understanding, without using any labels on downstream tasks. VideoCLIP trains a transformer for video and text by contrasting temporally overlapping positive video-text pairs with hard negatives from nearest neighbor retrieval. Our experiments on a diverse series of downstream tasks, including sequence-level text-video retrieval, VideoQA, token-level action localization, and action segmentation reveal state-of-the-art performance, surpassing prior work, and in some cases even outperforming supervised approaches. Code is made available at https://github.com/pytorch/fairseq/examples/MMPT.</abstract>
      <url hash="85cab672">2021.emnlp-main.544</url>
      <bibkey>xu-etal-2021-videoclip</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.544</doi>
      <video href="2021.emnlp-main.544.mp4"/>
      <pwccode url="https://github.com/pytorch/fairseq" additional="true">pytorch/fairseq</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/coin">COIN</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/crosstask">CrossTask</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/didemo">DiDeMo</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/howto100m">HowTo100M</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/msr-vtt">MSR-VTT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/youcook2">YouCook2</pwcdataset>
    </paper>
    <paper id="545">
      <title><fixed-case>N</fixed-case>ews<fixed-case>CLIP</fixed-case>pings: <fixed-case>A</fixed-case>utomatic <fixed-case>G</fixed-case>eneration of <fixed-case>O</fixed-case>ut-of-<fixed-case>C</fixed-case>ontext <fixed-case>M</fixed-case>ultimodal <fixed-case>M</fixed-case>edia</title>
      <author><first>Grace</first><last>Luo</last></author>
      <author><first>Trevor</first><last>Darrell</last></author>
      <author><first>Anna</first><last>Rohrbach</last></author>
      <pages>6801–6817</pages>
      <abstract>Online misinformation is a prevalent societal issue, with adversaries relying on tools ranging from cheap fakes to sophisticated deep fakes. We are motivated by the threat scenario where an image is used out of context to support a certain narrative. While some prior datasets for detecting image-text inconsistency generate samples via text manipulation, we propose a dataset where both image and text are unmanipulated but mismatched. We introduce several strategies for automatically retrieving convincing images for a given caption, capturing cases with inconsistent entities or semantic context. Our large-scale automatically generated the NewsCLIPpings Dataset: (1) demonstrates that machine-driven image repurposing is now a realistic threat, and (2) provides samples that represent challenging instances of mismatch between text and image in news that are able to mislead humans. We benchmark several state-of-the-art multimodal models on our dataset and analyze their performance across different pretraining domains and visual backbones.</abstract>
      <url hash="3b57d479">2021.emnlp-main.545</url>
      <bibkey>luo-etal-2021-newsclippings</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.545</doi>
      <video href="2021.emnlp-main.545.mp4"/>
      <pwccode url="https://github.com/g-luo/news_clippings" additional="false">g-luo/news_clippings</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/newsclippings">NewsCLIPpings</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptual-captions">Conceptual Captions</pwcdataset>
    </paper>
    <paper id="546">
      <title>Powering Comparative Classification with Sentiment Analysis via Domain Adaptive Knowledge Transfer</title>
      <author><first>Zeyu</first><last>Li</last></author>
      <author><first>Yilong</first><last>Qin</last></author>
      <author><first>Zihan</first><last>Liu</last></author>
      <author><first>Wei</first><last>Wang</last></author>
      <pages>6818–6830</pages>
      <abstract>We study Comparative Preference Classification (CPC) which aims at predicting whether a preference comparison exists between two entities in a given sentence and, if so, which entity is preferred over the other. High-quality CPC models can significantly benefit applications such as comparative question answering and review-based recommendation. Among the existing approaches, non-deep learning methods suffer from inferior performances. The state-of-the-art graph neural network-based ED-GAT (Ma et al., 2020) only considers syntactic information while ignoring the critical semantic relations and the sentiments to the compared entities. We propose Sentiment Analysis Enhanced COmparative Network (SAECON) which improves CPC accuracy with a sentiment analyzer that learns sentiments to individual entities via domain adaptive knowledge transfer. Experiments on the CompSent-19 (Panchenko et al., 2019) dataset present a significant improvement on the F1 scores over the best existing CPC approaches.</abstract>
      <url hash="5efbcdc0">2021.emnlp-main.546</url>
      <bibkey>li-etal-2021-powering</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.546</doi>
      <video href="2021.emnlp-main.546.mp4"/>
      <pwccode url="https://github.com/zyli93/saecon" additional="false">zyli93/saecon</pwccode>
    </paper>
    <paper id="547">
      <title>Tribrid: Stance Classification with Neural Inconsistency Detection</title>
      <author><first>Song</first><last>Yang</last></author>
      <author><first>Jacopo</first><last>Urbani</last></author>
      <pages>6831–6843</pages>
      <abstract>We study the problem of performing automatic stance classification on social media with neural architectures such as BERT. Although these architectures deliver impressive results, their level is not yet comparable to the one of humans and they might produce errors that have a significant impact on the downstream task (e.g., fact-checking). To improve the performance, we present a new neural architecture where the input also includes automatically generated negated perspectives over a given claim. The model is jointly learned to make simultaneously multiple predictions, which can be used either to improve the classification of the original perspective or to filter out doubtful predictions. In the first case, we propose a weakly supervised method for combining the predictions into a final one. In the second case, we show that using the confidence scores to remove doubtful predictions allows our method to achieve human-like performance over the retained information, which is still a sizable part of the original input.</abstract>
      <url hash="0e426dcf">2021.emnlp-main.547</url>
      <attachment type="Software" hash="42d7ad82">2021.emnlp-main.547.Software.zip</attachment>
      <bibkey>yang-urbani-2021-tribrid</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.547</doi>
      <video href="2021.emnlp-main.547.mp4"/>
      <pwccode url="https://github.com/karmaresearch/tribrid" additional="false">karmaresearch/tribrid</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/perspectrum">Perspectrum</pwcdataset>
    </paper>
    <paper id="548">
      <title><fixed-case>SYSML</fixed-case>: <fixed-case>S</fixed-case>t<fixed-case>Y</fixed-case>lometry with <fixed-case>S</fixed-case>tructure and <fixed-case>M</fixed-case>ultitask <fixed-case>L</fixed-case>earning: <fixed-case>I</fixed-case>mplications for <fixed-case>D</fixed-case>arknet Forum Migrant Analysis</title>
      <author><first>Pranav</first><last>Maneriker</last></author>
      <author><first>Yuntian</first><last>He</last></author>
      <author><first>Srinivasan</first><last>Parthasarathy</last></author>
      <pages>6844–6857</pages>
      <abstract>Darknet market forums are frequently used to exchange illegal goods and services between parties who use encryption to conceal their identities. The Tor network is used to host these markets, which guarantees additional anonymization from IP and location tracking, making it challenging to link across malicious users using multiple accounts (sybils). Additionally, users migrate to new forums when one is closed further increasing the difficulty of linking users across multiple forums. We develop a novel stylometry-based multitask learning approach for natural language and model interactions using graph embeddings to construct low-dimensional representations of short episodes of user activity for authorship attribution. We provide a comprehensive evaluation of our methods across four different darknet forums demonstrating its efficacy over the state-of-the-art, with a lift of up to 2.5X on Mean Retrieval Rank and 2X on Recall@10.</abstract>
      <url hash="c3ac7c97">2021.emnlp-main.548</url>
      <bibkey>maneriker-etal-2021-sysml</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.548</doi>
      <video href="2021.emnlp-main.548.mp4"/>
      <pwccode url="https://github.com/pranavmaneriker/sysml" additional="false">pranavmaneriker/sysml</pwccode>
    </paper>
    <paper id="549">
      <title>Few-Shot Emotion Recognition in Conversation with Sequential Prototypical Networks</title>
      <author><first>Gaël</first><last>Guibon</last></author>
      <author><first>Matthieu</first><last>Labeau</last></author>
      <author><first>Hélène</first><last>Flamein</last></author>
      <author><first>Luce</first><last>Lefeuvre</last></author>
      <author><first>Chloé</first><last>Clavel</last></author>
      <pages>6858–6870</pages>
      <abstract>Several recent studies on dyadic human-human interactions have been done on conversations without specific business objectives. However, many companies might benefit from studies dedicated to more precise environments such as after sales services or customer satisfaction surveys. In this work, we place ourselves in the scope of a live chat customer service in which we want to detect emotions and their evolution in the conversation flow. This context leads to multiple challenges that range from exploiting restricted, small and mostly unlabeled datasets to finding and adapting methods for such context. We tackle these challenges by using Few-Shot Learning while making the hypothesis it can serve conversational emotion classification for different languages and sparse labels. We contribute by proposing a variation of Prototypical Networks for sequence labeling in conversation that we name ProtoSeq. We test this method on two datasets with different languages: daily conversations in English and customer service chat conversations in French. When applied to emotion classification in conversations, our method proved to be competitive even when compared to other ones.</abstract>
      <url hash="29d970cb">2021.emnlp-main.549</url>
      <bibkey>guibon-etal-2021-shot</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.549</doi>
      <video href="2021.emnlp-main.549.mp4"/>
      <pwccode url="https://github.com/gguibon/protoseq" additional="false">gguibon/protoseq</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
    </paper>
    <paper id="550">
      <title><fixed-case>CLASSIC</fixed-case>: Continual and Contrastive Learning of Aspect Sentiment Classification Tasks</title>
      <author><first>Zixuan</first><last>Ke</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Hu</first><last>Xu</last></author>
      <author><first>Lei</first><last>Shu</last></author>
      <pages>6871–6883</pages>
      <abstract>This paper studies continual learning (CL) of a sequence of aspect sentiment classification (ASC) tasks in a particular CL setting called domain incremental learning (DIL). Each task is from a different domain or product. The DIL setting is particularly suited to ASC because in testing the system needs not know the task/domain to which the test data belongs. To our knowledge, this setting has not been studied before for ASC. This paper proposes a novel model called CLASSIC. The key novelty is a contrastive continual learning method that enables both knowledge transfer across tasks and knowledge distillation from old tasks to the new task, which eliminates the need for task ids in testing. Experimental results show the high effectiveness of CLASSIC.</abstract>
      <url hash="a9de4091">2021.emnlp-main.550</url>
      <bibkey>ke-etal-2021-classic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.550</doi>
      <video href="2021.emnlp-main.550.mp4"/>
      <pwccode url="https://github.com/zixuanke/pycontinual" additional="false">zixuanke/pycontinual</pwccode>
    </paper>
    <paper id="551">
      <title>Implicit Sentiment Analysis with Event-centered Text Representation</title>
      <author><first>Deyu</first><last>Zhou</last></author>
      <author><first>Jianan</first><last>Wang</last></author>
      <author><first>Linhai</first><last>Zhang</last></author>
      <author><first>Yulan</first><last>He</last></author>
      <pages>6884–6893</pages>
      <abstract>Implicit sentiment analysis, aiming at detecting the sentiment of a sentence without sentiment words, has become an attractive research topic in recent years. In this paper, we focus on event-centric implicit sentiment analysis that utilizes the sentiment-aware event contained in a sentence to infer its sentiment polarity. Most existing methods in implicit sentiment analysis simply view noun phrases or entities in text as events or indirectly model events with sophisticated models. Since events often trigger sentiments in sentences, we argue that this task would benefit from explicit modeling of events and event representation learning. To this end, we represent an event as the combination of its event type and the event triplet &lt;subject, predicate, object&gt;. Based on such event representation, we further propose a novel model with hierarchical tensor-based composition mechanism to detect sentiment in text. In addition, we present a dataset for event-centric implicit sentiment analysis where each sentence is labeled with the event representation described above. Experimental results on our constructed dataset and an existing benchmark dataset show the effectiveness of the proposed approach.</abstract>
      <url hash="abef830f">2021.emnlp-main.551</url>
      <bibkey>zhou-etal-2021-implicit</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.551</doi>
      <video href="2021.emnlp-main.551.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/framenet">FrameNet</pwcdataset>
    </paper>
    <paper id="552">
      <title><fixed-case>S</fixed-case>im<fixed-case>CSE</fixed-case>: Simple Contrastive Learning of Sentence Embeddings</title>
      <author><first>Tianyu</first><last>Gao</last></author>
      <author><first>Xingcheng</first><last>Yao</last></author>
      <author><first>Danqi</first><last>Chen</last></author>
      <pages>6894–6910</pages>
      <abstract>This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using “entailment” pairs as positives and “contradiction” pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman’s correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show—both theoretically and empirically—that contrastive learning objective regularizes pre-trained embeddings’ anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.</abstract>
      <url hash="290434e1">2021.emnlp-main.552</url>
      <bibkey>gao-etal-2021-simcse</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.552</doi>
      <video href="2021.emnlp-main.552.mp4"/>
      <pwccode url="https://github.com/princeton-nlp/SimCSE" additional="true">princeton-nlp/SimCSE</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sts-benchmark">STS Benchmark</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/semantic-textual-similarity-2012-2016">Semantic Textual Similarity (2012 - 2016)</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="553">
      <title>When is Wall a Pared and when a Muro?: Extracting Rules Governing Lexical Selection</title>
      <author><first>Aditi</first><last>Chaudhary</last></author>
      <author><first>Kayo</first><last>Yin</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>6911–6929</pages>
      <abstract>Learning fine-grained distinctions between vocabulary items is a key challenge in learning a new language. For example, the noun “wall” has different lexical manifestations in Spanish – “pared” refers to an indoor wall while “muro” refers to an outside wall. However, this variety of lexical distinction may not be obvious to non-native learners unless the distinction is explained in such a way. In this work, we present a method for automatically identifying fine-grained lexical distinctions, and extracting rules explaining these distinctions in a human- and machine-readable format. We confirm the quality of these extracted rules in a language learning setup for two languages, Spanish and Greek, where we use the rules to teach non-native speakers when to translate a given ambiguous word into its different possible translations.</abstract>
      <url hash="f56b9e90">2021.emnlp-main.553</url>
      <bibkey>chaudhary-etal-2021-wall</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.553</doi>
      <video href="2021.emnlp-main.553.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="554">
      <title>Aligning Actions Across Recipe Graphs</title>
      <author><first>Lucia</first><last>Donatelli</last></author>
      <author><first>Theresa</first><last>Schmidt</last></author>
      <author><first>Debanjali</first><last>Biswas</last></author>
      <author><first>Arne</first><last>Köhn</last></author>
      <author><first>Fangzhou</first><last>Zhai</last></author>
      <author><first>Alexander</first><last>Koller</last></author>
      <pages>6930–6942</pages>
      <abstract>Recipe texts are an idiosyncratic form of instructional language that pose unique challenges for automatic understanding. One challenge is that a cooking step in one recipe can be explained in another recipe in different words, at a different level of abstraction, or not at all. Previous work has annotated correspondences between recipe instructions at the sentence level, often glossing over important correspondences between cooking steps across recipes. We present a novel and fully-parsed English recipe corpus, ARA (Aligned Recipe Actions), which annotates correspondences between individual actions across similar recipes with the goal of capturing information implicit for accurate recipe understanding. We represent this information in the form of recipe graphs, and we train a neural model for predicting correspondences on ARA. We find that substantial gains in accuracy can be obtained by taking fine-grained structural information about the recipes into account.</abstract>
      <url hash="257f187e">2021.emnlp-main.554</url>
      <bibkey>donatelli-etal-2021-aligning</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.554</doi>
      <video href="2021.emnlp-main.554.mp4"/>
      <pwccode url="https://github.com/coli-saar/ara" additional="false">coli-saar/ara</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/microsoft-research-multimodal-aligned-recipe">Microsoft Research Multimodal Aligned Recipe Corpus</pwcdataset>
    </paper>
    <paper id="555">
      <title>Generating Datasets with Pretrained Language Models</title>
      <author><first>Timo</first><last>Schick</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>6943–6951</pages>
      <abstract>To obtain high-quality sentence embeddings from pretrained language models (PLMs), they must either be augmented with additional pretraining objectives or finetuned on a large set of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to generate suitable datasets of sufficient size. In this paper, we show how PLMs can be leveraged to obtain high-quality sentence embeddings without the need for labeled data, finetuning or modifications to the pretraining objective: We utilize the generative abilities of large and high-performing PLMs to generate entire datasets of labeled text pairs from scratch, which we then use for finetuning much smaller and more efficient models. Our fully unsupervised approach outperforms strong baselines on several semantic textual similarity datasets.</abstract>
      <url hash="62dc576c">2021.emnlp-main.555</url>
      <bibkey>schick-schutze-2021-generating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.555</doi>
      <video href="2021.emnlp-main.555.mp4"/>
      <pwccode url="https://github.com/timoschick/dino" additional="true">timoschick/dino</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sts-benchmark">STS Benchmark</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/semantic-textual-similarity-2012-2016">Semantic Textual Similarity (2012 - 2016)</pwcdataset>
    </paper>
    <paper id="556">
      <title>Continuous Entailment Patterns for Lexical Inference in Context</title>
      <author><first>Martin</first><last>Schmitt</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>6952–6959</pages>
      <abstract>Combining a pretrained language model (PLM) with textual patterns has been shown to help in both zero- and few-shot settings. For zero-shot performance, it makes sense to design patterns that closely resemble the text seen during self-supervised pretraining because the model has never seen anything else. Supervised training allows for more flexibility. If we allow for tokens outside the PLM’s vocabulary, patterns can be adapted more flexibly to a PLM’s idiosyncrasies. Contrasting patterns where a “token” can be any continuous vector from those where a discrete choice between vocabulary elements has to be made, we call our method CONtinous pAtterNs (CONAN). We evaluate CONAN on two established benchmarks for lexical inference in context (LIiC) a.k.a. predicate entailment, a challenging natural language understanding task with relatively small training data. In a direct comparison with discrete patterns, CONAN consistently leads to improved performance, setting a new state of the art. Our experiments give valuable insights on the kind of pattern that enhances a PLM’s performance on LIiC and raise important questions regarding our understanding of PLMs using text patterns.</abstract>
      <url hash="c34c3f03">2021.emnlp-main.556</url>
      <bibkey>schmitt-schutze-2021-continuous</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.556</doi>
      <video href="2021.emnlp-main.556.mp4"/>
      <pwccode url="https://github.com/mnschmit/conan" additional="false">mnschmit/conan</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sherliic">SherLIiC</pwcdataset>
    </paper>
    <paper id="557">
      <title>Numeracy enhances the Literacy of Language Models</title>
      <author><first>Avijit</first><last>Thawani</last></author>
      <author><first>Jay</first><last>Pujara</last></author>
      <author><first>Filip</first><last>Ilievski</last></author>
      <pages>6960–6967</pages>
      <abstract>Specialized number representations in NLP have shown improvements on numerical reasoning tasks like arithmetic word problems and masked number prediction. But humans also use numeracy to make better sense of world concepts, e.g., you can seat 5 people in your ‘room’ but not 500. Does a better grasp of numbers improve a model’s understanding of other concepts and words? This paper studies the effect of using six different number encoders on the task of masked word prediction (MWP), as a proxy for evaluating literacy. To support this investigation, we develop Wiki-Convert, a 900,000 sentence dataset annotated with numbers and units, to avoid conflating nominal and ordinal number occurrences. We find a significant improvement in MWP for sentences containing numbers, that exponent embeddings are the best number encoders, yielding over 2 points jump in prediction accuracy over a BERT baseline, and that these enhanced literacy skills also generalize to contexts without annotated numbers. We release all code at https://git.io/JuZXn.</abstract>
      <url hash="a885570e">2021.emnlp-main.557</url>
      <attachment type="Software" hash="e6eebc73">2021.emnlp-main.557.Software.zip</attachment>
      <bibkey>thawani-etal-2021-numeracy</bibkey>
      <revision id="1" href="2021.emnlp-main.557v1" hash="ecf6ae2a"/>
      <revision id="2" href="2021.emnlp-main.557v2" hash="a885570e" date="2021-11-09">Added acknowledgements.</revision>
      <doi>10.18653/v1/2021.emnlp-main.557</doi>
      <video href="2021.emnlp-main.557.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiconvert">WikiConvert</pwcdataset>
    </paper>
    <paper id="558">
      <title>Students Who Study Together Learn Better: On the Importance of Collective Knowledge Distillation for Domain Transfer in Fact Verification</title>
      <author><first>Mitch Paul</first><last>Mithun</last></author>
      <author><first>Sandeep</first><last>Suntwal</last></author>
      <author><first>Mihai</first><last>Surdeanu</last></author>
      <pages>6968–6973</pages>
      <abstract>While neural networks produce state-of-the- art performance in several NLP tasks, they generally depend heavily on lexicalized information, which transfer poorly between domains. Previous works have proposed delexicalization as a form of knowledge distillation to reduce the dependency on such lexical artifacts. However, a critical unsolved issue that remains is how much delexicalization to apply: a little helps reduce overfitting, but too much discards useful information. We propose Group Learning, a knowledge and model distillation approach for fact verification in which multiple student models have access to different delexicalized views of the data, but are encouraged to learn from each other through pair-wise consistency losses. In several cross-domain experiments between the FEVER and FNC fact verification datasets, we show that our approach learns the best delexicalization strategy for the given training dataset, and outperforms state-of-the-art classifiers that rely on the original data.</abstract>
      <url hash="60106c55">2021.emnlp-main.558</url>
      <bibkey>mithun-etal-2021-students</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.558</doi>
      <video href="2021.emnlp-main.558.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/figer">FIGER</pwcdataset>
    </paper>
    <paper id="559">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>EURLEX</fixed-case> - A multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer</title>
      <author><first>Ilias</first><last>Chalkidis</last></author>
      <author><first>Manos</first><last>Fergadiotis</last></author>
      <author><first>Ion</first><last>Androutsopoulos</last></author>
      <pages>6974–6996</pages>
      <abstract>We introduce MULTI-EURLEX, a new multilingual dataset for topic classification of legal documents. The dataset comprises 65k European Union (EU) laws, officially translated in 23 languages, annotated with multiple labels from the EUROVOC taxonomy. We highlight the effect of temporal concept drift and the importance of chronological, instead of random splits. We use the dataset as a testbed for zero-shot cross-lingual transfer, where we exploit annotated training documents in one language (source) to classify documents in another language (target). We find that fine-tuning a multilingually pretrained model (XLM-ROBERTA, MT5) in a single source language leads to catastrophic forgetting of multilingual knowledge and, consequently, poor zero-shot transfer to other languages. Adaptation strategies, namely partial fine-tuning, adapters, BITFIT, LNFIT, originally proposed to accelerate fine-tuning for new end-tasks, help retain multilingual knowledge from pretraining, substantially improving zero-shot cross-lingual transfer, but their impact also depends on the pretrained model used and the size of the label set.</abstract>
      <url hash="293dd3d4">2021.emnlp-main.559</url>
      <bibkey>chalkidis-etal-2021-multieurlex</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.559</doi>
      <revision id="1" href="2021.emnlp-main.559v1" hash="f03d0da8"/>
      <revision id="2" href="2021.emnlp-main.559v2" hash="293dd3d4" date="2022-04-29">Added missing acknowledgment.</revision>
      <video href="2021.emnlp-main.559.mp4"/>
      <pwccode url="https://github.com/nlpaueb/multi-eurlex" additional="false">nlpaueb/multi-eurlex</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/eurlex57k">EURLEX57K</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="560">
      <title>Joint Passage Ranking for Diverse Multi-Answer Retrieval</title>
      <author><first>Sewon</first><last>Min</last></author>
      <author><first>Kenton</first><last>Lee</last></author>
      <author><first>Ming-Wei</first><last>Chang</last></author>
      <author><first>Kristina</first><last>Toutanova</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <pages>6997–7008</pages>
      <abstract>We study multi-answer retrieval, an under-explored problem that requires retrieving passages to cover multiple distinct answers for a given question. This task requires joint modeling of retrieved passages, as models should not repeatedly retrieve passages containing the same answer at the cost of missing a different valid answer. Prior work focusing on single-answer retrieval is limited as it cannot reason about the set of passages jointly. In this paper, we introduce JPR, a joint passage retrieval model focusing on reranking. To model the joint probability of the retrieved passages, JPR makes use of an autoregressive reranker that selects a sequence of passages, equipped with novel training and decoding algorithms. Compared to prior approaches, JPR achieves significantly better answer coverage on three multi-answer datasets. When combined with downstream question answering, the improved retrieval enables larger answer generation models since they need to consider fewer passages, establishing a new state-of-the-art.</abstract>
      <url hash="b7938bc1">2021.emnlp-main.560</url>
      <bibkey>min-etal-2021-joint</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.560</doi>
      <video href="2021.emnlp-main.560.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="561">
      <title>Generative Context Pair Selection for Multi-hop Question Answering</title>
      <author><first>Dheeru</first><last>Dua</last></author>
      <author><first>Cicero</first><last>Nogueira dos Santos</last></author>
      <author><first>Patrick</first><last>Ng</last></author>
      <author><first>Ben</first><last>Athiwaratkun</last></author>
      <author><first>Bing</first><last>Xiang</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <pages>7009–7015</pages>
      <abstract>Compositional reasoning tasks such as multi-hop question answering require models to learn how to make latent decisions using only weak supervision from the final answer. Crowdsourced datasets gathered for these tasks, however, often contain only a slice of the underlying task distribution, which can induce unanticipated biases such as shallow word overlap between the question and context. Recent works have shown that discriminative training results in models that exploit these underlying biases to achieve a better held-out performance, without learning the right way to reason. We propose a generative context selection model for multi-hop QA that reasons about how the given question could have been generated given a context pair and not just independent contexts. We show that on HotpotQA, while being comparable to the state-of-the-art answering performance, our proposed generative passage selection model has a better performance (4.9% higher than baseline) on adversarial held-out set which tests robustness of model’s multi-hop reasoning capabilities.</abstract>
      <url hash="e38e9806">2021.emnlp-main.561</url>
      <bibkey>dua-etal-2021-generative</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.561</doi>
      <video href="2021.emnlp-main.561.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihop">WikiHop</pwcdataset>
    </paper>
    <paper id="562">
      <title>Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question Answering</title>
      <author><first>Arij</first><last>Riabi</last></author>
      <author><first>Thomas</first><last>Scialom</last></author>
      <author><first>Rachel</first><last>Keraron</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Djamé</first><last>Seddah</last></author>
      <author><first>Jacopo</first><last>Staiano</last></author>
      <pages>7016–7030</pages>
      <abstract>Coupled with the availability of large scale datasets, deep learning architectures have enabled rapid progress on the Question Answering task. However, most of those datasets are in English, and the performances of state-of-the-art multilingual models are significantly lower when evaluated on non-English data. Due to high data collection costs, it is not realistic to obtain annotated data for each language one desires to support. We propose a method to improve the Cross-lingual Question Answering performance without requiring additional annotated data, leveraging Question Generation models to produce synthetic samples in a cross-lingual fashion. We show that the proposed method allows to significantly outperform the baselines trained on English data only. We report a new state-of-the-art on four datasets: MLQA, XQuAD, SQuAD-it and PIAF (fr).</abstract>
      <url hash="f63f7199">2021.emnlp-main.562</url>
      <bibkey>riabi-etal-2021-synthetic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.562</doi>
      <video href="2021.emnlp-main.562.mp4"/>
      <pwccode url="https://github.com/microsoft/unilm" additional="false">microsoft/unilm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="563">
      <title>Have You Seen That Number? Investigating Extrapolation in Question Answering Models</title>
      <author><first>Jeonghwan</first><last>Kim</last></author>
      <author><first>Giwon</first><last>Hong</last></author>
      <author><first>Kyung-min</first><last>Kim</last></author>
      <author><first>Junmo</first><last>Kang</last></author>
      <author><first>Sung-Hyon</first><last>Myaeng</last></author>
      <pages>7031–7037</pages>
      <abstract>Numerical reasoning in machine reading comprehension (MRC) has shown drastic improvements over the past few years. While the previous models for numerical MRC are able to interpolate the learned numerical reasoning capabilities, it is not clear whether they can perform just as well on numbers unseen in the training dataset. Our work rigorously tests state-of-the-art models on DROP, a numerical MRC dataset, to see if they can handle passages that contain out-of-range numbers. One of the key findings is that the models fail to extrapolate to unseen numbers. Presenting numbers as digit-by-digit input to the model, we also propose the <i>E-digit</i> number form that alleviates the lack of extrapolation in models and reveals the need to treat numbers differently from regular words in the text. Our work provides a valuable insight into the numerical MRC models and the way to represent number forms in MRC.</abstract>
      <url hash="64f481c2">2021.emnlp-main.563</url>
      <bibkey>kim-etal-2021-seen</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.563</doi>
      <video href="2021.emnlp-main.563.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
    </paper>
    <paper id="564">
      <title>Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right</title>
      <author><first>Ari</first><last>Holtzman</last></author>
      <author><first>Peter</first><last>West</last></author>
      <author><first>Vered</first><last>Shwartz</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <pages>7038–7051</pages>
      <abstract>Large language models have shown promising results in zero-shot settings. For example, they can perform multiple choice tasks simply by conditioning on a question and selecting the answer with the highest probability. However, ranking by string probability can be problematic due to surface form competition—wherein different surface forms compete for probability mass, even if they represent the same underlying concept in a given context, e.g. “computer” and “PC.” Since probability mass is finite, this lowers the probability of the correct answer, due to competition from other strings that are valid answers (but not one of the multiple choice options). We introduce Domain Conditional Pointwise Mutual Information, an alternative scoring function that directly compensates for surface form competition by simply reweighing each option according to its a priori likelihood within the context of a specific task. It achieves consistent gains in zero-shot performance over both calibrated and uncalibrated scoring functions on all GPT-2 and GPT-3 models on a variety of multiple choice datasets.</abstract>
      <url hash="10f8ae31">2021.emnlp-main.564</url>
      <bibkey>holtzman-etal-2021-surface</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.564</doi>
      <video href="2021.emnlp-main.564.mp4"/>
      <pwccode url="https://github.com/peterwestuw/surface-form-competition" additional="false">peterwestuw/surface-form-competition</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/arc">ARC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hellaswag">HellaSwag</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="565">
      <title>Entity-Based Knowledge Conflicts in Question Answering</title>
      <author><first>Shayne</first><last>Longpre</last></author>
      <author><first>Kartik</first><last>Perisetla</last></author>
      <author><first>Anthony</first><last>Chen</last></author>
      <author><first>Nikhil</first><last>Ramesh</last></author>
      <author><first>Chris</first><last>DuBois</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <pages>7052–7063</pages>
      <abstract>Knowledge-dependent tasks typically use two sources of knowledge: parametric, learned at training time, and contextual, given as a passage at inference time. To understand how models use these sources together, we formalize the problem of knowledge conflicts, where the contextual information contradicts the learned information. Analyzing the behaviour of popular models, we measure their over-reliance on memorized information (the cause of hallucinations), and uncover important factors that exacerbate this behaviour. Lastly, we propose a simple method to mitigate over-reliance on parametric knowledge, which minimizes hallucination, and improves out-of-distribution generalization by 4% - 7%. Our findings demonstrate the importance for practitioners to evaluate model tendency to hallucinate rather than read, and show that our mitigation strategy encourages generalization to evolving information (i.e. time-dependent queries). To encourage these practices, we have released our framework for generating knowledge conflicts.</abstract>
      <url hash="18b7f5d7">2021.emnlp-main.565</url>
      <bibkey>longpre-etal-2021-entity</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.565</doi>
      <video href="2021.emnlp-main.565.mp4"/>
      <pwccode url="https://github.com/apple/ml-knowledge-conflicts" additional="false">apple/ml-knowledge-conflicts</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
    </paper>
    <paper id="566">
      <title>Back-Training excels Self-Training at Unsupervised Domain Adaptation of Question Generation and Passage Retrieval</title>
      <author><first>Devang</first><last>Kulshreshtha</last></author>
      <author><first>Robert</first><last>Belfer</last></author>
      <author><first>Iulian Vlad</first><last>Serban</last></author>
      <author><first>Siva</first><last>Reddy</last></author>
      <pages>7064–7078</pages>
      <abstract>In this work, we introduce back-training, an alternative to self-training for unsupervised domain adaptation (UDA). While self-training generates synthetic training data where natural inputs are aligned with noisy outputs, back-training results in natural outputs aligned with noisy inputs. This significantly reduces the gap between target domain and synthetic data distribution, and reduces model overfitting to source domain. We run UDA experiments on question generation and passage retrieval from the Natural Questions domain to machine learning and biomedical domains. We find that back-training vastly outperforms self-training by a mean improvement of 7.8 BLEU-4 points on generation, and 17.6% top-20 retrieval accuracy across both domains. We further propose consistency filters to remove low-quality synthetic data before training. We also release a new domain-adaptation dataset - MLQuestions containing 35K unaligned questions, 50K unaligned passages, and 3K aligned question-passage pairs.</abstract>
      <url hash="1f7cfb78">2021.emnlp-main.566</url>
      <attachment type="Software" hash="58cd5a39">2021.emnlp-main.566.Software.zip</attachment>
      <bibkey>kulshreshtha-etal-2021-back</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.566</doi>
      <video href="2021.emnlp-main.566.mp4"/>
      <pwccode url="https://github.com/McGill-NLP/MLQuestions" additional="false">McGill-NLP/MLQuestions</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mlquestions">MLQuestions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/pubmedqa">PubMedQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vqg">VQG</pwcdataset>
    </paper>
    <paper id="567">
      <title><fixed-case>DWUG</fixed-case>: A large Resource of Diachronic Word Usage Graphs in Four Languages</title>
      <author><first>Dominik</first><last>Schlechtweg</last></author>
      <author><first>Nina</first><last>Tahmasebi</last></author>
      <author><first>Simon</first><last>Hengchen</last></author>
      <author><first>Haim</first><last>Dubossarsky</last></author>
      <author><first>Barbara</first><last>McGillivray</last></author>
      <pages>7079–7091</pages>
      <abstract>Word meaning is notoriously difficult to capture, both synchronically and diachronically. In this paper, we describe the creation of the largest resource of graded contextualized, diachronic word meaning annotation in four different languages, based on 100,000 human semantic proximity judgments. We describe in detail the multi-round incremental annotation process, the choice for a clustering algorithm to group usages into senses, and possible – diachronic and synchronic – uses for this dataset.</abstract>
      <url hash="20183e05">2021.emnlp-main.567</url>
      <bibkey>schlechtweg-etal-2021-dwug</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.567</doi>
    </paper>
    <paper id="568">
      <title><fixed-case>I</fixed-case> Wish <fixed-case>I</fixed-case> Would Have Loved This One, But <fixed-case>I</fixed-case> Didn’t – A Multilingual Dataset for Counterfactual Detection in Product Review</title>
      <author><first>James</first><last>O’Neill</last></author>
      <author><first>Polina</first><last>Rozenshtein</last></author>
      <author><first>Ryuichi</first><last>Kiryo</last></author>
      <author><first>Motoko</first><last>Kubota</last></author>
      <author><first>Danushka</first><last>Bollegala</last></author>
      <pages>7092–7108</pages>
      <abstract>Counterfactual statements describe events that did not or cannot take place. We consider the problem of counterfactual detection (CFD) in product reviews. For this purpose, we annotate a multilingual CFD dataset from Amazon product reviews covering counterfactual statements written in English, German, and Japanese languages. The dataset is unique as it contains counterfactuals in multiple languages, covers a new application area of e-commerce reviews, and provides high quality professional annotations. We train CFD models using different text representation methods and classifiers. We find that these models are robust against the selectional biases introduced due to cue phrase-based sentence selection. Moreover, our CFD dataset is compatible with prior datasets and can be merged to learn accurate CFD models. Applying machine translation on English counterfactual examples to create multilingual data performs poorly, demonstrating the language-specificity of this problem, which has been ignored so far.</abstract>
      <url hash="99ac75f7">2021.emnlp-main.568</url>
      <bibkey>oneill-etal-2021-wish</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.568</doi>
      <video href="2021.emnlp-main.568.mp4"/>
      <pwccode url="https://github.com/amazon-research/amazon-multilingual-counterfactual-dataset" additional="false">amazon-research/amazon-multilingual-counterfactual-dataset</pwccode>
    </paper>
    <paper id="569">
      <title>Does It Capture <fixed-case>STEL</fixed-case>? A Modular, Similarity-based Linguistic Style Evaluation Framework</title>
      <author><first>Anna</first><last>Wegmann</last></author>
      <author><first>Dong</first><last>Nguyen</last></author>
      <pages>7109–7130</pages>
      <abstract>Style is an integral part of natural language. However, evaluation methods for style measures are rare, often task-specific and usually do not control for content. We propose the modular, fine-grained and content-controlled similarity-based STyle EvaLuation framework (STEL) to test the performance of any model that can compare two sentences on style. We illustrate STEL with two general dimensions of style (formal/informal and simple/complex) as well as two specific characteristics of style (contrac’tion and numb3r substitution). We find that BERT-based methods outperform simple versions of commonly used style measures like 3-grams, punctuation frequency and LIWC-based approaches. We invite the addition of further tasks and task instances to STEL and hope to facilitate the improvement of style-sensitive measures.</abstract>
      <url hash="62a1220b">2021.emnlp-main.569</url>
      <bibkey>wegmann-nguyen-2021-capture</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.569</doi>
      <video href="2021.emnlp-main.569.mp4"/>
      <pwccode url="https://github.com/nlpsoc/stel" additional="false">nlpsoc/stel</pwccode>
    </paper>
    <paper id="570">
      <title>Evaluating the Morphosyntactic Well-formedness of Generated Texts</title>
      <author><first>Adithya</first><last>Pratapa</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <author><first>Shruti</first><last>Rijhwani</last></author>
      <author><first>Aditi</first><last>Chaudhary</last></author>
      <author><first>David R.</first><last>Mortensen</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <pages>7131–7150</pages>
      <abstract>Text generation systems are ubiquitous in natural language processing applications. However, evaluation of these systems remains a challenge, especially in multilingual settings. In this paper, we propose L’AMBRE – a metric to evaluate the morphosyntactic well-formedness of text using its dependency parse and morphosyntactic rules of the language. We present a way to automatically extract various rules governing morphosyntax directly from dependency treebanks. To tackle the noisy outputs from text generation systems, we propose a simple methodology to train robust parsers. We show the effectiveness of our metric on the task of machine translation through a diachronic study of systems translating into morphologically-rich languages.</abstract>
      <url hash="aab8f609">2021.emnlp-main.570</url>
      <bibkey>pratapa-etal-2021-evaluating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.570</doi>
      <video href="2021.emnlp-main.570.mp4"/>
      <pwccode url="https://github.com/adithya7/lambre" additional="false">adithya7/lambre</pwccode>
    </paper>
    <paper id="571">
      <title><fixed-case>AM</fixed-case>2i<fixed-case>C</fixed-case>o: Evaluating Word Meaning in Context across Low-Resource Languages with Adversarial Examples</title>
      <author><first>Qianchu</first><last>Liu</last></author>
      <author><first>Edoardo Maria</first><last>Ponti</last></author>
      <author><first>Diana</first><last>McCarthy</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <pages>7151–7162</pages>
      <abstract>Capturing word meaning in context and distinguishing between correspondences and variations across languages is key to building successful multilingual and cross-lingual text representation models. However, existing multilingual evaluation datasets that evaluate lexical semantics “in-context” have various limitations. In particular, 1) their language coverage is restricted to high-resource languages and skewed in favor of only a few language families and areas, 2) a design that makes the task solvable via superficial cues, which results in artificially inflated (and sometimes super-human) performances of pretrained encoders, and 3) no support for cross-lingual evaluation. In order to address these gaps, we present AM2iCo (Adversarial and Multilingual Meaning in Context), a wide-coverage cross-lingual and multilingual evaluation set; it aims to faithfully assess the ability of state-of-the-art (SotA) representation models to understand the identity of word meaning in cross-lingual contexts for 14 language pairs. We conduct a series of experiments in a wide range of setups and demonstrate the challenging nature of AM2iCo. The results reveal that current SotA pretrained encoders substantially lag behind human performance, and the largest gaps are observed for low-resource languages and languages dissimilar to English.</abstract>
      <url hash="637ad1c2">2021.emnlp-main.571</url>
      <bibkey>liu-etal-2021-am2ico</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.571</doi>
      <video href="2021.emnlp-main.571.mp4"/>
      <pwccode url="https://github.com/cambridgeltl/AM2iCo" additional="false">cambridgeltl/AM2iCo</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/am2ico">AM2iCo</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wic">WiC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xl-wic">XL-WiC</pwcdataset>
    </paper>
    <paper id="572">
      <title><fixed-case>C</fixed-case>ross<fixed-case>F</fixed-case>it: A Few-shot Learning Challenge for Cross-task Generalization in <fixed-case>NLP</fixed-case></title>
      <author><first>Qinyuan</first><last>Ye</last></author>
      <author><first>Bill Yuchen</first><last>Lin</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <pages>7163–7189</pages>
      <abstract>Humans can learn a new language task efficiently with only few examples, by leveraging their knowledge obtained when learning prior tasks. In this paper, we explore whether and how such cross-task generalization ability can be acquired, and further applied to build better few-shot learners across diverse NLP tasks. We introduce CrossFit, a problem setup for studying cross-task generalization ability, which standardizes seen/unseen task partitions, data access during different learning stages, and the evaluation protocols. To instantiate different seen/unseen task partitions in CrossFit and facilitate in-depth analysis, we present the NLP Few-shot Gym, a repository of 160 diverse few-shot NLP tasks created from open-access NLP datasets and converted to a unified text-to-text format. Our analysis reveals that the few-shot learning ability on unseen tasks can be improved via an upstream learning stage using a set of seen tasks. We also observe that the selection of upstream learning tasks can significantly influence few-shot performance on unseen tasks, asking further analysis on task similarity and transferability.</abstract>
      <url hash="0d074ebf">2021.emnlp-main.572</url>
      <attachment type="Software" hash="058222f1">2021.emnlp-main.572.Software.rar</attachment>
      <bibkey>ye-etal-2021-crossfit</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.572</doi>
      <video href="2021.emnlp-main.572.mp4"/>
      <pwccode url="https://github.com/INK-USC/CrossFit" additional="true">INK-USC/CrossFit</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/arc">ARC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quoref">Quoref</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
    </paper>
    <paper id="573">
      <title>On the Influence of Masking Policies in Intermediate Pre-training</title>
      <author><first>Qinyuan</first><last>Ye</last></author>
      <author><first>Belinda Z.</first><last>Li</last></author>
      <author><first>Sinong</first><last>Wang</last></author>
      <author><first>Benjamin</first><last>Bolte</last></author>
      <author><first>Hao</first><last>Ma</last></author>
      <author><first>Wen-tau</first><last>Yih</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <author><first>Madian</first><last>Khabsa</last></author>
      <pages>7190–7202</pages>
      <abstract>Current NLP models are predominantly trained through a two-stage “pre-train then fine-tune” pipeline. Prior work has shown that inserting an intermediate pre-training stage, using heuristic masking policies for masked language modeling (MLM), can significantly improve final performance. However, it is still unclear (1) in what cases such intermediate pre-training is helpful, (2) whether hand-crafted heuristic objectives are optimal for a given task, and (3) whether a masking policy designed for one task is generalizable beyond that task. In this paper, we perform a large-scale empirical study to investigate the effect of various masking policies in intermediate pre-training with nine selected tasks across three categories. Crucially, we introduce methods to automate the discovery of optimal masking policies via direct supervision or meta-learning. We conclude that the success of intermediate pre-training is dependent on appropriate pre-train corpus, selection of output format (i.e., masked spans or full sentence), and clear understanding of the role that MLM plays for the downstream task. In addition, we find our learned masking policies outperform the heuristic of masking named entities on TriviaQA, and policies learned from one task can positively transfer to other tasks in certain cases, inviting future research in this direction.</abstract>
      <url hash="f7e52f8b">2021.emnlp-main.573</url>
      <bibkey>ye-etal-2021-influence</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.573</doi>
      <video href="2021.emnlp-main.573.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/kilt">KILT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quartz">QuaRTz</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ropes">ROPES</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestions">WebQuestions</pwcdataset>
    </paper>
    <paper id="574">
      <title><fixed-case>V</fixed-case>al<fixed-case>N</fixed-case>orm Quantifies Semantics to Reveal Consistent Valence Biases Across Languages and Over Centuries</title>
      <author><first>Autumn</first><last>Toney</last></author>
      <author><first>Aylin</first><last>Caliskan</last></author>
      <pages>7203–7218</pages>
      <abstract>Word embeddings learn implicit biases from linguistic regularities captured by word co-occurrence statistics. By extending methods that quantify human-like biases in word embeddings, we introduce ValNorm, a novel intrinsic evaluation task and method to quantify the valence dimension of affect in human-rated word sets from social psychology. We apply ValNorm on static word embeddings from seven languages (Chinese, English, German, Polish, Portuguese, Spanish, and Turkish) and from historical English text spanning 200 years. ValNorm achieves consistently high accuracy in quantifying the valence of non-discriminatory, non-social group word sets. Specifically, ValNorm achieves a Pearson correlation of r=0.88 for human judgment scores of valence for 399 words collected to establish pleasantness norms in English. In contrast, we measure gender stereotypes using the same set of word embeddings and find that social biases vary across languages. Our results indicate that valence associations of non-discriminatory, non-social group words represent widely-shared associations, in seven languages and over 200 years.</abstract>
      <url hash="e9e7c2b5">2021.emnlp-main.574</url>
      <bibkey>toney-caliskan-2021-valnorm</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.574</doi>
      <video href="2021.emnlp-main.574.mp4"/>
      <pwccode url="https://github.com/autumntoney/ValNorm" additional="true">autumntoney/ValNorm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="575">
      <title>Perturbation <fixed-case>C</fixed-case>heck<fixed-case>L</fixed-case>ists for Evaluating <fixed-case>NLG</fixed-case> Evaluation Metrics</title>
      <author><first>Ananya B.</first><last>Sai</last></author>
      <author><first>Tanay</first><last>Dixit</last></author>
      <author><first>Dev Yashpal</first><last>Sheth</last></author>
      <author><first>Sreyas</first><last>Mohan</last></author>
      <author><first>Mitesh M.</first><last>Khapra</last></author>
      <pages>7219–7234</pages>
      <abstract>Natural Language Generation (NLG) evaluation is a multifaceted task requiring assessment of multiple desirable criteria, e.g., fluency, coherency, coverage, relevance, adequacy, overall quality, etc. Across existing datasets for 6 NLG tasks, we observe that the human evaluation scores on these multiple criteria are often not correlated. For example, there is a very low correlation between human scores on fluency and data coverage for the task of structured data to text generation. This suggests that the current recipe of proposing new automatic evaluation metrics for NLG by showing that they correlate well with scores assigned by humans for a single criteria (overall quality) alone is inadequate. Indeed, our extensive study involving 25 automatic evaluation metrics across 6 different tasks and 18 different evaluation criteria shows that there is no single metric which correlates well with human scores on all desirable criteria, for most NLG tasks. Given this situation, we propose CheckLists for better design and evaluation of automatic metrics. We design templates which target a specific criteria (e.g., coverage) and perturb the output such that the quality gets affected only along this specific criteria (e.g., the coverage drops). We show that existing evaluation metrics are not robust against even such simple perturbations and disagree with scores assigned by humans to the perturbed output. The proposed templates thus allow for a fine-grained assessment of automatic evaluation metrics exposing their limitations and will facilitate better design, analysis and evaluation of such metrics. Our templates and code are available at https://iitmnlp.github.io/EvalEval/</abstract>
      <url hash="7bf7f3b6">2021.emnlp-main.575</url>
      <bibkey>sai-etal-2021-perturbation</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.575</doi>
      <video href="2021.emnlp-main.575.mp4"/>
      <pwccode url="https://github.com/iitmnlp/evaleval" additional="false">iitmnlp/evaleval</pwccode>
    </paper>
    <paper id="576">
      <title>Robust Open-Vocabulary Translation from Visual Text Representations</title>
      <author><first>Elizabeth</first><last>Salesky</last></author>
      <author><first>David</first><last>Etter</last></author>
      <author><first>Matt</first><last>Post</last></author>
      <pages>7235–7252</pages>
      <abstract>Machine translation models have discrete vocabularies and commonly use subword segmentation techniques to achieve an ‘open vocabulary.’ This approach relies on consistent and correct underlying unicode sequences, and makes models susceptible to degradation from common types of noise and variation. Motivated by the robustness of human language processing, we propose the use of visual text representations, which dispense with a finite set of text embeddings in favor of continuous vocabularies created by processing visually rendered text with sliding windows. We show that models using visual text representations approach or match performance of traditional text models on small and larger datasets. More importantly, models with visual embeddings demonstrate significant robustness to varied types of noise, achieving e.g., 25.9 BLEU on a character permuted German–English task where subword models degrade to 1.9.</abstract>
      <url hash="2c2bd918">2021.emnlp-main.576</url>
      <bibkey>salesky-etal-2021-robust</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.576</doi>
      <video href="2021.emnlp-main.576.mp4"/>
      <pwccode url="https://github.com/esalesky/visrep" additional="false">esalesky/visrep</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mtnt">MTNT</pwcdataset>
    </paper>
    <paper id="577">
      <title>Don’t Go Far Off: An Empirical Study on Neural Poetry Translation</title>
      <author><first>Tuhin</first><last>Chakrabarty</last></author>
      <author><first>Arkadiy</first><last>Saakyan</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <pages>7253–7265</pages>
      <abstract>Despite constant improvements in machine translation quality, automatic poetry translation remains a challenging problem due to the lack of open-sourced parallel poetic corpora, and to the intrinsic complexities involved in preserving the semantics, style and figurative nature of poetry. We present an empirical investigation for poetry translation along several dimensions: 1) size and style of training data (poetic vs. non-poetic), including a zero-shot setup; 2) bilingual vs. multilingual learning; and 3) language-family-specific models vs. mixed-language-family models. To accomplish this, we contribute a parallel dataset of poetry translations for several language pairs. Our results show that multilingual fine-tuning on poetic text significantly outperforms multilingual fine-tuning on non-poetic text that is 35X larger in size, both in terms of automatic metrics (BLEU, BERTScore, COMET) and human evaluation metrics such as faithfulness (meaning and poetic style). Moreover, multilingual fine-tuning on poetic data outperforms bilingual fine-tuning on poetic data.</abstract>
      <url hash="89f96c72">2021.emnlp-main.577</url>
      <bibkey>chakrabarty-etal-2021-dont</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.577</doi>
      <video href="2021.emnlp-main.577.mp4"/>
    </paper>
    <paper id="578">
      <title>Improving Multilingual Translation by Representation and Gradient Regularization</title>
      <author><first>Yilin</first><last>Yang</last></author>
      <author><first>Akiko</first><last>Eriguchi</last></author>
      <author><first>Alexandre</first><last>Muzio</last></author>
      <author><first>Prasad</first><last>Tadepalli</last></author>
      <author><first>Stefan</first><last>Lee</last></author>
      <author><first>Hany</first><last>Hassan</last></author>
      <pages>7266–7279</pages>
      <abstract>Multilingual Neural Machine Translation (NMT) enables one model to serve all translation directions, including ones that are unseen during training, i.e. zero-shot translation. Despite being theoretically attractive, current models often produce low quality translations – commonly failing to even produce outputs in the right target language. In this work, we observe that off-target translation is dominant even in strong multilingual systems, trained on massive multilingual corpora. To address this issue, we propose a joint approach to regularize NMT models at both representation-level and gradient-level. At the representation level, we leverage an auxiliary target language prediction task to regularize decoder outputs to retain information about the target language. At the gradient level, we leverage a small amount of direct data (in thousands of sentence pairs) to regularize model gradients. Our results demonstrate that our approach is highly effective in both reducing off-target translation occurrences and improving zero-shot translation performance by +5.59 and +10.38 BLEU on WMT and OPUS datasets respectively. Moreover, experiments show that our method also works well when the small amount of direct data is not available.</abstract>
      <url hash="1d4afce6">2021.emnlp-main.578</url>
      <bibkey>yang-etal-2021-improving-multilingual</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.578</doi>
      <video href="2021.emnlp-main.578.mp4"/>
      <pwccode url="https://github.com/yilinyang7/fairseq_multi_fix" additional="false">yilinyang7/fairseq_multi_fix</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/opus-100">OPUS-100</pwcdataset>
    </paper>
    <paper id="579">
      <title>Learning Kernel-Smoothed Machine Translation with Retrieved Examples</title>
      <author><first>Qingnan</first><last>Jiang</last></author>
      <author><first>Mingxuan</first><last>Wang</last></author>
      <author><first>Jun</first><last>Cao</last></author>
      <author><first>Shanbo</first><last>Cheng</last></author>
      <author><first>Shujian</first><last>Huang</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <pages>7280–7290</pages>
      <abstract>How to effectively adapt neural machine translation (NMT) models according to emerging cases without retraining? Despite the great success of neural machine translation, updating the deployed models online remains a challenge. Existing non-parametric approaches that retrieve similar examples from a database to guide the translation process are promising but are prone to overfit the retrieved examples. However, non-parametric methods are prone to overfit the retrieved examples. In this work, we propose to learn Kernel-Smoothed Translation with Example Retrieval (KSTER), an effective approach to adapt neural machine translation models online. Experiments on domain adaptation and multi-domain machine translation datasets show that even without expensive retraining, KSTER is able to achieve improvement of 1.1 to 1.5 BLEU scores over the best existing online adaptation methods. The code and trained models are released at https://github.com/jiangqn/KSTER.</abstract>
      <url hash="09faadd8">2021.emnlp-main.579</url>
      <bibkey>jiang-etal-2021-learning</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.579</doi>
      <video href="2021.emnlp-main.579.mp4"/>
      <pwccode url="https://github.com/jiangqn/kster" additional="true">jiangqn/kster</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="580">
      <title>Uncertainty-Aware Balancing for Multilingual and Multi-Domain Neural Machine Translation Training</title>
      <author><first>Minghao</first><last>Wu</last></author>
      <author><first>Yitong</first><last>Li</last></author>
      <author><first>Meng</first><last>Zhang</last></author>
      <author><first>Liangyou</first><last>Li</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>7291–7305</pages>
      <abstract>Learning multilingual and multi-domain translation model is challenging as the heterogeneous and imbalanced data make the model converge inconsistently over different corpora in real world. One common practice is to adjust the share of each corpus in the training, so that the learning process is balanced and low-resource cases can benefit from the high resource ones. However, automatic balancing methods usually depend on the intra- and inter-dataset characteristics, which is usually agnostic or requires human priors. In this work, we propose an approach, MultiUAT, that dynamically adjusts the training data usage based on the model’s uncertainty on a small set of trusted clean data for multi-corpus machine translation. We experiments with two classes of uncertainty measures on multilingual (16 languages with 4 settings) and multi-domain settings (4 for in-domain and 2 for out-of-domain on English-German translation) and demonstrate our approach MultiUAT substantially outperforms its baselines, including both static and dynamic strategies. We analyze the cross-domain transfer and show the deficiency of static and similarity based methods.</abstract>
      <url hash="38f0fc7a">2021.emnlp-main.580</url>
      <bibkey>wu-etal-2021-uncertainty</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.580</doi>
      <video href="2021.emnlp-main.580.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
    </paper>
    <paper id="581">
      <title>Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy</title>
      <author><first>Shaolei</first><last>Zhang</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <pages>7306–7317</pages>
      <abstract>Simultaneous machine translation (SiMT) generates translation before reading the entire source sentence and hence it has to trade off between translation quality and latency. To fulfill the requirements of different translation quality and latency in practical applications, the previous methods usually need to train multiple SiMT models for different latency levels, resulting in large computational costs. In this paper, we propose a universal SiMT model with Mixture-of-Experts Wait-k Policy to achieve the best translation quality under arbitrary latency with only one trained model. Specifically, our method employs multi-head attention to accomplish the mixture of experts where each head is treated as a wait-k expert with its own waiting words number, and given a test latency and source inputs, the weights of the experts are accordingly adjusted to produce the best translation. Experiments on three datasets show that our method outperforms all the strong baselines under different latency, including the state-of-the-art adaptive policy.</abstract>
      <url hash="96ade5c8">2021.emnlp-main.581</url>
      <bibkey>zhang-feng-2021-universal</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.581</doi>
      <video href="2021.emnlp-main.581.mp4"/>
      <pwccode url="https://github.com/ictnlp/moe-waitk" additional="false">ictnlp/moe-waitk</pwccode>
    </paper>
    <paper id="582">
      <title>How much coffee was consumed during <fixed-case>EMNLP</fixed-case> 2019? Fermi Problems: A New Reasoning Challenge for <fixed-case>AI</fixed-case></title>
      <author><first>Ashwin</first><last>Kalyan</last></author>
      <author><first>Abhinav</first><last>Kumar</last></author>
      <author><first>Arjun</first><last>Chandrasekaran</last></author>
      <author><first>Ashish</first><last>Sabharwal</last></author>
      <author><first>Peter</first><last>Clark</last></author>
      <pages>7318–7328</pages>
      <abstract>Many real-world problems require the combined application of multiple reasoning abilities—employing suitable abstractions, commonsense knowledge, and creative synthesis of problem-solving strategies. To help advance AI systems towards such capabilities, we propose a new reasoning challenge, namely Fermi Problems (FPs), which are questions whose answers can only be approximately estimated because their precise computation is either impractical or impossible. For example, “How much would the sea level rise if all ice in the world melted?” FPs are commonly used in quizzes and interviews to bring out and evaluate the creative reasoning abilities of humans. To do the same for AI systems, we present two datasets: 1) A collection of 1k real-world FPs sourced from quizzes and olympiads; and 2) a bank of 10k synthetic FPs of intermediate complexity to serve as a sandbox for the harder real-world challenge. In addition to question-answer pairs, the datasets contain detailed solutions in the form of an executable program and supporting facts, helping in supervision and evaluation of intermediate steps. We demonstrate that even extensively fine-tuned large-scale language models perform poorly on these datasets, on average making estimates that are off by two orders of magnitude. Our contribution is thus the crystallization of several unsolved AI problems into a single, new challenge that we hope will spur further advances in building systems that can reason.</abstract>
      <url hash="37be56eb">2021.emnlp-main.582</url>
      <bibkey>kalyan-etal-2021-much</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.582</doi>
      <video href="2021.emnlp-main.582.mp4"/>
    </paper>
    <paper id="583">
      <title>Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering</title>
      <author><first>Siddhant</first><last>Garg</last></author>
      <author><first>Alessandro</first><last>Moschitti</last></author>
      <pages>7329–7346</pages>
      <abstract>In this paper we propose a novel approach towards improving the efficiency of Question Answering (QA) systems by filtering out questions that will not be answered by them. This is based on an interesting new finding: the answer confidence scores of state-of-the-art QA systems can be approximated well by models solely using the input question text. This enables preemptive filtering of questions that are not answered by the system due to their answer confidence scores being lower than the system threshold. Specifically, we learn Transformer-based question models by distilling Transformer-based answering models. Our experiments on three popular QA datasets and one industrial QA benchmark demonstrate the ability of our question models to approximate the Precision/Recall curves of the target QA system well. These question models, when used as filters, can effectively trade off lower computation cost of QA systems for lower Recall, e.g., reducing computation by ~60%, while only losing ~3-4% of Recall.</abstract>
      <url hash="e28c0f93">2021.emnlp-main.583</url>
      <bibkey>garg-moschitti-2021-will</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.583</doi>
      <video href="2021.emnlp-main.583.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/asnq">ASNQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paralex">Paralex</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiqa">WikiQA</pwcdataset>
    </paper>
    <paper id="584">
      <title>Learning with Instance Bundles for Reading Comprehension</title>
      <author><first>Dheeru</first><last>Dua</last></author>
      <author><first>Pradeep</first><last>Dasigi</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <pages>7347–7357</pages>
      <abstract>When training most modern reading comprehension models, all the questions associated with a context are treated as being independent from each other. However, closely related questions and their corresponding answers are not independent, and leveraging these relationships could provide a strong supervision signal to a model. Drawing on ideas from contrastive estimation, we introduce several new supervision losses that compare question-answer scores across multiple related instances. Specifically, we normalize these scores across various neighborhoods of closely contrasting questions and/or answers, adding a cross entropy loss term in addition to traditional maximum likelihood estimation. Our techniques require bundles of related question-answer pairs, which we either mine from within existing data or create using automated heuristics. We empirically demonstrate the effectiveness of training with instance bundles on two datasets—HotpotQA and ROPES—showing up to 9% absolute gains in accuracy.</abstract>
      <url hash="2db9cc2c">2021.emnlp-main.584</url>
      <bibkey>dua-etal-2021-learning</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.584</doi>
      <video href="2021.emnlp-main.584.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quoref">Quoref</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ropes">ROPES</pwcdataset>
    </paper>
    <paper id="585">
      <title>Explaining Answers with Entailment Trees</title>
      <author><first>Bhavana</first><last>Dalvi</last></author>
      <author><first>Peter</first><last>Jansen</last></author>
      <author><first>Oyvind</first><last>Tafjord</last></author>
      <author><first>Zhengnan</first><last>Xie</last></author>
      <author><first>Hannah</first><last>Smith</last></author>
      <author><first>Leighanna</first><last>Pipatanangkura</last></author>
      <author><first>Peter</first><last>Clark</last></author>
      <pages>7358–7370</pages>
      <abstract>Our goal, in the context of open-domain textual question-answering (QA), is to explain answers by showing the line of reasoning from what is known to the answer, rather than simply showing a fragment of textual evidence (a “rationale”). If this could be done, new opportunities for understanding and debugging the system’s reasoning become possible. Our approach is to generate explanations in the form of entailment trees, namely a tree of multipremise entailment steps from facts that are known, through intermediate conclusions, to the hypothesis of interest (namely the question + answer). To train a model with this skill, we created ENTAILMENTBANK, the first dataset to contain multistep entailment trees. Given a hypothesis (question + answer), we define three increasingly difficult explanation tasks: generate a valid entailment tree given (a) all relevant sentences (b) all relevant and some irrelevant sentences, or (c) a corpus. We show that a strong language model can partially solve these tasks, in particular when the relevant sentences are included in the input (e.g., 35% of trees for (a) are perfect), and with indications of generalization to other domains. This work is significant as it provides a new type of dataset (multistep entailments) and baselines, offering a new avenue for the community to generate richer, more systematic explanations.</abstract>
      <url hash="52b7078a">2021.emnlp-main.585</url>
      <bibkey>dalvi-etal-2021-explaining</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.585</doi>
      <video href="2021.emnlp-main.585.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/entailmentbank">EntailmentBank</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/arc">ARC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/proofwriter">ProofWriter</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/worldtree">Worldtree</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/eqasc">eQASC</pwcdataset>
    </paper>
    <paper id="586">
      <title><fixed-case>S</fixed-case>ituated<fixed-case>QA</fixed-case>: Incorporating Extra-Linguistic Contexts into <fixed-case>QA</fixed-case></title>
      <author><first>Michael</first><last>Zhang</last></author>
      <author><first>Eunsol</first><last>Choi</last></author>
      <pages>7371–7387</pages>
      <abstract>Answers to the same question may change depending on the extra-linguistic contexts (when and where the question was asked). To study this challenge, we introduce SituatedQA, an open-retrieval QA dataset where systems must produce the correct answer to a question given the temporal or geographical context. To construct SituatedQA, we first identify such questions in existing QA datasets. We find that a significant proportion of information seeking questions have context-dependent answers (e.g. roughly 16.5% of NQ-Open). For such context-dependent questions, we then crowdsource alternative contexts and their corresponding answers. Our study shows that existing models struggle with producing answers that are frequently updated or from uncommon locations. We further quantify how existing models, which are trained on data collected in the past, fail to generalize to answering questions asked in the present, even when provided with an updated evidence corpus (a roughly 15 point drop in accuracy). Our analysis suggests that open-retrieval QA benchmarks should incorporate extra-linguistic context to stay relevant globally and in the future. Our data, code, and datasheet are available at https://situatedqa.github.io/.</abstract>
      <url hash="67f4a06c">2021.emnlp-main.586</url>
      <bibkey>zhang-choi-2021-situatedqa</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.586</doi>
      <video href="2021.emnlp-main.586.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/situatedqa">SituatedQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="587">
      <title><fixed-case>C</fixed-case>onv<fixed-case>A</fixed-case>buse: Data, Analysis, and Benchmarks for Nuanced Abuse Detection in Conversational <fixed-case>AI</fixed-case></title>
      <author><first>Amanda</first><last>Cercas Curry</last></author>
      <author><first>Gavin</first><last>Abercrombie</last></author>
      <author><first>Verena</first><last>Rieser</last></author>
      <pages>7388–7403</pages>
      <abstract>We present the first English corpus study on abusive language towards three conversational AI systems gathered ‘in the wild’: an open-domain social bot, a rule-based chatbot, and a task-based system. To account for the complexity of the task, we take a more ‘nuanced’ approach where our ConvAI dataset reflects fine-grained notions of abuse, as well as views from multiple expert annotators. We find that the distribution of abuse is vastly different compared to other commonly used datasets, with more sexually tinted aggression towards the virtual persona of these systems. Finally, we report results from bench-marking existing models against this data. Unsurprisingly, we find that there is substantial room for improvement with F1 scores below 90%.</abstract>
      <url hash="b155b6d9">2021.emnlp-main.587</url>
      <bibkey>cercas-curry-etal-2021-convabuse</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.587</doi>
      <video href="2021.emnlp-main.587.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/olid">OLID</pwcdataset>
    </paper>
    <paper id="588">
      <title>Conversational Multi-Hop Reasoning with Neural Commonsense Knowledge and Symbolic Logic Rules</title>
      <author><first>Forough</first><last>Arabshahi</last></author>
      <author><first>Jennifer</first><last>Lee</last></author>
      <author><first>Antoine</first><last>Bosselut</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <author><first>Tom</first><last>Mitchell</last></author>
      <pages>7404–7418</pages>
      <abstract>One of the challenges faced by conversational agents is their inability to identify unstated presumptions of their users’ commands, a task trivial for humans due to their common sense. In this paper, we propose a zero-shot commonsense reasoning system for conversational agents in an attempt to achieve this. Our reasoner uncovers unstated presumptions from user commands satisfying a general template of if-(state), then-(action), because-(goal). Our reasoner uses a state-of-the-art transformer-based generative commonsense knowledge base (KB) as its source of background knowledge for reasoning. We propose a novel and iterative knowledge query mechanism to extract multi-hop reasoning chains from the neural KB which uses symbolic logic rules to significantly reduce the search space. Similar to any KBs gathered to date, our commonsense KB is prone to missing knowledge. Therefore, we propose to conversationally elicit the missing knowledge from human users with our novel dynamic question generation strategy, which generates and presents contextualized queries to human users. We evaluate the model with a user study with human users that achieves a 35% higher success rate compared to SOTA.</abstract>
      <url hash="3fab6882">2021.emnlp-main.588</url>
      <bibkey>arabshahi-etal-2021-conversational</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.588</doi>
      <video href="2021.emnlp-main.588.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="589">
      <title>Towards Automatic Evaluation of Dialog Systems: A Model-Free Off-Policy Evaluation Approach</title>
      <author><first>Haoming</first><last>Jiang</last></author>
      <author><first>Bo</first><last>Dai</last></author>
      <author><first>Mengjiao</first><last>Yang</last></author>
      <author><first>Tuo</first><last>Zhao</last></author>
      <author><first>Wei</first><last>Wei</last></author>
      <pages>7419–7451</pages>
      <abstract>Reliable automatic evaluation of dialogue systems under an interactive environment has long been overdue. An ideal environment for evaluating dialog systems, also known as the Turing test, needs to involve human interaction, which is usually not affordable for large-scale experiments. Though researchers have attempted to use metrics for language generation tasks (e.g., perplexity, BLEU) or some model-based reinforcement learning methods (e.g., self-play evaluation) for automatic evaluation, these methods only show very weak correlation with the actual human evaluation in practice. To bridge such a gap, we propose a new framework named ENIGMA for estimating human evaluation scores based on recent advances of off-policy evaluation in reinforcement learning. ENIGMA only requires a handful of pre-collected experience data, and therefore does not involve human interaction with the target policy during the evaluation, making automatic evaluations feasible. More importantly, ENIGMA is model-free and agnostic to the behavior policies for collecting the experience data, which significantly alleviates the technical difficulties of modeling complex dialogue environments and human behaviors. Our experiments show that ENIGMA significantly outperforms existing methods in terms of correlation with human evaluation scores.</abstract>
      <url hash="038c959c">2021.emnlp-main.589</url>
      <bibkey>jiang-etal-2021-towards</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.589</doi>
      <video href="2021.emnlp-main.589.mp4"/>
      <pwccode url="https://github.com/google-research/google-research" additional="false">google-research/google-research</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/convai2">ConvAI2</pwcdataset>
    </paper>
    <paper id="590">
      <title>Continual Learning in Task-Oriented Dialogue Systems</title>
      <author><first>Andrea</first><last>Madotto</last></author>
      <author><first>Zhaojiang</first><last>Lin</last></author>
      <author><first>Zhenpeng</first><last>Zhou</last></author>
      <author><first>Seungwhan</first><last>Moon</last></author>
      <author><first>Paul</first><last>Crook</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <author><first>Eunjoon</first><last>Cho</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <author><first>Zhiguang</first><last>Wang</last></author>
      <pages>7452–7467</pages>
      <abstract>Continual learning in task-oriented dialogue systems allows the system to add new domains and functionalities overtime after deployment, without incurring the high cost of retraining the whole system each time. In this paper, we propose a first-ever continual learning benchmark for task-oriented dialogue systems with 37 domains to be learned continuously in both modularized and end-to-end learning settings. In addition, we implement and compare multiple existing continual learning baselines, and we propose a simple yet effective architectural method based on residual adapters. We also suggest that the upper bound performance of continual learning should be equivalent to multitask learning when data from all domain is available at once. Our experiments demonstrate that the proposed architectural method and a simple replay-based strategy perform better, by a large margin, compared to other continuous learning techniques, and only slightly worse than the multitask learning upper bound while being 20X faster in learning new domains. We also report several trade-offs in terms of parameter usage, memory size and training time, which are important in the design of a task-oriented dialogue system. The proposed benchmark is released to promote more research in this direction.</abstract>
      <url hash="09fc2da8">2021.emnlp-main.590</url>
      <bibkey>madotto-etal-2021-continual</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.590</doi>
      <video href="2021.emnlp-main.590.mp4"/>
      <pwccode url="https://github.com/andreamad8/ToDCL" additional="false">andreamad8/ToDCL</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
    </paper>
    <paper id="591">
      <title>Multilingual and Cross-Lingual Intent Detection from Spoken Data</title>
      <author><first>Daniela</first><last>Gerz</last></author>
      <author><first>Pei-Hao</first><last>Su</last></author>
      <author><first>Razvan</first><last>Kusztos</last></author>
      <author><first>Avishek</first><last>Mondal</last></author>
      <author><first>Michał</first><last>Lis</last></author>
      <author><first>Eshan</first><last>Singhal</last></author>
      <author><first>Nikola</first><last>Mrkšić</last></author>
      <author><first>Tsung-Hsien</first><last>Wen</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <pages>7468–7475</pages>
      <abstract>We present a systematic study on multilingual and cross-lingual intent detection (ID) from spoken data. The study leverages a new resource put forth in this work, termed MInDS-14, a first training and evaluation resource for the ID task with spoken data. It covers 14 intents extracted from a commercial system in the e-banking domain, associated with spoken examples in 14 diverse language varieties. Our key results indicate that combining machine translation models with state-of-the-art multilingual sentence encoders (e.g., LaBSE) yield strong intent detectors in the majority of target languages covered in MInDS-14, and offer comparative analyses across different axes: e.g., translation direction, impact of speech recognition, data augmentation from a related domain. We see this work as an important step towards more inclusive development and evaluation of multilingual ID from spoken data, hopefully in a much wider spectrum of languages compared to prior work.</abstract>
      <url hash="055347ee">2021.emnlp-main.591</url>
      <bibkey>gerz-etal-2021-multilingual</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.591</doi>
      <video href="2021.emnlp-main.591.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
    </paper>
    <paper id="592">
      <title>Investigating Robustness of Dialog Models to Popular Figurative Language Constructs</title>
      <author><first>Harsh</first><last>Jhamtani</last></author>
      <author><first>Varun</first><last>Gangal</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last></author>
      <pages>7476–7485</pages>
      <abstract>Humans often employ figurative language use in communication, including during interactions with dialog systems. Thus, it is important for real-world dialog systems to be able to handle popular figurative language constructs like metaphor and simile. In this work, we analyze the performance of existing dialog models in situations where the input dialog context exhibits use of figurative language. We observe large gaps in handling of figurative language when evaluating the models on two open domain dialog datasets. When faced with dialog contexts consisting of figurative language, some models show very large drops in performance compared to contexts without figurative language. We encourage future research in dialog modeling to separately analyze and report results on figurative language in order to better test model capabilities relevant to real-world use. Finally, we propose lightweight solutions to help existing models become more robust to figurative language by simply using an external resource to translate figurative language to literal (non-figurative) forms while preserving the meaning to the best extent possible.</abstract>
      <url hash="701271a2">2021.emnlp-main.592</url>
      <bibkey>jhamtani-etal-2021-investigating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.592</doi>
      <video href="2021.emnlp-main.592.mp4"/>
      <pwccode url="https://github.com/vgtomahawk/dialog-fig-speech-robust" additional="false">vgtomahawk/dialog-fig-speech-robust</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dailydialog">DailyDialog</pwcdataset>
    </paper>
    <paper id="593">
      <title>Effective Sequence-to-Sequence Dialogue State Tracking</title>
      <author><first>Jeffrey</first><last>Zhao</last></author>
      <author><first>Mahdis</first><last>Mahdieh</last></author>
      <author><first>Ye</first><last>Zhang</last></author>
      <author><first>Yuan</first><last>Cao</last></author>
      <author><first>Yonghui</first><last>Wu</last></author>
      <pages>7486–7493</pages>
      <abstract>Sequence-to-sequence models have been applied to a wide variety of NLP tasks, but how to properly use them for dialogue state tracking has not been systematically investigated. In this paper, we study this problem from the perspectives of pre-training objectives as well as the formats of context representations. We demonstrate that the choice of pre-training objective makes a significant difference to the state tracking quality. In particular, we find that masked span prediction is more effective than auto-regressive language modeling. We also explore using Pegasus, a span prediction-based pre-training objective for text summarization, for the state tracking model. We found that pre-training for the seemingly distant summarization task works surprisingly well for dialogue state tracking. In addition, we found that while recurrent state context representation works also reasonably well, the model may have a hard time recovering from earlier mistakes. We conducted experiments on the MultiWOZ 2.1-2.4, WOZ 2.0, and DSTC2 datasets with consistent observations.</abstract>
      <url hash="a821d5ba">2021.emnlp-main.593</url>
      <bibkey>zhao-etal-2021-effective-sequence</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.593</doi>
      <video href="2021.emnlp-main.593.mp4"/>
      <pwccode url="https://github.com/smartyfh/MultiWOZ2.4" additional="false">smartyfh/MultiWOZ2.4</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dialogue-state-tracking-challenge">Dialogue State Tracking Challenge</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-oz">Wizard-of-Oz</pwcdataset>
    </paper>
    <paper id="594">
      <title><fixed-case>MS</fixed-case>ˆ2: Multi-Document Summarization of Medical Studies</title>
      <author><first>Jay</first><last>DeYoung</last></author>
      <author><first>Iz</first><last>Beltagy</last></author>
      <author><first>Madeleine</first><last>van Zuylen</last></author>
      <author><first>Bailey</first><last>Kuehl</last></author>
      <author><first>Lucy Lu</first><last>Wang</last></author>
      <pages>7494–7513</pages>
      <abstract>To assess the effectiveness of any medical intervention, researchers must conduct a time-intensive and manual literature review. NLP systems can help to automate or assist in parts of this expensive process. In support of this goal, we release MSˆ2 (Multi-Document Summarization of Medical Studies), a dataset of over 470k documents and 20K summaries derived from the scientific literature. This dataset facilitates the development of systems that can assess and aggregate contradictory evidence across multiple studies, and is the first large-scale, publicly available multi-document summarization dataset in the biomedical domain. We experiment with a summarization system based on BART, with promising early results, though significant work remains to achieve higher summarization quality. We formulate our summarization inputs and targets in both free text and structured forms and modify a recently proposed metric to assess the quality of our system’s generated summaries. Data and models are available at https://github.com/allenai/ms2.</abstract>
      <url hash="c7d79f15">2021.emnlp-main.594</url>
      <attachment type="Software" hash="cfc4dc5a">2021.emnlp-main.594.Software.zip</attachment>
      <bibkey>deyoung-etal-2021-ms</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.594</doi>
      <pwccode url="https://github.com/allenai/ms2" additional="false">allenai/ms2</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ebm-nlp">EBM-NLP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/evidence-inference">Evidence Inference</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multi-news">Multi-News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/semantic-scholar">Semantic Scholar</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisum">WikiSum</pwcdataset>
    </paper>
    <paper id="595">
      <title><fixed-case>CLIPS</fixed-case>core: A Reference-free Evaluation Metric for Image Captioning</title>
      <author><first>Jack</first><last>Hessel</last></author>
      <author><first>Ari</first><last>Holtzman</last></author>
      <author><first>Maxwell</first><last>Forbes</last></author>
      <author><first>Ronan</first><last>Le Bras</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>7514–7528</pages>
      <abstract>Image captioning has conventionally relied on reference-based automatic evaluations, where machine captions are compared against captions written by humans. This is in contrast to the reference-free manner in which humans assess caption quality. In this paper, we report the surprising empirical finding that CLIP (Radford et al., 2021), a cross-modal model pretrained on 400M image+caption pairs from the web, can be used for robust automatic evaluation of image captioning without the need for references. Experiments spanning several corpora demonstrate that our new reference-free metric, CLIPScore, achieves the highest correlation with human judgements, outperforming existing reference-based metrics like CIDEr and SPICE. Information gain experiments demonstrate that CLIPScore, with its tight focus on image-text compatibility, is complementary to existing reference-based metrics that emphasize text-text similarities. Thus, we also present a reference-augmented version, RefCLIPScore, which achieves even higher correlation. Beyond literal description tasks, several case studies reveal domains where CLIPScore performs well (clip-art images, alt-text rating), but also where it is relatively weaker in comparison to reference-based metrics, e.g., news captions that require richer contextual knowledge.</abstract>
      <url hash="c820e9c2">2021.emnlp-main.595</url>
      <bibkey>hessel-etal-2021-clipscore</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.595</doi>
      <revision id="1" href="2021.emnlp-main.595v1" hash="6d54687c"/>
      <revision id="2" href="2021.emnlp-main.595v2" hash="c820e9c2" date="2022-03-28">Fixed results in Table 7 for reproducability purposes.</revision>
      <video href="2021.emnlp-main.595.mp4"/>
      <pwccode url="https://github.com/jmhessel/clipscore" additional="true">jmhessel/clipscore</pwccode>
    </paper>
    <paper id="596">
      <title>On the Challenges of Evaluating Compositional Explanations in Multi-Hop Inference: Relevance, Completeness, and Expert Ratings</title>
      <author><first>Peter</first><last>Jansen</last></author>
      <author><first>Kelly J.</first><last>Smith</last></author>
      <author><first>Dan</first><last>Moreno</last></author>
      <author><first>Huitzilin</first><last>Ortiz</last></author>
      <pages>7529–7542</pages>
      <abstract>Building compositional explanations requires models to combine two or more facts that, together, describe why the answer to a question is correct. Typically, these “multi-hop” explanations are evaluated relative to one (or a small number of) gold explanations. In this work, we show these evaluations substantially underestimate model performance, both in terms of the relevance of included facts, as well as the completeness of model-generated explanations, because models regularly discover and produce valid explanations that are different than gold explanations. To address this, we construct a large corpus of 126k domain-expert (science teacher) relevance ratings that augment a corpus of explanations to standardized science exam questions, discovering 80k additional relevant facts not rated as gold. We build three strong models based on different methodologies (generation, ranking, and schemas), and empirically show that while expert-augmented ratings provide better estimates of explanation quality, both original (gold) and expert-augmented automatic evaluations still substantially underestimate performance by up to 36% when compared with full manual expert judgements, with different models being disproportionately affected. This poses a significant methodological challenge to accurately evaluating explanations produced by compositional reasoning models.</abstract>
      <url hash="8adfa133">2021.emnlp-main.596</url>
      <bibkey>jansen-etal-2021-challenges</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.596</doi>
      <video href="2021.emnlp-main.596.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/qasc">QASC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/worldtree">Worldtree</pwcdataset>
    </paper>
    <paper id="597">
      <title><fixed-case>ESTER</fixed-case>: A Machine Reading Comprehension Dataset for Reasoning about Event Semantic Relations</title>
      <author><first>Rujun</first><last>Han</last></author>
      <author><first>I-Hung</first><last>Hsu</last></author>
      <author><first>Jiao</first><last>Sun</last></author>
      <author><first>Julia</first><last>Baylon</last></author>
      <author><first>Qiang</first><last>Ning</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>7543–7559</pages>
      <abstract>Understanding how events are semantically related to each other is the essence of reading comprehension. Recent event-centric reading comprehension datasets focus mostly on event arguments or temporal relations. While these tasks partially evaluate machines’ ability of narrative understanding, human-like reading comprehension requires the capability to process event-based information beyond arguments and temporal reasoning. For example, to understand causality between events, we need to infer motivation or purpose; to establish event hierarchy, we need to understand the composition of events. To facilitate these tasks, we introduce **ESTER**, a comprehensive machine reading comprehension (MRC) dataset for Event Semantic Relation Reasoning. The dataset leverages natural language queries to reason about the five most common event semantic relations, provides more than 6K questions, and captures 10.1K event relation pairs. Experimental results show that the current SOTA systems achieve 22.1%, 63.3% and 83.5% for token-based exact-match (**EM**), **F1** and event-based **HIT@1** scores, which are all significantly below human performances (36.0%, 79.6%, 100% respectively), highlighting our dataset as a challenging benchmark.</abstract>
      <url hash="985aae2f">2021.emnlp-main.597</url>
      <bibkey>han-etal-2021-ester</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.597</doi>
      <video href="2021.emnlp-main.597.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/torque">Torque</pwcdataset>
    </paper>
    <paper id="598">
      <title><fixed-case>RICA</fixed-case>: Evaluating Robust Inference Capabilities Based on Commonsense Axioms</title>
      <author><first>Pei</first><last>Zhou</last></author>
      <author><first>Rahul</first><last>Khanna</last></author>
      <author><first>Seyeon</first><last>Lee</last></author>
      <author><first>Bill Yuchen</first><last>Lin</last></author>
      <author><first>Daniel</first><last>Ho</last></author>
      <author><first>Jay</first><last>Pujara</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <pages>7560–7579</pages>
      <abstract>Pre-trained language models (PTLMs) have achieved impressive performance on commonsense inference benchmarks, but their ability to employ commonsense to make robust inferences, which is crucial for effective communications with humans, is debated. In the pursuit of advancing fluid human-AI communication, we propose a new challenge, RICA: Robust Inference using Commonsense Axioms, that evaluates robust commonsense inference despite textual perturbations. To generate data for this challenge, we develop a systematic and scalable procedure using commonsense knowledge bases and probe PTLMs across two different evaluation settings. Extensive experiments on our generated probe sets with more than 10k statements show that PTLMs perform no better than random guessing on the zero-shot setting, are heavily impacted by statistical biases, and are not robust to perturbation attacks. We also find that fine-tuning on similar statements offer limited gains, as PTLMs still fail to generalize to unseen inferences. Our new large-scale benchmark exposes a significant gap between PTLMs and human-level language understanding and offers a new challenge for PTLMs to demonstrate commonsense.</abstract>
      <url hash="17445795">2021.emnlp-main.598</url>
      <bibkey>zhou-etal-2021-rica</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.598</doi>
      <video href="2021.emnlp-main.598.mp4"/>
    </paper>
    <paper id="599">
      <title>Compression, Transduction, and Creation: A Unified Framework for Evaluating Natural Language Generation</title>
      <author><first>Mingkai</first><last>Deng</last></author>
      <author><first>Bowen</first><last>Tan</last></author>
      <author><first>Zhengzhong</first><last>Liu</last></author>
      <author><first>Eric</first><last>Xing</last></author>
      <author><first>Zhiting</first><last>Hu</last></author>
      <pages>7580–7605</pages>
      <abstract>Natural language generation (NLG) spans a broad range of tasks, each of which serves for specific objectives and desires different properties of generated text. The complexity makes automatic evaluation of NLG particularly challenging. Previous work has typically focused on a single task and developed individual evaluation metrics based on specific intuitions. In this paper, we propose a unifying perspective based on the nature of information change in NLG tasks, including compression (e.g., summarization), transduction (e.g., text rewriting), and creation (e.g., dialog). _Information alignment_ between input, context, and output text plays a common central role in characterizing the generation. With automatic alignment prediction models, we develop a family of interpretable metrics that are suitable for evaluating key aspects of different NLG tasks, often without need of gold reference data. Experiments show the uniformly designed metrics achieve stronger or comparable correlations with human judgement compared to state-of-the-art metrics in each of diverse tasks, including text summarization, style transfer, and knowledge-grounded dialog.</abstract>
      <url hash="78e5edad">2021.emnlp-main.599</url>
      <attachment type="Software" hash="c0285721">2021.emnlp-main.599.Software.zip</attachment>
      <bibkey>deng-etal-2021-compression</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.599</doi>
      <video href="2021.emnlp-main.599.mp4"/>
      <pwccode url="https://github.com/tanyuqian/ctc-gen-eval" additional="false">tanyuqian/ctc-gen-eval</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
    </paper>
    <paper id="600">
      <title><fixed-case>MATE</fixed-case>: Multi-view Attention for Table Transformer Efficiency</title>
      <author><first>Julian</first><last>Eisenschlos</last></author>
      <author><first>Maharshi</first><last>Gor</last></author>
      <author><first>Thomas</first><last>Müller</last></author>
      <author><first>William</first><last>Cohen</last></author>
      <pages>7606–7619</pages>
      <abstract>This work presents a sparse-attention Transformer architecture for modeling documents that contain large tables. Tables are ubiquitous on the web, and are rich in information. However, more than 20% of relational tables on the web have 20 or more rows (Cafarella et al., 2008), and these large tables present a challenge for current Transformer models, which are typically limited to 512 tokens. Here we propose MATE, a novel Transformer architecture designed to model the structure of web tables. MATE uses sparse attention in a way that allows heads to efficiently attend to either rows or columns in a table. This architecture scales linearly with respect to speed and memory, and can handle documents containing more than 8000 tokens with current accelerators. MATE also has a more appropriate inductive bias for tabular data, and sets a new state-of-the-art for three table reasoning datasets. For HybridQA (Chen et al., 2020), a dataset that involves large documents containing tables, we improve the best prior result by 19 points.</abstract>
      <url hash="91232325">2021.emnlp-main.600</url>
      <bibkey>eisenschlos-etal-2021-mate</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.600</doi>
      <video href="2021.emnlp-main.600.mp4"/>
      <pwccode url="https://github.com/google-research/tapas" additional="false">google-research/tapas</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hybridqa">HybridQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sqa">SQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tabfact">TabFact</pwcdataset>
    </paper>
    <paper id="601">
      <title>Learning with Different Amounts of Annotation: From Zero to Many Labels</title>
      <author><first>Shujian</first><last>Zhang</last></author>
      <author><first>Chengyue</first><last>Gong</last></author>
      <author><first>Eunsol</first><last>Choi</last></author>
      <pages>7620–7632</pages>
      <abstract>Training NLP systems typically assumes access to annotated data that has a single human label per example. Given imperfect labeling from annotators and inherent ambiguity of language, we hypothesize that single label is not sufficient to learn the spectrum of language interpretation. We explore new annotation distribution schemes, assigning multiple labels per example for a small subset of training examples. Introducing such multi label examples at the cost of annotating fewer examples brings clear gains on natural language inference task and entity typing task, even when we simply first train with a single label data and then fine tune with multi label examples. Extending a MixUp data augmentation framework, we propose a learning algorithm that can learn from training examples with different amount of annotation (with zero, one, or multiple labels). This algorithm efficiently combines signals from uneven training data and brings additional gains in low annotation budget and cross domain settings. Together, our method achieves consistent gains in two tasks, suggesting distributing labels unevenly among training examples can be beneficial for many NLP tasks.</abstract>
      <url hash="d27b81bf">2021.emnlp-main.601</url>
      <bibkey>zhang-etal-2021-learning-different</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.601</doi>
      <video href="2021.emnlp-main.601.mp4"/>
      <pwccode url="https://github.com/szhang42/uneven_training_data" additional="false">szhang42/uneven_training_data</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/chaosnli">ChaosNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="602">
      <title>When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute</title>
      <author><first>Tao</first><last>Lei</last></author>
      <pages>7633–7648</pages>
      <abstract>Large language models have become increasingly difficult to train because of the growing computation time and cost. In this work, we present SRU++, a highly-efficient architecture that combines fast recurrence and attention for sequence modeling. SRU++ exhibits strong modeling capacity and training efficiency. On standard language modeling tasks such as Enwik8, Wiki-103 and Billion Word datasets, our model obtains better bits-per-character and perplexity while using 3x-10x less training cost compared to top-performing Transformer models. For instance, our model achieves a state-of-the-art result on the Enwik8 dataset using 1.6 days of training on an 8-GPU machine. We further demonstrate that SRU++ requires minimal attention for near state-of-the-art performance. Our results suggest jointly leveraging fast recurrence with little attention as a promising direction for accelerating model training and inference.</abstract>
      <url hash="edcf9a70">2021.emnlp-main.602</url>
      <bibkey>lei-2021-attention</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.602</doi>
      <video href="2021.emnlp-main.602.mp4"/>
      <pwccode url="https://github.com/asappresearch/sru" additional="false">asappresearch/sru</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/billion-word-benchmark">Billion Word Benchmark</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="603">
      <title>Universal-<fixed-case>KD</fixed-case>: Attention-based Output-Grounded Intermediate Layer Knowledge Distillation</title>
      <author><first>Yimeng</first><last>Wu</last></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last></author>
      <author><first>Abbas</first><last>Ghaddar</last></author>
      <author><first>Md Akmal</first><last>Haidar</last></author>
      <author><first>Ali</first><last>Ghodsi</last></author>
      <pages>7649–7661</pages>
      <abstract>Intermediate layer matching is shown as an effective approach for improving knowledge distillation (KD). However, this technique applies matching in the hidden spaces of two different networks (i.e. student and teacher), which lacks clear interpretability. Moreover, intermediate layer KD cannot easily deal with other problems such as layer mapping search and architecture mismatch (i.e. it requires the teacher and student to be of the same model type). To tackle the aforementioned problems all together, we propose Universal-KD to match intermediate layers of the teacher and the student in the output space (by adding pseudo classifiers on intermediate layers) via the attention-based layer projection. By doing this, our unified approach has three merits: (i) it can be flexibly combined with current intermediate layer distillation techniques to improve their results (ii) the pseudo classifiers of the teacher can be deployed instead of extra expensive teacher assistant networks to address the capacity gap problem in KD which is a common issue when the gap between the size of the teacher and student networks becomes too large; (iii) it can be used in cross-architecture intermediate layer KD. We did comprehensive experiments in distilling BERT-base into BERT-4, RoBERTa-large into DistilRoBERTa and BERT-base into CNN and LSTM-based models. Results on the GLUE tasks show that our approach is able to outperform other KD techniques.</abstract>
      <url hash="2f8f7f56">2021.emnlp-main.603</url>
      <bibkey>wu-etal-2021-universal</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.603</doi>
      <video href="2021.emnlp-main.603.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="604">
      <title>Highly Parallel Autoregressive Entity Linking with Discriminative Correction</title>
      <author><first>Nicola</first><last>De Cao</last></author>
      <author><first>Wilker</first><last>Aziz</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <pages>7662–7669</pages>
      <abstract>Generative approaches have been recently shown to be effective for both Entity Disambiguation and Entity Linking (i.e., joint mention detection and disambiguation). However, the previously proposed autoregressive formulation for EL suffers from i) high computational cost due to a complex (deep) decoder, ii) non-parallelizable decoding that scales with the source sequence length, and iii) the need for training on a large amount of data. In this work, we propose a very efficient approach that parallelizes autoregressive linking across all potential mentions and relies on a shallow and efficient decoder. Moreover, we augment the generative objective with an extra discriminative component, i.e., a correction term which lets us directly optimize the generator’s ranking. When taken together, these techniques tackle all the above issues: our model is &gt;70 times faster and more accurate than the previous generative method, outperforming state-of-the-art approaches on the standard English dataset AIDA-CoNLL. Source code available at https://github.com/nicola-decao/efficient-autoregressive-EL</abstract>
      <url hash="0d81d37b">2021.emnlp-main.604</url>
      <bibkey>de-cao-etal-2021-highly</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.604</doi>
      <video href="2021.emnlp-main.604.mp4"/>
      <pwccode url="https://github.com/nicola-decao/efficient-autoregressive-EL" additional="false">nicola-decao/efficient-autoregressive-EL</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/aida-conll-yago">AIDA CoNLL-YAGO</pwcdataset>
    </paper>
    <paper id="605">
      <title>Word-Level Coreference Resolution</title>
      <author><first>Vladimir</first><last>Dobrovolskii</last></author>
      <pages>7670–7675</pages>
      <abstract>Recent coreference resolution models rely heavily on span representations to find coreference links between word spans. As the number of spans is <tex-math>O(n^2)</tex-math> in the length of text and the number of potential links is <tex-math>O(n^4)</tex-math>, various pruning techniques are necessary to make this approach computationally feasible. We propose instead to consider coreference links between individual words rather than word spans and then reconstruct the word spans. This reduces the complexity of the coreference model to <tex-math>O(n^2)</tex-math> and allows it to consider all potential mentions without pruning any of them out. We also demonstrate that, with these changes, SpanBERT for coreference resolution will be significantly outperformed by RoBERTa. While being highly efficient, our model performs competitively with recent coreference resolution systems on the OntoNotes benchmark.</abstract>
      <url hash="37503770">2021.emnlp-main.605</url>
      <bibkey>dobrovolskii-2021-word</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.605</doi>
      <video href="2021.emnlp-main.605.mp4"/>
      <pwccode url="https://github.com/vdobrovolskii/wl-coref" additional="false">vdobrovolskii/wl-coref</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2012-1">CoNLL-2012</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ontonotes-5-0">OntoNotes 5.0</pwcdataset>
    </paper>
    <paper id="606">
      <title>A Secure and Efficient Federated Learning Framework for <fixed-case>NLP</fixed-case></title>
      <author><first>Chenghong</first><last>Wang</last></author>
      <author><first>Jieren</first><last>Deng</last></author>
      <author><first>Xianrui</first><last>Meng</last></author>
      <author><first>Yijue</first><last>Wang</last></author>
      <author><first>Ji</first><last>Li</last></author>
      <author><first>Sheng</first><last>Lin</last></author>
      <author><first>Shuo</first><last>Han</last></author>
      <author><first>Fei</first><last>Miao</last></author>
      <author><first>Sanguthevar</first><last>Rajasekaran</last></author>
      <author><first>Caiwen</first><last>Ding</last></author>
      <pages>7676–7682</pages>
      <abstract>In this work, we consider the problem of designing secure and efficient federated learning (FL) frameworks for NLP. Existing solutions under this literature either consider a trusted aggregator or require heavy-weight cryptographic primitives, which makes the performance significantly degraded. Moreover, many existing secure FL designs work only under the restrictive assumption that none of the clients can be dropped out from the training protocol. To tackle these problems, we propose SEFL, a secure and efficient federated learning framework that (1) eliminates the need for the trusted entities; (2) achieves similar and even better model accuracy compared with existing FL designs; (3) is resilient to client dropouts.</abstract>
      <url hash="5624e960">2021.emnlp-main.606</url>
      <bibkey>wang-etal-2021-secure</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.606</doi>
      <video href="2021.emnlp-main.606.mp4"/>
    </paper>
    <paper id="607">
      <title>Controllable Semantic Parsing via Retrieval Augmentation</title>
      <author><first>Panupong</first><last>Pasupat</last></author>
      <author><first>Yuan</first><last>Zhang</last></author>
      <author><first>Kelvin</first><last>Guu</last></author>
      <pages>7683–7698</pages>
      <abstract>In practical applications of semantic parsing, we often want to rapidly change the behavior of the parser, such as enabling it to handle queries in a new domain, or changing its predictions on certain targeted queries. While we can introduce new training examples exhibiting the target behavior, a mechanism for enacting such behavior changes without expensive model re-training would be preferable. To this end, we propose ControllAble Semantic Parser via Exemplar Retrieval (CASPER). Given an input query, the parser retrieves related exemplars from a retrieval index, augments them to the query, and then applies a generative seq2seq model to produce an output parse. The exemplars act as a control mechanism over the generic generative model: by manipulating the retrieval index or how the augmented query is constructed, we can manipulate the behavior of the parser. On the MTOP dataset, in addition to achieving state-of-the-art on the standard setup, we show that CASPER can parse queries in a new domain, adapt the prediction toward the specified patterns, or adapt to new semantic schemas without having to further re-train the model.</abstract>
      <url hash="961adf51">2021.emnlp-main.607</url>
      <bibkey>pasupat-etal-2021-controllable</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.607</doi>
      <video href="2021.emnlp-main.607.mp4"/>
      <pwccode url="https://github.com/google-research/language/tree/master/language/casper" additional="false">google-research/language</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mtop">MTOP</pwcdataset>
    </paper>
    <paper id="608">
      <title>Constrained Language Models Yield Few-Shot Semantic Parsers</title>
      <author><first>Richard</first><last>Shin</last></author>
      <author><first>Christopher</first><last>Lin</last></author>
      <author><first>Sam</first><last>Thomson</last></author>
      <author><first>Charles</first><last>Chen</last></author>
      <author><first>Subhro</first><last>Roy</last></author>
      <author><first>Emmanouil Antonios</first><last>Platanios</last></author>
      <author><first>Adam</first><last>Pauls</last></author>
      <author><first>Dan</first><last>Klein</last></author>
      <author><first>Jason</first><last>Eisner</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>7699–7715</pages>
      <abstract>We explore the use of large pretrained language models as few-shot semantic parsers. The goal in semantic parsing is to generate a structured meaning representation given a natural language input. However, language models are trained to generate natural language. To bridge the gap, we use language models to paraphrase inputs into a controlled sublanguage resembling English that can be automatically mapped to a target meaning representation. Our results demonstrate that with only a small amount of data and very little code to convert into English-like representations, our blueprint for rapidly bootstrapping semantic parsers leads to surprisingly effective performance on multiple community tasks, greatly exceeding baseline methods also trained on the same limited data.</abstract>
      <url hash="de57de7a">2021.emnlp-main.608</url>
      <bibkey>shin-etal-2021-constrained</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.608</doi>
      <video href="2021.emnlp-main.608.mp4"/>
      <pwccode url="https://github.com/microsoft/semantic_parsing_with_constrained_lm" additional="false">microsoft/semantic_parsing_with_constrained_lm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/break">BREAK</pwcdataset>
    </paper>
    <paper id="609">
      <title><fixed-case>E</fixed-case>xpla<fixed-case>G</fixed-case>raphs: An Explanation Graph Generation Task for Structured Commonsense Reasoning</title>
      <author><first>Swarnadeep</first><last>Saha</last></author>
      <author><first>Prateek</first><last>Yadav</last></author>
      <author><first>Lisa</first><last>Bauer</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>7716–7740</pages>
      <abstract>Recent commonsense-reasoning tasks are typically discriminative in nature, where a model answers a multiple-choice question for a certain context. Discriminative tasks are limiting because they fail to adequately evaluate the model’s ability to reason and explain predictions with underlying commonsense knowledge. They also allow such models to use reasoning shortcuts and not be “right for the right reasons”. In this work, we present ExplaGraphs, a new generative and structured commonsense-reasoning task (and an associated dataset) of explanation graph generation for stance prediction. Specifically, given a belief and an argument, a model has to predict if the argument supports or counters the belief and also generate a commonsense-augmented graph that serves as non-trivial, complete, and unambiguous explanation for the predicted stance. We collect explanation graphs through a novel Create-Verify-And-Refine graph collection framework that improves the graph quality (up to 90%) via multiple rounds of verification and refinement. A significant 79% of our graphs contain external commonsense nodes with diverse structures and reasoning depths. Next, we propose a multi-level evaluation framework, consisting of automatic metrics and human evaluation, that check for the structural and semantic correctness of the generated graphs and their degree of match with ground-truth graphs. Finally, we present several structured, commonsense-augmented, and text generation models as strong starting points for this explanation graph generation task, and observe that there is a large gap with human performance, thereby encouraging future work for this new challenging task.</abstract>
      <url hash="6c027f9c">2021.emnlp-main.609</url>
      <bibkey>saha-etal-2021-explagraphs</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.609</doi>
      <video href="2021.emnlp-main.609.mp4"/>
      <pwccode url="https://github.com/swarnaHub/ExplaGraphs" additional="false">swarnaHub/ExplaGraphs</pwccode>
    </paper>
    <paper id="610">
      <title>Connect-the-Dots: Bridging Semantics between Words and Definitions via Aligning Word Sense Inventories</title>
      <author><first>Wenlin</first><last>Yao</last></author>
      <author><first>Xiaoman</first><last>Pan</last></author>
      <author><first>Lifeng</first><last>Jin</last></author>
      <author><first>Jianshu</first><last>Chen</last></author>
      <author><first>Dian</first><last>Yu</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <pages>7741–7751</pages>
      <abstract>Word Sense Disambiguation (WSD) aims to automatically identify the exact meaning of one word according to its context. Existing supervised models struggle to make correct predictions on rare word senses due to limited training data and can only select the best definition sentence from one predefined word sense inventory (e.g., WordNet). To address the data sparsity problem and generalize the model to be independent of one predefined inventory, we propose a gloss alignment algorithm that can align definition sentences (glosses) with the same meaning from different sense inventories to collect rich lexical knowledge. We then train a model to identify semantic equivalence between a target word in context and one of its glosses using these aligned inventories, which exhibits strong transfer capability to many WSD tasks. Experiments on benchmark datasets show that the proposed method improves predictions on both frequent and rare word senses, outperforming prior work by 1.2% on the All-Words WSD Task and 4.3% on the Low-Shot WSD Task. Evaluation on WiC Task also indicates that our method can better capture word meanings in context.</abstract>
      <url hash="1817095b">2021.emnlp-main.610</url>
      <bibkey>yao-etal-2021-connect</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.610</doi>
      <video href="2021.emnlp-main.610.mp4"/>
      <pwccode url="https://github.com/tencent-ailab/EMNLP21_SemEq" additional="true">tencent-ailab/EMNLP21_SemEq</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wic">WiC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="611">
      <title><fixed-case>LM</fixed-case>-Critic: Language Models for Unsupervised Grammatical Error Correction</title>
      <author><first>Michihiro</first><last>Yasunaga</last></author>
      <author><first>Jure</first><last>Leskovec</last></author>
      <author><first>Percy</first><last>Liang</last></author>
      <pages>7752–7763</pages>
      <abstract>Grammatical error correction (GEC) requires a set of labeled ungrammatical / grammatical sentence pairs for training, but obtaining such annotation can be prohibitively expensive. Recently, the Break-It-Fix-It (BIFI) framework has demonstrated strong results on learning to repair a broken program without any labeled examples, but this relies on a perfect critic (e.g., a compiler) that returns whether an example is valid or not, which does not exist for the GEC task. In this work, we show how to leverage a pretrained language model (LM) in defining an LM-Critic, which judges a sentence to be grammatical if the LM assigns it a higher probability than its local perturbations. We apply this LM-Critic and BIFI along with a large set of unlabeled sentences to bootstrap realistic ungrammatical / grammatical pairs for training a corrector. We evaluate our approach on GEC datasets on multiple domains (CoNLL-2014, BEA-2019, GMEG-wiki and GMEG-yahoo) and show that it outperforms existing methods in both the unsupervised setting (+7.7 F0.5) and the supervised setting (+0.5 F0.5).</abstract>
      <url hash="dd0f2b8a">2021.emnlp-main.611</url>
      <bibkey>yasunaga-etal-2021-lm</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.611</doi>
      <video href="2021.emnlp-main.611.mp4"/>
      <pwccode url="https://github.com/michiyasunaga/LM-Critic" additional="true">michiyasunaga/LM-Critic</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2014-shared-task-grammatical-error">CoNLL-2014 Shared Task: Grammatical Error Correction</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gmeg-wiki">GMEG-wiki</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gmeg-yahoo">GMEG-yahoo</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/jfleg">JFLEG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/locness-corpus">WI-LOCNESS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/yahoo-answers">Yahoo! Answers</pwcdataset>
    </paper>
    <paper id="612">
      <title>Language-agnostic Representation from Multilingual Sentence Encoders for Cross-lingual Similarity Estimation</title>
      <author><first>Nattapong</first><last>Tiyajamorn</last></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last></author>
      <author><first>Yuki</first><last>Arase</last></author>
      <author><first>Makoto</first><last>Onizuka</last></author>
      <pages>7764–7774</pages>
      <abstract>We propose a method to distill a language-agnostic meaning embedding from a multilingual sentence encoder. By removing language-specific information from the original embedding, we retrieve an embedding that fully represents the sentence’s meaning. The proposed method relies only on parallel corpora without any human annotations. Our meaning embedding allows efficient cross-lingual sentence similarity estimation by simple cosine similarity calculation. Experimental results on both quality estimation of machine translation and cross-lingual semantic textual similarity tasks reveal that our method consistently outperforms the strong baselines using the original multilingual embedding. Our method consistently improves the performance of any pre-trained multilingual sentence encoder, even in low-resource language pairs where only tens of thousands of parallel sentence pairs are available.</abstract>
      <url hash="2a67ff5d">2021.emnlp-main.612</url>
      <bibkey>tiyajamorn-etal-2021-language</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.612</doi>
      <video href="2021.emnlp-main.612.mp4"/>
      <pwccode url="https://github.com/nattaptiy/qe_disentangled" additional="false">nattaptiy/qe_disentangled</pwccode>
    </paper>
    <paper id="613">
      <title>Classifying Dyads for Militarized Conflict Analysis</title>
      <author><first>Niklas</first><last>Stoehr</last></author>
      <author><first>Lucas</first><last>Torroba Hennigen</last></author>
      <author><first>Samin</first><last>Ahbab</last></author>
      <author><first>Robert</first><last>West</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <pages>7775–7784</pages>
      <abstract>Understanding the origins of militarized conflict is a complex, yet important undertaking. Existing research seeks to build this understanding by considering bi-lateral relationships between entity pairs (dyadic causes) and multi-lateral relationships among multiple entities (systemic causes). The aim of this work is to compare these two causes in terms of how they correlate with conflict between two entities. We do this by devising a set of textual and graph-based features which represent each of the causes. The features are extracted from Wikipedia and modeled as a large graph. Nodes in this graph represent entities connected by labeled edges representing ally or enemy-relationships. This allows casting the problem as an edge classification task, which we term dyad classification. We propose and evaluate classifiers to determine if a particular pair of entities are allies or enemies. Our results suggest that our systemic features might be slightly better correlates of conflict. Further, we find that Wikipedia articles of allies are semantically more similar than enemies.</abstract>
      <url hash="c2008001">2021.emnlp-main.613</url>
      <bibkey>stoehr-etal-2021-classifying</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.613</doi>
      <video href="2021.emnlp-main.613.mp4"/>
      <pwccode url="https://github.com/conflict-ai/conflictwiki" additional="false">conflict-ai/conflictwiki</pwccode>
    </paper>
    <paper id="614">
      <title>Point-of-Interest Type Prediction using Text and Images</title>
      <author><first>Danae</first><last>Sánchez Villegas</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <pages>7785–7797</pages>
      <abstract>Point-of-interest (POI) type prediction is the task of inferring the type of a place from where a social media post was shared. Inferring a POI’s type is useful for studies in computational social science including sociolinguistics, geosemiotics, and cultural geography, and has applications in geosocial networking technologies such as recommendation and visualization systems. Prior efforts in POI type prediction focus solely on text, without taking visual information into account. However in reality, the variety of modalities, as well as their semiotic relationships with one another, shape communication and interactions in social media. This paper presents a study on POI type prediction using multimodal information from text and images available at posting time. For that purpose, we enrich a currently available data set for POI type prediction with the images that accompany the text messages. Our proposed method extracts relevant information from each modality to effectively capture interactions between text and image achieving a macro F1 of 47.21 across 8 categories significantly outperforming the state-of-the-art method for POI type prediction based on text-only methods. Finally, we provide a detailed analysis to shed light on cross-modal interactions and the limitations of our best performing model.</abstract>
      <url hash="60365974">2021.emnlp-main.614</url>
      <bibkey>sanchez-villegas-aletras-2021-point</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.614</doi>
      <video href="2021.emnlp-main.614.mp4"/>
      <pwccode url="https://github.com/danaesavi/poi-type-prediction" additional="false">danaesavi/poi-type-prediction</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
    <paper id="615">
      <title>Come hither or go away? Recognising pre-electoral coalition signals in the news</title>
      <author><first>Ines</first><last>Rehbein</last></author>
      <author><first>Simone Paolo</first><last>Ponzetto</last></author>
      <author><first>Anna</first><last>Adendorf</last></author>
      <author><first>Oke</first><last>Bahnsen</last></author>
      <author><first>Lukas</first><last>Stoetzer</last></author>
      <author><first>Heiner</first><last>Stuckenschmidt</last></author>
      <pages>7798–7810</pages>
      <abstract>In this paper, we introduce the task of political coalition signal prediction from text, that is, the task of recognizing from the news coverage leading up to an election the (un)willingness of political parties to form a government coalition. We decompose our problem into two related, but distinct tasks: (i) predicting whether a reported statement from a politician or a journalist refers to a potential coalition and (ii) predicting the polarity of the signal – namely, whether the speaker is in favour of or against the coalition. For this, we explore the benefits of multi-task learning and investigate which setup and task formulation is best suited for each sub-task. We evaluate our approach, based on hand-coded newspaper articles, covering elections in three countries (Ireland, Germany, Austria) and two languages (English, German). Our results show that the multi-task learning approach can further improve results over a strong monolingual transfer learning baseline.</abstract>
      <url hash="8a06a6f1">2021.emnlp-main.615</url>
      <bibkey>rehbein-etal-2021-come</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.615</doi>
      <video href="2021.emnlp-main.615.mp4"/>
    </paper>
    <paper id="616">
      <title>#<fixed-case>H</fixed-case>ow<fixed-case>Y</fixed-case>ou<fixed-case>T</fixed-case>ag<fixed-case>T</fixed-case>weets: Learning User Hashtagging Preferences via Personalized Topic Attention</title>
      <author><first>Yuji</first><last>Zhang</last></author>
      <author><first>Yubo</first><last>Zhang</last></author>
      <author><first>Chunpu</first><last>Xu</last></author>
      <author><first>Jing</first><last>Li</last></author>
      <author><first>Ziyan</first><last>Jiang</last></author>
      <author><first>Baolin</first><last>Peng</last></author>
      <pages>7811–7820</pages>
      <abstract>Millions of hashtags are created on social media every day to cross-refer messages concerning similar topics. To help people find the topics they want to discuss, this paper characterizes a user’s hashtagging preferences via predicting how likely they will post with a hashtag. It is hypothesized that one’s interests in a hashtag are related with what they said before (user history) and the existing posts present the hashtag (hashtag contexts). These factors are married in the deep semantic space built with a pre-trained BERT and a neural topic model via multitask learning. In this way, user interests learned from the past can be customized to match future hashtags, which is beyond the capability of existing methods assuming unchanged hashtag semantics. Furthermore, we propose a novel personalized topic attention to capture salient contents to personalize hashtag contexts. Experiments on a large-scale Twitter dataset show that our model significantly outperforms the state-of-the-art recommendation approach without exploiting latent topics.</abstract>
      <url hash="76dc4a17">2021.emnlp-main.616</url>
      <bibkey>zhang-etal-2021-howyoutagtweets</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.616</doi>
      <video href="2021.emnlp-main.616.mp4"/>
      <pwccode url="https://github.com/polyusmart/personalized-hashtag-preferences" additional="false">polyusmart/personalized-hashtag-preferences</pwccode>
    </paper>
    <paper id="617">
      <title>Learning Neural Templates for Recommender Dialogue System</title>
      <author><first>Zujie</first><last>Liang</last></author>
      <author><first>Huang</first><last>Hu</last></author>
      <author><first>Can</first><last>Xu</last></author>
      <author><first>Jian</first><last>Miao</last></author>
      <author><first>Yingying</first><last>He</last></author>
      <author><first>Yining</first><last>Chen</last></author>
      <author><first>Xiubo</first><last>Geng</last></author>
      <author><first>Fan</first><last>Liang</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <pages>7821–7833</pages>
      <abstract>The task of Conversational Recommendation System (CRS), i.e., recommender dialog system, aims to recommend precise items to users through natural language interactions. Though recent end-to-end neural models have shown promising progress on this task, two key challenges still remain. First, the recommended items cannot be always incorporated into the generated response precisely and appropriately. Second, only the items mentioned in the training corpus have a chance to be recommended in the conversation. To tackle these challenges, we introduce a novel framework called NTRD for recommender dialogue system that can decouple the dialogue generation from the item recommendation. NTRD has two key components, i.e., response template generator and item selector. The former adopts an encoder-decoder model to generate a response template with slot locations tied to target items, while the latter fills in slot locations with the proper items using a sufficient attention mechanism. Our approach combines the strengths of both classical slot filling approaches (that are generally controllable) and modern neural NLG approaches (that are generally more natural and accurate). Extensive experiments on the benchmark ReDial show our approach significantly outperforms the previous state-of-the-art methods. Besides, our approach has the unique advantage to produce novel items that do not appear in the training set of dialogue corpus. The code is available at <url>https://github.com/jokieleung/NTRD</url>.</abstract>
      <url hash="47c035d8">2021.emnlp-main.617</url>
      <bibkey>liang-etal-2021-learning-neural</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.617</doi>
      <video href="2021.emnlp-main.617.mp4"/>
      <pwccode url="https://github.com/jokieleung/ntrd" additional="false">jokieleung/ntrd</pwccode>
    </paper>
    <paper id="618">
      <title>Proxy Indicators for the Quality of Open-domain Dialogues</title>
      <author><first>Rostislav</first><last>Nedelchev</last></author>
      <author><first>Jens</first><last>Lehmann</last></author>
      <author><first>Ricardo</first><last>Usbeck</last></author>
      <pages>7834–7855</pages>
      <abstract>The automatic evaluation of open-domain dialogues remains a largely unsolved challenge. Despite the abundance of work done in the field, human judges have to evaluate dialogues’ quality. As a consequence, performing such evaluations at scale is usually expensive. This work investigates using a deep-learning model trained on the General Language Understanding Evaluation (GLUE) benchmark to serve as a quality indication of open-domain dialogues. The aim is to use the various GLUE tasks as different perspectives on judging the quality of conversation, thus reducing the need for additional training data or responses that serve as quality references. Due to this nature, the method can infer various quality metrics and can derive a component-based overall score. We achieve statistically significant correlation coefficients of up to 0.7.</abstract>
      <url hash="abc74a8a">2021.emnlp-main.618</url>
      <bibkey>nedelchev-etal-2021-proxy</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.618</doi>
      <video href="2021.emnlp-main.618.mp4"/>
      <pwccode url="https://github.com/smartdataanalytics/proxy_indicators" additional="false">smartdataanalytics/proxy_indicators</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/topical-chat">Topical-Chat</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/usr-personachat">USR-PersonaChat</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/usr-topicalchat">USR-TopicalChat</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
    </paper>
    <paper id="619">
      <title><tex-math>Q^{2}</tex-math>: <fixed-case>E</fixed-case>valuating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering</title>
      <author><first>Or</first><last>Honovich</last></author>
      <author><first>Leshem</first><last>Choshen</last></author>
      <author><first>Roee</first><last>Aharoni</last></author>
      <author><first>Ella</first><last>Neeman</last></author>
      <author><first>Idan</first><last>Szpektor</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <pages>7856–7870</pages>
      <abstract>Neural knowledge-grounded generative models for dialogue often produce content that is factually inconsistent with the knowledge they rely on, making them unreliable and limiting their applicability. Inspired by recent work on evaluating factual consistency in abstractive summarization, we propose an automatic evaluation metric for factual consistency in knowledge-grounded dialogue using automatic question generation and question answering. Our metric, denoted <tex-math>Q^2</tex-math>, compares answer spans using natural language inference (NLI), instead of token-based matching as done in previous work. To foster proper evaluation, we curate a novel dataset of dialogue system outputs for the Wizard-of-Wikipedia dataset, manually annotated for factual consistency. We perform a thorough meta-evaluation of <tex-math>Q^2</tex-math> against other metrics using this dataset and two others, where it consistently shows higher correlation with human judgements.</abstract>
      <url hash="b7b3f546">2021.emnlp-main.619</url>
      <bibkey>honovich-etal-2021-q2</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.619</doi>
      <video href="2021.emnlp-main.619.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wizard-of-wikipedia">Wizard of Wikipedia</pwcdataset>
    </paper>
    <paper id="620">
      <title>Knowledge-Aware Graph-Enhanced <fixed-case>GPT</fixed-case>-2 for Dialogue State Tracking</title>
      <author><first>Weizhe</first><last>Lin</last></author>
      <author><first>Bo-Hsiang</first><last>Tseng</last></author>
      <author id="bill-byrne"><first>Bill</first><last>Byrne</last></author>
      <pages>7871–7881</pages>
      <abstract>Dialogue State Tracking is central to multi-domain task-oriented dialogue systems, responsible for extracting information from user utterances. We present a novel hybrid architecture that augments GPT-2 with representations derived from Graph Attention Networks in such a way to allow causal, sequential prediction of slot values. The model architecture captures inter-slot relationships and dependencies across domains that otherwise can be lost in sequential prediction. We report improvements in state tracking performance in MultiWOZ 2.0 against a strong GPT-2 baseline and investigate a simplified sparse training scenario in which DST models are trained only on session-level annotations but evaluated at the turn level. We further report detailed analyses to demonstrate the effectiveness of graph models in DST by showing that the proposed graph modules capture inter-slot dependencies and improve the predictions of values that are common to multiple domains.</abstract>
      <url hash="a3f4fee1">2021.emnlp-main.620</url>
      <bibkey>lin-etal-2021-knowledge</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.620</doi>
      <video href="2021.emnlp-main.620.mp4"/>
      <pwccode url="https://github.com/linweizhedragon/knowledge-aware-graph-enhanced-gpt-2-for-dialogue-state-tracking" additional="false">linweizhedragon/knowledge-aware-graph-enhanced-gpt-2-for-dialogue-state-tracking</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
    </paper>
    <paper id="621">
      <title>A Collaborative Multi-agent Reinforcement Learning Framework for Dialog Action Decomposition</title>
      <author><first>Huimin</first><last>Wang</last></author>
      <author><first>Kam-Fai</first><last>Wong</last></author>
      <pages>7882–7889</pages>
      <abstract>Most reinforcement learning methods for dialog policy learning train a centralized agent that selects a predefined joint action concatenating domain name, intent type, and slot name. The centralized dialog agent suffers from a great many user-agent interaction requirements due to the large action space. Besides, designing the concatenated actions is laborious to engineers and maybe struggled with edge cases. To solve these problems, we model the dialog policy learning problem with a novel multi-agent framework, in which each part of the action is led by a different agent. The framework reduces labor costs for action templates and decreases the size of the action space for each agent. Furthermore, we relieve the non-stationary problem caused by the changing dynamics of the environment as evolving of agents’ policies by introducing a joint optimization process that makes agents can exchange their policy information. Concurrently, an independent experience replay buffer mechanism is integrated to reduce the dependence between gradients of samples to improve training efficiency. The effectiveness of the proposed framework is demonstrated in a multi-domain environment with both user simulator evaluation and human evaluation.</abstract>
      <url hash="946e81be">2021.emnlp-main.621</url>
      <bibkey>wang-wong-2021-collaborative</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.621</doi>
      <video href="2021.emnlp-main.621.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/multiwoz">MultiWOZ</pwcdataset>
    </paper>
    <paper id="622">
      <title>Zero-Shot Dialogue State Tracking via Cross-Task Transfer</title>
      <author><first>Zhaojiang</first><last>Lin</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Andrea</first><last>Madotto</last></author>
      <author><first>Seungwhan</first><last>Moon</last></author>
      <author><first>Zhenpeng</first><last>Zhou</last></author>
      <author><first>Paul</first><last>Crook</last></author>
      <author><first>Zhiguang</first><last>Wang</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <author><first>Eunjoon</first><last>Cho</last></author>
      <author><first>Rajen</first><last>Subba</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>7890–7900</pages>
      <abstract>Zero-shot transfer learning for dialogue state tracking (DST) enables us to handle a variety of task-oriented dialogue domains without the expense of collecting in-domain data. In this work, we propose to transfer the cross-task knowledge from general question answering (QA) corpora for the zero-shot DST task. Specifically, we propose TransferQA, a transferable generative QA model that seamlessly combines extractive QA and multi-choice QA via a text-to-text transformer framework, and tracks both categorical slots and non-categorical slots in DST. In addition, we introduce two effective ways to construct unanswerable questions, namely, negative question sampling and context truncation, which enable our model to handle none value slots in the zero-shot DST setting. The extensive experiments show that our approaches substantially improve the existing zero-shot and few-shot results on MultiWoz. Moreover, compared to the fully trained baseline on the Schema-Guided Dialogue dataset, our approach shows better generalization ability in unseen domains.</abstract>
      <url hash="6efc0fb4">2021.emnlp-main.622</url>
      <bibkey>lin-etal-2021-zero</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.622</doi>
      <video href="2021.emnlp-main.622.mp4"/>
      <pwccode url="https://github.com/facebookresearch/Zero-Shot-DST" additional="false">facebookresearch/Zero-Shot-DST</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dream">DREAM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sgd">SGD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/searchqa">SearchQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="623">
      <title>Uncertainty Measures in Neural Belief Tracking and the Effects on Dialogue Policy Performance</title>
      <author><first>Carel</first><last>van Niekerk</last></author>
      <author><first>Andrey</first><last>Malinin</last></author>
      <author><first>Christian</first><last>Geishauser</last></author>
      <author><first>Michael</first><last>Heck</last></author>
      <author><first>Hsien-chin</first><last>Lin</last></author>
      <author><first>Nurul</first><last>Lubis</last></author>
      <author><first>Shutong</first><last>Feng</last></author>
      <author><first>Milica</first><last>Gasic</last></author>
      <pages>7901–7914</pages>
      <abstract>The ability to identify and resolve uncertainty is crucial for the robustness of a dialogue system. Indeed, this has been confirmed empirically on systems that utilise Bayesian approaches to dialogue belief tracking. However, such systems consider only confidence estimates and have difficulty scaling to more complex settings. Neural dialogue systems, on the other hand, rarely take uncertainties into account. They are therefore overconfident in their decisions and less robust. Moreover, the performance of the tracking task is often evaluated in isolation, without consideration of its effect on the downstream policy optimisation. We propose the use of different uncertainty measures in neural belief tracking. The effects of these measures on the downstream task of policy optimisation are evaluated by adding selected measures of uncertainty to the feature space of the policy and training policies through interaction with a user simulator. Both human and simulated user results show that incorporating these measures leads to improvements both of the performance and of the robustness of the downstream dialogue policy. This highlights the importance of developing neural dialogue belief trackers that take uncertainty into account.</abstract>
      <url hash="671b4403">2021.emnlp-main.623</url>
      <bibkey>van-niekerk-etal-2021-uncertainty</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.623</doi>
      <video href="2021.emnlp-main.623.mp4"/>
    </paper>
    <paper id="624">
      <title>Dynamic Forecasting of Conversation Derailment</title>
      <author><first>Yova</first><last>Kementchedjhieva</last></author>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>7915–7919</pages>
      <abstract>Online conversations can sometimes take a turn for the worse, either due to systematic cultural differences, accidental misunderstandings, or mere malice. Automatically forecasting derailment in public online conversations provides an opportunity to take early action to moderate it. Previous work in this space is limited, and we extend it in several ways. We apply a pretrained language encoder to the task, which outperforms earlier approaches. We further experiment with shifting the training paradigm for the task from a static to a dynamic one to increase the forecast horizon. This approach shows mixed results: in a high-quality data setting, a longer average forecast horizon can be achieved at the cost of a small drop in F1; in a low-quality data setting, however, dynamic training propagates the noise and is highly detrimental to performance.</abstract>
      <url hash="4c08ad5a">2021.emnlp-main.624</url>
      <bibkey>kementchedjhieva-sogaard-2021-dynamic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.624</doi>
      <video href="2021.emnlp-main.624.mp4"/>
    </paper>
    <paper id="625">
      <title>A Semantic Filter Based on Relations for Knowledge Graph Completion</title>
      <author><first>Zongwei</first><last>Liang</last></author>
      <author><first>Junan</first><last>Yang</last></author>
      <author><first>Hui</first><last>Liu</last></author>
      <author><first>Keju</first><last>Huang</last></author>
      <pages>7920–7929</pages>
      <abstract>Knowledge graph embedding, representing entities and relations in the knowledge graphs with high-dimensional vectors, has made significant progress in link prediction. More researchers have explored the representational capabilities of models in recent years. That is, they investigate better representational models to fit symmetry/antisymmetry and combination relationships. The current embedding models are more inclined to utilize the identical vector for the same entity in various triples to measure the matching performance. The observation that measuring the rationality of specific triples means comparing the matching degree of the specific attributes associated with the relations is well-known. Inspired by this fact, this paper designs Semantic Filter Based on Relations(SFBR) to extract the required attributes of the entities. Then the rationality of triples is compared under these extracted attributes through the traditional embedding models. The semantic filter module can be added to most geometric and tensor decomposition models with minimal additional memory. experiments on the benchmark datasets show that the semantic filter based on relations can suppress the impact of other attribute dimensions and improve link prediction performance. The tensor decomposition models with SFBR have achieved state-of-the-art.</abstract>
      <url hash="908cfa2a">2021.emnlp-main.625</url>
      <attachment type="Software" hash="4829229c">2021.emnlp-main.625.Software.zip</attachment>
      <bibkey>liang-etal-2021-semantic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.625</doi>
      <video href="2021.emnlp-main.625.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k-237">FB15k-237</pwcdataset>
    </paper>
    <paper id="626">
      <title><fixed-case>AdapterDrop</fixed-case>: <fixed-case>O</fixed-case>n the Efficiency of Adapters in Transformers</title>
      <author><first>Andreas</first><last>Rücklé</last></author>
      <author><first>Gregor</first><last>Geigle</last></author>
      <author><first>Max</first><last>Glockner</last></author>
      <author><first>Tilman</first><last>Beck</last></author>
      <author><first>Jonas</first><last>Pfeiffer</last></author>
      <author><first>Nils</first><last>Reimers</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>7930–7946</pages>
      <abstract>Transformer models are expensive to fine-tune, slow for inference, and have large storage requirements. Recent approaches tackle these shortcomings by training smaller models, dynamically reducing the model size, and by training light-weight adapters. In this paper, we propose AdapterDrop, removing adapters from lower transformer layers during training and inference, which incorporates concepts from all three directions. We show that AdapterDrop can dynamically reduce the computational overhead when performing inference over multiple tasks simultaneously, with minimal decrease in task performances. We further prune adapters from AdapterFusion, which improves the inference efficiency while maintaining the task performances entirely.</abstract>
      <url hash="1a607544">2021.emnlp-main.626</url>
      <bibkey>ruckle-etal-2021-adapterdrop</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.626</doi>
      <video href="2021.emnlp-main.626.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="627">
      <title>Understanding and Overcoming the Challenges of Efficient Transformer Quantization</title>
      <author><first>Yelysei</first><last>Bondarenko</last></author>
      <author><first>Markus</first><last>Nagel</last></author>
      <author><first>Tijmen</first><last>Blankevoort</last></author>
      <pages>7947–7969</pages>
      <abstract>Transformer-based architectures have become the de-facto standard models for a wide range of Natural Language Processing tasks. However, their memory footprint and high latency are prohibitive for efficient deployment and inference on resource-limited devices. In this work, we explore quantization for transformers. We show that transformers have unique quantization challenges – namely, high dynamic activation ranges that are difficult to represent with a low bit fixed-point format. We establish that these activations contain structured outliers in the residual connections that encourage specific attention patterns, such as attending to the special separator token. To combat these challenges, we present three solutions based on post-training quantization and quantization-aware training, each with a different set of compromises for accuracy, model size, and ease of use. In particular, we introduce a novel quantization scheme – per-embedding-group quantization. We demonstrate the effectiveness of our methods on the GLUE benchmark using BERT, establishing state-of-the-art results for post-training quantization. Finally, we show that transformer weights and embeddings can be quantized to ultra-low bit-widths, leading to significant memory savings with a minimum accuracy loss. Our source code is available at <url>https://github.com/qualcomm-ai-research/transformer-quantization</url>.</abstract>
      <url hash="55ffd085">2021.emnlp-main.627</url>
      <bibkey>bondarenko-etal-2021-understanding</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.627</doi>
      <video href="2021.emnlp-main.627.mp4"/>
      <pwccode url="https://github.com/qualcomm-ai-research/transformer-quantization" additional="false">qualcomm-ai-research/transformer-quantization</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="628">
      <title><fixed-case>CAPE</fixed-case>: Context-Aware Private Embeddings for Private Language Learning</title>
      <author><first>Richard</first><last>Plant</last></author>
      <author><first>Dimitra</first><last>Gkatzia</last></author>
      <author><first>Valerio</first><last>Giuffrida</last></author>
      <pages>7970–7978</pages>
      <abstract>Neural language models have contributed to state-of-the-art results in a number of downstream applications including sentiment analysis, intent classification and others. However, obtaining text representations or embeddings using these models risks encoding personally identifiable information learned from language and context cues that may lead to privacy leaks. To ameliorate this issue, we propose Context-Aware Private Embeddings (CAPE), a novel approach which combines differential privacy and adversarial learning to preserve privacy during training of embeddings. Specifically, CAPE firstly applies calibrated noise through differential privacy to maintain the privacy of text representations by preserving the encoded semantic links while obscuring sensitive information. Next, CAPE employs an adversarial training regime that obscures identified private variables. Experimental results demonstrate that our proposed approach is more effective in reducing private information leakage than either single intervention, with approximately a 3% reduction in attacker performance compared to the best-performing current method.</abstract>
      <url hash="b207e072">2021.emnlp-main.628</url>
      <bibkey>plant-etal-2021-cape</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.628</doi>
      <video href="2021.emnlp-main.628.mp4"/>
      <revision id="1" href="2021.emnlp-main.628v1" hash="78618ae7"/>
      <revision id="2" href="2021.emnlp-main.628v2" hash="b207e072" date="2022-06-22">Updated some experiment results.</revision>
      <pwccode url="" additional="true"/>
    </paper>
    <paper id="629">
      <title>Text Detoxification using Large Pre-trained Neural Models</title>
      <author><first>David</first><last>Dale</last></author>
      <author><first>Anton</first><last>Voronov</last></author>
      <author><first>Daryna</first><last>Dementieva</last></author>
      <author><first>Varvara</first><last>Logacheva</last></author>
      <author><first>Olga</first><last>Kozlova</last></author>
      <author><first>Nikita</first><last>Semenov</last></author>
      <author><first>Alexander</first><last>Panchenko</last></author>
      <pages>7979–7996</pages>
      <abstract>We present two novel unsupervised methods for eliminating toxicity in text. Our first method combines two recent ideas: (1) guidance of the generation process with small style-conditional language models and (2) use of paraphrasing models to perform style transfer. We use a well-performing paraphraser guided by style-trained language models to keep the text content and remove toxicity. Our second method uses BERT to replace toxic words with their non-offensive synonyms. We make the method more flexible by enabling BERT to replace mask tokens with a variable number of words. Finally, we present the first large-scale comparative study of style transfer models on the task of toxicity removal. We compare our models with a number of methods for style transfer. The models are evaluated in a reference-free way using a combination of unsupervised style transfer metrics. Both methods we suggest yield new SOTA results.</abstract>
      <url hash="b4578d6f">2021.emnlp-main.629</url>
      <bibkey>dale-etal-2021-text</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.629</doi>
      <video href="2021.emnlp-main.629.mp4"/>
      <pwccode url="https://github.com/skoltech-nlp/detox" additional="false">skoltech-nlp/detox</pwccode>
    </paper>
    <paper id="630">
      <title>Document-Level Text Simplification: Dataset, Criteria and Baseline</title>
      <author><first>Renliang</first><last>Sun</last></author>
      <author><first>Hanqi</first><last>Jin</last></author>
      <author><first>Xiaojun</first><last>Wan</last></author>
      <pages>7997–8013</pages>
      <abstract>Text simplification is a valuable technique. However, current research is limited to sentence simplification. In this paper, we define and investigate a new task of document-level text simplification, which aims to simplify a document consisting of multiple sentences. Based on Wikipedia dumps, we first construct a large-scale dataset named D-Wikipedia and perform analysis and human evaluation on it to show that the dataset is reliable. Then, we propose a new automatic evaluation metric called D-SARI that is more suitable for the document-level simplification task. Finally, we select several representative models as baseline models for this task and perform automatic evaluation and human evaluation. We analyze the results and point out the shortcomings of the baseline models.</abstract>
      <url hash="c72d2d5a">2021.emnlp-main.630</url>
      <bibkey>sun-etal-2021-document</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.630</doi>
      <video href="2021.emnlp-main.630.mp4"/>
      <pwccode url="https://github.com/rlsnlp/document-level-text-simplification" additional="false">rlsnlp/document-level-text-simplification</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/newsela">Newsela</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikilarge">WikiLarge</pwcdataset>
    </paper>
    <paper id="631">
      <title>A Bag of Tricks for Dialogue Summarization</title>
      <author><first>Muhammad</first><last>Khalifa</last></author>
      <author><first>Miguel</first><last>Ballesteros</last></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <pages>8014–8022</pages>
      <abstract>Dialogue summarization comes with its own peculiar challenges as opposed to news or scientific articles summarization. In this work, we explore four different challenges of the task: handling and differentiating parts of the dialogue belonging to multiple speakers, negation understanding, reasoning about the situation, and informal language understanding. Using a pretrained sequence-to-sequence language model, we explore speaker name substitution, negation scope highlighting, multi-task learning with relevant tasks, and pretraining on in-domain data. Our experiments show that our proposed techniques indeed improve summarization performance, outperforming strong baselines.</abstract>
      <url hash="c8d26f1e">2021.emnlp-main.631</url>
      <bibkey>khalifa-etal-2021-bag</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.631</doi>
      <video href="2021.emnlp-main.631.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/commongen">CommonGen</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/samsum-corpus">SAMSum Corpus</pwcdataset>
    </paper>
    <paper id="632">
      <title>Paraphrasing Compound Nominalizations</title>
      <author><first>John</first><last>Lee</last></author>
      <author><first>Ho Hung</first><last>Lim</last></author>
      <author><first>Carol</first><last>Webster</last></author>
      <pages>8023–8028</pages>
      <abstract>A nominalization uses a deverbal noun to describe an event associated with its underlying verb. Commonly found in academic and formal texts, nominalizations can be difficult to interpret because of ambiguous semantic relations between the deverbal noun and its arguments. Our goal is to interpret nominalizations by generating clausal paraphrases. We address compound nominalizations with both nominal and adjectival modifiers, as well as prepositional phrases. In evaluations on a number of unsupervised methods, we obtained the strongest performance by using a pre-trained contextualized language model to re-rank paraphrase candidates identified by a textual entailment model.</abstract>
      <url hash="0093e445">2021.emnlp-main.632</url>
      <bibkey>lee-etal-2021-paraphrasing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.632</doi>
    </paper>
    <paper id="633">
      <title>Data-<fixed-case>Q</fixed-case>uest<fixed-case>E</fixed-case>val: A Referenceless Metric for Data-to-Text Semantic Evaluation</title>
      <author><first>Clement</first><last>Rebuffel</last></author>
      <author><first>Thomas</first><last>Scialom</last></author>
      <author><first>Laure</first><last>Soulier</last></author>
      <author><first>Benjamin</first><last>Piwowarski</last></author>
      <author><first>Sylvain</first><last>Lamprier</last></author>
      <author><first>Jacopo</first><last>Staiano</last></author>
      <author><first>Geoffrey</first><last>Scoutheeten</last></author>
      <author><first>Patrick</first><last>Gallinari</last></author>
      <pages>8029–8036</pages>
      <abstract>QuestEval is a reference-less metric used in text-to-text tasks, that compares the generated summaries directly to the source text, by automatically asking and answering questions. Its adaptation to Data-to-Text tasks is not straightforward, as it requires multimodal Question Generation and Answering systems on the considered tasks, which are seldom available. To this purpose, we propose a method to build synthetic multimodal corpora enabling to train multimodal components for a data-QuestEval metric. The resulting metric is reference-less and multimodal; it obtains state-of-the-art correlations with human judgment on the WebNLG and WikiBio benchmarks. We make data-QuestEval’s code and models available for reproducibility purpose, as part of the QuestEval project.</abstract>
      <url hash="53912d38">2021.emnlp-main.633</url>
      <bibkey>rebuffel-etal-2021-data</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.633</doi>
      <video href="2021.emnlp-main.633.mp4"/>
      <pwccode url="https://github.com/ThomasScialom/QuestEval" additional="true">ThomasScialom/QuestEval</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikibio">WikiBio</pwcdataset>
    </paper>
    <paper id="634">
      <title>Low-Rank Subspaces for Unsupervised Entity Linking</title>
      <author><first>Akhil</first><last>Arora</last></author>
      <author><first>Alberto</first><last>Garcia-Duran</last></author>
      <author><first>Robert</first><last>West</last></author>
      <pages>8037–8054</pages>
      <abstract>Entity linking is an important problem with many applications. Most previous solutions were designed for settings where annotated training data is available, which is, however, not the case in numerous domains. We propose a light-weight and scalable entity linking method, Eigenthemes, that relies solely on the availability of entity names and a referent knowledge base. Eigenthemes exploits the fact that the entities that are truly mentioned in a document (the “gold entities”) tend to form a semantically dense subset of the set of all candidate entities in the document. Geometrically speaking, when representing entities as vectors via some given embedding, the gold entities tend to lie in a low-rank subspace of the full embedding space. Eigenthemes identifies this subspace using the singular value decomposition and scores candidate entities according to their proximity to the subspace. On the empirical front, we introduce multiple strong baselines that compare favorably to (and sometimes even outperform) the existing state of the art. Extensive experiments on benchmark datasets from a variety of real-world domains showcase the effectiveness of our approach.</abstract>
      <url hash="e168162d">2021.emnlp-main.634</url>
      <bibkey>arora-etal-2021-low</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.634</doi>
      <pwccode url="https://github.com/epfl-dlab/eigenthemes" additional="false">epfl-dlab/eigenthemes</pwccode>
    </paper>
    <paper id="635">
      <title><fixed-case>TDEER</fixed-case>: An Efficient Translating Decoding Schema for Joint Extraction of Entities and Relations</title>
      <author><first>Xianming</first><last>Li</last></author>
      <author><first>Xiaotian</first><last>Luo</last></author>
      <author><first>Chenghao</first><last>Dong</last></author>
      <author><first>Daichuan</first><last>Yang</last></author>
      <author><first>Beidi</first><last>Luan</last></author>
      <author><first>Zhen</first><last>He</last></author>
      <pages>8055–8064</pages>
      <abstract>Joint extraction of entities and relations from unstructured texts to form factual triples is a fundamental task of constructing a Knowledge Base (KB). A common method is to decode triples by predicting entity pairs to obtain the corresponding relation. However, it is still challenging to handle this task efficiently, especially for the overlapping triple problem. To address such a problem, this paper proposes a novel efficient entities and relations extraction model called <b>TDEER</b>, which stands for <b>T</b>ranslating <b>D</b>ecoding Schema for Joint <b>E</b>xtraction of <b>E</b>ntities and <b>R</b>elations. Unlike the common approaches, the proposed translating decoding schema regards the relation as a translating operation from subject to objects, i.e., TDEER decodes triples as <tex-math>subject + relation \rightarrow objects</tex-math>. TDEER can naturally handle the overlapping triple problem, because the translating decoding schema can recognize all possible triples, including overlapping and non-overlapping triples. To enhance model robustness, we introduce negative samples to alleviate error accumulation at different stages. Extensive experiments on public datasets demonstrate that TDEER produces competitive results compared with the state-of-the-art (SOTA) baselines. Furthermore, the computation complexity analysis indicates that TDEER is more efficient than powerful baselines. Especially, the proposed TDEER is 2 times faster than the recent SOTA models. The code is available at <url>https://github.com/4AI/TDEER</url>.</abstract>
      <url hash="566e0c62">2021.emnlp-main.635</url>
      <bibkey>li-etal-2021-tdeer</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.635</doi>
      <video href="2021.emnlp-main.635.mp4"/>
      <pwccode url="https://github.com/4ai/tdeer" additional="false">4ai/tdeer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/nyt11-hrl">NYT11-HRL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webnlg">WebNLG</pwcdataset>
    </paper>
    <paper id="636">
      <title>Extracting Event Temporal Relations via Hyperbolic Geometry</title>
      <author><first>Xingwei</first><last>Tan</last></author>
      <author><first>Gabriele</first><last>Pergola</last></author>
      <author><first>Yulan</first><last>He</last></author>
      <pages>8065–8077</pages>
      <abstract>Detecting events and their evolution through time is a crucial task in natural language understanding. Recent neural approaches to event temporal relation extraction typically map events to embeddings in the Euclidean space and train a classifier to detect temporal relations between event pairs. However, embeddings in the Euclidean space cannot capture richer asymmetric relations such as event temporal relations. We thus propose to embed events into hyperbolic spaces, which are intrinsically oriented at modeling hierarchical structures. We introduce two approaches to encode events and their temporal relations in hyperbolic spaces. One approach leverages hyperbolic embeddings to directly infer event relations through simple geometrical operations. In the second one, we devise an end-to-end architecture composed of hyperbolic neural units tailored for the temporal relation extraction task. Thorough experimental assessments on widely used datasets have shown the benefits of revisiting the tasks on a different geometrical space, resulting in state-of-the-art performance on several standard metrics. Finally, the ablation study and several qualitative analyses highlighted the rich event semantics implicitly encoded into hyperbolic spaces.</abstract>
      <url hash="298bb25e">2021.emnlp-main.636</url>
      <bibkey>tan-etal-2021-extracting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.636</doi>
      <video href="2021.emnlp-main.636.mp4"/>
      <pwccode url="https://github.com/xingwei-warwick/hyper-event-temprel" additional="false">xingwei-warwick/hyper-event-temprel</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/tcr">TCR</pwcdataset>
    </paper>
    <paper id="637">
      <title>Honey or Poison? Solving the Trigger Curse in Few-shot Event Detection via Causal Intervention</title>
      <author><first>Jiawei</first><last>Chen</last></author>
      <author><first>Hongyu</first><last>Lin</last></author>
      <author><first>Xianpei</first><last>Han</last></author>
      <author><first>Le</first><last>Sun</last></author>
      <pages>8078–8088</pages>
      <abstract>Event detection has long been troubled by the trigger curse: overfitting the trigger will harm the generalization ability while underfitting it will hurt the detection performance. This problem is even more severe in few-shot scenario. In this paper, we identify and solve the trigger curse problem in few-shot event detection (FSED) from a causal view. By formulating FSED with a structural causal model (SCM), we found that the trigger is a confounder of the context and the result, which makes previous FSED methods much easier to overfit triggers. To resolve this problem, we propose to intervene on the context via backdoor adjustment during training. Experiments show that our method significantly improves the FSED on both ACE05 and MAVEN datasets.</abstract>
      <url hash="c64bb571">2021.emnlp-main.637</url>
      <bibkey>chen-etal-2021-honey</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.637</doi>
      <video href="2021.emnlp-main.637.mp4"/>
      <pwccode url="https://github.com/chen700564/causalfsed" additional="false">chen700564/causalfsed</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/maven">MAVEN</pwcdataset>
    </paper>
    <paper id="638">
      <title>Back to the Basics: A Quantitative Analysis of Statistical and Graph-Based Term Weighting Schemes for Keyword Extraction</title>
      <author><first>Asahi</first><last>Ushio</last></author>
      <author><first>Federico</first><last>Liberatore</last></author>
      <author><first>Jose</first><last>Camacho-Collados</last></author>
      <pages>8089–8103</pages>
      <abstract>Term weighting schemes are widely used in Natural Language Processing and Information Retrieval. In particular, term weighting is the basis for keyword extraction. However, there are relatively few evaluation studies that shed light about the strengths and shortcomings of each weighting scheme. In fact, in most cases researchers and practitioners resort to the well-known tf-idf as default, despite the existence of other suitable alternatives, including graph-based models. In this paper, we perform an exhaustive and large-scale empirical comparison of both statistical and graph-based term weighting methods in the context of keyword extraction. Our analysis reveals some interesting findings such as the advantages of the less-known lexical specificity with respect to tf-idf, or the qualitative differences between statistical and graph-based methods. Finally, based on our findings we discuss and devise some suggestions for practitioners. Source code to reproduce our experimental results, including a keyword extraction library, are available in the following repository: https://github.com/asahi417/kex</abstract>
      <url hash="5795ffdc">2021.emnlp-main.638</url>
      <bibkey>ushio-etal-2021-back</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.638</doi>
      <video href="2021.emnlp-main.638.mp4"/>
      <pwccode url="https://github.com/asahi417/kex" additional="false">asahi417/kex</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/semeval-2018-task-9-hypernym-discovery">SemEval-2018 Task 9: Hypernym Discovery</pwcdataset>
    </paper>
    <paper id="639">
      <title>Time-dependent Entity Embedding is not All You Need: A Re-evaluation of Temporal Knowledge Graph Completion Models under a Unified Framework</title>
      <author><first>Zhen</first><last>Han</last></author>
      <author><first>Gengyuan</first><last>Zhang</last></author>
      <author><first>Yunpu</first><last>Ma</last></author>
      <author><first>Volker</first><last>Tresp</last></author>
      <pages>8104–8118</pages>
      <abstract>Various temporal knowledge graph (KG) completion models have been proposed in the recent literature. The models usually contain two parts, a temporal embedding layer and a score function derived from existing static KG modeling approaches. Since the approaches differ along several dimensions, including different score functions and training strategies, the individual contributions of different temporal embedding techniques to model performance are not always clear. In this work, we systematically study six temporal embedding approaches and empirically quantify their performance across a wide range of configurations with about 3000 experiments and 13159 GPU hours. We classify the temporal embeddings into two classes: (1) timestamp embeddings and (2) time-dependent entity embeddings. Despite the common belief that the latter is more expressive, an extensive experimental study shows that timestamp embeddings can achieve on-par or even better performance with significantly fewer parameters. Moreover, we find that when trained appropriately, the relative performance differences between various temporal embeddings often shrink and sometimes even reverse when compared to prior results. For example, TTransE (CITATION), one of the first temporal KG models, can outperform more recent architectures on ICEWS datasets. To foster further research, we provide the first unified open-source framework for temporal KG completion models with full composability, where temporal embeddings, score functions, loss functions, regularizers, and the explicit modeling of reciprocal relations can be combined arbitrarily.</abstract>
      <url hash="2f6c17f9">2021.emnlp-main.639</url>
      <bibkey>han-etal-2021-time</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.639</doi>
      <video href="2021.emnlp-main.639.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/icews">ICEWS</pwcdataset>
    </paper>
    <paper id="640">
      <title>Matching-oriented Embedding Quantization For Ad-hoc Retrieval</title>
      <author><first>Shitao</first><last>Xiao</last></author>
      <author><first>Zheng</first><last>Liu</last></author>
      <author><first>Yingxia</first><last>Shao</last></author>
      <author><first>Defu</first><last>Lian</last></author>
      <author><first>Xing</first><last>Xie</last></author>
      <pages>8119–8129</pages>
      <abstract>Product quantization (PQ) is a widely used technique for ad-hoc retrieval. Recent studies propose supervised PQ, where the embedding and quantization models can be jointly trained with supervised learning. However, there is a lack of appropriate formulation of the joint training objective; thus, the improvements over previous non-supervised baselines are limited in reality. In this work, we propose the Matching-oriented Product Quantization (MoPQ), where a novel objective Multinoulli Contrastive Loss (MCL) is formulated. With the minimization of MCL, we are able to maximize the matching probability of query and ground-truth key, which contributes to the optimal retrieval accuracy. Given that the exact computation of MCL is intractable due to the demand of vast contrastive samples, we further propose the Differentiable Cross-device Sampling (DCS), which significantly augments the contrastive samples for precise approximation of MCL. We conduct extensive experimental studies on four real-world datasets, whose results verify the effectiveness of MoPQ. The code is available at https://github.com/microsoft/MoPQ.</abstract>
      <url hash="cbffa923">2021.emnlp-main.640</url>
      <bibkey>xiao-etal-2021-matching</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.640</doi>
      <video href="2021.emnlp-main.640.mp4"/>
      <pwccode url="https://github.com/microsoft/mopq" additional="false">microsoft/mopq</pwccode>
    </paper>
    <paper id="641">
      <title>Efficient Mind-Map Generation via Sequence-to-Graph and Reinforced Graph Refinement</title>
      <author><first>Mengting</first><last>Hu</last></author>
      <author><first>Honglei</first><last>Guo</last></author>
      <author><first>Shiwan</first><last>Zhao</last></author>
      <author><first>Hang</first><last>Gao</last></author>
      <author><first>Zhong</first><last>Su</last></author>
      <pages>8130–8141</pages>
      <abstract>A mind-map is a diagram that represents the central concept and key ideas in a hierarchical way. Converting plain text into a mind-map will reveal its key semantic structure and be easier to understand. Given a document, the existing automatic mind-map generation method extracts the relationships of every sentence pair to generate the directed semantic graph for this document. The computation complexity increases exponentially with the length of the document. Moreover, it is difficult to capture the overall semantics. To deal with the above challenges, we propose an efficient mind-map generation network that converts a document into a graph via sequence-to-graph. To guarantee a meaningful mind-map, we design a graph refinement module to adjust the relation graph in a reinforcement learning manner. Extensive experimental results demonstrate that the proposed approach is more effective and efficient than the existing methods. The inference time is reduced by thousands of times compared with the existing methods. The case studies verify that the generated mind-maps better reveal the underlying semantic structures of the document.</abstract>
      <url hash="48d46828">2021.emnlp-main.641</url>
      <bibkey>hu-etal-2021-efficient</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.641</doi>
      <video href="2021.emnlp-main.641.mp4"/>
    </paper>
    <paper id="642">
      <title>Deep Attention Diffusion Graph Neural Networks for Text Classification</title>
      <author><first>Yonghao</first><last>Liu</last></author>
      <author><first>Renchu</first><last>Guan</last></author>
      <author><first>Fausto</first><last>Giunchiglia</last></author>
      <author><first>Yanchun</first><last>Liang</last></author>
      <author><first>Xiaoyue</first><last>Feng</last></author>
      <pages>8142–8152</pages>
      <abstract>Text classification is a fundamental task with broad applications in natural language processing. Recently, graph neural networks (GNNs) have attracted much attention due to their powerful representation ability. However, most existing methods for text classification based on GNNs consider only one-hop neighborhoods and low-frequency information within texts, which cannot fully utilize the rich context information of documents. Moreover, these models suffer from over-smoothing issues if many graph layers are stacked. In this paper, a Deep Attention Diffusion Graph Neural Network (DADGNN) model is proposed to learn text representations, bridging the chasm of interaction difficulties between a word and its distant neighbors. Experimental results on various standard benchmark datasets demonstrate the superior performance of the present approach.</abstract>
      <url hash="cd2e10c7">2021.emnlp-main.642</url>
      <bibkey>liu-etal-2021-deep</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.642</doi>
      <video href="2021.emnlp-main.642.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="643">
      <title>Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution</title>
      <author><first>Yi</first><last>Huang</last></author>
      <author><first>Buse</first><last>Giledereli</last></author>
      <author><first>Abdullatif</first><last>Köksal</last></author>
      <author><first>Arzucan</first><last>Özgür</last></author>
      <author><first>Elif</first><last>Ozkirimli</last></author>
      <pages>8153–8161</pages>
      <abstract>Multi-label text classification is a challenging task because it requires capturing label dependencies. It becomes even more challenging when class distribution is long-tailed. Resampling and re-weighting are common approaches used for addressing the class imbalance problem, however, they are not effective when there is label dependency besides class imbalance because they result in oversampling of common labels. Here, we introduce the application of balancing loss functions for multi-label text classification. We perform experiments on a general domain dataset with 90 labels (Reuters-21578) and a domain-specific dataset from PubMed with 18211 labels. We find that a distribution-balanced loss function, which inherently addresses both the class imbalance and label linkage problems, outperforms commonly used loss functions. Distribution balancing methods have been successfully used in the image recognition field. Here, we show their effectiveness in natural language processing. Source code is available at https://github.com/blessu/BalancedLossNLP.</abstract>
      <url hash="d8f1174f">2021.emnlp-main.643</url>
      <bibkey>huang-etal-2021-balancing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.643</doi>
      <video href="2021.emnlp-main.643.mp4"/>
      <pwccode url="https://github.com/Roche/BalancedLossNLP" additional="true">Roche/BalancedLossNLP</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/reuters-21578">Reuters-21578</pwcdataset>
    </paper>
    <paper id="644">
      <title><fixed-case>B</fixed-case>ayesian Topic Regression for Causal Inference</title>
      <author><first>Maximilian</first><last>Ahrens</last></author>
      <author><first>Julian</first><last>Ashwin</last></author>
      <author><first>Jan-Peter</first><last>Calliess</last></author>
      <author><first>Vu</first><last>Nguyen</last></author>
      <pages>8162–8188</pages>
      <abstract>Causal inference using observational text data is becoming increasingly popular in many research areas. This paper presents the Bayesian Topic Regression (BTR) model that uses both text and numerical information to model an outcome variable. It allows estimation of both discrete and continuous treatment effects. Furthermore, it allows for the inclusion of additional numerical confounding factors next to text data. To this end, we combine a supervised Bayesian topic model with a Bayesian regression framework and perform supervised representation learning for the text features jointly with the regression parameter training, respecting the Frisch-Waugh-Lovell theorem. Our paper makes two main contributions. First, we provide a regression framework that allows causal inference in settings when both text and numerical confounders are of relevance. We show with synthetic and semi-synthetic datasets that our joint approach recovers ground truth with lower bias than any benchmark model, when text and numerical features are correlated. Second, experiments on two real-world datasets demonstrate that a joint and supervised learning strategy also yields superior prediction results compared to strategies that estimate regression weights for text and non-text features separately, being even competitive with more complex deep neural networks.</abstract>
      <url hash="be683200">2021.emnlp-main.644</url>
      <bibkey>ahrens-etal-2021-bayesian</bibkey>
      <revision id="1" href="2021.emnlp-main.644v1" hash="357e4b70"/>
      <revision id="2" href="2021.emnlp-main.644v2" hash="be683200" date="2021-11-29">Corrects a small typo in the text and in an unnumbered equation.</revision>
      <doi>10.18653/v1/2021.emnlp-main.644</doi>
      <video href="2021.emnlp-main.644.mp4"/>
      <pwccode url="https://github.com/maximilianahrens/data" additional="false">maximilianahrens/data</pwccode>
    </paper>
    <paper id="645">
      <title>Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience</title>
      <author><first>George</first><last>Chrysostomou</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <pages>8189–8200</pages>
      <abstract>Pretrained transformer-based models such as BERT have demonstrated state-of-the-art predictive performance when adapted into a range of natural language processing tasks. An open problem is how to improve the faithfulness of explanations (rationales) for the predictions of these models. In this paper, we hypothesize that salient information extracted a priori from the training data can complement the task-specific information learned by the model during fine-tuning on a downstream task. In this way, we aim to help BERT not to forget assigning importance to informative input tokens when making predictions by proposing SaLoss; an auxiliary loss function for guiding the multi-head attention mechanism during training to be close to salient information extracted a priori using TextRank. Experiments for explanation faithfulness across five datasets, show that models trained with SaLoss consistently provide more faithful explanations across four different feature attribution methods compared to vanilla BERT. Using the rationales extracted from vanilla BERT and SaLoss models to train inherently faithful classifiers, we further show that the latter result in higher predictive performance in downstream tasks.</abstract>
      <url hash="f606d6c1">2021.emnlp-main.645</url>
      <bibkey>chrysostomou-aletras-2021-enjoy</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.645</doi>
      <video href="2021.emnlp-main.645.mp4"/>
      <pwccode url="https://github.com/gchrysostomou/saloss" additional="false">gchrysostomou/saloss</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="646">
      <title><fixed-case>W</fixed-case>hat’s in Your Head? <fixed-case>E</fixed-case>mergent Behaviour in Multi-Task Transformer Models</title>
      <author><first>Mor</first><last>Geva</last></author>
      <author><first>Uri</first><last>Katz</last></author>
      <author><first>Aviv</first><last>Ben-Arie</last></author>
      <author><first>Jonathan</first><last>Berant</last></author>
      <pages>8201–8215</pages>
      <abstract>The primary paradigm for multi-task training in natural language processing is to represent the input with a shared pre-trained language model, and add a small, thin network (head) per task. Given an input, a target head is the head that is selected for outputting the final prediction. In this work, we examine the behaviour of non-target heads, that is, the output of heads when given input that belongs to a different task than the one they were trained for. We find that non-target heads exhibit emergent behaviour, which may either explain the target task, or generalize beyond their original task. For example, in a numerical reasoning task, a span extraction head extracts from the input the arguments to a computation that results in a number generated by a target generative head. In addition, a summarization head that is trained with a target question answering head, outputs query-based summaries when given a question and a context from which the answer is to be extracted. This emergent behaviour suggests that multi-task training leads to non-trivial extrapolation of skills, which can be harnessed for interpretability and generalization.</abstract>
      <url hash="d8f88eed">2021.emnlp-main.646</url>
      <bibkey>geva-etal-2021-whats</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.646</doi>
      <video href="2021.emnlp-main.646.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
    </paper>
    <paper id="647">
      <title>Don’t Search for a Search Method — Simple Heuristics Suffice for Adversarial Text Attacks</title>
      <author><first>Nathaniel</first><last>Berger</last></author>
      <author><first>Stefan</first><last>Riezler</last></author>
      <author><first>Sebastian</first><last>Ebert</last></author>
      <author><first>Artem</first><last>Sokolov</last></author>
      <pages>8216–8224</pages>
      <abstract>Recently more attention has been given to adversarial attacks on neural networks for natural language processing (NLP). A central research topic has been the investigation of search algorithms and search constraints, accompanied by benchmark algorithms and tasks. We implement an algorithm inspired by zeroth order optimization-based attacks and compare with the benchmark results in the TextAttack framework. Surprisingly, we find that optimization-based methods do not yield any improvement in a constrained setup and slightly benefit from approximate gradient information only in unconstrained setups where search spaces are larger. In contrast, simple heuristics exploiting nearest neighbors without querying the target function yield substantial success rates in constrained setups, and nearly full success rate in unconstrained setups, at an order of magnitude fewer queries. We conclude from these results that current TextAttack benchmark tasks are too easy and constraints are too strict, preventing meaningful research on black-box adversarial text attacks.</abstract>
      <url hash="103f9e6a">2021.emnlp-main.647</url>
      <bibkey>berger-etal-2021-dont</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.647</doi>
      <video href="2021.emnlp-main.647.mp4"/>
    </paper>
    <paper id="648">
      <title>Adversarial Attacks on Knowledge Graph Embeddings via Instance Attribution Methods</title>
      <author><first>Peru</first><last>Bhardwaj</last></author>
      <author><first>John</first><last>Kelleher</last></author>
      <author><first>Luca</first><last>Costabello</last></author>
      <author><first>Declan</first><last>O’Sullivan</last></author>
      <pages>8225–8239</pages>
      <abstract>Despite the widespread use of Knowledge Graph Embeddings (KGE), little is known about the security vulnerabilities that might disrupt their intended behaviour. We study data poisoning attacks against KGE models for link prediction. These attacks craft adversarial additions or deletions at training time to cause model failure at test time. To select adversarial deletions, we propose to use the model-agnostic instance attribution methods from Interpretable Machine Learning, which identify the training instances that are most influential to a neural model’s predictions on test instances. We use these influential triples as adversarial deletions. We further propose a heuristic method to replace one of the two entities in each influential triple to generate adversarial additions. Our experiments show that the proposed strategies outperform the state-of-art data poisoning attacks on KGE models and improve the MRR degradation due to the attacks by up to 62% over the baselines.</abstract>
      <url hash="96f19b38">2021.emnlp-main.648</url>
      <bibkey>bhardwaj-etal-2021-adversarial</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.648</doi>
      <video href="2021.emnlp-main.648.mp4"/>
      <pwccode url="https://github.com/perubhardwaj/attributionattack" additional="false">perubhardwaj/attributionattack</pwccode>
    </paper>
    <paper id="649">
      <title>Locke’s Holiday: Belief Bias in Machine Reading</title>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>8240–8245</pages>
      <abstract>I highlight a simple failure mode of state-of-the-art machine reading systems: when contexts do not align with commonly shared beliefs. For example, machine reading systems fail to answer <i>What did Elizabeth want?</i> correctly in the context of ‘My kingdom for a cough drop, cried Queen Elizabeth.’ Biased by co-occurrence statistics in the training data of pretrained language models, systems predict <i>my kingdom</i>, rather than <i>a cough drop</i>. I argue such biases are analogous to human belief biases and present a carefully designed challenge dataset for English machine reading, called Auto-Locke, to quantify such effects. Evaluations of machine reading systems on Auto-Locke show the pervasiveness of belief bias in machine reading.</abstract>
      <url hash="0494c636">2021.emnlp-main.649</url>
      <bibkey>sogaard-2021-lockes</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.649</doi>
      <video href="2021.emnlp-main.649.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
    </paper>
    <paper id="650">
      <title>Sequence Length is a Domain: Length-based Overfitting in Transformer Models</title>
      <author><first>Dusan</first><last>Varis</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <pages>8246–8257</pages>
      <abstract>Transformer-based sequence-to-sequence architectures, while achieving state-of-the-art results on a large number of NLP tasks, can still suffer from overfitting during training. In practice, this is usually countered either by applying regularization methods (e.g. dropout, L2-regularization) or by providing huge amounts of training data. Additionally, Transformer and other architectures are known to struggle when generating very long sequences. For example, in machine translation, the neural-based systems perform worse on very long sequences when compared to the preceding phrase-based translation approaches (Koehn and Knowles, 2017). We present results which suggest that the issue might also be in the mismatch between the length distributions of the training and validation data combined with the aforementioned tendency of the neural networks to overfit to the training data. We demonstrate on a simple string editing tasks and a machine translation task that the Transformer model performance drops significantly when facing sequences of length diverging from the length distribution in the training data. Additionally, we show that the observed drop in performance is due to the hypothesis length corresponding to the lengths seen by the model during training rather than the length of the input sequence.</abstract>
      <url hash="55a0b05d">2021.emnlp-main.650</url>
      <bibkey>varis-bojar-2021-sequence</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.650</doi>
      <video href="2021.emnlp-main.650.mp4"/>
    </paper>
    <paper id="651">
      <title>Contrasting Human- and Machine-Generated Word-Level Adversarial Examples for Text Classification</title>
      <author><first>Maximilian</first><last>Mozes</last></author>
      <author><first>Max</first><last>Bartolo</last></author>
      <author><first>Pontus</first><last>Stenetorp</last></author>
      <author><first>Bennett</first><last>Kleinberg</last></author>
      <author><first>Lewis</first><last>Griffin</last></author>
      <pages>8258–8270</pages>
      <abstract>Research shows that natural language processing models are generally considered to be vulnerable to adversarial attacks; but recent work has drawn attention to the issue of validating these adversarial inputs against certain criteria (e.g., the preservation of semantics and grammaticality). Enforcing constraints to uphold such criteria may render attacks unsuccessful, raising the question of whether valid attacks are actually feasible. In this work, we investigate this through the lens of human language ability. We report on crowdsourcing studies in which we task humans with iteratively modifying words in an input text, while receiving immediate model feedback, with the aim of causing a sentiment classification model to misclassify the example. Our findings suggest that humans are capable of generating a substantial amount of adversarial examples using semantics-preserving word substitutions. We analyze how human-generated adversarial examples compare to the recently proposed TextFooler, Genetic, BAE and SememePSO attack algorithms on the dimensions naturalness, preservation of sentiment, grammaticality and substitution rate. Our findings suggest that human-generated adversarial examples are not more able than the best algorithms to generate natural-reading, sentiment-preserving examples, though they do so by being much more computationally efficient.</abstract>
      <url hash="17cb1839">2021.emnlp-main.651</url>
      <bibkey>mozes-etal-2021-contrasting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.651</doi>
      <video href="2021.emnlp-main.651.mp4"/>
      <pwccode url="https://github.com/maximilianmozes/human_adversaries" additional="false">maximilianmozes/human_adversaries</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="652">
      <title>Is Information Density Uniform in Task-Oriented Dialogues?</title>
      <author><first>Mario</first><last>Giulianelli</last></author>
      <author><first>Arabella</first><last>Sinclair</last></author>
      <author><first>Raquel</first><last>Fernández</last></author>
      <pages>8271–8283</pages>
      <abstract>The Uniform Information Density principle states that speakers plan their utterances to reduce fluctuations in the density of the information transmitted. In this paper, we test whether, and within which contextual units this principle holds in task-oriented dialogues. We show that there is evidence supporting the principle in written dialogues where participants play a cooperative reference game as well as in spoken dialogues involving instruction giving and following. Our study underlines the importance of identifying the relevant contextual components, showing that information content increases particularly within topically and referentially related contextual units.</abstract>
      <url hash="4fc36c4b">2021.emnlp-main.652</url>
      <attachment type="Software" hash="de86f70c">2021.emnlp-main.652.Software.zip</attachment>
      <bibkey>giulianelli-etal-2021-information</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.652</doi>
      <video href="2021.emnlp-main.652.mp4"/>
      <pwccode url="https://github.com/dmg-illc/uid-dialogue" additional="false">dmg-illc/uid-dialogue</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/photobook">PhotoBook</pwcdataset>
    </paper>
    <paper id="653">
      <title>On Homophony and Rényi Entropy</title>
      <author><first>Tiago</first><last>Pimentel</last></author>
      <author><first>Clara</first><last>Meister</last></author>
      <author><first>Simone</first><last>Teufel</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <pages>8284–8293</pages>
      <abstract>Homophony’s widespread presence in natural languages is a controversial topic. Recent theories of language optimality have tried to justify its prevalence, despite its negative effects on cognitive processing time, e.g., Piantadosi et al. (2012) argued homophony enables the reuse of efficient wordforms and is thus beneficial for languages. This hypothesis has recently been challenged by Trott and Bergen (2020), who posit that good wordforms are more often homophonous simply because they are more phonotactically probable. In this paper, we join in on the debate. We first propose a new information-theoretic quantification of a language’s homophony: the sample Rényi entropy. Then, we use this quantification to revisit Trott and Bergen’s claims. While their point is theoretically sound, a specific methodological issue in their experiments raises doubts about their results. After addressing this issue, we find no clear pressure either towards or against homophony—a much more nuanced result than either Piantadosi et al.’s or Trott and Bergen’s findings.</abstract>
      <url hash="5ad1601b">2021.emnlp-main.653</url>
      <bibkey>pimentel-etal-2021-homophony</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.653</doi>
      <video href="2021.emnlp-main.653.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/celex">CELEX</pwcdataset>
    </paper>
    <paper id="654">
      <title>Synthetic Textual Features for the Large-Scale Detection of Basic-level Categories in <fixed-case>E</fixed-case>nglish and <fixed-case>M</fixed-case>andarin</title>
      <author><first>Yiwen</first><last>Chen</last></author>
      <author><first>Simone</first><last>Teufel</last></author>
      <pages>8294–8305</pages>
      <abstract>Basic-level categories (BLC) are an important psycholinguistic concept introduced by Rosch et al. (1976); they are defined as the most inclusive categories for which a concrete mental image of the category as a whole can be formed, and also as those categories which are acquired early in life. Rosch’s original algorithm for detecting BLC (called cue-validity) is based on the availability of semantic features such as “has tail” for “cat”, and has remained untested at large. An at-scale algorithm for the automatic determination of BLC exists, but it operates without Rosch-style semantic features, and is thus unable to verify Rosch’s hypothesis. We present the first method for the detection of BLC at scale that makes use of Rosch-style semantic features. For both English and Mandarin, we test three methods of generating such features for any synset within Wordnet (WN): extraction of textual features from Wikipedia pages, Distributional Memory (DM) and BART. The best of our methods outperforms the current SoA in BLC detection, with an accuracy of English BLC detection of 75.0%, and of Mandarin BLC detection 80.7% on a test set. When applied to all of WordNet, our model predicts that 1,118 synsets in English Wordnet (1.4%) are BLC, far fewer than existing methods, and with a precision improvement of over 200% over these. As well as confirming the usefulness of Rosch’s cue validity algorithm, we also developed and evaluated our own new indicator for BLC, which models the fact that BLC features tend to be BLC themselves.</abstract>
      <url hash="1a55c0cd">2021.emnlp-main.654</url>
      <bibkey>chen-teufel-2021-synthetic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.654</doi>
      <video href="2021.emnlp-main.654.mp4"/>
    </paper>
    <paper id="655">
      <title><fixed-case>T</fixed-case>ime<fixed-case>T</fixed-case>raveler: Reinforcement Learning for Temporal Knowledge Graph Forecasting</title>
      <author><first>Haohai</first><last>Sun</last></author>
      <author><first>Jialun</first><last>Zhong</last></author>
      <author><first>Yunpu</first><last>Ma</last></author>
      <author><first>Zhen</first><last>Han</last></author>
      <author><first>Kun</first><last>He</last></author>
      <pages>8306–8319</pages>
      <abstract>Temporal knowledge graph (TKG) reasoning is a crucial task that has gained increasing research interest in recent years. Most existing methods focus on reasoning at past timestamps to complete the missing facts, and there are only a few works of reasoning on known TKGs to forecast future facts. Compared with the completion task, the forecasting task is more difficult that faces two main challenges: (1) how to effectively model the time information to handle future timestamps? (2) how to make inductive inference to handle previously unseen entities that emerge over time? To address these challenges, we propose the first reinforcement learning method for forecasting. Specifically, the agent travels on historical knowledge graph snapshots to search for the answer. Our method defines a relative time encoding function to capture the timespan information, and we design a novel time-shaped reward based on Dirichlet distribution to guide the model learning. Furthermore, we propose a novel representation method for unseen entities to improve the inductive inference ability of the model. We evaluate our method for this link prediction task at future timestamps. Extensive experiments on four benchmark datasets demonstrate substantial performance improvement meanwhile with higher explainability, less calculation, and fewer parameters when compared with existing state-of-the-art methods.</abstract>
      <url hash="825c6aee">2021.emnlp-main.655</url>
      <attachment type="Software" hash="cd55d0a1">2021.emnlp-main.655.Software.zip</attachment>
      <bibkey>sun-etal-2021-timetraveler</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.655</doi>
      <video href="2021.emnlp-main.655.mp4"/>
      <pwccode url="https://github.com/jhl-hust/titer" additional="false">jhl-hust/titer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/icews">ICEWS</pwcdataset>
    </paper>
    <paper id="656">
      <title>Code-switched inspired losses for spoken dialog representations</title>
      <author><first>Pierre</first><last>Colombo</last></author>
      <author><first>Emile</first><last>Chapuis</last></author>
      <author><first>Matthieu</first><last>Labeau</last></author>
      <author><first>Chloé</first><last>Clavel</last></author>
      <pages>8320–8337</pages>
      <abstract>Spoken dialogue systems need to be able to handle both multiple languages and multilinguality inside a conversation (<i>e.g</i> in case of code-switching). In this work, we introduce new pretraining losses tailored to learn generic multilingual spoken dialogue representations. The goal of these losses is to expose the model to code-switched language. In order to scale up training, we automatically build a pretraining corpus composed of multilingual conversations in five different languages (French, Italian, English, German and Spanish) from OpenSubtitles, a huge multilingual corpus composed of 24.3G tokens. We test the generic representations on MIAM, a new benchmark composed of five dialogue act corpora on the same aforementioned languages as well as on two novel multilingual tasks (<i>i.e</i> multilingual mask utterance retrieval and multilingual inconsistency identification). Our experiments show that our new losses achieve a better performance in both monolingual and multilingual settings.</abstract>
      <url hash="c4853697">2021.emnlp-main.656</url>
      <bibkey>colombo-etal-2021-code</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.656</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
    </paper>
    <paper id="657">
      <title><fixed-case>BiQUE</fixed-case>: <fixed-case>B</fixed-case>iquaternionic Embeddings of Knowledge Graphs</title>
      <author><first>Jia</first><last>Guo</last></author>
      <author><first>Stanley</first><last>Kok</last></author>
      <pages>8338–8351</pages>
      <abstract>Knowledge graph embeddings (KGEs) compactly encode multi-relational knowledge graphs (KGs). Existing KGE models rely on geometric operations to model relational patterns. Euclidean (circular) rotation is useful for modeling patterns such as symmetry, but cannot represent hierarchical semantics. In contrast, hyperbolic models are effective at modeling hierarchical relations, but do not perform as well on patterns on which circular rotation excels. It is crucial for KGE models to unify multiple geometric transformations so as to fully cover the multifarious relations in KGs. To do so, we propose BiQUE, a novel model that employs <i>biquaternions</i> to integrate multiple geometric transformations, viz., scaling, translation, Euclidean rotation, and hyperbolic rotation. BiQUE makes the best trade-offs among geometric operators during training, picking the best one (or their best combination) for each relation. Experiments on five datasets show BiQUE’s effectiveness.</abstract>
      <url hash="8afd7baa">2021.emnlp-main.657</url>
      <attachment type="Software" hash="27dc4f2d">2021.emnlp-main.657.Software.zip</attachment>
      <bibkey>guo-kok-2021-bique</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.657</doi>
      <video href="2021.emnlp-main.657.mp4"/>
      <pwccode url="https://github.com/guojiapub/bique" additional="false">guojiapub/bique</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k-237">FB15k-237</pwcdataset>
    </paper>
    <paper id="658">
      <title>Learning Neural Ordinary Equations for Forecasting Future Links on Temporal Knowledge Graphs</title>
      <author><first>Zhen</first><last>Han</last></author>
      <author><first>Zifeng</first><last>Ding</last></author>
      <author><first>Yunpu</first><last>Ma</last></author>
      <author><first>Yujia</first><last>Gu</last></author>
      <author><first>Volker</first><last>Tresp</last></author>
      <pages>8352–8364</pages>
      <abstract>There has been an increasing interest in inferring future links on temporal knowledge graphs (KG). While links on temporal KGs vary continuously over time, the existing approaches model the temporal KGs in discrete state spaces. To this end, we propose a novel continuum model by extending the idea of neural ordinary differential equations (ODEs) to multi-relational graph convolutional networks. The proposed model preserves the continuous nature of dynamic multi-relational graph data and encodes both temporal and structural information into continuous-time dynamic embeddings. In addition, a novel graph transition layer is applied to capture the transitions on the dynamic graph, i.e., edge formation and dissolution. We perform extensive experiments on five benchmark datasets for temporal KG reasoning, showing our model’s superior performance on the future link forecasting task.</abstract>
      <url hash="0bcf4aaf">2021.emnlp-main.658</url>
      <bibkey>han-etal-2021-learning-neural</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.658</doi>
      <video href="2021.emnlp-main.658.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/icews">ICEWS</pwcdataset>
    </paper>
    <paper id="659">
      <title><fixed-case>RAP</fixed-case>: <fixed-case>R</fixed-case>obustness-<fixed-case>A</fixed-case>ware <fixed-case>P</fixed-case>erturbations for Defending against Backdoor Attacks on <fixed-case>NLP</fixed-case> Models</title>
      <author><first>Wenkai</first><last>Yang</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <pages>8365–8381</pages>
      <abstract>Backdoor attacks, which maliciously control a well-trained model’s outputs of the instances with specific triggers, are recently shown to be serious threats to the safety of reusing deep neural networks (DNNs). In this work, we propose an efficient online defense mechanism based on robustness-aware perturbations. Specifically, by analyzing the backdoor training process, we point out that there exists a big gap of robustness between poisoned and clean samples. Motivated by this observation, we construct a word-based robustness-aware perturbation to distinguish poisoned samples from clean samples to defend against the backdoor attacks on natural language processing (NLP) models. Moreover, we give a theoretical analysis about the feasibility of our robustness-aware perturbation-based defense method. Experimental results on sentiment analysis and toxic detection tasks show that our method achieves better defending performance and much lower computational costs than existing online defense methods. Our code is available at https://github.com/lancopku/RAP.</abstract>
      <url hash="7cd6e2cd">2021.emnlp-main.659</url>
      <bibkey>yang-etal-2021-rap</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.659</doi>
      <video href="2021.emnlp-main.659.mp4"/>
      <pwccode url="https://github.com/lancopku/rap" additional="false">lancopku/rap</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
    </paper>
    <paper id="660">
      <title><fixed-case>FAME</fixed-case>: <fixed-case>F</fixed-case>eature-Based Adversarial Meta-Embeddings for Robust Input Representations</title>
      <author><first>Lukas</first><last>Lange</last></author>
      <author><first>Heike</first><last>Adel</last></author>
      <author><first>Jannik</first><last>Strötgen</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <pages>8382–8395</pages>
      <abstract>Combining several embeddings typically improves performance in downstream tasks as different embeddings encode different information. It has been shown that even models using embeddings from transformers still benefit from the inclusion of standard word embeddings. However, the combination of embeddings of different types and dimensions is challenging. As an alternative to attention-based meta-embeddings, we propose feature-based adversarial meta-embeddings (FAME) with an attention function that is guided by features reflecting word-specific properties, such as shape and frequency, and show that this is beneficial to handle subword-based embeddings. In addition, FAME uses adversarial training to optimize the mappings of differently-sized embeddings to the same space. We demonstrate that FAME works effectively across languages and domains for sequence labeling and sentence classification, in particular in low-resource settings. FAME sets the new state of the art for POS tagging in 27 languages, various NER settings and question classification in different domains.</abstract>
      <url hash="87fcf6fe">2021.emnlp-main.660</url>
      <bibkey>lange-etal-2021-fame</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.660</doi>
      <video href="2021.emnlp-main.660.mp4"/>
      <pwccode url="https://github.com/boschresearch/adversarial_meta_embeddings" additional="false">boschresearch/adversarial_meta_embeddings</pwccode>
    </paper>
    <paper id="661">
      <title>A Strong Baseline for Query Efficient Attacks in a Black Box Setting</title>
      <author><first>Rishabh</first><last>Maheshwary</last></author>
      <author><first>Saket</first><last>Maheshwary</last></author>
      <author><first>Vikram</first><last>Pudi</last></author>
      <pages>8396–8409</pages>
      <abstract>Existing black box search methods have achieved high success rate in generating adversarial attacks against NLP models. However, such search methods are inefficient as they do not consider the amount of queries required to generate adversarial attacks. Also, prior attacks do not maintain a consistent search space while comparing different search methods. In this paper, we propose a query efficient attack strategy to generate plausible adversarial examples on text classification and entailment tasks. Our attack jointly leverages attention mechanism and locality sensitive hashing (LSH) to reduce the query count. We demonstrate the efficacy of our approach by comparing our attack with four baselines across three different search spaces. Further, we benchmark our results across the same search space used in prior attacks. In comparison to attacks proposed, on an average, we are able to reduce the query count by 75% across all datasets and target models. We also demonstrate that our attack achieves a higher success rate when compared to prior attacks in a limited query setting.</abstract>
      <url hash="950f6912">2021.emnlp-main.661</url>
      <bibkey>maheshwary-etal-2021-strong</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.661</doi>
      <video href="2021.emnlp-main.661.mp4"/>
      <pwccode url="https://github.com/rishabhmaheshwary/query-attack" additional="false">rishabhmaheshwary/query-attack</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="662">
      <title>Machine Translation Decoding beyond Beam Search</title>
      <author><first>Rémi</first><last>Leblond</last></author>
      <author><first>Jean-Baptiste</first><last>Alayrac</last></author>
      <author><first>Laurent</first><last>Sifre</last></author>
      <author><first>Miruna</first><last>Pislar</last></author>
      <author><first>Lespiau</first><last>Jean-Baptiste</last></author>
      <author><first>Ioannis</first><last>Antonoglou</last></author>
      <author><first>Karen</first><last>Simonyan</last></author>
      <author><first>Oriol</first><last>Vinyals</last></author>
      <pages>8410–8434</pages>
      <abstract>Beam search is the go-to method for decoding auto-regressive machine translation models. While it yields consistent improvements in terms of BLEU, it is only concerned with finding outputs with high model likelihood, and is thus agnostic to whatever end metric or score practitioners care about. Our aim is to establish whether beam search can be replaced by a more powerful metric-driven search technique. To this end, we explore numerous decoding algorithms, including some which rely on a value function parameterised by a neural network, and report results on a variety of metrics. Notably, we introduce a Monte-Carlo Tree Search (MCTS) based method and showcase its competitiveness. We provide a blueprint for how to use MCTS fruitfully in language applications, which opens promising future directions. We find that which algorithm is best heavily depends on the characteristics of the goal metric; we believe that our extensive experiments and analysis will inform further research in this area.</abstract>
      <url hash="8c4a01cc">2021.emnlp-main.662</url>
      <bibkey>leblond-etal-2021-machine</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.662</doi>
      <video href="2021.emnlp-main.662.mp4"/>
    </paper>
    <paper id="663">
      <title>Document Graph for Neural Machine Translation</title>
      <author><first>Mingzhou</first><last>Xu</last></author>
      <author><first>Liangyou</first><last>Li</last></author>
      <author><first>Derek F.</first><last>Wong</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <author><first>Lidia S.</first><last>Chao</last></author>
      <pages>8435–8448</pages>
      <abstract>Previous works have shown that contextual information can improve the performance of neural machine translation (NMT). However, most existing document-level NMT methods failed to leverage contexts beyond a few set of previous sentences. How to make use of the whole document as global contexts is still a challenge. To address this issue, we hypothesize that a document can be represented as a graph that connects relevant contexts regardless of their distances. We employ several types of relations, including adjacency, syntactic dependency, lexical consistency, and coreference, to construct the document graph. Then, we incorporate both source and target graphs into the conventional Transformer architecture with graph convolutional networks. Experiments on various NMT benchmarks, including IWSLT English–French, Chinese-English, WMT English–German and Opensubtitle English–Russian, demonstrate that using document graphs can significantly improve the translation quality. Extensive analysis verifies that the document graph is beneficial for capturing discourse phenomena.</abstract>
      <url hash="77613875">2021.emnlp-main.663</url>
      <bibkey>xu-etal-2021-document-graph</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.663</doi>
      <video href="2021.emnlp-main.663.mp4"/>
    </paper>
    <paper id="664">
      <title>An Empirical Investigation of Word Alignment Supervision for Zero-Shot Multilingual Neural Machine Translation</title>
      <author><first>Alessandro</first><last>Raganato</last></author>
      <author><first>Raúl</first><last>Vázquez</last></author>
      <author><first>Mathias</first><last>Creutz</last></author>
      <author><first>Jörg</first><last>Tiedemann</last></author>
      <pages>8449–8456</pages>
      <abstract>Zero-shot translations is a fascinating feature of Multilingual Neural Machine Translation (MNMT) systems. These MNMT models are usually trained on English-centric data, i.e. English either as the source or target language, and with a language label prepended to the input indicating the target language. However, recent work has highlighted several flaws of these models in zero-shot scenarios where language labels are ignored and the wrong language is generated or different runs show highly unstable results. In this paper, we investigate the benefits of an explicit alignment to language labels in Transformer-based MNMT models in the zero-shot context, by jointly training one cross attention head with word alignment supervision to stress the focus on the target language label. We compare and evaluate several MNMT systems on three multilingual MT benchmarks of different sizes, showing that simply supervising one cross attention head to focus both on word alignments and language labels reduces the bias towards translating into the wrong language, improving the zero-shot performance overall. Moreover, as an additional advantage, we find that our alignment supervision leads to more stable results across different training runs.</abstract>
      <url hash="2955ecdc">2021.emnlp-main.664</url>
      <bibkey>raganato-etal-2021-empirical</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.664</doi>
      <video href="2021.emnlp-main.664.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/opus-100">OPUS-100</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2018">WMT 2018</pwcdataset>
    </paper>
    <paper id="665">
      <title>Graph Algorithms for Multiparallel Word Alignment</title>
      <author><first>Ayyoob</first><last>ImaniGooghari</last></author>
      <author><first>Masoud</first><last>Jalili Sabet</last></author>
      <author><first>Lutfi Kerem</first><last>Senel</last></author>
      <author><first>Philipp</first><last>Dufter</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>8457–8469</pages>
      <abstract>With the advent of end-to-end deep learning approaches in machine translation, interest in word alignments initially decreased; however, they have again become a focus of research more recently. Alignments are useful for typological research, transferring formatting like markup to translated texts, and can be used in the decoding of machine translation systems. At the same time, massively multilingual processing is becoming an important NLP scenario, and pretrained language and machine translation models that are truly multilingual are proposed. However, most alignment algorithms rely on bitexts only and do not leverage the fact that many parallel corpora are multiparallel. In this work, we exploit the multiparallelity of corpora by representing an initial set of bilingual alignments as a graph and then predicting additional edges in the graph. We present two graph algorithms for edge prediction: one inspired by recommender systems and one based on network link prediction. Our experimental results show absolute improvements in F1 of up to 28% over the baseline bilingual word aligner in different datasets.</abstract>
      <url hash="96387566">2021.emnlp-main.665</url>
      <bibkey>imanigooghari-etal-2021-graph</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.665</doi>
      <video href="2021.emnlp-main.665.mp4"/>
      <pwccode url="https://github.com/cisnlp/graph-align" additional="false">cisnlp/graph-align</pwccode>
    </paper>
    <paper id="666">
      <title>Improving the Quality Trade-Off for Neural Machine Translation Multi-Domain Adaptation</title>
      <author><first>Eva</first><last>Hasler</last></author>
      <author><first>Tobias</first><last>Domhan</last></author>
      <author><first>Jonay</first><last>Trenous</last></author>
      <author><first>Ke</first><last>Tran</last></author>
      <author id="bill-byrne"><first>Bill</first><last>Byrne</last></author>
      <author><first>Felix</first><last>Hieber</last></author>
      <pages>8470–8477</pages>
      <abstract>Building neural machine translation systems to perform well on a specific target domain is a well-studied problem. Optimizing system performance for multiple, diverse target domains however remains a challenge. We study this problem in an adaptation setting where the goal is to preserve the existing system quality while incorporating data for domains that were not the focus of the original translation system. We find that we can improve over the performance trade-off offered by Elastic Weight Consolidation with a relatively simple data mixing strategy. At comparable performance on the new domains, catastrophic forgetting is mitigated significantly on strong WMT baselines. Combining both approaches improves the Pareto frontier on this task.</abstract>
      <url hash="52d871ba">2021.emnlp-main.666</url>
      <bibkey>hasler-etal-2021-improving</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.666</doi>
      <video href="2021.emnlp-main.666.mp4"/>
    </paper>
    <paper id="667">
      <title>Language Modeling, Lexical Translation, Reordering: The Training Process of <fixed-case>NMT</fixed-case> through the Lens of Classical <fixed-case>SMT</fixed-case></title>
      <author><first>Elena</first><last>Voita</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <pages>8478–8491</pages>
      <abstract>Differently from the traditional statistical MT that decomposes the translation task into distinct separately learned components, neural machine translation uses a single neural network to model the entire translation process. Despite neural machine translation being de-facto standard, it is still not clear how NMT models acquire different competences over the course of training, and how this mirrors the different models in traditional SMT. In this work, we look at the competences related to three core SMT components and find that during training, NMT first focuses on learning target-side language modeling, then improves translation quality approaching word-by-word translation, and finally learns more complicated reordering patterns. We show that this behavior holds for several models and language pairs. Additionally, we explain how such an understanding of the training process can be useful in practice and, as an example, show how it can be used to improve vanilla non-autoregressive neural machine translation by guiding teacher model selection.</abstract>
      <url hash="c13d7926">2021.emnlp-main.667</url>
      <bibkey>voita-etal-2021-language</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.667</doi>
      <video href="2021.emnlp-main.667.mp4"/>
    </paper>
    <paper id="668">
      <title>Effective Fine-Tuning Methods for Cross-lingual Adaptation</title>
      <author><first>Tao</first><last>Yu</last></author>
      <author><first>Shafiq</first><last>Joty</last></author>
      <pages>8492–8501</pages>
      <abstract>Large scale multilingual pre-trained language models have shown promising results in zero- and few-shot cross-lingual tasks. However, recent studies have shown their lack of generalizability when the languages are structurally dissimilar. In this work, we propose a novel fine-tuning method based on co-training that aims to learn more generalized semantic equivalences as a complementary to multilingual language modeling using the unlabeled data in the target language. We also propose an adaption method based on contrastive learning to better capture the semantic relationship in the parallel data, when a few translation pairs are available. To show our method’s effectiveness, we conduct extensive experiments on cross-lingual inference and review classification tasks across various languages. We report significant gains compared to directly fine-tuning multilingual pre-trained models and other semi-supervised alternatives.</abstract>
      <url hash="ee99e4db">2021.emnlp-main.668</url>
      <bibkey>yu-joty-2021-effective</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.668</doi>
      <video href="2021.emnlp-main.668.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="669">
      <title>Rethinking Data Augmentation for Low-Resource Neural Machine Translation: A Multi-Task Learning Approach</title>
      <author><first>Víctor M.</first><last>Sánchez-Cartagena</last></author>
      <author><first>Miquel</first><last>Esplà-Gomis</last></author>
      <author><first>Juan Antonio</first><last>Pérez-Ortiz</last></author>
      <author><first>Felipe</first><last>Sánchez-Martínez</last></author>
      <pages>8502–8516</pages>
      <abstract>In the context of neural machine translation, data augmentation (DA) techniques may be used for generating additional training samples when the available parallel data are scarce. Many DA approaches aim at expanding the support of the empirical data distribution by generating new sentence pairs that contain infrequent words, thus making it closer to the true data distribution of parallel sentences. In this paper, we propose to follow a completely different approach and present a multi-task DA approach in which we generate new sentence pairs with transformations, such as reversing the order of the target sentence, which produce unfluent target sentences. During training, these augmented sentences are used as auxiliary tasks in a multi-task framework with the aim of providing new contexts where the target prefix is not informative enough to predict the next word. This strengthens the encoder and forces the decoder to pay more attention to the source representations of the encoder. Experiments carried out on six low-resource translation tasks show consistent improvements over the baseline and over DA methods aiming at extending the support of the empirical data distribution. The systems trained with our approach rely more on the source tokens, are more robust against domain shift and suffer less hallucinations.</abstract>
      <url hash="dc420392">2021.emnlp-main.669</url>
      <bibkey>sanchez-cartagena-etal-2021-rethinking</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.669</doi>
      <video href="2021.emnlp-main.669.mp4"/>
      <pwccode url="https://github.com/transducens/mtl-da-emnlp" additional="false">transducens/mtl-da-emnlp</pwccode>
    </paper>
    <paper id="670">
      <title>Wino-<fixed-case>X</fixed-case>: Multilingual <fixed-case>W</fixed-case>inograd Schemas for Commonsense Reasoning and Coreference Resolution</title>
      <author><first>Denis</first><last>Emelin</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <pages>8517–8532</pages>
      <abstract>Winograd schemas are a well-established tool for evaluating coreference resolution (CoR) and commonsense reasoning (CSR) capabilities of computational models. So far, schemas remained largely confined to English, limiting their utility in multilingual settings. This work presents Wino-X, a parallel dataset of German, French, and Russian schemas, aligned with their English counterparts. We use this resource to investigate whether neural machine translation (NMT) models can perform CoR that requires commonsense knowledge and whether multilingual language models (MLLMs) are capable of CSR across multiple languages. Our findings show Wino-X to be exceptionally challenging for NMT systems that are prone to undesirable biases and unable to detect disambiguating information. We quantify biases using established statistical methods and define ways to address both of these issues. We furthermore present evidence of active cross-lingual knowledge transfer in MLLMs, whereby fine-tuning models on English schemas yields CSR improvements in other languages.</abstract>
      <url hash="8c685c97">2021.emnlp-main.670</url>
      <attachment type="Software" hash="837e5225">2021.emnlp-main.670.Software.zip</attachment>
      <bibkey>emelin-sennrich-2021-wino</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.670</doi>
      <video href="2021.emnlp-main.670.mp4"/>
      <pwccode url="https://github.com/demelin/wino-x" additional="false">demelin/wino-x</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/winogrande">WinoGrande</pwcdataset>
    </paper>
    <paper id="671">
      <title>One Source, Two Targets: <fixed-case>C</fixed-case>hallenges and Rewards of Dual Decoding</title>
      <author><first>Jitao</first><last>Xu</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <pages>8533–8546</pages>
      <abstract>Machine translation is generally understood as generating one target text from an input source document. In this paper, we consider a stronger requirement: to jointly generate two texts so that each output side effectively depends on the other. As we discuss, such a device serves several practical purposes, from multi-target machine translation to the generation of controlled variations of the target text. We present an analysis of possible implementations of dual decoding, and experiment with four applications. Viewing the problem from multiple angles allows us to better highlight the challenges of dual decoding and to also thoroughly analyze the benefits of generating matched, rather than independent, translations.</abstract>
      <url hash="20e34fa8">2021.emnlp-main.671</url>
      <bibkey>xu-yvon-2021-one</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.671</doi>
      <video href="2021.emnlp-main.671.mp4"/>
      <pwccode url="https://github.com/jitao-xu/dual-decoding" additional="false">jitao-xu/dual-decoding</pwccode>
    </paper>
    <paper id="672">
      <title>Discrete and Soft Prompting for Multilingual Models</title>
      <author><first>Mengjie</first><last>Zhao</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>8547–8555</pages>
      <abstract>It has been shown for English that discrete and soft prompting perform strongly in few-shot learning with pretrained language models (PLMs). In this paper, we show that discrete and soft prompting perform better than finetuning in multilingual cases: Crosslingual transfer and in-language training of multilingual natural language inference. For example, with 48 English training examples, finetuning obtains 33.74% accuracy in crosslingual transfer, barely surpassing the majority baseline (33.33%). In contrast, discrete and soft prompting outperform finetuning, achieving 36.43% and 38.79%. We also demonstrate good performance of prompting with training data in multiple languages other than English.</abstract>
      <url hash="ab628c18">2021.emnlp-main.672</url>
      <bibkey>zhao-schutze-2021-discrete</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.672</doi>
      <video href="2021.emnlp-main.672.mp4"/>
      <pwccode url="https://github.com/mprompting/xlmrprompt" additional="false">mprompting/xlmrprompt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="673">
      <title>Vision Matters When It Should: Sanity Checking Multimodal Machine Translation Models</title>
      <author><first>Jiaoda</first><last>Li</last></author>
      <author><first>Duygu</first><last>Ataman</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <pages>8556–8562</pages>
      <abstract>Multimodal machine translation (MMT) systems have been shown to outperform their text-only neural machine translation (NMT) counterparts when visual context is available. However, recent studies have also shown that the performance of MMT models is only marginally impacted when the associated image is replaced with an unrelated image or noise, which suggests that the visual context might not be exploited by the model at all. We hypothesize that this might be caused by the nature of the commonly used evaluation benchmark, also known as Multi30K, where the translations of image captions were prepared without actually showing the images to human translators. In this paper, we present a qualitative study that examines the role of datasets in stimulating the leverage of visual modality and we propose methods to highlight the importance of visual signals in the datasets which demonstrate improvements in reliance of models on the source images. Our findings suggest the research on effective MMT architectures is currently impaired by the lack of suitable datasets and careful consideration must be taken in creation of future MMT datasets, for which we also provide useful insights.</abstract>
      <url hash="5e6e1c92">2021.emnlp-main.673</url>
      <bibkey>li-etal-2021-vision</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.673</doi>
      <video href="2021.emnlp-main.673.mp4"/>
      <pwccode url="https://github.com/jiaodali/vision-matters-when-it-should" additional="false">jiaodali/vision-matters-when-it-should</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptual-captions">Conceptual Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multisense">MultiSense</pwcdataset>
    </paper>
    <paper id="674">
      <title>Efficient Inference for Multilingual Neural Machine Translation</title>
      <author><first>Alexandre</first><last>Berard</last></author>
      <author><first>Dain</first><last>Lee</last></author>
      <author><first>Stephane</first><last>Clinchant</last></author>
      <author><first>Kweonwoo</first><last>Jung</last></author>
      <author><first>Vassilina</first><last>Nikoulina</last></author>
      <pages>8563–8583</pages>
      <abstract>Multilingual NMT has become an attractive solution for MT deployment in production. But to match bilingual quality, it comes at the cost of larger and slower models. In this work, we consider several ways to make multilingual NMT faster at inference without degrading its quality. We experiment with several “light decoder” architectures in two 20-language multi-parallel settings: small-scale on TED Talks and large-scale on ParaCrawl. Our experiments demonstrate that combining a shallow decoder with vocabulary filtering leads to almost 2 times faster inference with no loss in translation quality. We validate our findings with BLEU and chrF (on 380 language pairs), robustness evaluation and human evaluation.</abstract>
      <url hash="9f3a7029">2021.emnlp-main.674</url>
      <bibkey>berard-etal-2021-efficient</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.674</doi>
      <video href="2021.emnlp-main.674.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/paracrawl">ParaCrawl</pwcdataset>
    </paper>
    <paper id="675">
      <title>Role of <fixed-case>L</fixed-case>anguage <fixed-case>R</fixed-case>elatedness in <fixed-case>M</fixed-case>ultilingual <fixed-case>F</fixed-case>ine-tuning of <fixed-case>L</fixed-case>anguage <fixed-case>M</fixed-case>odels: <fixed-case>A</fixed-case> <fixed-case>C</fixed-case>ase <fixed-case>S</fixed-case>tudy in <fixed-case>I</fixed-case>ndo-<fixed-case>A</fixed-case>ryan <fixed-case>L</fixed-case>anguages</title>
      <author><first>Tejas</first><last>Dhamecha</last></author>
      <author><first>Rudra</first><last>Murthy</last></author>
      <author><first>Samarth</first><last>Bharadwaj</last></author>
      <author><first>Karthik</first><last>Sankaranarayanan</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>8584–8595</pages>
      <abstract>We explore the impact of leveraging the relatedness of languages that belong to the same family in NLP models using multilingual fine-tuning. We hypothesize and validate that multilingual fine-tuning of pre-trained language models can yield better performance on downstream NLP applications, compared to models fine-tuned on individual languages. A first of its kind detailed study is presented to track performance change as languages are added to a base language in a graded and greedy (in the sense of best boost of performance) manner; which reveals that careful selection of subset of related languages can significantly improve performance than utilizing all related languages. The Indo-Aryan (IA) language family is chosen for the study, the exact languages being Bengali, Gujarati, Hindi, Marathi, Oriya, Punjabi and Urdu. The script barrier is crossed by simple rule-based transliteration of the text of all languages to Devanagari. Experiments are performed on mBERT, IndicBERT, MuRIL and two RoBERTa-based LMs, the last two being pre-trained by us. Low resource languages, such as Oriya and Punjabi, are found to be the largest beneficiaries of multilingual fine-tuning. Textual Entailment, Entity Classification, Section Title Prediction, tasks of IndicGLUE and POS tagging form our test bed. Compared to monolingual fine tuning we get relative performance improvement of up to 150% in the downstream tasks. The surprise take-away is that for any language there is a particular combination of other languages which yields the best performance, and any additional language is in fact detrimental.</abstract>
      <url hash="bb21fcbb">2021.emnlp-main.675</url>
      <attachment type="Software" hash="ab82f874">2021.emnlp-main.675.Software.zip</attachment>
      <bibkey>dhamecha-etal-2021-role</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.675</doi>
      <video href="2021.emnlp-main.675.mp4"/>
      <pwccode url="https://github.com/ibm/indo-aryan-language-family-model" additional="false">ibm/indo-aryan-language-family-model</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/indicglue">IndicGLUE</pwcdataset>
    </paper>
    <paper id="676">
      <title>Comparing Feature-Engineering and Feature-Learning Approaches for Multilingual Translationese Classification</title>
      <author><first>Daria</first><last>Pylypenko</last></author>
      <author><first>Kwabena</first><last>Amponsah-Kaakyire</last></author>
      <author><first>Koel</first><last>Dutta Chowdhury</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <author><first>Cristina</first><last>España-Bonet</last></author>
      <pages>8596–8611</pages>
      <abstract>Traditional hand-crafted linguistically-informed features have often been used for distinguishing between translated and original non-translated texts. By contrast, to date, neural architectures without manual feature engineering have been less explored for this task. In this work, we (i) compare the traditional feature-engineering-based approach to the feature-learning-based one and (ii) analyse the neural architectures in order to investigate how well the hand-crafted features explain the variance in the neural models’ predictions. We use pre-trained neural word embeddings, as well as several end-to-end neural architectures in both monolingual and multilingual settings and compare them to feature-engineering-based SVM classifiers. We show that (i) neural architectures outperform other approaches by more than 20 accuracy points, with the BERT-based model performing the best in both the monolingual and multilingual settings; (ii) while many individual hand-crafted translationese features correlate with neural model predictions, feature importance analysis shows that the most important features for neural and classical architectures differ; and (iii) our multilingual experiments provide empirical evidence for translationese universals across languages.</abstract>
      <url hash="25d8b020">2021.emnlp-main.676</url>
      <bibkey>pylypenko-etal-2021-comparing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.676</doi>
      <video href="2021.emnlp-main.676.mp4"/>
    </paper>
    <paper id="677">
      <title>Multi-Sentence Resampling: A Simple Approach to Alleviate Dataset Length Bias and Beam-Search Degradation</title>
      <author><first>Ivan</first><last>Provilkov</last></author>
      <author><first>Andrey</first><last>Malinin</last></author>
      <pages>8612–8621</pages>
      <abstract>Neural Machine Translation (NMT) is known to suffer from a beam-search problem: after a certain point, increasing beam size causes an overall drop in translation quality. This effect is especially pronounced for long sentences. While much work was done analyzing this phenomenon, primarily for autoregressive NMT models, there is still no consensus on its underlying cause. In this work, we analyze errors that cause major quality degradation with large beams in NMT and Automatic Speech Recognition (ASR). We show that a factor that strongly contributes to the quality degradation with large beams is dataset length-bias - NMT datasets are strongly biased towards short sentences. To mitigate this issue, we propose a new data augmentation technique – Multi-Sentence Resampling (MSR). This technique extends the training examples by concatenating several sentences from the original dataset to make a long training example. We demonstrate that MSR significantly reduces degradation with growing beam size and improves final translation quality on the IWSTL15 En-Vi, IWSTL17 En-Fr, and WMT14 En-De datasets.</abstract>
      <url hash="4f89bfde">2021.emnlp-main.677</url>
      <bibkey>provilkov-malinin-2021-multi</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.677</doi>
      <video href="2021.emnlp-main.677.mp4"/>
      <pwccode url="https://github.com/yandex-research/msr" additional="false">yandex-research/msr</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
    </paper>
    <paper id="678">
      <title>Cross-Policy Compliance Detection via Question Answering</title>
      <author><first>Marzieh</first><last>Saeidi</last></author>
      <author><first>Majid</first><last>Yazdani</last></author>
      <author><first>Andreas</first><last>Vlachos</last></author>
      <pages>8622–8632</pages>
      <abstract>Policy compliance detection is the task of ensuring that a scenario conforms to a policy (e.g. a claim is valid according to government rules or a post in an online platform conforms to community guidelines). This task has been previously instantiated as a form of textual entailment, which results in poor accuracy due to the complexity of the policies. In this paper we propose to address policy compliance detection via decomposing it into question answering, where questions check whether the conditions stated in the policy apply to the scenario, and an expression tree combines the answers to obtain the label. Despite the initial upfront annotation cost, we demonstrate that this approach results in better accuracy, especially in the cross-policy setup where the policies during testing are unseen in training. In addition, it allows us to use existing question answering models pre-trained on existing large datasets. Finally, it explicitly identifies the information missing from a scenario in case policy compliance cannot be determined. We conduct our experiments using a recent dataset consisting of government policies, which we augment with expert annotations and find that the cost of annotating question answering decomposition is largely offset by improved inter-annotator agreement and speed.</abstract>
      <url hash="e4acd2d5">2021.emnlp-main.678</url>
      <bibkey>saeidi-etal-2021-cross</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.678</doi>
      <video href="2021.emnlp-main.678.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sharc">ShARC</pwcdataset>
    </paper>
    <paper id="679">
      <title>Meta-<fixed-case>LMTC</fixed-case>: Meta-Learning for Large-Scale Multi-Label Text Classification</title>
      <author><first>Ran</first><last>Wang</last></author>
      <author><first>Xi’ao</first><last>Su</last></author>
      <author><first>Siyu</first><last>Long</last></author>
      <author><first>Xinyu</first><last>Dai</last></author>
      <author><first>Shujian</first><last>Huang</last></author>
      <author><first>Jiajun</first><last>Chen</last></author>
      <pages>8633–8646</pages>
      <abstract>Large-scale multi-label text classification (LMTC) tasks often face long-tailed label distributions, where many labels have few or even no training instances. Although current methods can exploit prior knowledge to handle these few/zero-shot labels, they neglect the meta-knowledge contained in the dataset that can guide models to learn with few samples. In this paper, for the first time, this problem is addressed from a meta-learning perspective. However, the simple extension of meta-learning approaches to multi-label classification is sub-optimal for LMTC tasks due to long-tailed label distribution and coexisting of few- and zero-shot scenarios. We propose a meta-learning approach named META-LMTC. Specifically, it constructs more faithful and more diverse tasks according to well-designed sampling strategies and directly incorporates the objective of adapting to new low-resource tasks into the meta-learning phase. Extensive experiments show that META-LMTC achieves state-of-the-art performance against strong baselines and can still enhance powerful BERTlike models.</abstract>
      <url hash="7e5317de">2021.emnlp-main.679</url>
      <bibkey>wang-etal-2021-meta-lmtc</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.679</doi>
      <video href="2021.emnlp-main.679.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/eurlex57k">EURLEX57K</pwcdataset>
    </paper>
    <paper id="680">
      <title>Unsupervised Multi-View Post-<fixed-case>OCR</fixed-case> Error Correction With Language Models</title>
      <author><first>Harsh</first><last>Gupta</last></author>
      <author><first>Luciano</first><last>Del Corro</last></author>
      <author><first>Samuel</first><last>Broscheit</last></author>
      <author><first>Johannes</first><last>Hoffart</last></author>
      <author><first>Eliot</first><last>Brenner</last></author>
      <pages>8647–8652</pages>
      <abstract>We investigate post-OCR correction in a setting where we have access to different OCR views of the same document. The goal of this study is to understand if a pretrained language model (LM) can be used in an unsupervised way to reconcile the different OCR views such that their combination contains fewer errors than each individual view. This approach is motivated by scenarios in which unconstrained text generation for error correction is too risky. We evaluated different pretrained LMs on two datasets and found significant gains in realistic scenarios with up to 15% WER improvement over the best OCR view. We also show the importance of domain adaptation for post-OCR correction on out-of-domain documents.</abstract>
      <url hash="b47ab55e">2021.emnlp-main.680</url>
      <bibkey>gupta-etal-2021-unsupervised-multi</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.680</doi>
      <video href="2021.emnlp-main.680.mp4"/>
    </paper>
    <paper id="681">
      <title>Parallel Refinements for Lexically Constrained Text Generation with <fixed-case>BART</fixed-case></title>
      <author><first>Xingwei</first><last>He</last></author>
      <pages>8653–8666</pages>
      <abstract>Lexically constrained text generation aims to control the generated text by incorporating certain pre-specified keywords into the output. Previous work injects lexical constraints into the output by controlling the decoding process or refining the candidate output iteratively, which tends to generate generic or ungrammatical sentences, and has high computational complexity. To address these challenges, we proposed Constrained BART (CBART) for lexically constrained text generation. CBART leverages the pre-trained model, BART and transfers part of the generation burden from the decoder to the encoder by decomposing this task into two sub-tasks, thereby improving the sentence quality. Concretely, we extended BART by adding a token-level classifier over the encoder, aiming at instructing the decoder where to replace and insert. Guided by the encoder, the decoder refines multiple tokens of the input in one step by inserting tokens before specific positions and re-predicting tokens at a low confidence level. To further reduce the inference latency, the decoder predicts all tokens in parallel. Experiment results on One-Billion-Word and Yelp show that CBART can generate plausible text with high quality and diversity while largely accelerating inference.</abstract>
      <url hash="2d960f40">2021.emnlp-main.681</url>
      <bibkey>he-2021-parallel</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.681</doi>
      <video href="2021.emnlp-main.681.mp4"/>
      <pwccode url="https://github.com/nlpcode/cbart" additional="false">nlpcode/cbart</pwccode>
    </paper>
    <paper id="682">
      <title><fixed-case>BERT</fixed-case>-Beta: A Proactive Probabilistic Approach to Text Moderation</title>
      <author><first>Fei</first><last>Tan</last></author>
      <author><first>Yifan</first><last>Hu</last></author>
      <author><first>Kevin</first><last>Yen</last></author>
      <author><first>Changwei</first><last>Hu</last></author>
      <pages>8667–8675</pages>
      <abstract>Text moderation for user generated content, which helps to promote healthy interaction among users, has been widely studied and many machine learning models have been proposed. In this work, we explore an alternative perspective by augmenting reactive reviews with proactive forecasting. Specifically, we propose a new concept text toxicity propensity to characterize the extent to which a text tends to attract toxic comments. Beta regression is then introduced to do the probabilistic modeling, which is demonstrated to function well in comprehensive experiments. We also propose an explanation method to communicate the model decision clearly. Both propensity scoring and interpretation benefit text moderation in a novel manner. Finally, the proposed scaling mechanism for the linear model offers useful insights beyond this work.</abstract>
      <url hash="2a0644f6">2021.emnlp-main.682</url>
      <bibkey>tan-etal-2021-bert</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.682</doi>
      <video href="2021.emnlp-main.682.mp4"/>
    </paper>
    <paper id="683">
      <title><fixed-case>ST</fixed-case>a<fixed-case>CK</fixed-case>: Sentence Ordering with Temporal Commonsense Knowledge</title>
      <author><first>Deepanway</first><last>Ghosal</last></author>
      <author><first>Navonil</first><last>Majumder</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <author><first>Soujanya</first><last>Poria</last></author>
      <pages>8676–8686</pages>
      <abstract>Sentence order prediction is the task of finding the correct order of sentences in a randomly ordered document. Correctly ordering the sentences requires an understanding of coherence with respect to the chronological sequence of events described in the text. Document-level contextual understanding and commonsense knowledge centered around these events are often essential in uncovering this coherence and predicting the exact chronological order. In this paper, we introduce STaCK — a framework based on graph neural networks and temporal commonsense knowledge to model global information and predict the relative order of sentences. Our graph network accumulates temporal evidence using knowledge of ‘past’ and ‘future’ and formulates sentence ordering as a constrained edge classification problem. We report results on five different datasets, and empirically show that the proposed method is naturally suitable for order prediction. The implementation of this work is available at: https://github.com/declare-lab/sentence-ordering.</abstract>
      <url hash="2dc5b8b2">2021.emnlp-main.683</url>
      <bibkey>ghosal-etal-2021-stack</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.683</doi>
      <video href="2021.emnlp-main.683.mp4"/>
      <pwccode url="https://github.com/declare-lab/sentence-ordering" additional="false">declare-lab/sentence-ordering</pwccode>
    </paper>
    <paper id="684">
      <title>Preventing Author Profiling through Zero-Shot Multilingual Back-Translation</title>
      <author><first>David</first><last>Adelani</last></author>
      <author><first>Miaoran</first><last>Zhang</last></author>
      <author><first>Xiaoyu</first><last>Shen</last></author>
      <author><first>Ali</first><last>Davody</last></author>
      <author><first>Thomas</first><last>Kleinbauer</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <pages>8687–8695</pages>
      <abstract>Documents as short as a single sentence may inadvertently reveal sensitive information about their authors, including e.g. their gender or ethnicity. Style transfer is an effective way of transforming texts in order to remove any information that enables author profiling. However, for a number of current state-of-the-art approaches the improved privacy is accompanied by an undesirable drop in the down-stream utility of the transformed data. In this paper, we propose a simple, zero-shot way to effectively lower the risk of author profiling through multilingual back-translation using off-the-shelf translation models. We compare our models with five representative text style transfer models on three datasets across different domains. Results from both an automatic and a human evaluation show that our approach achieves the best overall performance while requiring no training data. We are able to lower the adversarial prediction of gender and race by up to 22% while retaining 95% of the original utility on downstream tasks.</abstract>
      <url hash="bf2b4076">2021.emnlp-main.684</url>
      <bibkey>adelani-etal-2021-preventing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.684</doi>
      <video href="2021.emnlp-main.684.mp4"/>
      <pwccode url="https://github.com/uds-lsv/author-profiling-prevention-bt" additional="false">uds-lsv/author-profiling-prevention-bt</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
    </paper>
    <paper id="685">
      <title><fixed-case>C</fixed-case>ode<fixed-case>T</fixed-case>5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation</title>
      <author><first>Yue</first><last>Wang</last></author>
      <author><first>Weishi</first><last>Wang</last></author>
      <author><first>Shafiq</first><last>Joty</last></author>
      <author><first>Steven C.H.</first><last>Hoi</last></author>
      <pages>8696–8708</pages>
      <abstract>Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5.</abstract>
      <url hash="56481874">2021.emnlp-main.685</url>
      <bibkey>wang-etal-2021-codet5</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.685</doi>
      <video href="2021.emnlp-main.685.mp4"/>
      <pwccode url="https://github.com/salesforce/codet5" additional="true">salesforce/codet5</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/concode">CONCODE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/codesearchnet">CodeSearchNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/codexglue">CodeXGLUE</pwcdataset>
    </paper>
    <paper id="686">
      <title>Detect and Classify – Joint Span Detection and Classification for Health Outcomes</title>
      <author><first>Micheal</first><last>Abaho</last></author>
      <author><first>Danushka</first><last>Bollegala</last></author>
      <author><first>Paula</first><last>Williamson</last></author>
      <author><first>Susanna</first><last>Dodd</last></author>
      <pages>8709–8721</pages>
      <abstract>A health outcome is a measurement or an observation used to capture and assess the effect of a treatment. Automatic detection of health outcomes from text would undoubtedly speed up access to evidence necessary in healthcare decision making. Prior work on outcome detection has modelled this task as either (a) a sequence labelling task, where the goal is to detect which text spans describe health outcomes, or (b) a classification task, where the goal is to classify a text into a predefined set of categories depending on an outcome that is mentioned somewhere in that text. However, this decoupling of span detection and classification is problematic from a modelling perspective and ignores global structural correspondences between sentence-level and word-level information present in a given text. To address this, we propose a method that uses both word-level and sentence-level information to simultaneously perform outcome span detection and outcome type classification. In addition to injecting contextual information to hidden vectors, we use label attention to appropriately weight both word and sentence level information. Experimental results on several benchmark datasets for health outcome detection show that our proposed method consistently outperforms decoupled methods, reporting competitive results.</abstract>
      <url hash="47258cf7">2021.emnlp-main.686</url>
      <attachment type="Software" hash="ff027604">2021.emnlp-main.686.Software.zip</attachment>
      <bibkey>abaho-etal-2021-detect</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.686</doi>
      <video href="2021.emnlp-main.686.mp4"/>
      <pwccode url="https://github.com/MichealAbaho/Label-Context-Aware-Attention-Model" additional="false">MichealAbaho/Label-Context-Aware-Attention-Model</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ebm-nlp">EBM-NLP</pwcdataset>
    </paper>
    <paper id="687">
      <title><fixed-case>M</fixed-case>ulti-Class Grammatical Error Detection for Correction: <fixed-case>A</fixed-case> Tale of Two Systems</title>
      <author><first>Zheng</first><last>Yuan</last></author>
      <author><first>Shiva</first><last>Taslimipoor</last></author>
      <author><first>Christopher</first><last>Davis</last></author>
      <author><first>Christopher</first><last>Bryant</last></author>
      <pages>8722–8736</pages>
      <abstract>In this paper, we show how a multi-class grammatical error detection (GED) system can be used to improve grammatical error correction (GEC) for English. Specifically, we first develop a new state-of-the-art binary detection system based on pre-trained ELECTRA, and then extend it to multi-class detection using different error type tagsets derived from the ERRANT framework. Output from this detection system is used as auxiliary input to fine-tune a novel encoder-decoder GEC model, and we subsequently re-rank the N-best GEC output to find the hypothesis that most agrees with the GED output. Results show that fine-tuning the GEC system using 4-class GED produces the best model, but re-ranking using 55-class GED leads to the best performance overall. This suggests that different multi-class GED systems benefit GEC in different ways. Ultimately, our system outperforms all other previous work that combines GED and GEC, and achieves a new single-model NMT-based state of the art on the BEA-test benchmark.</abstract>
      <url hash="5d98ddfc">2021.emnlp-main.687</url>
      <bibkey>yuan-etal-2021-multi</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.687</doi>
      <video href="2021.emnlp-main.687.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/fce">FCE</pwcdataset>
    </paper>
    <paper id="688">
      <title>Towards Zero-shot Commonsense Reasoning with Self-supervised Refinement of Language Models</title>
      <author><first>Tassilo</first><last>Klein</last></author>
      <author><first>Moin</first><last>Nabi</last></author>
      <pages>8737–8743</pages>
      <abstract>Can we get existing language models and refine them for zero-shot commonsense reasoning? This paper presents an initial study exploring the feasibility of zero-shot commonsense reasoning for the Winograd Schema Challenge by formulating the task as self-supervised refinement of a pre-trained language model. In contrast to previous studies that rely on fine-tuning annotated datasets, we seek to boost conceptualization via loss landscape refinement. To this end, we propose a novel self-supervised learning approach that refines the language model utilizing a set of linguistic perturbations of similar concept relationships. Empirical analysis of our conceptually simple framework demonstrates the viability of zero-shot commonsense reasoning on multiple benchmarks.</abstract>
      <url hash="0f483603">2021.emnlp-main.688</url>
      <bibkey>klein-nabi-2021-towards</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.688</doi>
      <video href="2021.emnlp-main.688.mp4"/>
      <pwccode url="https://github.com/sap-samples/emnlp2021-contrastive-refinement" additional="false">sap-samples/emnlp2021-contrastive-refinement</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gap-coreference-dataset">GAP Coreference Dataset</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winobias">WinoBias</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winogrande">WinoGrande</pwcdataset>
    </paper>
    <paper id="689">
      <title>To Share or not to Share: <fixed-case>P</fixed-case>redicting Sets of Sources for Model Transfer Learning</title>
      <author><first>Lukas</first><last>Lange</last></author>
      <author><first>Jannik</first><last>Strötgen</last></author>
      <author><first>Heike</first><last>Adel</last></author>
      <author><first>Dietrich</first><last>Klakow</last></author>
      <pages>8744–8753</pages>
      <abstract>In low-resource settings, model transfer can help to overcome a lack of labeled data for many tasks and domains. However, predicting useful transfer sources is a challenging problem, as even the most similar sources might lead to unexpected negative transfer results. Thus, ranking methods based on task and text similarity — as suggested in prior work — may not be sufficient to identify promising sources. To tackle this problem, we propose a new approach to automatically determine which and how many sources should be exploited. For this, we study the effects of model transfer on sequence labeling across various domains and tasks and show that our methods based on model similarity and support vector machines are able to predict promising sources, resulting in performance increases of up to 24 F1 points.</abstract>
      <url hash="787d85a3">2021.emnlp-main.689</url>
      <bibkey>lange-etal-2021-share</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.689</doi>
      <video href="2021.emnlp-main.689.mp4"/>
      <pwccode url="https://github.com/boschresearch/predicting_sets_of_sources" additional="false">boschresearch/predicting_sets_of_sources</pwccode>
    </paper>
    <paper id="690">
      <title>Self-Supervised Detection of Contextual Synonyms in a Multi-Class Setting: Phenotype Annotation Use Case</title>
      <author><first>Jingqing</first><last>Zhang</last></author>
      <author><first>Luis</first><last>Bolanos Trujillo</last></author>
      <author><first>Tong</first><last>Li</last></author>
      <author><first>Ashwani</first><last>Tanwar</last></author>
      <author><first>Guilherme</first><last>Freire</last></author>
      <author><first>Xian</first><last>Yang</last></author>
      <author><first>Julia</first><last>Ive</last></author>
      <author><first>Vibhor</first><last>Gupta</last></author>
      <author><first>Yike</first><last>Guo</last></author>
      <pages>8754–8769</pages>
      <abstract>Contextualised word embeddings is a powerful tool to detect contextual synonyms. However, most of the current state-of-the-art (SOTA) deep learning concept extraction methods remain supervised and underexploit the potential of the context. In this paper, we propose a self-supervised pre-training approach which is able to detect contextual synonyms of concepts being training on the data created by shallow matching. We apply our methodology in the sparse multi-class setting (over 15,000 concepts) to extract phenotype information from electronic health records. We further investigate data augmentation techniques to address the problem of the class sparsity. Our approach achieves a new SOTA for the unsupervised phenotype concept annotation on clinical text on F1 and Recall outperforming the previous SOTA with a gain of up to 4.5 and 4.0 absolute points, respectively. After fine-tuning with as little as 20% of the labelled data, we also outperform BioBERT and ClinicalBERT. The extrinsic evaluation on three ICU benchmarks also shows the benefit of using the phenotypes annotated by our model as features.</abstract>
      <url hash="c63323dc">2021.emnlp-main.690</url>
      <bibkey>zhang-etal-2021-self</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.690</doi>
      <video href="2021.emnlp-main.690.mp4"/>
    </paper>
    <paper id="691">
      <title><fixed-case>C</fixed-case>lause<fixed-case>R</fixed-case>ec: A Clause Recommendation Framework for <fixed-case>AI</fixed-case>-aided Contract Authoring</title>
      <author><first>Vinay</first><last>Aggarwal</last></author>
      <author><first>Aparna</first><last>Garimella</last></author>
      <author><first>Balaji Vasan</first><last>Srinivasan</last></author>
      <author><first>Anandhavelu</first><last>N</last></author>
      <author><first>Rajiv</first><last>Jain</last></author>
      <pages>8770–8776</pages>
      <abstract>Contracts are a common type of legal document that frequent in several day-to-day business workflows. However, there has been very limited NLP research in processing such documents, and even lesser in generating them. These contracts are made up of clauses, and the unique nature of these clauses calls for specific methods to understand and generate such documents. In this paper, we introduce the task of clause recommendation, as a first step to aid and accelerate the authoring of contract documents. We propose a two-staged pipeline to first predict if a specific clause type is relevant to be added in a contract, and then recommend the top clauses for the given type based on the contract context. We pre-train BERT on an existing library of clauses with two additional tasks and use it for our prediction and recommendation. We experiment with classification methods and similarity-based heuristics for clause relevance prediction, and generation-based methods for clause recommendation, and evaluate the results from various methods on several clause types. We provide analyses on the results, and further outline the limitations and future directions of this line of research.</abstract>
      <url hash="04b4c4d0">2021.emnlp-main.691</url>
      <bibkey>aggarwal-etal-2021-clauserec</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.691</doi>
      <video href="2021.emnlp-main.691.mp4"/>
    </paper>
    <paper id="692">
      <title><fixed-case>F</fixed-case>innish Dialect Identification: The Effect of Audio and Text</title>
      <author><first>Mika</first><last>Hämäläinen</last></author>
      <author><first>Khalid</first><last>Alnajjar</last></author>
      <author><first>Niko</first><last>Partanen</last></author>
      <author><first>Jack</first><last>Rueter</last></author>
      <pages>8777–8783</pages>
      <abstract>Finnish is a language with multiple dialects that not only differ from each other in terms of accent (pronunciation) but also in terms of morphological forms and lexical choice. We present the first approach to automatically detect the dialect of a speaker based on a dialect transcript and transcript with audio recording in a dataset consisting of 23 different dialects. Our results show that the best accuracy is received by combining both of the modalities, as text only reaches to an overall accuracy of 57%, where as text and audio reach to 85%. Our code, models and data have been released openly on Github and Zenodo.</abstract>
      <url hash="c18020e9">2021.emnlp-main.692</url>
      <bibkey>hamalainen-etal-2021-finnish</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.692</doi>
      <video href="2021.emnlp-main.692.mp4"/>
      <pwccode url="https://github.com/rootroo-ltd/finnishdialectidentification" additional="false">rootroo-ltd/finnishdialectidentification</pwccode>
    </paper>
    <paper id="693">
      <title><fixed-case>E</fixed-case>nglish Machine Reading Comprehension Datasets: A Survey</title>
      <author><first>Daria</first><last>Dzendzik</last></author>
      <author><first>Jennifer</first><last>Foster</last></author>
      <author><first>Carl</first><last>Vogel</last></author>
      <pages>8784–8804</pages>
      <abstract>This paper surveys 60 English Machine Reading Comprehension datasets, with a view to providing a convenient resource for other researchers interested in this problem. We categorize the datasets according to their question and answer form and compare them across various dimensions including size, vocabulary, data source, method of creation, human performance level, and first question word. Our analysis reveals that Wikipedia is by far the most common data source and that there is a relative lack of why, when, and where questions across datasets.</abstract>
      <url hash="d3058e6e">2021.emnlp-main.693</url>
      <bibkey>dzendzik-etal-2021-english</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.693</doi>
      <video href="2021.emnlp-main.693.mp4"/>
      <pwccode url="https://github.com/dariad/rczoo" additional="false">dariad/rczoo</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/arc">ARC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/amazonqa">AmazonQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/bipar">BiPaR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/bookcorpus">BookCorpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/clicr">CliCR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/coqa">CoQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cosmosqa">CosmosQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/duorc">DuoRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/iirc">IIRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/lambada">LAMBADA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mcscript">MCScript</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mctest">MCTest</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ms-marco">MS MARCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/medhop">MedHop</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/movieqa">MovieQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multirc">MultiRC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/narrativeqa">NarrativeQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsqa">NewsQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/openbookqa">OpenBookQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qasc">QASC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quasar-1">QUASAR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quasar-s">QUASAR-S</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quasar-t">QUASAR-T</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/race">RACE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reco">ReCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/reclor">ReClor</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/record">ReCoRD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/recipeqa">RecipeQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sciq">SciQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/searchqa">SearchQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sharc">ShARC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/subjqa">SubjQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tweetqa">TweetQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/who-did-what">Who-did-What</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikihop">WikiHop</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikimovies">WikiMovies</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikiqa">WikiQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikireading">WikiReading</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/worldtree">Worldtree</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/emrqa">emrQA</pwcdataset>
    </paper>
    <paper id="694">
      <title>Expanding End-to-End Question Answering on Differentiable Knowledge Graphs with Intersection</title>
      <author><first>Priyanka</first><last>Sen</last></author>
      <author><first>Armin</first><last>Oliya</last></author>
      <author><first>Amir</first><last>Saffari</last></author>
      <pages>8805–8812</pages>
      <abstract>End-to-end question answering using a differentiable knowledge graph is a promising technique that requires only weak supervision, produces interpretable results, and is fully differentiable. Previous implementations of this technique (Cohen et al, 2020) have focused on single-entity questions using a relation following operation. In this paper, we propose a model that explicitly handles multiple-entity questions by implementing a new intersection operation, which identifies the shared elements between two sets of entities. We find that introducing intersection improves performance over a baseline model on two datasets, WebQuestionsSP (69.6% to 73.3% Hits@1) and ComplexWebQuestions (39.8% to 48.7% Hits@1), and in particular, improves performance on questions with multiple entities by over 14% on WebQuestionsSP and by 19% on ComplexWebQuestions.</abstract>
      <url hash="3c5f48cd">2021.emnlp-main.694</url>
      <bibkey>sen-etal-2021-expanding</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.694</doi>
      <video href="2021.emnlp-main.694.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/complexwebquestions">ComplexWebQuestions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestionssp">WebQuestionsSP</pwcdataset>
    </paper>
    <paper id="695">
      <title>Structured Context and High-Coverage Grammar for Conversational Question Answering over Knowledge Graphs</title>
      <author><first>Pierre</first><last>Marion</last></author>
      <author><first>Pawel</first><last>Nowak</last></author>
      <author><first>Francesco</first><last>Piccinno</last></author>
      <pages>8813–8829</pages>
      <abstract>We tackle the problem of weakly-supervised conversational Question Answering over large Knowledge Graphs using a neural semantic parsing approach. We introduce a new Logical Form (LF) grammar that can model a wide range of queries on the graph while remaining sufficiently simple to generate supervision data efficiently. Our Transformer-based model takes a JSON-like structure as input, allowing us to easily incorporate both Knowledge Graph and conversational contexts. This structured input is transformed to lists of embeddings and then fed to standard attention layers. We validate our approach, both in terms of grammar coverage and LF execution accuracy, on two publicly available datasets, CSQA and ConvQuestions, both grounded in Wikidata. On CSQA, our approach increases the coverage from 80% to 96.2%, and the LF execution accuracy from 70.6% to 75.6%, with respect to previous state-of-the-art results. On ConvQuestions, we achieve competitive results with respect to the state-of-the-art.</abstract>
      <url hash="d5c1d6ee">2021.emnlp-main.695</url>
      <bibkey>marion-etal-2021-structured</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.695</doi>
      <video href="2021.emnlp-main.695.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/csqa">CSQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/convquestions">ConvQuestions</pwcdataset>
    </paper>
    <paper id="696">
      <title>Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation</title>
      <author><first>Max</first><last>Bartolo</last></author>
      <author><first>Tristan</first><last>Thrush</last></author>
      <author><first>Robin</first><last>Jia</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <author><first>Pontus</first><last>Stenetorp</last></author>
      <author><first>Douwe</first><last>Kiela</last></author>
      <pages>8830–8848</pages>
      <abstract>Despite recent progress, state-of-the-art question answering models remain vulnerable to a variety of adversarial attacks. While dynamic adversarial data collection, in which a human annotator tries to write examples that fool a model-in-the-loop, can improve model robustness, this process is expensive which limits the scale of the collected data. In this work, we are the first to use synthetic adversarial data generation to make question answering models more robust to human adversaries. We develop a data generation pipeline that selects source passages, identifies candidate answers, generates questions, then finally filters or re-labels them to improve quality. Using this approach, we amplify a smaller human-written adversarial dataset to a much larger set of synthetic question-answer pairs. By incorporating our synthetic data, we improve the state-of-the-art on the AdversarialQA dataset by 3.7F1 and improve model generalisation on nine of the twelve MRQA datasets. We further conduct a novel human-in-the-loop evaluation and show that our models are considerably more robust to new human-written adversarial examples: crowdworkers can fool our model only 8.8% of the time on average, compared to 17.6% for a model trained without synthetic data.</abstract>
      <url hash="3ec9154d">2021.emnlp-main.696</url>
      <bibkey>bartolo-etal-2021-improving</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.696</doi>
      <video href="2021.emnlp-main.696.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/adversarialqa">AdversarialQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrqa-2019">MRQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="697">
      <title><fixed-case>B</fixed-case>elief<fixed-case>B</fixed-case>ank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief</title>
      <author><first>Nora</first><last>Kassner</last></author>
      <author><first>Oyvind</first><last>Tafjord</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <author><first>Peter</first><last>Clark</last></author>
      <pages>8849–8861</pages>
      <abstract>Although pretrained language models (PTLMs) contain significant amounts of world knowledge, they can still produce inconsistent answers to questions when probed, even after specialized training. As a result, it can be hard to identify what the model actually “believes” about the world, making it susceptible to inconsistent behavior and simple errors. Our goal is to reduce these problems. Our approach is to embed a PTLM in a broader system that also includes an evolving, symbolic memory of beliefs – a BeliefBank – that records but then may modify the raw PTLM answers. We describe two mechanisms to improve belief consistency in the overall system. First, a reasoning component – a weighted MaxSAT solver – revises beliefs that significantly clash with others. Second, a feedback component issues future queries to the PTLM using known beliefs as context. We show that, in a controlled experimental setting, these two mechanisms result in more consistent beliefs in the overall system, improving both the accuracy and consistency of its answers over time. This is significant as it is a first step towards PTLM-based architectures with a systematic notion of belief, enabling them to construct a more coherent picture of the world, and improve over time without model retraining.</abstract>
      <url hash="5ab09962">2021.emnlp-main.697</url>
      <bibkey>kassner-etal-2021-beliefbank</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.697</doi>
      <video href="2021.emnlp-main.697.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
    </paper>
    <paper id="698">
      <title><fixed-case>MLEC-QA</fixed-case>: <fixed-case>A</fixed-case> <fixed-case>C</fixed-case>hinese <fixed-case>M</fixed-case>ulti-<fixed-case>C</fixed-case>hoice <fixed-case>B</fixed-case>iomedical <fixed-case>Q</fixed-case>uestion <fixed-case>A</fixed-case>nswering <fixed-case>D</fixed-case>ataset</title>
      <author><first>Jing</first><last>Li</last></author>
      <author><first>Shangping</first><last>Zhong</last></author>
      <author><first>Kaizhi</first><last>Chen</last></author>
      <pages>8862–8874</pages>
      <abstract>Question Answering (QA) has been successfully applied in scenarios of human-computer interaction such as chatbots and search engines. However, for the specific biomedical domain, QA systems are still immature due to expert-annotated datasets being limited by category and scale. In this paper, we present MLEC-QA, the largest-scale Chinese multi-choice biomedical QA dataset, collected from the National Medical Licensing Examination in China. The dataset is composed of five subsets with 136,236 biomedical multi-choice questions with extra materials (images or tables) annotated by human experts, and first covers the following biomedical sub-fields: Clinic, Stomatology, Public Health, Traditional Chinese Medicine, and Traditional Chinese Medicine Combined with Western Medicine. We implement eight representative control methods and open-domain QA methods as baselines. Experimental results demonstrate that even the current best model can only achieve accuracies between 40% to 55% on five subsets, especially performing poorly on questions that require sophisticated reasoning ability. We hope the release of the MLEC-QA dataset can serve as a valuable resource for research and evaluation in open-domain QA, and also make advances for biomedical QA systems.</abstract>
      <url hash="2a6e4581">2021.emnlp-main.698</url>
      <bibkey>li-etal-2021-mlec</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.698</doi>
      <video href="2021.emnlp-main.698.mp4"/>
      <pwccode url="https://github.com/judenpech/mlec-qa" additional="false">judenpech/mlec-qa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/headqa">HeadQA</pwcdataset>
    </paper>
    <paper id="699">
      <title><fixed-case>I</fixed-case>ndo<fixed-case>NLG</fixed-case>: Benchmark and Resources for Evaluating <fixed-case>I</fixed-case>ndonesian Natural Language Generation</title>
      <author><first>Samuel</first><last>Cahyawijaya</last></author>
      <author><first>Genta Indra</first><last>Winata</last></author>
      <author><first>Bryan</first><last>Wilie</last></author>
      <author><first>Karissa</first><last>Vincentio</last></author>
      <author><first>Xiaohong</first><last>Li</last></author>
      <author><first>Adhiguna</first><last>Kuncoro</last></author>
      <author><first>Sebastian</first><last>Ruder</last></author>
      <author><first>Zhi Yuan</first><last>Lim</last></author>
      <author><first>Syafri</first><last>Bahar</last></author>
      <author><first>Masayu</first><last>Khodra</last></author>
      <author><first>Ayu</first><last>Purwarianti</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <pages>8875–8898</pages>
      <abstract>Natural language generation (NLG) benchmarks provide an important avenue to measure progress and develop better NLG systems. Unfortunately, the lack of publicly available NLG benchmarks for low-resource languages poses a challenging barrier for building NLG systems that work well for languages with limited amounts of data. Here we introduce IndoNLG, the first benchmark to measure natural language generation (NLG) progress in three low-resource—yet widely spoken—languages of Indonesia: Indonesian, Javanese, and Sundanese. Altogether, these languages are spoken by more than 100 million native speakers, and hence constitute an important use case of NLG systems today. Concretely, IndoNLG covers six tasks: summarization, question answering, chit-chat, and three different pairs of machine translation (MT) tasks. We collate a clean pretraining corpus of Indonesian, Sundanese, and Javanese datasets, Indo4B-Plus, which is used to pretrain our models: IndoBART and IndoGPT. We show that IndoBART and IndoGPT achieve competitive performance on all tasks—despite using only one-fifth the parameters of a larger multilingual model, mBART-large (Liu et al., 2020). This finding emphasizes the importance of pretraining on closely related, localized languages to achieve more efficient learning and faster inference at very low-resource languages like Javanese and Sundanese.</abstract>
      <url hash="5f98a1d4">2021.emnlp-main.699</url>
      <bibkey>cahyawijaya-etal-2021-indonlg</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.699</doi>
      <video href="2021.emnlp-main.699.mp4"/>
      <pwccode url="" additional="true"/>
      <pwcdataset url="https://paperswithcode.com/dataset/indonlg">IndoNLG</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gem">GEM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/liputan6">Liputan6</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDi QA</pwcdataset>
    </paper>
    <paper id="700">
      <title>Is Multi-Hop Reasoning Really Explainable? Towards Benchmarking Reasoning Interpretability</title>
      <author><first>Xin</first><last>Lv</last></author>
      <author><first>Yixin</first><last>Cao</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Yichi</first><last>Zhang</last></author>
      <author><first>Zelin</first><last>Dai</last></author>
      <pages>8899–8911</pages>
      <abstract>Multi-hop reasoning has been widely studied in recent years to obtain more interpretable link prediction. However, we find in experiments that many paths given by these models are actually unreasonable, while little work has been done on interpretability evaluation for them. In this paper, we propose a unified framework to quantitatively evaluate the interpretability of multi-hop reasoning models so as to advance their development. In specific, we define three metrics, including path recall, local interpretability, and global interpretability for evaluation, and design an approximate strategy to calculate these metrics using the interpretability scores of rules. We manually annotate all possible rules and establish a benchmark. In experiments, we verify the effectiveness of our benchmark. Besides, we run nine representative baselines on our benchmark, and the experimental results show that the interpretability of current multi-hop reasoning models is less satisfactory and is 51.7% lower than the upper bound given by our benchmark. Moreover, the rule-based models outperform the multi-hop reasoning models in terms of performance and interpretability, which points to a direction for future research, i.e., how to better incorporate rule information into the multi-hop reasoning model. We will publish our codes and datasets upon acceptance.</abstract>
      <url hash="4e12918c">2021.emnlp-main.700</url>
      <bibkey>lv-etal-2021-multi</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.700</doi>
      <video href="2021.emnlp-main.700.mp4"/>
      <pwccode url="https://github.com/THU-KEG/BIMR" additional="false">THU-KEG/BIMR</pwccode>
    </paper>
    <paper id="701">
      <title>Global Explainability of <fixed-case>BERT</fixed-case>-Based Evaluation Metrics by Disentangling along Linguistic Factors</title>
      <author><first>Marvin</first><last>Kaster</last></author>
      <author><first>Wei</first><last>Zhao</last></author>
      <author><first>Steffen</first><last>Eger</last></author>
      <pages>8912–8925</pages>
      <abstract>Evaluation metrics are a key ingredient for progress of text generation systems. In recent years, several BERT-based evaluation metrics have been proposed (including BERTScore, MoverScore, BLEURT, etc.) which correlate much better with human assessment of text generation quality than BLEU or ROUGE, invented two decades ago. However, little is known what these metrics, which are based on black-box language model representations, actually capture (it is typically assumed they model semantic similarity). In this work, we use a simple regression based global explainability technique to disentangle metric scores along linguistic factors, including semantics, syntax, morphology, and lexical overlap. We show that the different metrics capture all aspects to some degree, but that they are all substantially sensitive to lexical overlap, just like BLEU and ROUGE. This exposes limitations of these novelly proposed metrics, which we also highlight in an adversarial test scenario.</abstract>
      <url hash="bf414303">2021.emnlp-main.701</url>
      <bibkey>kaster-etal-2021-global</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.701</doi>
      <video href="2021.emnlp-main.701.mp4"/>
      <pwccode url="https://github.com/steffeneger/global-explainability-metrics" additional="false">steffeneger/global-explainability-metrics</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/paws">PAWS</pwcdataset>
    </paper>
    <paper id="702">
      <title>Exploring Underexplored Limitations of Cross-Domain Text-to-<fixed-case>SQL</fixed-case> Generalization</title>
      <author><first>Yujian</first><last>Gan</last></author>
      <author><first>Xinyun</first><last>Chen</last></author>
      <author><first>Matthew</first><last>Purver</last></author>
      <pages>8926–8931</pages>
      <abstract>Recently, there has been significant progress in studying neural networks for translating text descriptions into SQL queries under the zero-shot cross-domain setting. Despite achieving good performance on some public benchmarks, we observe that existing text-to-SQL models do not generalize when facing domain knowledge that does not frequently appear in the training data, which may render the worse prediction performance for unseen domains. In this work, we investigate the robustness of text-to-SQL models when the questions require rarely observed domain knowledge. In particular, we define five types of domain knowledge and introduce Spider-DK (DK is the abbreviation of domain knowledge), a human-curated dataset based on the Spider benchmark for text-to-SQL translation. NL questions in Spider-DK are selected from Spider, and we modify some samples by adding domain knowledge that reflects real-world question paraphrases. We demonstrate that the prediction accuracy dramatically drops on samples that require such domain knowledge, even if the domain knowledge appears in the training set, and the model provides the correct predictions for related training samples.</abstract>
      <url hash="4b6052c0">2021.emnlp-main.702</url>
      <bibkey>gan-etal-2021-exploring</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.702</doi>
      <video href="2021.emnlp-main.702.mp4"/>
      <pwccode url="https://github.com/ygan/spider-dk" additional="false">ygan/spider-dk</pwccode>
    </paper>
    <paper id="703">
      <title>What happens if you treat ordinal ratings as interval data? Human evaluations in <fixed-case>NLP</fixed-case> are even more under-powered than you think</title>
      <author><first>David M.</first><last>Howcroft</last></author>
      <author><first>Verena</first><last>Rieser</last></author>
      <pages>8932–8939</pages>
      <abstract>Previous work has shown that human evaluations in NLP are notoriously under-powered. Here, we argue that there are two common factors which make this problem even worse: NLP studies usually (a) treat ordinal data as interval data and (b) operate under high variance settings while the differences they are hoping to detect are often subtle. We demonstrate through simulation that ordinal mixed effects models are better able to detect small differences between models, especially in high variance settings common in evaluations of generated texts. We release tools for researchers to conduct their own power analysis and test their assumptions. We also make recommendations for improving statistical power.</abstract>
      <url hash="0ae161fc">2021.emnlp-main.703</url>
      <bibkey>howcroft-rieser-2021-happens</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.703</doi>
      <video href="2021.emnlp-main.703.mp4"/>
    </paper>
    <paper id="704">
      <title><fixed-case>N</fixed-case>eu<fixed-case>T</fixed-case>ral <fixed-case>R</fixed-case>ewriter: <fixed-case>A</fixed-case> Rule-Based and Neural Approach to Automatic Rewriting into Gender Neutral Alternatives</title>
      <author><first>Eva</first><last>Vanmassenhove</last></author>
      <author><first>Chris</first><last>Emmery</last></author>
      <author><first>Dimitar</first><last>Shterionov</last></author>
      <pages>8940–8948</pages>
      <abstract>Recent years have seen an increasing need for gender-neutral and inclusive language. Within the field of NLP, there are various mono- and bilingual use cases where gender inclusive language is appropriate, if not preferred due to ambiguity or uncertainty in terms of the gender of referents. In this work, we present a rule-based and a neural approach to gender-neutral rewriting for English along with manually curated synthetic data (WinoBias+) and natural data (OpenSubtitles and Reddit) benchmarks. A detailed manual and automatic evaluation highlights how our NeuTral Rewriter, trained on data generated by the rule-based approach, obtains word error rates (WER) below 0.18% on synthetic, in-domain and out-domain test sets.</abstract>
      <url hash="f7e486db">2021.emnlp-main.704</url>
      <bibkey>vanmassenhove-etal-2021-neutral</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.704</doi>
      <video href="2021.emnlp-main.704.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/opensubtitles">OpenSubtitles</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winobias">WinoBias</pwcdataset>
    </paper>
    <paper id="705">
      <title>Benchmarking Commonsense Knowledge Base Population with an Effective Evaluation Dataset</title>
      <author><first>Tianqing</first><last>Fang</last></author>
      <author><first>Weiqi</first><last>Wang</last></author>
      <author><first>Sehyun</first><last>Choi</last></author>
      <author><first>Shibo</first><last>Hao</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Yangqiu</first><last>Song</last></author>
      <author><first>Bin</first><last>He</last></author>
      <pages>8949–8964</pages>
      <abstract>Reasoning over commonsense knowledge bases (CSKB) whose elements are in the form of free-text is an important yet hard task in NLP. While CSKB completion only fills the missing links within the domain of the CSKB, CSKB population is alternatively proposed with the goal of reasoning unseen assertions from external resources. In this task, CSKBs are grounded to a large-scale eventuality (activity, state, and event) graph to discriminate whether novel triples from the eventuality graph are plausible or not. However, existing evaluations on the population task are either not accurate (automatic evaluation with randomly sampled negative examples) or of small scale (human annotation). In this paper, we benchmark the CSKB population task with a new large-scale dataset by first aligning four popular CSKBs, and then presenting a high-quality human-annotated evaluation set to probe neural models’ commonsense reasoning ability. We also propose a novel inductive commonsense reasoning model that reasons over graphs. Experimental results show that generalizing commonsense reasoning on unseen assertions is inherently a hard task. Models achieving high accuracy during training perform poorly on the evaluation set, with a large gap between human performance. We will make the data publicly available for future contributions. Codes and data are available at https://github.com/HKUST-KnowComp/CSKB-Population.</abstract>
      <url hash="4c7070dc">2021.emnlp-main.705</url>
      <bibkey>fang-etal-2021-benchmarking</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.705</doi>
      <video href="2021.emnlp-main.705.mp4"/>
      <pwccode url="https://github.com/hkust-knowcomp/cskb-population" additional="true">hkust-knowcomp/cskb-population</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glucose">GLUCOSE</pwcdataset>
    </paper>
    <paper id="706">
      <title>Enhancing the Context Representation in Similarity-based Word Sense Disambiguation</title>
      <author><first>Ming</first><last>Wang</last></author>
      <author><first>Jianzhang</first><last>Zhang</last></author>
      <author><first>Yinglin</first><last>Wang</last></author>
      <pages>8965–8973</pages>
      <abstract>In previous similarity-based WSD systems, studies have allocated much effort on learning comprehensive sense embeddings using contextual representations and knowledge sources. However, the context embedding of an ambiguous word is learned using only the sentence where the word appears, neglecting its global context. In this paper, we investigate the contribution of both word-level and sense-level global context of an ambiguous word for disambiguation. Experiments have shown that the Context-Oriented Embedding (COE) can enhance a similarity-based system’s performance on WSD by relatively large margins, achieving state-of-the-art on all-words WSD benchmarks in knowledge-based category.</abstract>
      <url hash="64971644">2021.emnlp-main.706</url>
      <attachment type="Software" hash="71b95bd0">2021.emnlp-main.706.Software.zip</attachment>
      <bibkey>wang-etal-2021-enhancing-context</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.706</doi>
      <video href="2021.emnlp-main.706.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="707">
      <title>Data Augmentation with Hierarchical <fixed-case>SQL</fixed-case>-to-Question Generation for Cross-domain Text-to-<fixed-case>SQL</fixed-case> Parsing</title>
      <author><first>Kun</first><last>Wu</last></author>
      <author><first>Lijie</first><last>Wang</last></author>
      <author><first>Zhenghua</first><last>Li</last></author>
      <author><first>Ao</first><last>Zhang</last></author>
      <author><first>Xinyan</first><last>Xiao</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <pages>8974–8983</pages>
      <abstract>Data augmentation has attracted a lot of research attention in the deep learning era for its ability in alleviating data sparseness. The lack of labeled data for unseen evaluation databases is exactly the major challenge for cross-domain text-to-SQL parsing. Previous works either require human intervention to guarantee the quality of generated data, or fail to handle complex SQL queries. This paper presents a simple yet effective data augmentation framework. First, given a database, we automatically produce a large number of SQL queries based on an abstract syntax tree grammar. For better distribution matching, we require that at least 80% of SQL patterns in the training data are covered by generated queries. Second, we propose a hierarchical SQL-to-question generation model to obtain high-quality natural language questions, which is the major contribution of this work. Finally, we design a simple sampling strategy that can greatly improve training efficiency given large amounts of generated data. Experiments on three cross-domain datasets, i.e., WikiSQL and Spider in English, and DuSQL in Chinese, show that our proposed data augmentation framework can consistently improve performance over strong baselines, and the hierarchical generation component is the key for the improvement.</abstract>
      <url hash="cd38f313">2021.emnlp-main.707</url>
      <bibkey>wu-etal-2021-data</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.707</doi>
      <video href="2021.emnlp-main.707.mp4"/>
      <pwccode url="https://github.com/PaddlePaddle/Research" additional="false">PaddlePaddle/Research</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikisql">WikiSQL</pwcdataset>
    </paper>
    <paper id="708">
      <title><fixed-case>SPARQL</fixed-case>ing Database Queries from Intermediate Question Decompositions</title>
      <author><first>Irina</first><last>Saparina</last></author>
      <author><first>Anton</first><last>Osokin</last></author>
      <pages>8984–8998</pages>
      <abstract>To translate natural language questions into executable database queries, most approaches rely on a fully annotated training set. Annotating a large dataset with queries is difficult as it requires query-language expertise. We reduce this burden using grounded in databases intermediate question representations. These representations are simpler to collect and were originally crowdsourced within the Break dataset (Wolfson et al., 2020). Our pipeline consists of two parts: a neural semantic parser that converts natural language questions into the intermediate representations and a non-trainable transpiler to the SPARQL query language (a standard language for accessing knowledge graphs and semantic web). We chose SPARQL because its queries are structurally closer to our intermediate representations (compared to SQL). We observe that the execution accuracy of queries constructed by our model on the challenging Spider dataset is comparable with the state-of-the-art text-to-SQL methods trained with annotated SQL queries. Our code and data are publicly available (https://github.com/yandex-research/sparqling-queries).</abstract>
      <url hash="6f1874d5">2021.emnlp-main.708</url>
      <attachment type="Software" hash="d580fc77">2021.emnlp-main.708.Software.zip</attachment>
      <bibkey>saparina-osokin-2021-sparqling</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.708</doi>
      <video href="2021.emnlp-main.708.mp4"/>
      <pwccode url="https://github.com/yandex-research/sparqling-queries" additional="false">yandex-research/sparqling-queries</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/break">BREAK</pwcdataset>
    </paper>
    <paper id="709">
      <title>Time-aware Graph Neural Network for Entity Alignment between Temporal Knowledge Graphs</title>
      <author><first>Chengjin</first><last>Xu</last></author>
      <author><first>Fenglong</first><last>Su</last></author>
      <author><first>Jens</first><last>Lehmann</last></author>
      <pages>8999–9010</pages>
      <abstract>Entity alignment aims to identify equivalent entity pairs between different knowledge graphs (KGs). Recently, the availability of temporal KGs (TKGs) that contain time information created the need for reasoning over time in such TKGs. Existing embedding-based entity alignment approaches disregard time information that commonly exists in many large-scale KGs, leaving much room for improvement. In this paper, we focus on the task of aligning entity pairs between TKGs and propose a novel Time-aware Entity Alignment approach based on Graph Neural Networks (TEA-GNN). We embed entities, relations and timestamps of different KGs into a vector space and use GNNs to learn entity representations. To incorporate both relation and time information into the GNN structure of our model, we use a self-attention mechanism which assigns different weights to different nodes with orthogonal transformation matrices computed from embeddings of the relevant relations and timestamps in a neighborhood. Experimental results on multiple real-world TKG datasets show that our method significantly outperforms the state-of-the-art methods due to the inclusion of time information.</abstract>
      <url hash="e9ac0f22">2021.emnlp-main.709</url>
      <bibkey>xu-etal-2021-time</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.709</doi>
      <video href="2021.emnlp-main.709.mp4"/>
      <pwccode url="https://github.com/soledad921/tea-gnn" additional="false">soledad921/tea-gnn</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/yago">YAGO</pwcdataset>
    </paper>
    <paper id="710">
      <title>Cross-Domain Label-Adaptive Stance Detection</title>
      <author><first>Momchil</first><last>Hardalov</last></author>
      <author><first>Arnav</first><last>Arora</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <author><first>Isabelle</first><last>Augenstein</last></author>
      <pages>9011–9028</pages>
      <abstract>Stance detection concerns the classification of a writer’s viewpoint towards a target. There are different task variants, e.g., stance of a tweet vs. a full article, or stance with respect to a claim vs. an (implicit) topic. Moreover, task definitions vary, which includes the label inventory, the data collection, and the annotation protocol. All these aspects hinder cross-domain studies, as they require changes to standard domain adaptation approaches. In this paper, we perform an in-depth analysis of 16 stance detection datasets, and we explore the possibility for cross-domain learning from them. Moreover, we propose an end-to-end unsupervised framework for out-of-domain prediction of unseen, user-defined labels. In particular, we combine domain adaptation techniques such as mixture of experts and domain-adversarial training with label embeddings, and we demonstrate sizable performance gains over strong baselines, both (i) in-domain, i.e., for seen targets, and (ii) out-of-domain, i.e., for unseen targets. Finally, we perform an exhaustive analysis of the cross-domain results, and we highlight the important factors influencing the model performance.</abstract>
      <url hash="0a04bf62">2021.emnlp-main.710</url>
      <bibkey>hardalov-etal-2021-cross</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.710</doi>
      <video href="2021.emnlp-main.710.mp4"/>
      <pwccode url="https://github.com/checkstep/mole-stance" additional="false">checkstep/mole-stance</pwccode>
    </paper>
    <paper id="711">
      <title>Text <fixed-case>A</fixed-case>uto<fixed-case>A</fixed-case>ugment: Learning Compositional Augmentation Policy for Text Classification</title>
      <author><first>Shuhuai</first><last>Ren</last></author>
      <author><first>Jinchao</first><last>Zhang</last></author>
      <author><first>Lei</first><last>Li</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>9029–9043</pages>
      <abstract>Data augmentation aims to enrich training samples for alleviating the overfitting issue in low-resource or class-imbalanced situations. Traditional methods first devise task-specific operations such as Synonym Substitute, then preset the corresponding parameters such as the substitution rate artificially, which require a lot of prior knowledge and are prone to fall into the sub-optimum. Besides, the number of editing operations is limited in the previous methods, which decreases the diversity of the augmented data and thus restricts the performance gain. To overcome the above limitations, we propose a framework named Text AutoAugment (TAA) to establish a compositional and learnable paradigm for data augmentation. We regard a combination of various operations as an augmentation policy and utilize an efficient Bayesian Optimization algorithm to automatically search for the best policy, which substantially improves the generalization capability of models. Experiments on six benchmark datasets show that TAA boosts classification accuracy in low-resource and class-imbalanced regimes by an average of 8.8% and 9.7%, respectively, outperforming strong baselines.</abstract>
      <url hash="1fddc3bb">2021.emnlp-main.711</url>
      <bibkey>ren-etal-2021-text</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.711</doi>
      <video href="2021.emnlp-main.711.mp4"/>
      <pwccode url="https://github.com/lancopku/text-autoaugment" additional="false">lancopku/text-autoaugment</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="712">
      <title>Distilling Relation Embeddings from Pretrained Language Models</title>
      <author><first>Asahi</first><last>Ushio</last></author>
      <author><first>Jose</first><last>Camacho-Collados</last></author>
      <author><first>Steven</first><last>Schockaert</last></author>
      <pages>9044–9062</pages>
      <abstract>Pre-trained language models have been found to capture a surprisingly rich amount of lexical knowledge, ranging from commonsense properties of everyday concepts to detailed factual knowledge about named entities. Among others, this makes it possible to distill high-quality word vectors from pre-trained language models. However, it is currently unclear to what extent it is possible to distill relation embeddings, i.e. vectors that characterize the relationship between two words. Such relation embeddings are appealing because they can, in principle, encode relational knowledge in a more fine-grained way than is possible with knowledge graphs. To obtain relation embeddings from a pre-trained language model, we encode word pairs using a (manually or automatically generated) prompt, and we fine-tune the language model such that relationally similar word pairs yield similar output vectors. We find that the resulting relation embeddings are highly competitive on analogy (unsupervised) and relation classification (supervised) benchmarks, even without any task-specific fine-tuning. Source code to reproduce our experimental results and the model checkpoints are available in the following repository: https://github.com/asahi417/relbert</abstract>
      <url hash="c43be5b6">2021.emnlp-main.712</url>
      <bibkey>ushio-etal-2021-distilling</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.712</doi>
      <video href="2021.emnlp-main.712.mp4"/>
      <pwccode url="https://github.com/asahi417/relbert" additional="false">asahi417/relbert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptnet">ConceptNet</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/evalution">EVALution</pwcdataset>
    </paper>
    <paper id="713">
      <title>Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning</title>
      <author><first>Prasetya</first><last>Utama</last></author>
      <author><first>Nafise Sadat</first><last>Moosavi</last></author>
      <author><first>Victor</first><last>Sanh</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>9063–9074</pages>
      <abstract>Recent prompt-based approaches allow pretrained language models to achieve strong performances on few-shot finetuning by reformulating downstream tasks as a language modeling problem. In this work, we demonstrate that, despite its advantages on low data regimes, finetuned prompt-based models for sentence pair classification tasks still suffer from a common pitfall of adopting inference heuristics based on lexical overlap, e.g., models incorrectly assuming a sentence pair is of the same meaning because they consist of the same set of words. Interestingly, we find that this particular inference heuristic is significantly less present in the zero-shot evaluation of the prompt-based model, indicating how finetuning can be destructive to useful knowledge learned during the pretraining. We then show that adding a regularization that preserves pretraining weights is effective in mitigating this destructive tendency of few-shot finetuning. Our evaluation on three datasets demonstrates promising improvements on the three corresponding challenge datasets used to diagnose the inference heuristics.</abstract>
      <url hash="87d8a3f1">2021.emnlp-main.713</url>
      <bibkey>utama-etal-2021-avoiding</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.713</doi>
      <video href="2021.emnlp-main.713.mp4"/>
      <pwccode url="https://github.com/ukplab/emnlp2021-prompt-ft-heuristics" additional="false">ukplab/emnlp2021-prompt-ft-heuristics</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws">PAWS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="714">
      <title>A Differentiable Relaxation of Graph Segmentation and Alignment for <fixed-case>AMR</fixed-case> Parsing</title>
      <author><first>Chunchuan</first><last>Lyu</last></author>
      <author><first>Shay B.</first><last>Cohen</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <pages>9075–9091</pages>
      <abstract>Abstract Meaning Representations (AMR) are a broad-coverage semantic formalism which represents sentence meaning as a directed acyclic graph. To train most AMR parsers, one needs to segment the graph into subgraphs and align each such subgraph to a word in a sentence; this is normally done at preprocessing, relying on hand-crafted rules. In contrast, we treat both alignment and segmentation as latent variables in our model and induce them as part of end-to-end training. As marginalizing over the structured latent variables is infeasible, we use the variational autoencoding framework. To ensure end-to-end differentiable optimization, we introduce a differentiable relaxation of the segmentation and alignment problems. We observe that inducing segmentation yields substantial gains over using a ‘greedy’ segmentation heuristic. The performance of our method also approaches that of a model that relies on the segmentation rules of Lyu and Titov (2018), which were hand-crafted to handle individual AMR constructions.</abstract>
      <url hash="3c4f9833">2021.emnlp-main.714</url>
      <bibkey>lyu-etal-2021-differentiable</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.714</doi>
      <video href="2021.emnlp-main.714.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ldc2017t10">LDC2017T10</pwcdataset>
    </paper>
    <paper id="715">
      <title>Integrating Personalized <fixed-case>P</fixed-case>age<fixed-case>R</fixed-case>ank into Neural Word Sense Disambiguation</title>
      <author><first>Ahmed</first><last>El Sheikh</last></author>
      <author><first>Michele</first><last>Bevilacqua</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>9092–9098</pages>
      <abstract>Neural Word Sense Disambiguation (WSD) has recently been shown to benefit from the incorporation of pre-existing knowledge, such as that coming from the WordNet graph. However, state-of-the-art approaches have been successful in exploiting only the local structure of the graph, with only close neighbors of a given synset influencing the prediction. In this work, we improve a classification model by recomputing logits as a function of both the vanilla independently produced logits and the global WordNet graph. We achieve this by incorporating an online neural approximated PageRank, which enables us to refine edge weights as well. This method exploits the global graph structure while keeping space requirements linear in the number of edges. We obtain strong improvements, matching the current state of the art. Code is available at https://github.com/SapienzaNLP/neural-pagerank-wsd</abstract>
      <url hash="4e2100ea">2021.emnlp-main.715</url>
      <bibkey>el-sheikh-etal-2021-integrating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.715</doi>
      <video href="2021.emnlp-main.715.mp4"/>
      <pwccode url="https://github.com/sapienzanlp/neural-pagerank-wsd" additional="false">sapienzanlp/neural-pagerank-wsd</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="716">
      <title>Cross-lingual Sentence Embedding using Multi-Task Learning</title>
      <author><first>Koustava</first><last>Goswami</last></author>
      <author><first>Sourav</first><last>Dutta</last></author>
      <author><first>Haytham</first><last>Assem</last></author>
      <author><first>Theodorus</first><last>Fransen</last></author>
      <author><first>John P.</first><last>McCrae</last></author>
      <pages>9099–9113</pages>
      <abstract>Multilingual sentence embeddings capture rich semantic information not only for measuring similarity between texts but also for catering to a broad range of downstream cross-lingual NLP tasks. State-of-the-art multilingual sentence embedding models require large parallel corpora to learn efficiently, which confines the scope of these models. In this paper, we propose a novel sentence embedding framework based on an unsupervised loss function for generating effective multilingual sentence embeddings, eliminating the need for parallel corpora. We capture semantic similarity and relatedness between sentences using a multi-task loss function for training a dual encoder model mapping different languages onto the same vector space. We demonstrate the efficacy of an unsupervised as well as a weakly supervised variant of our framework on STS, BUCC and Tatoeba benchmark tasks. The proposed unsupervised sentence embedding framework outperforms even supervised state-of-the-art methods for certain under-resourced languages on the Tatoeba dataset and on a monolingual benchmark. Further, we show enhanced zero-shot learning capabilities for more than 30 languages, with the model being trained on only 13 languages. Our model can be extended to a wide range of languages from any language family, as it overcomes the requirement of parallel corpora for training.</abstract>
      <url hash="3d526338">2021.emnlp-main.716</url>
      <bibkey>goswami-etal-2021-cross</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.716</doi>
      <video href="2021.emnlp-main.716.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/bucc">BUCC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="717">
      <title><fixed-case>NB</fixed-case>-<fixed-case>MLM</fixed-case>: Efficient Domain Adaptation of Masked Language Models for Sentiment Analysis</title>
      <author><first>Nikolay</first><last>Arefyev</last></author>
      <author><first>Dmitrii</first><last>Kharchev</last></author>
      <author><first>Artem</first><last>Shelmanov</last></author>
      <pages>9114–9124</pages>
      <abstract>While Masked Language Models (MLM) are pre-trained on massive datasets, the additional training with the MLM objective on domain or task-specific data before fine-tuning for the final task is known to improve the final performance. This is usually referred to as the domain or task adaptation step. However, unlike the initial pre-training, this step is performed for each domain or task individually and is still rather slow, requiring several GPU days compared to several GPU hours required for the final task fine-tuning. We argue that the standard MLM objective leads to inefficiency when it is used for the adaptation step because it mostly learns to predict the most frequent words, which are not necessarily related to a final task. We propose a technique for more efficient adaptation that focuses on predicting words with large weights of the Naive Bayes classifier trained for the task at hand, which are likely more relevant than the most frequent words. The proposed method provides faster adaptation and better final performance for sentiment analysis compared to the standard approach.</abstract>
      <url hash="e329a0a1">2021.emnlp-main.717</url>
      <bibkey>arefyev-etal-2021-nb</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.717</doi>
      <video href="2021.emnlp-main.717.mp4"/>
      <pwccode url="https://github.com/nvanva/nb-mlm" additional="false">nvanva/nb-mlm</pwccode>
    </paper>
    <paper id="718">
      <title>Revisiting Self-training for Few-shot Learning of Language Model</title>
      <author><first>Yiming</first><last>Chen</last></author>
      <author><first>Yan</first><last>Zhang</last></author>
      <author><first>Chen</first><last>Zhang</last></author>
      <author><first>Grandee</first><last>Lee</last></author>
      <author><first>Ran</first><last>Cheng</last></author>
      <author><first>Haizhou</first><last>Li</last></author>
      <pages>9125–9135</pages>
      <abstract>As unlabeled data carry rich task-relevant information, they are proven useful for few-shot learning of language model. The question is how to effectively make use of such data. In this work, we revisit the self-training technique for language model fine-tuning and present a state-of-the-art prompt-based few-shot learner, SFLM. Given two views of a text sample via weak and strong augmentation techniques, SFLM generates a pseudo label on the weakly augmented version. Then, the model predicts the same pseudo label when fine-tuned with the strongly augmented version. This simple approach is shown to outperform other state-of-the-art supervised and semi-supervised counterparts on six sentence classification and six sentence-pair classification benchmarking tasks. In addition, SFLM only relies on a few in-domain unlabeled data. We conduct a comprehensive analysis to demonstrate the robustness of our proposed approach under various settings, including augmentation techniques, model scale, and few-shot knowledge transfer across tasks.</abstract>
      <url hash="5f86eba3">2021.emnlp-main.718</url>
      <bibkey>chen-etal-2021-revisiting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.718</doi>
      <video href="2021.emnlp-main.718.mp4"/>
      <pwccode url="https://github.com/matthewcym/sflm" additional="false">matthewcym/sflm</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="719">
      <title>Bridging Perception, Memory, and Inference through Semantic Relations</title>
      <author><first>Johanna</first><last>Björklund</last></author>
      <author><first>Adam</first><last>Dahlgren Lindström</last></author>
      <author><first>Frank</first><last>Drewes</last></author>
      <pages>9136–9142</pages>
      <abstract>There is a growing consensus that surface form alone does not enable models to learn meaning and gain language understanding. This warrants an interest in hybrid systems that combine the strengths of neural and symbolic methods. We favour triadic systems consisting of neural networks, knowledge bases, and inference engines. The network provides perception, that is, the interface between the system and its environment. The knowledge base provides explicit memory and thus immediate access to established facts. Finally, inference capabilities are provided by the inference engine which reflects on the perception, supported by memory, to reason and discover new facts. In this work, we probe six popular language models for semantic relations and outline a future line of research to study how the constituent subsystems can be jointly realised and integrated.</abstract>
      <url hash="e57ef23e">2021.emnlp-main.719</url>
      <bibkey>bjorklund-etal-2021-bridging</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.719</doi>
      <video href="2021.emnlp-main.719.mp4"/>
    </paper>
    <paper id="720">
      <title>Unimodal and Crossmodal Refinement Network for Multimodal Sequence Fusion</title>
      <author><first>Xiaobao</first><last>Guo</last></author>
      <author><first>Adams</first><last>Kong</last></author>
      <author><first>Huan</first><last>Zhou</last></author>
      <author><first>Xianfeng</first><last>Wang</last></author>
      <author><first>Min</first><last>Wang</last></author>
      <pages>9143–9153</pages>
      <abstract>Effective unimodal representation and complementary crossmodal representation fusion are both important in multimodal representation learning. Prior works often modulate one modal feature to another straightforwardly and thus, underutilizing both unimodal and crossmodal representation refinements, which incurs a bottleneck of performance improvement. In this paper, Unimodal and Crossmodal Refinement Network (UCRN) is proposed to enhance both unimodal and crossmodal representations. Specifically, to improve unimodal representations, a unimodal refinement module is designed to refine modality-specific learning via iteratively updating the distribution with transformer-based attention layers. Self-quality improvement layers are followed to generate the desired weighted representations progressively. Subsequently, those unimodal representations are projected into a common latent space, regularized by a multimodal Jensen-Shannon divergence loss for better crossmodal refinement. Lastly, a crossmodal refinement module is employed to integrate all information. By hierarchical explorations on unimodal, bimodal, and trimodal interactions, UCRN is highly robust against missing modality and noisy data. Experimental results on MOSI and MOSEI datasets illustrated that the proposed UCRN outperforms recent state-of-the-art techniques and its robustness is highly preferred in real multimodal sequence fusion scenarios. Codes will be shared publicly.</abstract>
      <url hash="cdccbf14">2021.emnlp-main.720</url>
      <bibkey>guo-etal-2021-unimodal</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.720</doi>
      <video href="2021.emnlp-main.720.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cmu-mosei">CMU-MOSEI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multimodal-opinionlevel-sentiment-intensity">Multimodal Opinionlevel Sentiment Intensity</pwcdataset>
    </paper>
    <paper id="721">
      <title><fixed-case>YASO</fixed-case>: <fixed-case>A</fixed-case> Targeted Sentiment Analysis Evaluation Dataset for Open-Domain Reviews</title>
      <author><first>Matan</first><last>Orbach</last></author>
      <author><first>Orith</first><last>Toledo-Ronen</last></author>
      <author><first>Artem</first><last>Spector</last></author>
      <author><first>Ranit</first><last>Aharonov</last></author>
      <author><first>Yoav</first><last>Katz</last></author>
      <author><first>Noam</first><last>Slonim</last></author>
      <pages>9154–9173</pages>
      <abstract>Current TSA evaluation in a cross-domain setup is restricted to the small set of review domains available in existing datasets. Such an evaluation is limited, and may not reflect true performance on sites like Amazon or Yelp that host diverse reviews from many domains. To address this gap, we present YASO – a new TSA evaluation dataset of open-domain user reviews. YASO contains 2,215 English sentences from dozens of review domains, annotated with target terms and their sentiment. Our analysis verifies the reliability of these annotations, and explores the characteristics of the collected data. Benchmark results using five contemporary TSA systems show there is ample room for improvement on this challenging new dataset. YASO is available at https://github.com/IBM/yaso-tsa.</abstract>
      <url hash="6a787f6e">2021.emnlp-main.721</url>
      <bibkey>orbach-etal-2021-yaso</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.721</doi>
      <video href="2021.emnlp-main.721.mp4"/>
      <pwccode url="https://github.com/IBM/yaso-tsa" additional="true">IBM/yaso-tsa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/yaso">YASO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="722">
      <title>An Empirical Study on Leveraging Position Embeddings for Target-oriented Opinion Words Extraction</title>
      <author><first>Samuel</first><last>Mensah</last></author>
      <author><first>Kai</first><last>Sun</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <pages>9174–9179</pages>
      <abstract>Target-oriented opinion words extraction (TOWE) (Fan et al., 2019b) is a new subtask of target-oriented sentiment analysis that aims to extract opinion words for a given aspect in text. Current state-of-the-art methods leverage position embeddings to capture the relative position of a word to the target. However, the performance of these methods depends on the ability to incorporate this information into word representations. In this paper, we explore a variety of text encoders based on pretrained word embeddings or language models that leverage part-of-speech and position embeddings, aiming to examine the actual contribution of each component in TOWE. We also adapt a graph convolutional network (GCN) to enhance word representations by incorporating syntactic information. Our experimental results demonstrate that BiLSTM-based models can effectively encode position information into word representations while using a GCN only achieves marginal gains. Interestingly, our simple methods outperform several state-of-the-art complex neural structures.</abstract>
      <url hash="5bbc563c">2021.emnlp-main.722</url>
      <bibkey>mensah-etal-2021-empirical</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.722</doi>
      <video href="2021.emnlp-main.722.mp4"/>
      <pwccode url="https://github.com/samensah/encoders_towe_emnlp2021" additional="false">samensah/encoders_towe_emnlp2021</pwccode>
    </paper>
    <paper id="723">
      <title>Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis</title>
      <author><first>Wei</first><last>Han</last></author>
      <author><first>Hui</first><last>Chen</last></author>
      <author><first>Soujanya</first><last>Poria</last></author>
      <pages>9180–9192</pages>
      <abstract>In multimodal sentiment analysis (MSA), the performance of a model highly depends on the quality of synthesized embeddings. These embeddings are generated from the upstream process called multimodal fusion, which aims to extract and combine the input unimodal raw data to produce a richer multimodal representation. Previous work either back-propagates the task loss or manipulates the geometric property of feature spaces to produce favorable fusion results, which neglects the preservation of critical task-related information that flows from input to the fusion results. In this work, we propose a framework named MultiModal InfoMax (MMIM), which hierarchically maximizes the Mutual Information (MI) in unimodal input pairs (inter-modality) and between multimodal fusion result and unimodal input in order to maintain task-related information through multimodal fusion. The framework is jointly trained with the main task (MSA) to improve the performance of the downstream MSA task. To address the intractable issue of MI bounds, we further formulate a set of computationally simple parametric and non-parametric methods to approximate their truth value. Experimental results on the two widely used datasets demonstrate the efficacy of our approach.</abstract>
      <url hash="b4b944a6">2021.emnlp-main.723</url>
      <bibkey>han-etal-2021-improving</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.723</doi>
      <video href="2021.emnlp-main.723.mp4"/>
      <pwccode url="https://github.com/declare-lab/multimodal-deep-learning" additional="true">declare-lab/multimodal-deep-learning</pwccode>
    </paper>
    <paper id="724">
      <title><fixed-case>BERT</fixed-case>4<fixed-case>GCN</fixed-case>: Using <fixed-case>BERT</fixed-case> Intermediate Layers to Augment <fixed-case>GCN</fixed-case> for Aspect-based Sentiment Classification</title>
      <author><first>Zeguan</first><last>Xiao</last></author>
      <author><first>Jiarun</first><last>Wu</last></author>
      <author><first>Qingliang</first><last>Chen</last></author>
      <author><first>Congjian</first><last>Deng</last></author>
      <pages>9193–9200</pages>
      <abstract>Graph-based Aspect-based Sentiment Classification (ABSC) approaches have yielded state-of-the-art results, expecially when equipped with contextual word embedding from pre-training language models (PLMs). However, they ignore sequential features of the context and have not yet made the best of PLMs. In this paper, we propose a novel model, BERT4GCN, which integrates the grammatical sequential features from the PLM of BERT, and the syntactic knowledge from dependency graphs. BERT4GCN utilizes outputs from intermediate layers of BERT and positional information between words to augment GCN (Graph Convolutional Network) to better encode the dependency graphs for the downstream classification. Experimental results demonstrate that the proposed BERT4GCN outperforms all state-of-the-art baselines, justifying that augmenting GCN with the grammatical features from intermediate layers of BERT can significantly empower ABSC models.</abstract>
      <url hash="3b6f6e0c">2021.emnlp-main.724</url>
      <bibkey>xiao-etal-2021-bert4gcn</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.724</doi>
      <video href="2021.emnlp-main.724.mp4"/>
    </paper>
    <paper id="725">
      <title>Does Social Pressure Drive Persuasion in Online Fora?</title>
      <author><first>Ayush</first><last>Jain</last></author>
      <author><first>Shashank</first><last>Srivastava</last></author>
      <pages>9201–9208</pages>
      <abstract>Online forums such as ChangeMyView have been explored to research aspects of persuasion and argumentative quality in language. While previous research has focused on arguments between a view-holder and a persuader, we explore the premise that apart from the merits of arguments, persuasion is influenced by the ambient social community. We hypothesize that comments from the rest of the community can either affirm the original view or implicitly exert pressure to change it. We develop a structured model to capture the ambient community’s sentiment towards the discussion and its effect on persuasion. Our experiments show that social features themselves are significantly predictive of persuasion (even without looking at the actual content of discussion), with performance comparable to some earlier approaches that use content features. Combining community and content features leads to overall performance of 78.5% on the persuasion prediction task. Our analyses suggest that the effect of social pressure is comparable to the difference between persuasive and non-persuasive language strategies in driving persuasion and that social pressure might be a causal factor for persuasion.</abstract>
      <url hash="320a3ed5">2021.emnlp-main.725</url>
      <bibkey>jain-srivastava-2021-social</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.725</doi>
      <video href="2021.emnlp-main.725.mp4"/>
    </paper>
    <paper id="726">
      <title>Aspect Sentiment Quad Prediction as Paraphrase Generation</title>
      <author><first>Wenxuan</first><last>Zhang</last></author>
      <author><first>Yang</first><last>Deng</last></author>
      <author><first>Xin</first><last>Li</last></author>
      <author><first>Yifei</first><last>Yuan</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <pages>9209–9219</pages>
      <abstract>Aspect-based sentiment analysis (ABSA) has been extensively studied in recent years, which typically involves four fundamental sentiment elements, including the aspect category, aspect term, opinion term, and sentiment polarity. Existing studies usually consider the detection of partial sentiment elements, instead of predicting the four elements in one shot. In this work, we introduce the Aspect Sentiment Quad Prediction (ASQP) task, aiming to jointly detect all sentiment elements in quads for a given opinionated sentence, which can reveal a more comprehensive and complete aspect-level sentiment structure. We further propose a novel Paraphrase modeling paradigm to cast the ASQP task to a paraphrase generation process. On one hand, the generation formulation allows solving ASQP in an end-to-end manner, alleviating the potential error propagation in the pipeline solution. On the other hand, the semantics of the sentiment elements can be fully exploited by learning to generate them in the natural language form. Extensive experiments on benchmark datasets show the superiority of our proposed method and the capacity of cross-task transfer with the proposed unified Paraphrase modeling framework.</abstract>
      <url hash="e8e78567">2021.emnlp-main.726</url>
      <bibkey>zhang-etal-2021-aspect-sentiment</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.726</doi>
      <video href="2021.emnlp-main.726.mp4"/>
      <pwccode url="https://github.com/isakzhang/absa-quad" additional="false">isakzhang/absa-quad</pwccode>
    </paper>
    <paper id="727">
      <title>Cross-lingual Aspect-based Sentiment Analysis with Aspect Term Code-Switching</title>
      <author><first>Wenxuan</first><last>Zhang</last></author>
      <author><first>Ruidan</first><last>He</last></author>
      <author><first>Haiyun</first><last>Peng</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <pages>9220–9230</pages>
      <abstract>Many efforts have been made in solving the Aspect-based sentiment analysis (ABSA) task. While most existing studies focus on English texts, handling ABSA in resource-poor languages remains a challenging problem. In this paper, we consider the unsupervised cross-lingual transfer for the ABSA task, where only labeled data in the source language is available and we aim at transferring its knowledge to the target language having no labeled data. To this end, we propose an alignment-free label projection method to obtain high-quality pseudo-labeled data of the target language with the help of the translation system, which could preserve more accurate task-specific knowledge in the target language. For better utilizing the source and translated data, as well as enhancing the cross-lingual alignment, we design an aspect code-switching mechanism to augment the training data with code-switched bilingual sentences. To further investigate the importance of language-specific knowledge in solving the ABSA problem, we distill the above model on the unlabeled target language data which improves the performance to the same level of the supervised method.</abstract>
      <url hash="0b5024cc">2021.emnlp-main.727</url>
      <bibkey>zhang-etal-2021-cross</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.727</doi>
      <video href="2021.emnlp-main.727.mp4"/>
      <pwccode url="https://github.com/isakzhang/xabsa" additional="false">isakzhang/xabsa</pwccode>
    </paper>
    <paper id="728">
      <title>Towards Label-Agnostic Emotion Embeddings</title>
      <author><first>Sven</first><last>Buechel</last></author>
      <author><first>Luise</first><last>Modersohn</last></author>
      <author><first>Udo</first><last>Hahn</last></author>
      <pages>9231–9249</pages>
      <abstract>Research in emotion analysis is scattered across different label formats (e.g., polarity types, basic emotion categories, and affective dimensions), linguistic levels (word vs. sentence vs. discourse), and, of course, (few well-resourced but much more under-resourced) natural languages and text genres (e.g., product reviews, tweets, news). The resulting heterogeneity makes data and software developed under these conflicting constraints hard to compare and challenging to integrate. To resolve this unsatisfactory state of affairs we here propose a training scheme that learns a shared latent representation of emotion independent from different label formats, natural languages, and even disparate model architectures. Experiments on a wide range of datasets indicate that this approach yields the desired interoperability without penalizing prediction quality. Code and data are archived under DOI 10.5281/zenodo.5466068.</abstract>
      <url hash="c429039c">2021.emnlp-main.728</url>
      <bibkey>buechel-etal-2021-towards</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.728</doi>
      <video href="2021.emnlp-main.728.mp4"/>
    </paper>
    <paper id="729">
      <title>Collaborative Learning of Bidirectional Decoders for Unsupervised Text Style Transfer</title>
      <author><first>Yun</first><last>Ma</last></author>
      <author><first>Yangbin</first><last>Chen</last></author>
      <author><first>Xudong</first><last>Mao</last></author>
      <author><first>Qing</first><last>Li</last></author>
      <pages>9250–9266</pages>
      <abstract>Unsupervised text style transfer aims to alter the underlying style of the text to a desired value while keeping its style-independent semantics, without the support of parallel training corpora. Existing methods struggle to achieve both high style conversion rate and low content loss, exhibiting the over-transfer and under-transfer problems. We attribute these problems to the conflicting driving forces of the style conversion goal and content preservation goal. In this paper, we propose a collaborative learning framework for unsupervised text style transfer using a pair of bidirectional decoders, one decoding from left to right while the other decoding from right to left. In our collaborative learning mechanism, each decoder is regularized by knowledge from its peer which has a different knowledge acquisition process. The difference is guaranteed by their opposite decoding directions and a distinguishability constraint. As a result, mutual knowledge distillation drives both decoders to a better optimum and alleviates the over-transfer and under-transfer problems. Experimental results on two benchmark datasets show that our framework achieves strong empirical results on both style compatibility and content preservation.</abstract>
      <url hash="1dd92b9b">2021.emnlp-main.729</url>
      <bibkey>ma-etal-2021-collaborative</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.729</doi>
      <video href="2021.emnlp-main.729.mp4"/>
      <pwccode url="https://github.com/sunlight-ym/cbd_style_transfer" additional="false">sunlight-ym/cbd_style_transfer</pwccode>
    </paper>
    <paper id="730">
      <title>Exploring Non-Autoregressive Text Style Transfer</title>
      <author><first>Yun</first><last>Ma</last></author>
      <author><first>Qing</first><last>Li</last></author>
      <pages>9267–9278</pages>
      <abstract>In this paper, we explore Non-AutoRegressive (NAR) decoding for unsupervised text style transfer. We first propose a base NAR model by directly adapting the common training scheme from its AutoRegressive (AR) counterpart. Despite the faster inference speed over the AR model, this NAR model sacrifices its transfer performance due to the lack of conditional dependence between output tokens. To this end, we investigate three techniques, i.e., knowledge distillation, contrastive learning, and iterative decoding, for performance enhancement. Experimental results on two benchmark datasets suggest that, although the base NAR model is generally inferior to AR decoding, their performance gap can be clearly narrowed when empowering NAR decoding with knowledge distillation, contrastive learning, and iterative decoding.</abstract>
      <url hash="78b4edf6">2021.emnlp-main.730</url>
      <bibkey>ma-li-2021-exploring</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.730</doi>
      <video href="2021.emnlp-main.730.mp4"/>
      <pwccode url="https://github.com/sunlight-ym/nar_style_transfer" additional="false">sunlight-ym/nar_style_transfer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/gyafc">GYAFC</pwcdataset>
    </paper>
    <paper id="731">
      <title><fixed-case>PASTE</fixed-case>: A Tagging-Free Decoding Framework Using Pointer Networks for Aspect Sentiment Triplet Extraction</title>
      <author><first>Rajdeep</first><last>Mukherjee</last></author>
      <author><first>Tapas</first><last>Nayak</last></author>
      <author><first>Yash</first><last>Butala</last></author>
      <author><first>Sourangshu</first><last>Bhattacharya</last></author>
      <author><first>Pawan</first><last>Goyal</last></author>
      <pages>9279–9291</pages>
      <abstract>Aspect Sentiment Triplet Extraction (ASTE) deals with extracting opinion triplets, consisting of an opinion target or aspect, its associated sentiment, and the corresponding opinion term/span explaining the rationale behind the sentiment. Existing research efforts are majorly tagging-based. Among the methods taking a sequence tagging approach, some fail to capture the strong interdependence between the three opinion factors, whereas others fall short of identifying triplets with overlapping aspect/opinion spans. A recent grid tagging approach on the other hand fails to capture the span-level semantics while predicting the sentiment between an aspect-opinion pair. Different from these, we present a tagging-free solution for the task, while addressing the limitations of the existing works. We adapt an encoder-decoder architecture with a Pointer Network-based decoding framework that generates an entire opinion triplet at each time step thereby making our solution end-to-end. Interactions between the aspects and opinions are effectively captured by the decoder by considering their entire detected spans while predicting their connecting sentiment. Extensive experiments on several benchmark datasets establish the better efficacy of our proposed approach, especially in recall, and in predicting multiple and aspect/opinion-overlapped triplets from the same review sentence. We report our results both with and without BERT and also demonstrate the utility of domain-specific BERT post-training for the task.</abstract>
      <url hash="c219823b">2021.emnlp-main.731</url>
      <bibkey>mukherjee-etal-2021-paste</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.731</doi>
      <video href="2021.emnlp-main.731.mp4"/>
      <pwccode url="https://github.com/rajdeep345/paste" additional="false">rajdeep345/paste</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/aste-data-v2">ASTE-Data-V2</pwcdataset>
    </paper>
    <paper id="732">
      <title>Adaptive Proposal Generation Network for Temporal Sentence Localization in Videos</title>
      <author><first>Daizong</first><last>Liu</last></author>
      <author><first>Xiaoye</first><last>Qu</last></author>
      <author><first>Jianfeng</first><last>Dong</last></author>
      <author><first>Pan</first><last>Zhou</last></author>
      <pages>9292–9301</pages>
      <abstract>We address the problem of temporal sentence localization in videos (TSLV). Traditional methods follow a top-down framework which localizes the target segment with pre-defined segment proposals. Although they have achieved decent performance, the proposals are handcrafted and redundant. Recently, bottom-up framework attracts increasing attention due to its superior efficiency. It directly predicts the probabilities for each frame as a boundary. However, the performance of bottom-up model is inferior to the top-down counterpart as it fails to exploit the segment-level interaction. In this paper, we propose an Adaptive Proposal Generation Network (APGN) to maintain the segment-level interaction while speeding up the efficiency. Specifically, we first perform a foreground-background classification upon the video and regress on the foreground frames to adaptively generate proposals. In this way, the handcrafted proposal design is discarded and the redundant proposals are decreased. Then, a proposal consolidation module is further developed to enhance the semantics of the generated proposals. Finally, we locate the target moments with these generated proposals following the top-down framework. Extensive experiments show that our proposed APGN significantly outperforms previous state-of-the-art methods on three challenging benchmarks.</abstract>
      <url hash="87734e67">2021.emnlp-main.732</url>
      <bibkey>liu-etal-2021-adaptive</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.732</doi>
      <video href="2021.emnlp-main.732.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/activitynet-captions">ActivityNet Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/charades-sta">Charades-STA</pwcdataset>
    </paper>
    <paper id="733">
      <title>Progressively Guide to Attend: An Iterative Alignment Framework for Temporal Sentence Grounding</title>
      <author><first>Daizong</first><last>Liu</last></author>
      <author><first>Xiaoye</first><last>Qu</last></author>
      <author><first>Pan</first><last>Zhou</last></author>
      <pages>9302–9311</pages>
      <abstract>A key solution to temporal sentence grounding (TSG) exists in how to learn effective alignment between vision and language features extracted from an untrimmed video and a sentence description. Existing methods mainly leverage vanilla soft attention to perform the alignment in a single-step process. However, such single-step attention is insufficient in practice, since complicated relations between inter- and intra-modality are usually obtained through multi-step reasoning. In this paper, we propose an Iterative Alignment Network (IA-Net) for TSG task, which iteratively interacts inter- and intra-modal features within multiple steps for more accurate grounding. Specifically, during the iterative reasoning process, we pad multi-modal features with learnable parameters to alleviate the nowhere-to-attend problem of non-matched frame-word pairs, and enhance the basic co-attention mechanism in a parallel manner. To further calibrate the misaligned attention caused by each reasoning step, we also devise a calibration module following each attention module to refine the alignment knowledge. With such iterative alignment scheme, our IA-Net can robustly capture the fine-grained relations between vision and language domains step-by-step for progressively reasoning the temporal boundaries. Extensive experiments conducted on three challenging benchmarks demonstrate that our proposed model performs better than the state-of-the-arts.</abstract>
      <url hash="d7fa8de9">2021.emnlp-main.733</url>
      <bibkey>liu-etal-2021-progressively</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.733</doi>
      <video href="2021.emnlp-main.733.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/activitynet-captions">ActivityNet Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/charades">Charades</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/charades-sta">Charades-STA</pwcdataset>
    </paper>
    <paper id="734">
      <title>Language Models are Few-Shot Butlers</title>
      <author><first>Vincent</first><last>Micheli</last></author>
      <author><first>Francois</first><last>Fleuret</last></author>
      <pages>9312–9318</pages>
      <abstract>Pretrained language models demonstrate strong performance in most NLP tasks when fine-tuned on small task-specific datasets. Hence, these autoregressive models constitute ideal agents to operate in text-based environments where language understanding and generative capabilities are essential. Nonetheless, collecting expert demonstrations in such environments is a time-consuming endeavour. We introduce a two-stage procedure to learn from a small set of demonstrations and further improve by interacting with an environment. We show that language models fine-tuned with only 1.2% of the expert demonstrations and a simple reinforcement learning algorithm achieve a 51% absolute improvement in success rate over existing methods in the ALFWorld environment.</abstract>
      <url hash="433dab29">2021.emnlp-main.734</url>
      <bibkey>micheli-fleuret-2021-language</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.734</doi>
      <video href="2021.emnlp-main.734.mp4"/>
      <pwccode url="https://github.com/vmicheli/lm-butlers" additional="false">vmicheli/lm-butlers</pwccode>
    </paper>
    <paper id="735">
      <title><fixed-case>R</fixed-case>ˆ3<fixed-case>N</fixed-case>et:Relation-embedded Representation Reconstruction Network for Change Captioning</title>
      <author><first>Yunbin</first><last>Tu</last></author>
      <author><first>Liang</first><last>Li</last></author>
      <author><first>Chenggang</first><last>Yan</last></author>
      <author><first>Shengxiang</first><last>Gao</last></author>
      <author><first>Zhengtao</first><last>Yu</last></author>
      <pages>9319–9329</pages>
      <abstract>Change captioning is to use a natural language sentence to describe the fine-grained disagreement between two similar images. Viewpoint change is the most typical distractor in this task, because it changes the scale and location of the objects and overwhelms the representation of real change. In this paper, we propose a Relation-embedded Representation Reconstruction Network (Rˆ3Net) to explicitly distinguish the real change from the large amount of clutter and irrelevant changes. Specifically, a relation-embedded module is first devised to explore potential changed objects in the large amount of clutter. Then, based on the semantic similarities of corresponding locations in the two images, a representation reconstruction module (RRM) is designed to learn the reconstruction representation and further model the difference representation. Besides, we introduce a syntactic skeleton predictor (SSP) to enhance the semantic interaction between change localization and caption generation. Extensive experiments show that the proposed method achieves the state-of-the-art results on two public datasets.</abstract>
      <url hash="57e03dc5">2021.emnlp-main.735</url>
      <bibkey>tu-etal-2021-r</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.735</doi>
      <video href="2021.emnlp-main.735.mp4"/>
      <pwccode url="https://github.com/tuyunbin/r3net" additional="false">tuyunbin/r3net</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/spot-the-diff">Spot-the-diff</pwcdataset>
    </paper>
    <paper id="736">
      <title>Looking for Confirmations: An Effective and Human-Like Visual Dialogue Strategy</title>
      <author><first>Alberto</first><last>Testoni</last></author>
      <author><first>Raffaella</first><last>Bernardi</last></author>
      <pages>9330–9338</pages>
      <abstract>Generating goal-oriented questions in Visual Dialogue tasks is a challenging and longstanding problem. State-Of-The-Art systems are shown to generate questions that, although grammatically correct, often lack an effective strategy and sound unnatural to humans. Inspired by the cognitive literature on information search and cross-situational word learning, we design Confirm-it, a model based on a beam search re-ranking algorithm that guides an effective goal-oriented strategy by asking questions that confirm the model’s conjecture about the referent. We take the GuessWhat?! game as a case-study. We show that dialogues generated by Confirm-it are more natural and effective than beam search decoding without re-ranking.</abstract>
      <url hash="c24b5e9c">2021.emnlp-main.736</url>
      <bibkey>testoni-bernardi-2021-looking</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.736</doi>
      <video href="2021.emnlp-main.736.mp4"/>
      <pwccode url="https://github.com/albertotestoni/confirm_it" additional="false">albertotestoni/confirm_it</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/guesswhat">GuessWhat?!</pwcdataset>
    </paper>
    <paper id="737">
      <title>A Unified Speaker Adaptation Approach for <fixed-case>ASR</fixed-case></title>
      <author><first>Yingzhu</first><last>Zhao</last></author>
      <author><first>Chongjia</first><last>Ni</last></author>
      <author><first>Cheung-Chi</first><last>Leung</last></author>
      <author><first>Shafiq</first><last>Joty</last></author>
      <author><first>Eng Siong</first><last>Chng</last></author>
      <author><first>Bin</first><last>Ma</last></author>
      <pages>9339–9349</pages>
      <abstract>Transformer models have been used in automatic speech recognition (ASR) successfully and yields state-of-the-art results. However, its performance is still affected by speaker mismatch between training and test data. Further finetuning a trained model with target speaker data is the most natural approach for adaptation, but it takes a lot of compute and may cause catastrophic forgetting to the existing speakers. In this work, we propose a unified speaker adaptation approach consisting of feature adaptation and model adaptation. For feature adaptation, we employ a speaker-aware persistent memory model which generalizes better to unseen test speakers by making use of speaker i-vectors to form a persistent memory. For model adaptation, we use a novel gradual pruning method to adapt to target speakers without changing the model architecture, which to the best of our knowledge, has never been explored in ASR. Specifically, we gradually prune less contributing parameters on model encoder to a certain sparsity level, and use the pruned parameters for adaptation, while freezing the unpruned parameters to keep the original model performance. We conduct experiments on the Librispeech dataset. Our proposed approach brings relative 2.74-6.52% word error rate (WER) reduction on general speaker adaptation. On target speaker adaptation, our method outperforms the baseline with up to 20.58% relative WER reduction, and surpasses the finetuning method by up to relative 2.54%. Besides, with extremely low-resource adaptation data (e.g., 1 utterance), our method could improve the WER by relative 6.53% with only a few epochs of training.</abstract>
      <url hash="5270b09d">2021.emnlp-main.737</url>
      <bibkey>zhao-etal-2021-unified</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.737</doi>
      <video href="2021.emnlp-main.737.mp4"/>
      <pwccode url="https://github.com/zyzpower/gradprune_speaker" additional="false">zyzpower/gradprune_speaker</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/librispeech">LibriSpeech</pwcdataset>
    </paper>
    <paper id="738">
      <title>Caption Enriched Samples for Improving Hateful Memes Detection</title>
      <author><first>Efrat</first><last>Blaier</last></author>
      <author><first>Itzik</first><last>Malkiel</last></author>
      <author><first>Lior</first><last>Wolf</last></author>
      <pages>9350–9358</pages>
      <abstract>The recently introduced hateful meme challenge demonstrates the difficulty of determining whether a meme is hateful or not. Specifically, both unimodal language models and multimodal vision-language models cannot reach the human level of performance. Motivated by the need to model the contrast between the image content and the overlayed text, we suggest applying an off-the-shelf image captioning tool in order to capture the first. We demonstrate that the incorporation of such automatic captions during fine-tuning improves the results for various unimodal and multimodal models. Moreover, in the unimodal case, continuing the pre-training of language models on augmented and original caption pairs, is highly beneficial to the classification accuracy.</abstract>
      <url hash="90219fdc">2021.emnlp-main.738</url>
      <attachment type="Software" hash="1a72fda5">2021.emnlp-main.738.Software.zip</attachment>
      <bibkey>blaier-etal-2021-caption</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.738</doi>
      <video href="2021.emnlp-main.738.mp4"/>
      <pwccode url="https://github.com/efrat-safanov/caption-enriched-samples-research" additional="false">efrat-safanov/caption-enriched-samples-research</pwccode>
    </paper>
    <paper id="739">
      <title>Sparsity and Sentence Structure in Encoder-Decoder Attention of Summarization Systems</title>
      <author><first>Potsawee</first><last>Manakul</last></author>
      <author><first>Mark</first><last>Gales</last></author>
      <pages>9359–9368</pages>
      <abstract>Transformer models have achieved state-of-the-art results in a wide range of NLP tasks including summarization. Training and inference using large transformer models can be computationally expensive. Previous work has focused on one important bottleneck, the quadratic self-attention mechanism in the encoder. Modified encoder architectures such as LED or LoBART use local attention patterns to address this problem for summarization. In contrast, this work focuses on the transformer’s encoder-decoder attention mechanism. The cost of this attention becomes more significant in inference or training approaches that require model-generated histories. First, we examine the complexity of the encoder-decoder attention. We demonstrate empirically that there is a sparse sentence structure in document summarization that can be exploited by constraining the attention mechanism to a subset of input sentences, whilst maintaining system performance. Second, we propose a modified architecture that selects the subset of sentences to constrain the encoder-decoder attention. Experiments are carried out on abstractive summarization tasks, including CNN/DailyMail, XSum, Spotify Podcast, and arXiv.</abstract>
      <url hash="5a462584">2021.emnlp-main.739</url>
      <bibkey>manakul-gales-2021-sparsity</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.739</doi>
      <video href="2021.emnlp-main.739.mp4"/>
    </paper>
    <paper id="740">
      <title><fixed-case>BART</fixed-case>hez: a Skilled Pretrained <fixed-case>F</fixed-case>rench Sequence-to-Sequence Model</title>
      <author><first>Moussa</first><last>Kamal Eddine</last></author>
      <author><first>Antoine</first><last>Tixier</last></author>
      <author><first>Michalis</first><last>Vazirgiannis</last></author>
      <pages>9369–9390</pages>
      <abstract>Inductive transfer learning has taken the entire NLP field by storm, with models such as BERT and BART setting new state of the art on countless NLU tasks. However, most of the available models and research have been conducted for English. In this work, we introduce BARThez, the first large-scale pretrained seq2seq model for French. Being based on BART, BARThez is particularly well-suited for generative tasks. We evaluate BARThez on five discriminative tasks from the FLUE benchmark and two generative tasks from a novel summarization dataset, OrangeSum, that we created for this research. We show BARThez to be very competitive with state-of-the-art BERT-based French language models such as CamemBERT and FlauBERT. We also continue the pretraining of a multilingual BART on BARThez’ corpus, and show our resulting model, mBARThez, to significantly boost BARThez’ generative performance.</abstract>
      <url hash="afdbcc87">2021.emnlp-main.740</url>
      <bibkey>kamal-eddine-etal-2021-barthez</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.740</doi>
      <video href="2021.emnlp-main.740.mp4"/>
      <pwccode url="https://github.com/moussaKam/BARThez" additional="true">moussaKam/BARThez</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/orangesum">OrangeSum</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flue-french-language-understanding-evaluation">FLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
    </paper>
    <paper id="741">
      <title><fixed-case>ARMAN</fixed-case>: <fixed-case>P</fixed-case>re-training with <fixed-case>S</fixed-case>emantically <fixed-case>S</fixed-case>electing and <fixed-case>R</fixed-case>eordering of <fixed-case>S</fixed-case>entences for <fixed-case>P</fixed-case>ersian <fixed-case>A</fixed-case>bstractive <fixed-case>S</fixed-case>ummarization</title>
      <author><first>Alireza</first><last>Salemi</last></author>
      <author><first>Emad</first><last>Kebriaei</last></author>
      <author><first>Ghazal</first><last>Neisi Minaei</last></author>
      <author><first>Azadeh</first><last>Shakery</last></author>
      <pages>9391–9407</pages>
      <abstract>Abstractive text summarization is one of the areas influenced by the emergence of pre-trained language models. Current pre-training works in abstractive summarization give more points to the summaries with more words in common with the main text and pay less attention to the semantic similarity between generated sentences and the original document. We propose ARMAN, a Transformer-based encoder-decoder model pre-trained with three novel objectives to address this issue. In ARMAN, salient sentences from a document are selected according to a modified semantic score to be masked and form a pseudo summary. To summarize more accurately and similar to human writing patterns, we applied modified sentence reordering. We evaluated our proposed models on six downstream Persian summarization tasks. Experimental results show that our proposed model achieves state-of-the-art performance on all six summarization tasks measured by ROUGE and BERTScore. Our models also outperform prior works in textual entailment, question paraphrasing, and multiple choice question answering. Finally, we established a human evaluation and show that using the semantic score significantly improves summarization results.</abstract>
      <url hash="9e52027f">2021.emnlp-main.741</url>
      <bibkey>salemi-etal-2021-arman</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.741</doi>
      <video href="2021.emnlp-main.741.mp4"/>
      <pwccode url="https://github.com/alirezasalemi7/arman" additional="false">alirezasalemi7/arman</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cc100">CC100</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/perkey">PerKey</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/pn-summary">pn-summary</pwcdataset>
    </paper>
    <paper id="742">
      <title>Models and Datasets for Cross-Lingual Summarisation</title>
      <author><first>Laura</first><last>Perez-Beltrachini</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <pages>9408–9423</pages>
      <abstract>We present a cross-lingual summarisation corpus with long documents in a source language associated with multi-sentence summaries in a target language. The corpus covers twelve language pairs and directions for four European languages, namely Czech, English, French and German, and the methodology for its creation can be applied to several other languages. We derive cross-lingual document-summary instances from Wikipedia by combining lead paragraphs and articles’ bodies from language aligned Wikipedia titles. We analyse the proposed cross-lingual summarisation task with automatic metrics and validate it with a human study. To illustrate the utility of our dataset we report experiments with multi-lingual pre-trained models in supervised, zero- and few-shot, and out-of-domain scenarios.</abstract>
      <url hash="9222eb1e">2021.emnlp-main.742</url>
      <bibkey>perez-beltrachini-lapata-2021-models</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.742</doi>
      <video href="2021.emnlp-main.742.mp4"/>
      <pwccode url="https://github.com/lauhaide/clads" additional="false">lauhaide/clads</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikilingua">WikiLingua</pwcdataset>
    </paper>
    <paper id="743">
      <title>Learning Opinion Summarizers by Selecting Informative Reviews</title>
      <author><first>Arthur</first><last>Bražinskas</last></author>
      <author><first>Mirella</first><last>Lapata</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <pages>9424–9442</pages>
      <abstract>Opinion summarization has been traditionally approached with unsupervised, weakly-supervised and few-shot learning techniques. In this work, we collect a large dataset of summaries paired with user reviews for over 31,000 products, enabling supervised training. However, the number of reviews per product is large (320 on average), making summarization – and especially training a summarizer – impractical. Moreover, the content of many reviews is not reflected in the human-written summaries, and, thus, the summarizer trained on random review subsets hallucinates. In order to deal with both of these challenges, we formulate the task as jointly learning to select informative subsets of reviews and summarizing the opinions expressed in these subsets. The choice of the review subset is treated as a latent variable, predicted by a small and simple selector. The subset is then fed into a more powerful summarizer. For joint training, we use amortized variational inference and policy gradient methods. Our experiments demonstrate the importance of selecting informative reviews resulting in improved quality of summaries and reduced hallucinations.</abstract>
      <url hash="e5c8a7b5">2021.emnlp-main.743</url>
      <bibkey>brazinskas-etal-2021-learning</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.743</doi>
      <video href="2021.emnlp-main.743.mp4"/>
      <pwccode url="https://github.com/abrazinskas/selsum" additional="false">abrazinskas/selsum</pwccode>
    </paper>
    <paper id="744">
      <title>Enriching and Controlling Global Semantics for Text Summarization</title>
      <author><first>Thong</first><last>Nguyen</last></author>
      <author><first>Anh Tuan</first><last>Luu</last></author>
      <author><first>Truc</first><last>Lu</last></author>
      <author><first>Tho</first><last>Quan</last></author>
      <pages>9443–9456</pages>
      <abstract>Recently, Transformer-based models have been proven effective in the abstractive summarization task by creating fluent and informative summaries. Nevertheless, these models still suffer from the short-range dependency problem, causing them to produce summaries that miss the key points of document. In this paper, we attempt to address this issue by introducing a neural topic model empowered with normalizing flow to capture the global semantics of the document, which are then integrated into the summarization model. In addition, to avoid the overwhelming effect of global semantics on contextualized representation, we introduce a mechanism to control the amount of global semantics supplied to the text generation module. Our method outperforms state-of-the-art summarization models on five common text summarization datasets, namely CNN/DailyMail, XSum, Reddit TIFU, arXiv, and PubMed.</abstract>
      <url hash="62f529cc">2021.emnlp-main.744</url>
      <bibkey>nguyen-etal-2021-enriching</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.744</doi>
      <video href="2021.emnlp-main.744.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/reddit-tifu">Reddit TIFU</pwcdataset>
    </paper>
    <paper id="745">
      <title>Revisiting Tri-training of Dependency Parsers</title>
      <author><first>Joachim</first><last>Wagner</last></author>
      <author><first>Jennifer</first><last>Foster</last></author>
      <pages>9457–9473</pages>
      <abstract>We compare two orthogonal semi-supervised learning techniques, namely tri-training and pretrained word embeddings, in the task of dependency parsing. We explore language-specific FastText and ELMo embeddings and multilingual BERT embeddings. We focus on a low resource scenario as semi-supervised learning can be expected to have the most impact here. Based on treebank size and available ELMo models, we select Hungarian, Uyghur (a zero-shot language for mBERT) and Vietnamese. Furthermore, we include English in a simulated low-resource setting. We find that pretrained word embeddings make more effective use of unlabelled data than tri-training but that the two approaches can be successfully combined.</abstract>
      <url hash="e899d7c6">2021.emnlp-main.745</url>
      <bibkey>wagner-foster-2021-revisiting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.745</doi>
      <video href="2021.emnlp-main.745.mp4"/>
      <pwccode url="https://github.com/jowagner/mtb-tri-training" additional="true">jowagner/mtb-tri-training</pwccode>
    </paper>
    <paper id="746">
      <title>Bridge to Target Domain by Prototypical Contrastive Learning and Label Confusion: Re-explore Zero-Shot Learning for Slot Filling</title>
      <author><first>Liwen</first><last>Wang</last></author>
      <author><first>Xuefeng</first><last>Li</last></author>
      <author><first>Jiachi</first><last>Liu</last></author>
      <author><first>Keqing</first><last>He</last></author>
      <author><first>Yuanmeng</first><last>Yan</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <pages>9474–9480</pages>
      <abstract>Zero-shot cross-domain slot filling alleviates the data dependence in the case of data scarcity in the target domain, which has aroused extensive research. However, as most of the existing methods do not achieve effective knowledge transfer to the target domain, they just fit the distribution of the seen slot and show poor performance on unseen slot in the target domain. To solve this, we propose a novel approach based on prototypical contrastive learning with a dynamic label confusion strategy for zero-shot slot filling. The prototypical contrastive learning aims to reconstruct the semantic constraints of labels, and we introduce the label confusion strategy to establish the label dependence between the source domains and the target domain on-the-fly. Experimental results show that our model achieves significant improvement on the unseen slots, while also set new state-of-the-arts on slot filling task.</abstract>
      <url hash="5075be54">2021.emnlp-main.746</url>
      <bibkey>wang-etal-2021-bridge</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.746</doi>
      <video href="2021.emnlp-main.746.mp4"/>
      <pwccode url="https://github.com/w-lw/pclc" additional="false">w-lw/pclc</pwccode>
    </paper>
    <paper id="747">
      <title>Neuralizing Regular Expressions for Slot Filling</title>
      <author><first>Chengyue</first><last>Jiang</last></author>
      <author><first>Zijian</first><last>Jin</last></author>
      <author><first>Kewei</first><last>Tu</last></author>
      <pages>9481–9498</pages>
      <abstract>Neural models and symbolic rules such as regular expressions have their respective merits and weaknesses. In this paper, we study the integration of the two approaches for the slot filling task by converting regular expressions into neural networks. Specifically, we first convert regular expressions into a special form of finite-state transducers, then unfold its approximate inference algorithm as a bidirectional recurrent neural model that performs slot filling via sequence labeling. Experimental results show that our model has superior zero-shot and few-shot performance and stays competitive when there are sufficient training data.</abstract>
      <url hash="07485a04">2021.emnlp-main.747</url>
      <bibkey>jiang-etal-2021-neuralizing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.747</doi>
      <video href="2021.emnlp-main.747.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/atis">ATIS</pwcdataset>
    </paper>
    <paper id="748">
      <title>Causal Direction of Data Collection Matters: Implications of Causal and Anticausal Learning for <fixed-case>NLP</fixed-case></title>
      <author><first>Zhijing</first><last>Jin</last></author>
      <author><first>Julius</first><last>von Kügelgen</last></author>
      <author><first>Jingwei</first><last>Ni</last></author>
      <author><first>Tejas</first><last>Vaidhya</last></author>
      <author><first>Ayush</first><last>Kaushal</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last></author>
      <author><first>Bernhard</first><last>Schoelkopf</last></author>
      <pages>9499–9513</pages>
      <abstract>The principle of independent causal mechanisms (ICM) states that generative processes of real world data consist of independent modules which do not influence or inform each other. While this idea has led to fruitful developments in the field of causal inference, it is not widely-known in the NLP community. In this work, we argue that the causal direction of the data collection process bears nontrivial implications that can explain a number of published NLP findings, such as differences in semi-supervised learning (SSL) and domain adaptation (DA) performance across different settings. We categorize common NLP tasks according to their causal direction and empirically assay the validity of the ICM principle for text data using minimum description length. We conduct an extensive meta-analysis of over 100 published SSL and 30 DA studies, and find that the results are consistent with our expectations based on causal insights. This work presents the first attempt to analyze the ICM principle in NLP, and provides constructive suggestions for future modeling choices.</abstract>
      <url hash="71557b46">2021.emnlp-main.748</url>
      <bibkey>jin-etal-2021-causal</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.748</doi>
      <video href="2021.emnlp-main.748.mp4"/>
      <pwccode url="https://github.com/zhijing-jin/icm4nlp" additional="false">zhijing-jin/icm4nlp</pwccode>
    </paper>
    <paper id="749">
      <title>Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning</title>
      <author><first>Runxin</first><last>Xu</last></author>
      <author><first>Fuli</first><last>Luo</last></author>
      <author><first>Zhiyuan</first><last>Zhang</last></author>
      <author><first>Chuanqi</first><last>Tan</last></author>
      <author><first>Baobao</first><last>Chang</last></author>
      <author><first>Songfang</first><last>Huang</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <pages>9514–9528</pages>
      <abstract>Recent pretrained language models extend from millions to billions of parameters. Thus the need to fine-tune an extremely large pretrained model with a limited training corpus arises in various downstream tasks. In this paper, we propose a straightforward yet effective fine-tuning technique, Child-Tuning, which updates a subset of parameters (called child network) of large pretrained models via strategically masking out the gradients of the non-child network during the backward process. Experiments on various downstream tasks in GLUE benchmark show that Child-Tuning consistently outperforms the vanilla fine-tuning by 1.5 8.6 average score among four different pretrained models, and surpasses the prior fine-tuning techniques by 0.6 1.3 points. Furthermore, empirical results on domain transfer and task transfer show that Child-Tuning can obtain better generalization performance by large margins.</abstract>
      <url hash="0417c456">2021.emnlp-main.749</url>
      <bibkey>xu-etal-2021-raise</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.749</doi>
      <video href="2021.emnlp-main.749.mp4"/>
      <pwccode url="https://github.com/alibaba/AliceMind" additional="true">alibaba/AliceMind</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sick">SICK</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
    </paper>
    <paper id="750">
      <title>Knowledge Graph Representation Learning using Ordinary Differential Equations</title>
      <author><first>Mojtaba</first><last>Nayyeri</last></author>
      <author><first>Chengjin</first><last>Xu</last></author>
      <author><first>Franca</first><last>Hoffmann</last></author>
      <author><first>Mirza Mohtashim</first><last>Alam</last></author>
      <author><first>Jens</first><last>Lehmann</last></author>
      <author><first>Sahar</first><last>Vahdati</last></author>
      <pages>9529–9548</pages>
      <abstract>Knowledge Graph Embeddings (KGEs) have shown promising performance on link prediction tasks by mapping the entities and relations from a knowledge graph into a geometric space. The capability of KGEs in preserving graph characteristics including structural aspects and semantics, highly depends on the design of their score function, as well as the inherited abilities from the underlying geometry. Many KGEs use the Euclidean geometry which renders them incapable of preserving complex structures and consequently causes wrong inferences by the models. To address this problem, we propose a neuro differential KGE that embeds nodes of a KG on the trajectories of Ordinary Differential Equations (ODEs). To this end, we represent each relation (edge) in a KG as a vector field on several manifolds. We specifically parameterize ODEs by a neural network to represent complex manifolds and complex vector fields on the manifolds. Therefore, the underlying embedding space is capable to assume the shape of various geometric forms to encode heterogeneous subgraphs. Experiments on synthetic and benchmark datasets using state-of-the-art KGE models justify the ODE trajectories as a means to enable structure preservation and consequently avoiding wrong inferences.</abstract>
      <url hash="c81fa505">2021.emnlp-main.750</url>
      <bibkey>nayyeri-etal-2021-knowledge</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.750</doi>
      <video href="2021.emnlp-main.750.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k-237">FB15k-237</pwcdataset>
    </paper>
    <paper id="751">
      <title><fixed-case>K</fixed-case>now<fixed-case>MAN</fixed-case>: Weakly Supervised Multinomial Adversarial Networks</title>
      <author><first>Luisa</first><last>März</last></author>
      <author><first>Ehsaneddin</first><last>Asgari</last></author>
      <author><first>Fabienne</first><last>Braune</last></author>
      <author><first>Franziska</first><last>Zimmermann</last></author>
      <author><first>Benjamin</first><last>Roth</last></author>
      <pages>9549–9557</pages>
      <abstract>The absence of labeled data for training neural models is often addressed by leveraging knowledge about the specific task, resulting in heuristic but noisy labels. The knowledge is captured in labeling functions, which detect certain regularities or patterns in the training samples and annotate corresponding labels for training. This process of weakly supervised training may result in an over-reliance on the signals captured by the labeling functions and hinder models to exploit other signals or to generalize well. We propose KnowMAN, an adversarial scheme that enables to control influence of signals associated with specific labeling functions. KnowMAN forces the network to learn representations that are invariant to those signals and to pick up other signals that are more generally associated with an output label. KnowMAN strongly improves results compared to direct weakly supervised learning with a pre-trained transformer language model and a feature-based baseline.</abstract>
      <url hash="c439f29a">2021.emnlp-main.751</url>
      <bibkey>marz-etal-2021-knowman</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.751</doi>
      <video href="2021.emnlp-main.751.mp4"/>
      <pwccode url="https://github.com/luisamaerz/knowman" additional="false">luisamaerz/knowman</pwccode>
    </paper>
    <paper id="752">
      <title><fixed-case>ONION</fixed-case>: A Simple and Effective Defense Against Textual Backdoor Attacks</title>
      <author><first>Fanchao</first><last>Qi</last></author>
      <author><first>Yangyi</first><last>Chen</last></author>
      <author><first>Mukai</first><last>Li</last></author>
      <author><first>Yuan</first><last>Yao</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>9558–9566</pages>
      <abstract>Backdoor attacks are a kind of emergent training-time threat to deep neural networks (DNNs). They can manipulate the output of DNNs and possess high insidiousness. In the field of natural language processing, some attack methods have been proposed and achieve very high attack success rates on multiple popular models. Nevertheless, there are few studies on defending against textual backdoor attacks. In this paper, we propose a simple and effective textual backdoor defense named ONION, which is based on outlier word detection and, to the best of our knowledge, is the first method that can handle all the textual backdoor attack situations. Experiments demonstrate the effectiveness of our model in defending BiLSTM and BERT against five different backdoor attacks. All the code and data of this paper can be obtained at https://github.com/thunlp/ONION.</abstract>
      <url hash="1b8e14d1">2021.emnlp-main.752</url>
      <bibkey>qi-etal-2021-onion</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.752</doi>
      <video href="2021.emnlp-main.752.mp4"/>
      <pwccode url="https://github.com/thunlp/ONION" additional="true">thunlp/ONION</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="753">
      <title>Value-aware Approximate Attention</title>
      <author><first>Ankit</first><last>Gupta</last></author>
      <author><first>Jonathan</first><last>Berant</last></author>
      <pages>9567–9574</pages>
      <abstract>Following the success of dot-product attention in Transformers, numerous approximations have been recently proposed to address its quadratic complexity with respect to the input length. However, all approximations thus far have ignored the contribution of the *value vectors* to the quality of approximation. In this work, we argue that research efforts should be directed towards approximating the true output of the attention sub-layer, which includes the value vectors. We propose a value-aware objective, and show theoretically and empirically that an optimal approximation of a value-aware objective substantially outperforms an optimal approximation that ignores values, in the context of language modeling. Moreover, we show that the choice of kernel function for computing attention similarity can substantially affect the quality of sparse approximations, where kernel functions that are less skewed are more affected by the value vectors.</abstract>
      <url hash="5681b8c8">2021.emnlp-main.753</url>
      <bibkey>gupta-berant-2021-value</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.753</doi>
      <video href="2021.emnlp-main.753.mp4"/>
      <pwccode url="https://github.com/ag1988/value_aware_attn" additional="false">ag1988/value_aware_attn</pwccode>
    </paper>
    <paper id="754">
      <title>Contrastive Domain Adaptation for Question Answering using Limited Text Corpora</title>
      <author><first>Zhenrui</first><last>Yue</last></author>
      <author><first>Bernhard</first><last>Kratzwald</last></author>
      <author><first>Stefan</first><last>Feuerriegel</last></author>
      <pages>9575–9593</pages>
      <abstract>Question generation has recently shown impressive results in customizing question answering (QA) systems to new domains. These approaches circumvent the need for manually annotated training data from the new domain and, instead, generate synthetic question-answer pairs that are used for training. However, existing methods for question generation rely on large amounts of synthetically generated datasets and costly computational resources, which render these techniques widely inaccessible when the text corpora is of limited size. This is problematic as many niche domains rely on small text corpora, which naturally restricts the amount of synthetic data that can be generated. In this paper, we propose a novel framework for domain adaptation called contrastive domain adaptation for QA (CAQA). Specifically, CAQA combines techniques from question generation and domain-invariant learning to answer out-of-domain questions in settings with limited text corpora. Here, we train a QA system on both source data and generated data from the target domain with a contrastive adaptation loss that is incorporated in the training objective. By combining techniques from question generation and domain-invariant learning, our model achieved considerable improvements compared to state-of-the-art baselines.</abstract>
      <url hash="65cbf6e1">2021.emnlp-main.754</url>
      <bibkey>yue-etal-2021-contrastive</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.754</doi>
      <video href="2021.emnlp-main.754.mp4"/>
      <pwccode url="https://github.com/yueeeeeeee/caqa" additional="false">yueeeeeeee/caqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/hotpotqa">HotpotQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/searchqa">SearchQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="755">
      <title>Case-based Reasoning for Natural Language Queries over Knowledge Bases</title>
      <author><first>Rajarshi</first><last>Das</last></author>
      <author><first>Manzil</first><last>Zaheer</last></author>
      <author><first>Dung</first><last>Thai</last></author>
      <author><first>Ameya</first><last>Godbole</last></author>
      <author><first>Ethan</first><last>Perez</last></author>
      <author><first>Jay Yoon</first><last>Lee</last></author>
      <author><first>Lizhen</first><last>Tan</last></author>
      <author><first>Lazaros</first><last>Polymenakos</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>9594–9611</pages>
      <abstract>It is often challenging to solve a complex problem from scratch, but much easier if we can access other similar problems with their solutions — a paradigm known as case-based reasoning (CBR). We propose a neuro-symbolic CBR approach (CBR-KBQA) for question answering over large knowledge bases. CBR-KBQA consists of a nonparametric memory that stores cases (question and logical forms) and a parametric model that can generate a logical form for a new question by retrieving cases that are relevant to it. On several KBQA datasets that contain complex questions, CBR-KBQA achieves competitive performance. For example, on the CWQ dataset, CBR-KBQA outperforms the current state of the art by 11% on accuracy. Furthermore, we show that CBR-KBQA is capable of using new cases <i>without</i> any further training: by incorporating a few human-labeled examples in the case memory, CBR-KBQA is able to successfully generate logical forms containing unseen KB entities as well as relations.</abstract>
      <url hash="c8a8afde">2021.emnlp-main.755</url>
      <bibkey>das-etal-2021-case</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.755</doi>
      <video href="2021.emnlp-main.755.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/cfq">CFQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/complexwebquestions">ComplexWebQuestions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/simplequestions">SimpleQuestions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestions">WebQuestions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/webquestionssp">WebQuestionsSP</pwcdataset>
    </paper>
    <paper id="756">
      <title>Distantly-Supervised Dense Retrieval Enables Open-Domain Question Answering without Evidence Annotation</title>
      <author><first>Chen</first><last>Zhao</last></author>
      <author><first>Chenyan</first><last>Xiong</last></author>
      <author><first>Jordan</first><last>Boyd-Graber</last></author>
      <author><first>Hal</first><last>Daumé III</last></author>
      <pages>9612–9622</pages>
      <abstract>Open-domain question answering answers a question based on evidence retrieved from a large corpus. State-of-the-art neural approaches require intermediate evidence annotations for training. However, such intermediate annotations are expensive, and methods that rely on them cannot transfer to the more common setting, where only question–answer pairs are available. This paper investigates whether models can learn to find evidence from a large corpus, with only distant supervision from answer labels for model training, thereby generating no additional annotation cost. We introduce a novel approach (DistDR) that iteratively improves over a weak retriever by alternately finding evidence from the up-to-date model and encouraging the model to learn the most likely evidence. Without using any evidence labels, DistDR is on par with fully-supervised state-of-the-art methods on both multi-hop and single-hop QA benchmarks. Our analysis confirms that DistDR finds more accurate evidence over iterations, which leads to model improvements. The code is available at https://github.com/henryzhao5852/DistDR.</abstract>
      <url hash="f69c33b4">2021.emnlp-main.756</url>
      <bibkey>zhao-etal-2021-distantly</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.756</doi>
      <video href="2021.emnlp-main.756.mp4"/>
      <pwccode url="https://github.com/henryzhao5852/distdr" additional="false">henryzhao5852/distdr</pwccode>
    </paper>
    <paper id="757">
      <title>What’s in a Name? Answer Equivalence For Open-Domain Question Answering</title>
      <author><first>Chenglei</first><last>Si</last></author>
      <author><first>Chen</first><last>Zhao</last></author>
      <author><first>Jordan</first><last>Boyd-Graber</last></author>
      <pages>9623–9629</pages>
      <abstract>A flaw in QA evaluation is that annotations often only provide one gold answer. Thus, model predictions semantically equivalent to the answer but superficially different are considered incorrect. This work explores mining alias entities from knowledge bases and using them as additional gold answers (i.e., equivalent answers). We incorporate answers for two settings: evaluation with additional answers and model training with equivalent answers. We analyse three QA benchmarks: Natural Questions, TriviaQA, and SQuAD. Answer expansion increases the exact match score on all datasets for evaluation, while incorporating it helps model training over real-world datasets. We ensure the additional answers are valid through a human post hoc evaluation.</abstract>
      <url hash="debd85a2">2021.emnlp-main.757</url>
      <bibkey>si-etal-2021-whats</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.757</doi>
      <video href="2021.emnlp-main.757.mp4"/>
      <pwccode url="https://github.com/noviscl/answerequiv" additional="false">noviscl/answerequiv</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/triviaqa">TriviaQA</pwcdataset>
    </paper>
    <paper id="758">
      <title>Evaluation Paradigms in Question Answering</title>
      <author><first>Pedro</first><last>Rodriguez</last></author>
      <author><first>Jordan</first><last>Boyd-Graber</last></author>
      <pages>9630–9642</pages>
      <abstract>Question answering (QA) primarily descends from two branches of research: (1) Alan Turing’s investigation of machine intelligence at Manchester University and (2) Cyril Cleverdon’s comparison of library card catalog indices at Cranfield University. This position paper names and distinguishes these paradigms. Despite substantial overlap, subtle but significant distinctions exert an outsize influence on research. While one evaluation paradigm values creating more intelligent QA systems, the other paradigm values building QA systems that appeal to users. By better understanding the epistemic heritage of QA, researchers, academia, and industry can more effectively accelerate QA research.</abstract>
      <url hash="70b52233">2021.emnlp-main.758</url>
      <bibkey>rodriguez-boyd-graber-2021-evaluation</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.758</doi>
      <video href="2021.emnlp-main.758.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
    </paper>
    <paper id="759">
      <title>Numerical reasoning in machine reading comprehension tasks: are we there yet?</title>
      <author><first>Hadeel</first><last>Al-Negheimish</last></author>
      <author><first>Pranava</first><last>Madhyastha</last></author>
      <author><first>Alessandra</first><last>Russo</last></author>
      <pages>9643–9649</pages>
      <abstract>Numerical reasoning based machine reading comprehension is a task that involves reading comprehension along with using arithmetic operations such as addition, subtraction, sorting and counting. The DROP benchmark (Dua et al., 2019) is a recent dataset that has inspired the design of NLP models aimed at solving this task. The current standings of these models in the DROP leaderboard, over standard metrics, suggests that the models have achieved near-human performance. However, does this mean that these models have learned to reason? In this paper, we present a controlled study on some of the top-performing model architectures for the task of numerical reasoning. Our observations suggest that the standard metrics are incapable of measuring progress towards such tasks.</abstract>
      <url hash="71bbd697">2021.emnlp-main.759</url>
      <bibkey>al-negheimish-etal-2021-numerical</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.759</doi>
      <video href="2021.emnlp-main.759.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
    </paper>
    <paper id="760">
      <title>Set Generation Networks for End-to-End Knowledge Base Population</title>
      <author><first>Dianbo</first><last>Sui</last></author>
      <author><first>Chenhao</first><last>Wang</last></author>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <author><first>Wei</first><last>Bi</last></author>
      <pages>9650–9660</pages>
      <abstract>The task of knowledge base population (KBP) aims to discover facts about entities from texts and expand a knowledge base with these facts. Previous studies shape end-to-end KBP as a machine translation task, which is required to convert unordered fact into a sequence according to a pre-specified order. However, the facts stated in a sentence are unordered in essence. In this paper, we formulate end-to-end KBP as a direct set generation problem, avoiding considering the order of multiple facts. To solve the set generation problem, we propose networks featured by transformers with non-autoregressive parallel decoding. Unlike previous approaches that use an autoregressive decoder to generate facts one by one, the proposed networks can directly output the final set of facts in one shot. Furthermore, to train the networks, we also design a set-based loss that forces unique predictions via bipartite matching. Compared with cross-entropy loss that highly penalizes small shifts in fact order, the proposed bipartite matching loss is invariant to any permutation of predictions. Benefiting from getting rid of the burden of predicting the order of multiple facts, our proposed networks achieve state-of-the-art (SoTA) performance on two benchmark datasets.</abstract>
      <url hash="6f25b9c5">2021.emnlp-main.760</url>
      <bibkey>sui-etal-2021-set</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.760</doi>
      <video href="2021.emnlp-main.760.mp4"/>
    </paper>
    <paper id="761">
      <title>Knowing False Negatives: An Adversarial Training Method for Distantly Supervised Relation Extraction</title>
      <author><first>Kailong</first><last>Hao</last></author>
      <author><first>Botao</first><last>Yu</last></author>
      <author><first>Wei</first><last>Hu</last></author>
      <pages>9661–9672</pages>
      <abstract>Distantly supervised relation extraction (RE) automatically aligns unstructured text with relation instances in a knowledge base (KB). Due to the incompleteness of current KBs, sentences implying certain relations may be annotated as N/A instances, which causes the so-called false negative (FN) problem. Current RE methods usually overlook this problem, inducing improper biases in both training and testing procedures. To address this issue, we propose a two-stage approach. First, it finds out possible FN samples by heuristically leveraging the memory mechanism of deep neural networks. Then, it aligns those unlabeled data with the training data into a unified feature space by adversarial training to assign pseudo labels and further utilize the information contained in them. Experiments on two wildly-used benchmark datasets demonstrate the effectiveness of our approach.</abstract>
      <url hash="fd9f0c38">2021.emnlp-main.761</url>
      <bibkey>hao-etal-2021-knowing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.761</doi>
      <video href="2021.emnlp-main.761.mp4"/>
      <pwccode url="https://github.com/nju-websoft/fan" additional="false">nju-websoft/fan</pwccode>
    </paper>
    <paper id="762">
      <title>Progressive Adversarial Learning for Bootstrapping: A Case Study on Entity Set Expansion</title>
      <author><first>Lingyong</first><last>Yan</last></author>
      <author><first>Xianpei</first><last>Han</last></author>
      <author><first>Le</first><last>Sun</last></author>
      <pages>9673–9682</pages>
      <abstract>Bootstrapping has become the mainstream method for entity set expansion. Conventional bootstrapping methods mostly define the expansion boundary using seed-based distance metrics, which heavily depend on the quality of selected seeds and are hard to be adjusted due to the extremely sparse supervision. In this paper, we propose BootstrapGAN, a new learning method for bootstrapping which jointly models the bootstrapping process and the boundary learning process in a GAN framework. Specifically, the expansion boundaries of different bootstrapping iterations are learned via different discriminator networks; the bootstrapping network is the generator to generate new positive entities, and the discriminator networks identify the expansion boundaries by trying to distinguish the generated entities from known positive entities. By iteratively performing the above adversarial learning, the generator and the discriminators can reinforce each other and be progressively refined along the whole bootstrapping process. Experiments show that BootstrapGAN achieves the new state-of-the-art entity set expansion performance.</abstract>
      <url hash="a739d6d0">2021.emnlp-main.762</url>
      <bibkey>yan-etal-2021-progressive</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.762</doi>
      <video href="2021.emnlp-main.762.mp4"/>
      <pwccode url="https://github.com/lingyongyan/bootstrapgan" additional="false">lingyongyan/bootstrapgan</pwccode>
    </paper>
    <paper id="763">
      <title>Uncovering Main Causalities for Long-tailed Information Extraction</title>
      <author><first>Guoshun</first><last>Nan</last></author>
      <author><first>Jiaqi</first><last>Zeng</last></author>
      <author><first>Rui</first><last>Qiao</last></author>
      <author><first>Zhijiang</first><last>Guo</last></author>
      <author><first>Wei</first><last>Lu</last></author>
      <pages>9683–9695</pages>
      <abstract>Information Extraction (IE) aims to extract structural information from unstructured texts. In practice, long-tailed distributions caused by the selection bias of a dataset may lead to incorrect correlations, also known as spurious correlations, between entities and labels in the conventional likelihood models. This motivates us to propose counterfactual IE (CFIE), a novel framework that aims to uncover the main causalities behind data in the view of causal inference. Specifically, 1) we first introduce a unified structural causal model (SCM) for various IE tasks, describing the relationships among variables; 2) with our SCM, we then generate counterfactuals based on an explicit language structure to better calculate the direct causal effect during the inference stage; 3) we further propose a novel debiasing approach to yield more robust predictions. Experiments on three IE tasks across five public datasets show the effectiveness of our CFIE model in mitigating the spurious correlation issues.</abstract>
      <url hash="05dfecf0">2021.emnlp-main.763</url>
      <bibkey>nan-etal-2021-uncovering</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.763</doi>
      <video href="2021.emnlp-main.763.mp4"/>
      <pwccode url="https://github.com/heyyyyyyg/cfie" additional="false">heyyyyyyg/cfie</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/maven">MAVEN</pwcdataset>
    </paper>
    <paper id="764">
      <title>Maximal Clique Based Non-Autoregressive Open Information Extraction</title>
      <author><first>Bowen</first><last>Yu</last></author>
      <author><first>Yucheng</first><last>Wang</last></author>
      <author><first>Tingwen</first><last>Liu</last></author>
      <author><first>Hongsong</first><last>Zhu</last></author>
      <author><first>Limin</first><last>Sun</last></author>
      <author><first>Bin</first><last>Wang</last></author>
      <pages>9696–9706</pages>
      <abstract>Open Information Extraction (OpenIE) aims to discover textual facts from a given sentence. In essence, the facts contained in plain text are unordered. However, the popular OpenIE systems usually output facts sequentially in the way of predicting the next fact conditioned on the previous decoded ones, which enforce an unnecessary order on the facts and involve the error accumulation between autoregressive steps. To break this bottleneck, we propose MacroIE, a novel non-autoregressive framework for OpenIE. MacroIE firstly constructs a fact graph based on the table filling scheme, in which each node denotes a fact element, and an edge links two nodes that belong to the same fact. Then OpenIE can be reformulated as a non-parametric process of finding maximal cliques from the graph. It directly outputs the final set of facts in one go, thus getting rid of the burden of predicting fact order, as well as the error propagation between facts. Experiments conducted on two benchmark datasets show that our proposed model significantly outperforms current state-of-the-art methods, beats the previous systems by as much as 5.7 absolute gain in F1 score.</abstract>
      <url hash="a5848032">2021.emnlp-main.764</url>
      <bibkey>yu-etal-2021-maximal</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.764</doi>
      <video href="2021.emnlp-main.764.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/carb">CaRB</pwcdataset>
    </paper>
    <paper id="765">
      <title>A Relation-Oriented Clustering Method for Open Relation Extraction</title>
      <author><first>Jun</first><last>Zhao</last></author>
      <author><first>Tao</first><last>Gui</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Yaqian</first><last>Zhou</last></author>
      <pages>9707–9718</pages>
      <abstract>The clustering-based unsupervised relation discovery method has gradually become one of the important methods of open relation extraction (OpenRE). However, high-dimensional vectors can encode complex linguistic information which leads to the problem that the derived clusters cannot explicitly align with the relational semantic classes. In this work, we propose a relation-oriented clustering model and use it to identify the novel relations in the unlabeled data. Specifically, to enable the model to learn to cluster relational data, our method leverages the readily available labeled data of pre-defined relations to learn a relation-oriented representation. We minimize distance between the instance with same relation by gathering the instances towards their corresponding relation centroids to form a cluster structure, so that the learned representation is cluster-friendly. To reduce the clustering bias on predefined classes, we optimize the model by minimizing a joint objective on both labeled and unlabeled data. Experimental results show that our method reduces the error rate by 29.2% and 15.7%, on two datasets respectively, compared with current SOTA methods.</abstract>
      <url hash="fb4e65b2">2021.emnlp-main.765</url>
      <bibkey>zhao-etal-2021-relation</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.765</doi>
      <video href="2021.emnlp-main.765.mp4"/>
      <pwccode url="https://github.com/ac-zyx/rocore" additional="false">ac-zyx/rocore</pwccode>
    </paper>
    <paper id="766">
      <title>Exploring Methods for Generating Feedback Comments for Writing Learning</title>
      <author><first>Kazuaki</first><last>Hanawa</last></author>
      <author><first>Ryo</first><last>Nagata</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>9719–9730</pages>
      <abstract>The task of generating explanatory notes for language learners is known as feedback comment generation. Although various generation techniques are available, little is known about which methods are appropriate for this task. Nagata (2019) demonstrates the effectiveness of neural-retrieval-based methods in generating feedback comments for preposition use. Retrieval-based methods have limitations in that they can only output feedback comments existing in a given training data. Furthermore, feedback comments can be made on other grammatical and writing items than preposition use, which is still unaddressed. To shed light on these points, we investigate a wider range of methods for generating many feedback comments in this study. Our close analysis of the type of task leads us to investigate three different architectures for comment generation: (i) a neural-retrieval-based method as a baseline, (ii) a pointer-generator-based generation method as a neural seq2seq method, (iii) a retrieve-and-edit method, a hybrid of (i) and (ii). Intuitively, the pointer-generator should outperform neural-retrieval, and retrieve-and-edit should perform best. However, in our experiments, this expectation is completely overturned. We closely analyze the results to reveal the major causes of these counter-intuitive results and report on our findings from the experiments.</abstract>
      <url hash="28a61cb0">2021.emnlp-main.766</url>
      <bibkey>hanawa-etal-2021-exploring</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.766</doi>
      <video href="2021.emnlp-main.766.mp4"/>
      <pwccode url="https://github.com/k-hanawa/fcg_emnlp2021" additional="false">k-hanawa/fcg_emnlp2021</pwccode>
    </paper>
    <paper id="767">
      <title>A Role-Selected Sharing Network for Joint Machine-Human Chatting Handoff and Service Satisfaction Analysis</title>
      <author><first>Jiawei</first><last>Liu</last></author>
      <author><first>Kaisong</first><last>Song</last></author>
      <author><first>Yangyang</first><last>Kang</last></author>
      <author><first>Guoxiu</first><last>He</last></author>
      <author><first>Zhuoren</first><last>Jiang</last></author>
      <author><first>Changlong</first><last>Sun</last></author>
      <author><first>Wei</first><last>Lu</last></author>
      <author><first>Xiaozhong</first><last>Liu</last></author>
      <pages>9731–9741</pages>
      <abstract>Chatbot is increasingly thriving in different domains, however, because of unexpected discourse complexity and training data sparseness, its potential distrust hatches vital apprehension. Recently, Machine-Human Chatting Handoff (MHCH), predicting chatbot failure and enabling human-algorithm collaboration to enhance chatbot quality, has attracted increasing attention from industry and academia. In this study, we propose a novel model, Role-Selected Sharing Network (RSSN), which integrates both dialogue satisfaction estimation and handoff prediction in one multi-task learning framework. Unlike prior efforts in dialog mining, by utilizing local user satisfaction as a bridge, global satisfaction detector and handoff predictor can effectively exchange critical information. Specifically, we decouple the relation and interaction between the two tasks by the role information after the shared encoder. Extensive experiments on two public datasets demonstrate the effectiveness of our model.</abstract>
      <url hash="b9213d0d">2021.emnlp-main.767</url>
      <bibkey>liu-etal-2021-role</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.767</doi>
      <video href="2021.emnlp-main.767.mp4"/>
      <pwccode url="https://github.com/weijialau/rssn" additional="false">weijialau/rssn</pwccode>
    </paper>
    <paper id="768">
      <title>Meta Distant Transfer Learning for Pre-trained Language Models</title>
      <author><first>Chengyu</first><last>Wang</last></author>
      <author><first>Haojie</first><last>Pan</last></author>
      <author><first>Minghui</first><last>Qiu</last></author>
      <author><first>Jun</first><last>Huang</last></author>
      <author><first>Fei</first><last>Yang</last></author>
      <author><first>Yin</first><last>Zhang</last></author>
      <pages>9742–9752</pages>
      <abstract>With the wide availability of Pre-trained Language Models (PLMs), multi-task fine-tuning across domains has been extensively applied. For tasks related to distant domains with different class label sets, PLMs may memorize non-transferable knowledge for the target domain and suffer from negative transfer. Inspired by meta-learning, we propose the Meta Distant Transfer Learning (Meta-DTL) framework to learn the cross-task knowledge for PLM-based methods. Meta-DTL first employs task representation learning to mine implicit relations among multiple tasks and classes. Based on the results, it trains a PLM-based meta-learner to capture the transferable knowledge across tasks. The weighted maximum entropy regularizers are proposed to make meta-learner more task-agnostic and unbiased. Finally, the meta-learner can be fine-tuned to fit each task with better parameter initialization. We evaluate Meta-DTL using both BERT and ALBERT on seven public datasets. Experiment results confirm the superiority of Meta-DTL as it consistently outperforms strong baselines. We find that Meta-DTL is highly effective when very few data is available for the target task.</abstract>
      <url hash="86cfed8c">2021.emnlp-main.768</url>
      <bibkey>wang-etal-2021-meta-distant</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.768</doi>
      <video href="2021.emnlp-main.768.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="769">
      <title><fixed-case>U</fixed-case>ni<fixed-case>KER</fixed-case>: A Unified Framework for Combining Embedding and Definite Horn Rule Reasoning for Knowledge Graph Inference</title>
      <author><first>Kewei</first><last>Cheng</last></author>
      <author><first>Ziqing</first><last>Yang</last></author>
      <author><first>Ming</first><last>Zhang</last></author>
      <author><first>Yizhou</first><last>Sun</last></author>
      <pages>9753–9771</pages>
      <abstract>Knowledge graph inference has been studied extensively due to its wide applications. It has been addressed by two lines of research, i.e., the more traditional logical rule reasoning and the more recent knowledge graph embedding (KGE). Several attempts have been made to combine KGE and logical rules for better knowledge graph inference. Unfortunately, they either simply treat logical rules as additional constraints into KGE loss or use probabilistic model to approximate the exact logical inference (i.e., MAX-SAT). Even worse, both approaches need to sample ground rules to tackle the scalability issue, as the total number of ground rules is intractable in practice, making them less effective in handling logical rules. In this paper, we propose a novel framework UniKER to address these challenges by restricting logical rules to be definite Horn rules, which can fully exploit the knowledge in logical rules and enable the mutual enhancement of logical rule-based reasoning and KGE in an extremely efficient way. Extensive experiments have demonstrated that our approach is superior to existing state-of-the-art algorithms in terms of both efficiency and effectiveness.</abstract>
      <url hash="945bf38d">2021.emnlp-main.769</url>
      <bibkey>cheng-etal-2021-uniker</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.769</doi>
      <video href="2021.emnlp-main.769.mp4"/>
    </paper>
    <paper id="770">
      <title><fixed-case>W</fixed-case>asserstein Selective Transfer Learning for Cross-domain Text Mining</title>
      <author><first>Lingyun</first><last>Feng</last></author>
      <author><first>Minghui</first><last>Qiu</last></author>
      <author><first>Yaliang</first><last>Li</last></author>
      <author><first>Haitao</first><last>Zheng</last></author>
      <author><first>Ying</first><last>Shen</last></author>
      <pages>9772–9783</pages>
      <abstract>Transfer learning (TL) seeks to improve the learning of a data-scarce target domain by using information from source domains. However, the source and target domains usually have different data distributions, which may lead to negative transfer. To alleviate this issue, we propose a Wasserstein Selective Transfer Learning (WSTL) method. Specifically, the proposed method considers a reinforced selector to select helpful data for transfer learning. We further use a Wasserstein-based discriminator to maximize the empirical distance between the selected source data and target data. The TL module is then trained to minimize the estimated Wasserstein distance in an adversarial manner and provides domain invariant features for the reinforced selector. We adopt an evaluation metric based on the performance of the TL module as delayed reward and a Wasserstein-based metric as immediate rewards to guide the reinforced selector learning. Compared with the competing TL approaches, the proposed method selects data samples that are closer to the target domain. It also provides better state features and reward signals that lead to better performance with faster convergence. Extensive experiments on three real-world text mining tasks demonstrate the effectiveness of the proposed method.</abstract>
      <url hash="205e2e94">2021.emnlp-main.770</url>
      <attachment type="Software" hash="cc7b3ac0">2021.emnlp-main.770.Software.zip</attachment>
      <bibkey>feng-etal-2021-wasserstein</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.770</doi>
      <video href="2021.emnlp-main.770.mp4"/>
    </paper>
    <paper id="771">
      <title>Jointly Learning to Repair Code and Generate Commit Message</title>
      <author><first>Jiaqi</first><last>Bai</last></author>
      <author><first>Long</first><last>Zhou</last></author>
      <author><first>Ambrosio</first><last>Blanco</last></author>
      <author><first>Shujie</first><last>Liu</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <pages>9784–9795</pages>
      <abstract>We propose a novel task of jointly repairing program codes and generating commit messages. Code repair and commit message generation are two essential and related tasks for software development. However, existing work usually performs the two tasks independently. We construct a multilingual triple dataset including buggy code, fixed code, and commit messages for this novel task. We first introduce a cascaded method with two models, one is to generate the fixed code first, and the other generates the commit message based on the fixed and original codes. We enhance the cascaded method with different training approaches, including the teacher-student method, the multi-task method, and the back-translation method. To deal with the error propagation problem of the cascaded method, we also propose a joint model that can both repair the program code and generate the commit message in a unified framework. Massive experiments on our constructed buggy-fixed-commit dataset reflect the challenge of this task and that the enhanced cascaded model and the proposed joint model significantly outperform baselines in both quality of code and commit messages.</abstract>
      <url hash="4d39c31a">2021.emnlp-main.771</url>
      <bibkey>bai-etal-2021-jointly</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.771</doi>
      <video href="2021.emnlp-main.771.mp4"/>
    </paper>
    <paper id="772">
      <title>Inflate and Shrink:Enriching and Reducing Interactions for Fast Text-Image Retrieval</title>
      <author><first>Haoliang</first><last>Liu</last></author>
      <author><first>Tan</first><last>Yu</last></author>
      <author><first>Ping</first><last>Li</last></author>
      <pages>9796–9809</pages>
      <abstract>By exploiting the cross-modal attention, cross-BERT methods have achieved state-of-the-art accuracy in cross-modal retrieval. Nevertheless, the heavy text-image interactions in the cross-BERT model are prohibitively slow for large-scale retrieval. Late-interaction methods trade off retrieval accuracy and efficiency by exploiting cross-modal interaction only in the late stage, attaining a satisfactory retrieval speed. In this work, we propose an inflating and shrinking approach to further boost the efficiency and accuracy of late-interaction methods. The inflating operation plugs several codes in the input of the encoder to exploit the text-image interactions more thoroughly for higher retrieval accuracy. Then the shrinking operation gradually reduces the text-image interactions through knowledge distilling for higher efficiency. Through an inflating operation followed by a shrinking operation, both efficiency and accuracy of a late-interaction model are boosted. Systematic experiments on public benchmarks demonstrate the effectiveness of our inflating and shrinking approach.</abstract>
      <url hash="bb4ea1d8">2021.emnlp-main.772</url>
      <attachment type="Software" hash="52e60ac5">2021.emnlp-main.772.Software.txt</attachment>
      <bibkey>liu-etal-2021-inflate</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.772</doi>
      <video href="2021.emnlp-main.772.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
    </paper>
    <paper id="773">
      <title>On Pursuit of Designing Multi-modal Transformer for Video Grounding</title>
      <author><first>Meng</first><last>Cao</last></author>
      <author><first>Long</first><last>Chen</last></author>
      <author><first>Mike Zheng</first><last>Shou</last></author>
      <author><first>Can</first><last>Zhang</last></author>
      <author><first>Yuexian</first><last>Zou</last></author>
      <pages>9810–9823</pages>
      <abstract>Video grounding aims to localize the temporal segment corresponding to a sentence query from an untrimmed video. Almost all existing video grounding methods fall into two frameworks: 1) Top-down model: It predefines a set of segment candidates and then conducts segment classification and regression. 2) Bottom-up model: It directly predicts frame-wise probabilities of the referential segment boundaries. However, all these methods are not end-to-end, i.e., they always rely on some time-consuming post-processing steps to refine predictions. To this end, we reformulate video grounding as a set prediction task and propose a novel end-to-end multi-modal Transformer model, dubbed as GTR. Specifically, GTR has two encoders for video and language encoding, and a cross-modal decoder for grounding prediction. To facilitate the end-to-end training, we use a Cubic Embedding layer to transform the raw videos into a set of visual tokens. To better fuse these two modalities in the decoder, we design a new Multi-head Cross-Modal Attention. The whole GTR is optimized via a Many-to-One matching loss. Furthermore, we conduct comprehensive studies to investigate different model design choices. Extensive results on three benchmarks have validated the superiority of GTR. All three typical GTR variants achieve record-breaking performance on all datasets and metrics, with several times faster inference speed.</abstract>
      <url hash="67076422">2021.emnlp-main.773</url>
      <bibkey>cao-etal-2021-pursuit</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.773</doi>
      <revision id="1" href="2021.emnlp-main.773v1" hash="2bd6c814"/>
      <revision id="2" href="2021.emnlp-main.773v2" hash="67076422" date="2022-04-29">Added missing acknowledgement.</revision>
      <video href="2021.emnlp-main.773.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/activitynet-captions">ActivityNet Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/charades-sta">Charades-STA</pwcdataset>
    </paper>
    <paper id="774">
      <title><fixed-case>COVR</fixed-case>: A Test-Bed for Visually Grounded Compositional Generalization with Real Images</title>
      <author><first>Ben</first><last>Bogin</last></author>
      <author><first>Shivanshu</first><last>Gupta</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <author><first>Jonathan</first><last>Berant</last></author>
      <pages>9824–9846</pages>
      <abstract>While interest in models that generalize at test time to new compositions has risen in recent years, benchmarks in the visually-grounded domain have thus far been restricted to synthetic images. In this work, we propose COVR, a new test-bed for visually-grounded compositional generalization with real images. To create COVR, we use real images annotated with scene graphs, and propose an almost fully automatic procedure for generating question-answer pairs along with a set of context images. COVR focuses on questions that require complex reasoning, including higher-order operations such as quantification and aggregation. Due to the automatic generation process, COVR facilitates the creation of compositional splits, where models at test time need to generalize to new concepts and compositions in a zero- or few-shot setting. We construct compositional splits using COVR and demonstrate a myriad of cases where state-of-the-art pre-trained language-and-vision models struggle to compositionally generalize.</abstract>
      <url hash="1fe9e067">2021.emnlp-main.774</url>
      <bibkey>bogin-etal-2021-covr</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.774</doi>
      <video href="2021.emnlp-main.774.mp4"/>
      <pwccode url="https://github.com/benbogin/covr-dataset" additional="false">benbogin/covr-dataset</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-genome">Visual Genome</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="775">
      <title>Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers</title>
      <author><first>Stella</first><last>Frank</last></author>
      <author><first>Emanuele</first><last>Bugliarello</last></author>
      <author><first>Desmond</first><last>Elliott</last></author>
      <pages>9847–9857</pages>
      <abstract>Pretrained vision-and-language BERTs aim to learn representations that combine information from both modalities. We propose a diagnostic method based on cross-modal input ablation to assess the extent to which these models actually integrate cross-modal information. This method involves ablating inputs from one modality, either entirely or selectively based on cross-modal grounding alignments, and evaluating the model prediction performance on the other modality. Model performance is measured by modality-specific tasks that mirror the model pretraining objectives (e.g. masked language modelling for text). Models that have learned to construct cross-modal representations using both modalities are expected to perform worse when inputs are missing from a modality. We find that recently proposed models have much greater relative difficulty predicting text when visual information is ablated, compared to predicting visual object categories when text is ablated, indicating that these models are not symmetrically cross-modal.</abstract>
      <url hash="4105ff84">2021.emnlp-main.775</url>
      <bibkey>frank-etal-2021-vision</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.775</doi>
      <video href="2021.emnlp-main.775.mp4"/>
      <pwccode url="https://github.com/e-bug/volta" additional="true">e-bug/volta</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/conceptual-captions">Conceptual Captions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/flickr30k">Flickr30k</pwcdataset>
    </paper>
    <paper id="776">
      <title><fixed-case>H</fixed-case>yp<fixed-case>M</fixed-case>ix: Hyperbolic Interpolative Data Augmentation</title>
      <author><first>Ramit</first><last>Sawhney</last></author>
      <author><first>Megh</first><last>Thakkar</last></author>
      <author><first>Shivam</first><last>Agarwal</last></author>
      <author><first>Di</first><last>Jin</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <author><first>Lucie</first><last>Flek</last></author>
      <pages>9858–9868</pages>
      <abstract>Interpolation-based regularisation methods for data augmentation have proven to be effective for various tasks and modalities. These methods involve performing mathematical operations over the raw input samples or their latent states representations - vectors that often possess complex hierarchical geometries. However, these operations are performed in the Euclidean space, simplifying these representations, which may lead to distorted and noisy interpolations. We propose HypMix, a novel model-, data-, and modality-agnostic interpolative data augmentation technique operating in the hyperbolic space, which captures the complex geometry of input and hidden state hierarchies better than its contemporaries. We evaluate HypMix on benchmark and low resource datasets across speech, text, and vision modalities, showing that HypMix consistently outperforms state-of-the-art data augmentation techniques. In addition, we demonstrate the use of HypMix in semi-supervised settings. We further probe into the adversarial robustness and qualitative inferences we draw from HypMix that elucidate the efficacy of the Riemannian hyperbolic manifolds for interpolation-based data augmentation.</abstract>
      <url hash="893a4936">2021.emnlp-main.776</url>
      <bibkey>sawhney-etal-2021-hypmix</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.776</doi>
      <video href="2021.emnlp-main.776.mp4"/>
      <pwccode url="https://github.com/caisa-lab/hypmix-emnlp" additional="false">caisa-lab/hypmix-emnlp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cifar-10">CIFAR-10</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/cifar-100">CIFAR-100</pwcdataset>
    </paper>
    <paper id="777">
      <title>Integrating Deep Event-Level and Script-Level Information for Script Event Prediction</title>
      <author><first>Long</first><last>Bai</last></author>
      <author><first>Saiping</first><last>Guan</last></author>
      <author><first>Jiafeng</first><last>Guo</last></author>
      <author><first>Zixuan</first><last>Li</last></author>
      <author><first>Xiaolong</first><last>Jin</last></author>
      <author><first>Xueqi</first><last>Cheng</last></author>
      <pages>9869–9878</pages>
      <abstract>Scripts are structured sequences of events together with the participants, which are extracted from the texts. Script event prediction aims to predict the subsequent event given the historical events in the script. Two kinds of information facilitate this task, namely, the event-level information and the script-level information. At the event level, existing studies view an event as a verb with its participants, while neglecting other useful properties, such as the state of the participants. At the script level, most existing studies only consider a single event sequence corresponding to one common protagonist. In this paper, we propose a Transformer-based model, called MCPredictor, which integrates deep event-level and script-level information for script event prediction. At the event level, MCPredictor utilizes the rich information in the text to obtain more comprehensive event semantic representations. At the script-level, it considers multiple event sequences corresponding to different participants of the subsequent event. The experimental results on the widely-used New York Times corpus demonstrate the effectiveness and superiority of the proposed model.</abstract>
      <url hash="1df22a15">2021.emnlp-main.777</url>
      <bibkey>bai-etal-2021-integrating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.777</doi>
      <video href="2021.emnlp-main.777.mp4"/>
      <pwccode url="https://github.com/waltbai/MCPredictor" additional="false">waltbai/MCPredictor</pwccode>
    </paper>
    <paper id="778">
      <title><fixed-case>QA</fixed-case>-Align: Representing Cross-Text Content Overlap by Aligning Question-Answer Propositions</title>
      <author><first>Daniela</first><last>Brook Weiss</last></author>
      <author><first>Paul</first><last>Roit</last></author>
      <author><first>Ayal</first><last>Klein</last></author>
      <author><first>Ori</first><last>Ernst</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <pages>9879–9894</pages>
      <abstract>Multi-text applications, such as multi-document summarization, are typically required to model redundancies across related texts. Current methods confronting consolidation struggle to fuse overlapping information. In order to explicitly represent content overlap, we propose to align predicate-argument relations across texts, providing a potential scaffold for information consolidation. We go beyond clustering coreferring mentions, and instead model overlap with respect to redundancy at a propositional level, rather than merely detecting shared referents. Our setting exploits QA-SRL, utilizing question-answer pairs to capture predicate-argument relations, facilitating laymen annotation of cross-text alignments. We employ crowd-workers for constructing a dataset of QA-based alignments, and present a baseline QA alignment model trained over our dataset. Analyses show that our new task is semantically challenging, capturing content overlap beyond lexical similarity and complements cross-document coreference with proposition-level links, offering potential use for downstream tasks.</abstract>
      <url hash="6bd2ac49">2021.emnlp-main.778</url>
      <bibkey>brook-weiss-etal-2021-qa</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.778</doi>
      <video href="2021.emnlp-main.778.mp4"/>
      <pwccode url="https://github.com/danielabweiss/qa-align" additional="false">danielabweiss/qa-align</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ecb">ECB+</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qa-srl">QA-SRL</pwcdataset>
    </paper>
    <paper id="779">
      <title><fixed-case>PICARD</fixed-case>: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models</title>
      <author><first>Torsten</first><last>Scholak</last></author>
      <author><first>Nathan</first><last>Schucher</last></author>
      <author><first>Dzmitry</first><last>Bahdanau</last></author>
      <pages>9895–9901</pages>
      <abstract>Large pre-trained language models for textual data have an unconstrained output space; at each decoding step, they can produce any of 10,000s of sub-word tokens. When fine-tuned to target constrained formal languages like SQL, these models often generate invalid code, rendering it unusable. We propose PICARD (code available at https://github.com/ElementAI/picard), a method for constraining auto-regressive decoders of language models through incremental parsing. PICARD helps to find valid output sequences by rejecting inadmissible tokens at each decoding step. On the challenging Spider and CoSQL text-to-SQL translation tasks, we show that PICARD transforms fine-tuned T5 models with passable performance into state-of-the-art solutions.</abstract>
      <url hash="6b45cca4">2021.emnlp-main.779</url>
      <bibkey>scholak-etal-2021-picard</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.779</doi>
      <video href="2021.emnlp-main.779.mp4"/>
      <pwccode url="https://github.com/ElementAI/picard" additional="true">ElementAI/picard</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cosql">CoSQL</pwcdataset>
    </paper>
    <paper id="780">
      <title>Exploiting <fixed-case>T</fixed-case>witter as Source of Large Corpora of Weakly Similar Pairs for Semantic Sentence Embeddings</title>
      <author><first>Marco</first><last>Di Giovanni</last></author>
      <author><first>Marco</first><last>Brambilla</last></author>
      <pages>9902–9910</pages>
      <abstract>Semantic sentence embeddings are usually supervisedly built minimizing distances between pairs of embeddings of sentences labelled as semantically similar by annotators. Since big labelled datasets are rare, in particular for non-English languages, and expensive, recent studies focus on unsupervised approaches that require not-paired input sentences. We instead propose a language-independent approach to build large datasets of pairs of informal texts weakly similar, without manual human effort, exploiting Twitter’s intrinsic powerful signals of relatedness: replies and quotes of tweets. We use the collected pairs to train a Transformer model with triplet-like structures, and we test the generated embeddings on Twitter NLP similarity tasks (PIT and TURL) and STSb. We also introduce four new sentence ranking evaluation benchmarks of informal texts, carefully extracted from the initial collections of tweets, proving not only that our best model learns classical Semantic Textual Similarity, but also excels on tasks where pairs of sentences are not exact paraphrases. Ablation studies reveal how increasing the corpus size influences positively the results, even at 2M samples, suggesting that bigger collections of Tweets still do not contain redundant information about semantic similarities. Code available at https://github.com/marco-digio/Twitter4SSE</abstract>
      <url hash="fcbf27ea">2021.emnlp-main.780</url>
      <bibkey>di-giovanni-brambilla-2021-exploiting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.780</doi>
      <video href="2021.emnlp-main.780.mp4"/>
      <pwccode url="https://github.com/marco-digio/twitter4sse" additional="false">marco-digio/twitter4sse</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/pit">PIT</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/twitter-news-url-corpus">TURL</pwcdataset>
    </paper>
    <paper id="781">
      <title>Guilt by Association: Emotion Intensities in Lexical Representations</title>
      <author><first>Shahab</first><last>Raji</last></author>
      <author><first>Gerard</first><last>de Melo</last></author>
      <pages>9911–9917</pages>
      <abstract>What do linguistic models reveal about the emotions associated with words? In this study, we consider the task of estimating word-level emotion intensity scores for specific emotions, exploring unsupervised, supervised, and finally a self-supervised method of extracting emotional associations from pretrained vectors and models. Overall, we find that linguistic models carry substantial potential for inducing fine-grained emotion intensity scores, showing a far higher correlation with human ground truth ratings than state-of-the-art emotion lexicons based on labeled data.</abstract>
      <url hash="640e8a71">2021.emnlp-main.781</url>
      <bibkey>raji-de-melo-2021-guilt</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.781</doi>
      <video href="2021.emnlp-main.781.mp4"/>
    </paper>
    <paper id="782">
      <title>Using Sociolinguistic Variables to Reveal Changing Attitudes Towards Sexuality and Gender</title>
      <author><first>Sky</first><last>CH-Wang</last></author>
      <author><first>David</first><last>Jurgens</last></author>
      <pages>9918–9938</pages>
      <abstract>Individuals signal aspects of their identity and beliefs through linguistic choices. Studying these choices in aggregate allows us to examine large-scale attitude shifts within a population. Here, we develop computational methods to study word choice within a sociolinguistic lexical variable—alternate words used to express the same concept—in order to test for change in the United States towards sexuality and gender. We examine two variables: i) referents to significant others, such as the word “partner” and ii) referents to an indefinite person, both of which could optionally be marked with gender. The linguistic choices in each variable allow us to study increased rates of acceptances of gay marriage and gender equality, respectively. In longitudinal analyses across Twitter and Reddit over 87M messages, we demonstrate that attitudes are changing but that these changes are driven by specific demographics within the United States. Further, in a quasi-causal analysis, we show that passages of Marriage Equality Acts in different states are drivers of linguistic change.</abstract>
      <url hash="6175446a">2021.emnlp-main.782</url>
      <bibkey>ch-wang-jurgens-2021-using</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.782</doi>
    </paper>
    <paper id="783">
      <title>Identifying Morality Frames in Political Tweets using Relational Learning</title>
      <author><first>Shamik</first><last>Roy</last></author>
      <author><first>Maria Leonor</first><last>Pacheco</last></author>
      <author><first>Dan</first><last>Goldwasser</last></author>
      <pages>9939–9958</pages>
      <abstract>Extracting moral sentiment from text is a vital component in understanding public opinion, social movements, and policy decisions. The Moral Foundation Theory identifies five moral foundations, each associated with a positive and negative polarity. However, moral sentiment is often motivated by its targets, which can correspond to individuals or collective entities. In this paper, we introduce morality frames, a representation framework for organizing moral attitudes directed at different entities, and come up with a novel and high-quality annotated dataset of tweets written by US politicians. Then, we propose a relational learning model to predict moral attitudes towards entities and moral foundations jointly. We do qualitative and quantitative evaluations, showing that moral sentiment towards entities differs highly across political ideologies.</abstract>
      <url hash="142d81b0">2021.emnlp-main.783</url>
      <bibkey>roy-etal-2021-identifying</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.783</doi>
      <video href="2021.emnlp-main.783.mp4"/>
      <pwccode url="https://github.com/shamikroy/moral-role-prediction" additional="false">shamikroy/moral-role-prediction</pwccode>
    </paper>
    <paper id="784">
      <title>Measuring Sentence-Level and Aspect-Level (Un)certainty in Science Communications</title>
      <author><first>Jiaxin</first><last>Pei</last></author>
      <author><first>David</first><last>Jurgens</last></author>
      <pages>9959–10011</pages>
      <abstract>Certainty and uncertainty are fundamental to science communication. Hedges have widely been used as proxies for uncertainty. However, certainty is a complex construct, with authors expressing not only the degree but the type and aspects of uncertainty in order to give the reader a certain impression of what is known. Here, we introduce a new study of certainty that models both the level and the aspects of certainty in scientific findings. Using a new dataset of 2167 annotated scientific findings, we demonstrate that hedges alone account for only a partial explanation of certainty. We show that both the overall certainty and individual aspects can be predicted with pre-trained language models, providing a more complete picture of the author’s intended communication. Downstream analyses on 431K scientific findings from news and scientific abstracts demonstrate that modeling sentence-level and aspect-level certainty is meaningful for areas like science communication. Both the model and datasets used in this paper are released at https://blablablab.si.umich.edu/projects/certainty/.</abstract>
      <url hash="eeea8468">2021.emnlp-main.784</url>
      <bibkey>pei-jurgens-2021-measuring</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.784</doi>
      <video href="2021.emnlp-main.784.mp4"/>
    </paper>
    <paper id="785">
      <title>Assessing the Reliability of Word Embedding Gender Bias Measures</title>
      <author><first>Yupei</first><last>Du</last></author>
      <author><first>Qixiang</first><last>Fang</last></author>
      <author><first>Dong</first><last>Nguyen</last></author>
      <pages>10012–10034</pages>
      <abstract>Various measures have been proposed to quantify human-like social biases in word embeddings. However, bias scores based on these measures can suffer from measurement error. One indication of measurement quality is reliability, concerning the extent to which a measure produces consistent results. In this paper, we assess three types of reliability of word embedding gender bias measures, namely test-retest reliability, inter-rater consistency and internal consistency. Specifically, we investigate the consistency of bias scores across different choices of random seeds, scoring rules and words. Furthermore, we analyse the effects of various factors on these measures’ reliability scores. Our findings inform better design of word embedding gender bias measures. Moreover, we urge researchers to be more critical about the application of such measures</abstract>
      <url hash="1c32913d">2021.emnlp-main.785</url>
      <bibkey>du-etal-2021-assessing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.785</doi>
      <video href="2021.emnlp-main.785.mp4"/>
      <pwccode url="https://github.com/nlpsoc/reliability_bias" additional="false">nlpsoc/reliability_bias</pwccode>
    </paper>
    <paper id="786">
      <title>Rumor Detection on <fixed-case>T</fixed-case>witter with Claim-Guided Hierarchical Graph Attention Networks</title>
      <author><first>Hongzhan</first><last>Lin</last></author>
      <author><first>Jing</first><last>Ma</last></author>
      <author><first>Mingfei</first><last>Cheng</last></author>
      <author><first>Zhiwei</first><last>Yang</last></author>
      <author><first>Liangliang</first><last>Chen</last></author>
      <author><first>Guang</first><last>Chen</last></author>
      <pages>10035–10047</pages>
      <abstract>Rumors are rampant in the era of social media. Conversation structures provide valuable clues to differentiate between real and fake claims. However, existing rumor detection methods are either limited to the strict relation of user responses or oversimplify the conversation structure. In this study, to substantially reinforces the interaction of user opinions while alleviating the negative impact imposed by irrelevant posts, we first represent the conversation thread as an undirected interaction graph. We then present a Claim-guided Hierarchical Graph Attention Network for rumor classification, which enhances the representation learning for responsive posts considering the entire social contexts and attends over the posts that can semantically infer the target claim. Extensive experiments on three Twitter datasets demonstrate that our rumor detection method achieves much better performance than state-of-the-art methods and exhibits a superior capacity for detecting rumors at early stages.</abstract>
      <url hash="b6f272a5">2021.emnlp-main.786</url>
      <bibkey>lin-etal-2021-rumor</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.786</doi>
      <video href="2021.emnlp-main.786.mp4"/>
    </paper>
    <paper id="787">
      <title>Learning Bill Similarity with Annotated and Augmented Corpora of Bills</title>
      <author><first>Jiseon</first><last>Kim</last></author>
      <author><first>Elden</first><last>Griggs</last></author>
      <author><first>In Song</first><last>Kim</last></author>
      <author><first>Alice</first><last>Oh</last></author>
      <pages>10048–10064</pages>
      <abstract>Bill writing is a critical element of representative democracy. However, it is often overlooked that most legislative bills are derived, or even directly copied, from other bills. Despite the significance of bill-to-bill linkages for understanding the legislative process, existing approaches fail to address semantic similarities across bills, let alone reordering or paraphrasing which are prevalent in legal document writing. In this paper, we overcome these limitations by proposing a 5-class classification task that closely reflects the nature of the bill generation process. In doing so, we construct a human-labeled dataset of 4,721 bill-to-bill relationships at the subsection-level and release this annotated dataset to the research community. To augment the dataset, we generate synthetic data with varying degrees of similarity, mimicking the complex bill writing process. We use BERT variants and apply multi-stage training, sequentially fine-tuning our models with synthetic and human-labeled datasets. We find that the predictive performance significantly improves when training with both human-labeled and synthetic data. Finally, we apply our trained model to infer section- and bill-level similarities. Our analysis shows that the proposed methodology successfully captures the similarities across legal documents at various levels of aggregation.</abstract>
      <url hash="5fb4ee9e">2021.emnlp-main.787</url>
      <attachment type="Software" hash="bfe0a9b4">2021.emnlp-main.787.Software.zip</attachment>
      <bibkey>kim-etal-2021-learning</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.787</doi>
      <video href="2021.emnlp-main.787.mp4"/>
      <pwccode url="https://github.com/hikoseon12/learning-bill-similarity" additional="false">hikoseon12/learning-bill-similarity</pwccode>
    </paper>
    <paper id="788">
      <title><fixed-case>SWEAT</fixed-case>: Scoring Polarization of Topics across Different Corpora</title>
      <author><first>Federico</first><last>Bianchi</last></author>
      <author><first>Marco</first><last>Marelli</last></author>
      <author><first>Paolo</first><last>Nicoli</last></author>
      <author><first>Matteo</first><last>Palmonari</last></author>
      <pages>10065–10072</pages>
      <abstract>Understanding differences of viewpoints across corpora is a fundamental task for computational social sciences. In this paper, we propose the Sliced Word Embedding Association Test (SWEAT), a novel statistical measure to compute the relative polarization of a topical wordset across two distributional representations. To this end, SWEAT uses two additional wordsets, deemed to have opposite valence, to represent two different poles. We validate our approach and illustrate a case study to show the usefulness of the introduced measure.</abstract>
      <url hash="7b9953e7">2021.emnlp-main.788</url>
      <bibkey>bianchi-etal-2021-sweat</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.788</doi>
      <video href="2021.emnlp-main.788.mp4"/>
      <pwccode url="https://github.com/vinid/sweat" additional="false">vinid/sweat</pwccode>
    </paper>
    <paper id="789">
      <title>“So You Think You’re Funny?”: Rating the Humour Quotient in Standup Comedy</title>
      <author><first>Anirudh</first><last>Mittal</last></author>
      <author><first>Pranav Jeevan</first><last>P</last></author>
      <author><first>Prerak</first><last>Gandhi</last></author>
      <author><first>Diptesh</first><last>Kanojia</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>10073–10079</pages>
      <abstract>Computational Humour (CH) has attracted the interest of Natural Language Processing and Computational Linguistics communities. Creating datasets for automatic measurement of humour quotient is difficult due to multiple possible interpretations of the content. In this work, we create a multi-modal humour-annotated dataset (~40 hours) using stand-up comedy clips. We devise a novel scoring mechanism to annotate the training data with a humour quotient score using the audience’s laughter. The normalized duration (laughter duration divided by the clip duration) of laughter in each clip is used to compute this humour coefficient score on a five-point scale (0-4). This method of scoring is validated by comparing with manually annotated scores, wherein a quadratic weighted kappa of 0.6 is obtained. We use this dataset to train a model that provides a ‘funniness’ score, on a five-point scale, given the audio and its corresponding text. We compare various neural language models for the task of humour-rating and achieve an accuracy of 0.813 in terms of Quadratic Weighted Kappa (QWK). Our ‘Open Mic’ dataset is released for further research along with the code.</abstract>
      <url hash="06308fa1">2021.emnlp-main.789</url>
      <bibkey>mittal-etal-2021-think</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.789</doi>
      <video href="2021.emnlp-main.789.mp4"/>
      <pwccode url="https://github.com/theextrasemicolon/ai-openmic" additional="false">theextrasemicolon/ai-openmic</pwccode>
    </paper>
    <paper id="790">
      <title>“Was it “stated” or was it “claimed”?: How linguistic bias affects generative language models</title>
      <author><first>Roma</first><last>Patel</last></author>
      <author><first>Ellie</first><last>Pavlick</last></author>
      <pages>10080–10095</pages>
      <abstract>People use language in subtle and nuanced ways to convey their beliefs. For instance, saying <i>claimed</i> instead of <i>said</i> casts doubt on the truthfulness of the underlying proposition, thus representing the author’s opinion on the matter. Several works have identified such linguistic classes of words that occur frequently in natural language text and are bias-inducing by virtue of their framing effects. In this paper, we test whether generative language models (including GPT-2 (CITATION) are sensitive to these linguistic framing effects. In particular, we test whether prompts that contain linguistic markers of author bias (e.g., hedges, implicatives, subjective intensifiers, assertives) influence the distribution of the generated text. Although these framing effects are subtle and stylistic, we find evidence that they lead to measurable style and topic differences in the generated text, leading to language that is, on average, more polarised and more skewed towards controversial entities and events.</abstract>
      <url hash="f3f72810">2021.emnlp-main.790</url>
      <bibkey>patel-pavlick-2021-stated</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.790</doi>
    </paper>
    <paper id="791">
      <title><fixed-case>PAUSE</fixed-case>: Positive and Annealed Unlabeled Sentence Embedding</title>
      <author><first>Lele</first><last>Cao</last></author>
      <author><first>Emil</first><last>Larsson</last></author>
      <author><first>Vilhelm</first><last>von Ehrenheim</last></author>
      <author><first>Dhiana Deva</first><last>Cavalcanti Rocha</last></author>
      <author><first>Anna</first><last>Martin</last></author>
      <author><first>Sonja</first><last>Horn</last></author>
      <pages>10096–10107</pages>
      <abstract>Sentence embedding refers to a set of effective and versatile techniques for converting raw text into numerical vector representations that can be used in a wide range of natural language processing (NLP) applications. The majority of these techniques are either supervised or unsupervised. Compared to the unsupervised methods, the supervised ones make less assumptions about optimization objectives and usually achieve better results. However, the training requires a large amount of labeled sentence pairs, which is not available in many industrial scenarios. To that end, we propose a generic and end-to-end approach – PAUSE (Positive and Annealed Unlabeled Sentence Embedding), capable of learning high-quality sentence embeddings from a partially labeled dataset. We experimentally show that PAUSE achieves, and sometimes surpasses, state-of-the-art results using only a small fraction of labeled sentence pairs on various benchmark tasks. When applied to a real industrial use case where labeled samples are scarce, PAUSE encourages us to extend our dataset without the burden of extensive manual annotation work.</abstract>
      <url hash="51096494">2021.emnlp-main.791</url>
      <bibkey>cao-etal-2021-pause</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.791</doi>
      <video href="2021.emnlp-main.791.mp4"/>
      <pwccode url="https://github.com/eqtpartners/pause" additional="false">eqtpartners/pause</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="792">
      <title>A Simple Geometric Method for Cross-Lingual Linguistic Transformations with Pre-trained Autoencoders</title>
      <author><first>Maarten</first><last>De Raedt</last></author>
      <author><first>Fréderic</first><last>Godin</last></author>
      <author><first>Pieter</first><last>Buteneers</last></author>
      <author><first>Chris</first><last>Develder</last></author>
      <author><first>Thomas</first><last>Demeester</last></author>
      <pages>10108–10114</pages>
      <abstract>Powerful sentence encoders trained for multiple languages are on the rise. These systems are capable of embedding a wide range of linguistic properties into vector representations. While explicit probing tasks can be used to verify the presence of specific linguistic properties, it is unclear whether the vector representations can be manipulated to indirectly steer such properties. For efficient learning, we investigate the use of a geometric mapping in embedding space to transform linguistic properties, without any tuning of the pre-trained sentence encoder or decoder. We validate our approach on three linguistic properties using a pre-trained multilingual autoencoder and analyze the results in both monolingual and cross-lingual settings.</abstract>
      <url hash="c9568ff6">2021.emnlp-main.792</url>
      <bibkey>de-raedt-etal-2021-simple</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.792</doi>
      <video href="2021.emnlp-main.792.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/senteval">SentEval</pwcdataset>
    </paper>
    <paper id="793">
      <title>An Information-Theoretic Characterization of Morphological Fusion</title>
      <author><first>Neil</first><last>Rathi</last></author>
      <author><first>Michael</first><last>Hahn</last></author>
      <author><first>Richard</first><last>Futrell</last></author>
      <pages>10115–10120</pages>
      <abstract>Linguistic typology generally divides synthetic languages into groups based on their morphological fusion. However, this measure has long been thought to be best considered a matter of degree. We present an information-theoretic measure, called informational fusion, to quantify the degree of fusion of a given set of morphological features in a surface form, which naturally provides such a graded scale. Informational fusion is able to encapsulate not only concatenative, but also nonconcatenative morphological systems (e.g. Arabic), abstracting away from any notions of morpheme segmentation. We then show, on a sample of twenty-one languages, that our measure recapitulates the usual linguistic classifications for concatenative systems, and provides new measures for nonconcatenative ones. We also evaluate the long-standing hypotheses that more frequent forms are more fusional, and that paradigm size anticorrelates with degree of fusion. We do not find evidence for the idea that languages have characteristic levels of fusion; rather, the degree of fusion varies across part-of-speech within languages.</abstract>
      <url hash="3ffc2dba">2021.emnlp-main.793</url>
      <bibkey>rathi-etal-2021-information</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.793</doi>
      <video href="2021.emnlp-main.793.mp4"/>
      <pwccode url="https://github.com/neilrathi/morphological-fusion" additional="false">neilrathi/morphological-fusion</pwccode>
    </paper>
    <paper id="794">
      <title>The Effect of Efficient Messaging and Input Variability on Neural-Agent Iterated Language Learning</title>
      <author><first>Yuchen</first><last>Lian</last></author>
      <author><first>Arianna</first><last>Bisazza</last></author>
      <author><first>Tessa</first><last>Verhoef</last></author>
      <pages>10121–10129</pages>
      <abstract>Natural languages display a trade-off among different strategies to convey syntactic structure, such as word order or inflection. This trade-off, however, has not appeared in recent simulations of iterated language learning with neural network agents (Chaabouni et al., 2019b). We re-evaluate this result in light of three factors that play an important role in comparable experiments from the Language Evolution field: (i) speaker bias towards efficient messaging, (ii) non systematic input languages, and (iii) learning bottleneck. Our simulations show that neural agents mainly strive to maintain the utterance type distribution observed during learning, instead of developing a more efficient or systematic language.</abstract>
      <url hash="e97aea41">2021.emnlp-main.794</url>
      <bibkey>lian-etal-2021-effect</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.794</doi>
      <video href="2021.emnlp-main.794.mp4"/>
    </paper>
    <paper id="795">
      <title>On Classifying whether Two Texts are on the Same Side of an Argument</title>
      <author><first>Erik</first><last>Körner</last></author>
      <author><first>Gregor</first><last>Wiedemann</last></author>
      <author><first>Ahmad Dawar</first><last>Hakimi</last></author>
      <author><first>Gerhard</first><last>Heyer</last></author>
      <author><first>Martin</first><last>Potthast</last></author>
      <pages>10130–10138</pages>
      <abstract>To ease the difficulty of argument stance classification, the task of same side stance classification (S3C) has been proposed. In contrast to actual stance classification, which requires a substantial amount of domain knowledge to identify whether an argument is in favor or against a certain issue, it is argued that, for S3C, only argument similarity within stances needs to be learned to successfully solve the task. We evaluate several transformer-based approaches on the dataset of the recent S3C shared task, followed by an in-depth evaluation and error analysis of our model and the task’s hypothesis. We show that, although we achieve state-of-the-art results, our model fails to generalize both within as well as across topics and domains when adjusting the sampling strategy of the training and test set to a more adversarial scenario. Our evaluation shows that current state-of-the-art approaches cannot determine same side stance by considering only domain-independent linguistic similarity features, but appear to require domain knowledge and semantic inference, too.</abstract>
      <url hash="d8c53cf2">2021.emnlp-main.795</url>
      <bibkey>korner-etal-2021-classifying</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.795</doi>
      <video href="2021.emnlp-main.795.mp4"/>
      <pwccode url="https://github.com/webis-de/emnlp-21" additional="false">webis-de/emnlp-21</pwccode>
    </paper>
    <paper id="796">
      <title><fixed-case>C</fixed-case>hinese Opinion Role Labeling with Corpus Translation: A Pivot Study</title>
      <author><first>Ranran</first><last>Zhen</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Guohong</first><last>Fu</last></author>
      <author><first>Chengguo</first><last>Lv</last></author>
      <author><first>Meishan</first><last>Zhang</last></author>
      <pages>10139–10149</pages>
      <abstract>Opinion Role Labeling (ORL), aiming to identify the key roles of opinion, has received increasing interest. Unlike most of the previous works focusing on the English language, in this paper, we present the first work of Chinese ORL. We construct a Chinese dataset by manually translating and projecting annotations from a standard English MPQA dataset. Then, we investigate the effectiveness of cross-lingual transfer methods, including model transfer and corpus translation. We exploit multilingual BERT with Contextual Parameter Generator and Adapter methods to examine the potentials of unsupervised cross-lingual learning and our experiments and analyses for both bilingual and multilingual transfers establish a foundation for the future research of this task.</abstract>
      <url hash="06510e66">2021.emnlp-main.796</url>
      <attachment type="Software" hash="f67c6879">2021.emnlp-main.796.Software.zip</attachment>
      <bibkey>zhen-etal-2021-chinese</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.796</doi>
      <video href="2021.emnlp-main.796.mp4"/>
      <pwccode url="https://github.com/zenrran/chineseorl-with-corpus-translation" additional="false">zenrran/chineseorl-with-corpus-translation</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mpqa-opinion-corpus">MPQA Opinion Corpus</pwcdataset>
    </paper>
    <paper id="797">
      <title><fixed-case>M</fixed-case>assive<fixed-case>S</fixed-case>umm: a very large-scale, very multilingual, news summarisation dataset</title>
      <author><first>Daniel</first><last>Varab</last></author>
      <author><first>Natalie</first><last>Schluter</last></author>
      <pages>10150–10161</pages>
      <abstract>Current research in automatic summarisation is unapologetically anglo-centered–a persistent state-of-affairs, which also predates neural net approaches. High-quality automatic summarisation datasets are notoriously expensive to create, posing a challenge for any language. However, with digitalisation, archiving, and social media advertising of newswire articles, recent work has shown how, with careful methodology application, large-scale datasets can now be simply gathered instead of written. In this paper, we present a large-scale multilingual summarisation dataset containing articles in 92 languages, spread across 28.8 million articles, in more than 35 writing scripts. This is both the largest, most inclusive, existing automatic summarisation dataset, as well as one of the largest, most inclusive, ever published datasets for any NLP task. We present the first investigation on the efficacy of resource building from news platforms in the low-resource language setting. Finally, we provide some first insight on how low-resource language settings impact state-of-the-art automatic summarisation system performance.</abstract>
      <url hash="f78971c7">2021.emnlp-main.797</url>
      <bibkey>varab-schluter-2021-massivesumm</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.797</doi>
      <video href="2021.emnlp-main.797.mp4"/>
      <pwccode url="https://github.com/danielvarab/massive-summ" additional="false">danielvarab/massive-summ</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/danewsroom">DaNewsroom</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/global-voices">Global Voices</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mlsum">MLSUM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/newsroom">NEWSROOM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/new-york-times-annotated-corpus">New York Times Annotated Corpus</pwcdataset>
    </paper>
    <paper id="798">
      <title><fixed-case>AUTOSUMM</fixed-case>: Automatic Model Creation for Text Summarization</title>
      <author><first>Sharmila Reddy</first><last>Nangi</last></author>
      <author><first>Atharv</first><last>Tyagi</last></author>
      <author><first>Jay</first><last>Mundra</last></author>
      <author><first>Sagnik</first><last>Mukherjee</last></author>
      <author><first>Raj</first><last>Snehal</last></author>
      <author><first>Niyati</first><last>Chhaya</last></author>
      <author><first>Aparna</first><last>Garimella</last></author>
      <pages>10162–10172</pages>
      <abstract>Recent efforts to develop deep learning models for text generation tasks such as extractive and abstractive summarization have resulted in state-of-the-art performances on various datasets. However, obtaining the best model configuration for a given dataset requires an extensive knowledge of deep learning specifics like model architecture, tuning parameters etc., and is often extremely challenging for a non-expert. In this paper, we propose methods to automatically create deep learning models for the tasks of extractive and abstractive text summarization. Based on the recent advances in Automated Machine Learning and the success of large language models such as BERT and GPT-2 in encoding knowledge, we use a combination of Neural Architecture Search (NAS) and Knowledge Distillation (KD) techniques to perform model search and compression using the vast knowledge provided by these language models to develop smaller, customized models for any given dataset. We present extensive empirical results to illustrate the effectiveness of our model creation methods in terms of inference time and model size, while achieving near state-of-the-art performances in terms of accuracy across a range of datasets.</abstract>
      <url hash="612624ab">2021.emnlp-main.798</url>
      <bibkey>nangi-etal-2021-autosumm</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.798</doi>
      <video href="2021.emnlp-main.798.mp4"/>
    </paper>
    <paper id="799">
      <title>Investigating the Helpfulness of Word-Level Quality Estimation for Post-Editing Machine Translation Output</title>
      <author><first>Raksha</first><last>Shenoy</last></author>
      <author><first>Nico</first><last>Herbig</last></author>
      <author><first>Antonio</first><last>Krüger</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <pages>10173–10185</pages>
      <abstract>Compared to fully manual translation, post-editing (PE) machine translation (MT) output can save time and reduce errors. Automatic word-level quality estimation (QE) aims to predict the correctness of words in MT output and holds great promise to aid PE by flagging problematic output. Quality of QE is crucial, as incorrect QE might lead to translators missing errors or wasting time on already correct MT output. Achieving accurate automatic word-level QE is very hard, and it is currently not known (i) at what quality threshold QE is actually beginning to be useful for human PE, and (ii), how to best present word-level QE information to translators. In particular, should word-level QE visualization indicate uncertainty of the QE model or not? In this paper, we address both research questions with real and simulated word-level QE, visualizations, and user studies, where time, subjective ratings, and quality of the final translations are assessed. Results show that current word-level QE models are not yet good enough to support PE. Instead, quality levels of &gt; 80% F1 are required. For helpful quality levels, a visualization reflecting the uncertainty of the QE model is preferred. Our analysis further shows that speed gains achieved through QE are not merely a result of blindly trusting the QE system, but that the quality of the final translations also improves. The threshold results from the paper establish a quality goal for future word-level QE research.</abstract>
      <url hash="c3200638">2021.emnlp-main.799</url>
      <attachment type="Software" hash="9178ada6">2021.emnlp-main.799.Software.zip</attachment>
      <bibkey>shenoy-etal-2021-investigating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.799</doi>
      <video href="2021.emnlp-main.799.mp4"/>
      <pwccode url="https://github.com/nicoherbig/mmpe" additional="false">nicoherbig/mmpe</pwccode>
    </paper>
    <paper id="800">
      <title><fixed-case>UNK</fixed-case>s Everywhere: <fixed-case>A</fixed-case>dapting Multilingual Language Models to New Scripts</title>
      <author><first>Jonas</first><last>Pfeiffer</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <author><first>Sebastian</first><last>Ruder</last></author>
      <pages>10186–10203</pages>
      <abstract>Massively multilingual language models such as multilingual BERT offer state-of-the-art cross-lingual transfer performance on a range of NLP tasks. However, due to limited capacity and large differences in pretraining data sizes, there is a profound performance gap between resource-rich and resource-poor target languages. The ultimate challenge is dealing with under-resourced languages not covered at all by the models and written in scripts unseen during pretraining. In this work, we propose a series of novel data-efficient methods that enable quick and effective adaptation of pretrained multilingual models to such low-resource languages and unseen scripts. Relying on matrix factorization, our methods capitalize on the existing latent knowledge about multiple languages already available in the pretrained model’s embedding matrix. Furthermore, we show that learning of the new dedicated embedding matrix in the target language can be improved by leveraging a small number of vocabulary items (i.e., the so-called lexically overlapping tokens) shared between mBERT’s and target language vocabulary. Our adaptation techniques offer substantial performance gains for languages with unseen scripts. We also demonstrate that they can yield improvements for low-resource languages written in scripts covered by the pretrained model.</abstract>
      <url hash="9adf6cc2">2021.emnlp-main.800</url>
      <bibkey>pfeiffer-etal-2021-unks</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.800</doi>
      <video href="2021.emnlp-main.800.mp4"/>
      <pwccode url="https://github.com/adapter-hub/unks_everywhere" additional="true">adapter-hub/unks_everywhere</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="801">
      <title>Neural Machine Translation Quality and Post-Editing Performance</title>
      <author><first>Vilém</first><last>Zouhar</last></author>
      <author><first>Martin</first><last>Popel</last></author>
      <author><first>Ondřej</first><last>Bojar</last></author>
      <author><first>Aleš</first><last>Tamchyna</last></author>
      <pages>10204–10214</pages>
      <abstract>We test the natural expectation that using MT in professional translation saves human processing time. The last such study was carried out by Sanchez-Torron and Koehn (2016) with phrase-based MT, artificially reducing the translation quality. In contrast, we focus on neural MT (NMT) of high quality, which has become the state-of-the-art approach since then and also got adopted by most translation companies. Through an experimental study involving over 30 professional translators for English -&gt; Czech translation, we examine the relationship between NMT performance and post-editing time and quality. Across all models, we found that better MT systems indeed lead to fewer changes in the sentences in this industry setting. The relation between system quality and post-editing time is however not straightforward and, contrary to the results on phrase-based MT, BLEU is definitely not a stable predictor of the time or final output quality.</abstract>
      <url hash="53ee645e">2021.emnlp-main.801</url>
      <attachment type="Software" hash="4cb2c9fd">2021.emnlp-main.801.Software.zip</attachment>
      <bibkey>zouhar-etal-2021-neural</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.801</doi>
      <video href="2021.emnlp-main.801.mp4"/>
    </paper>
    <paper id="802">
      <title><fixed-case>XTREME</fixed-case>-<fixed-case>R</fixed-case>: Towards More Challenging and Nuanced Multilingual Evaluation</title>
      <author><first>Sebastian</first><last>Ruder</last></author>
      <author><first>Noah</first><last>Constant</last></author>
      <author><first>Jan</first><last>Botha</last></author>
      <author><first>Aditya</first><last>Siddhant</last></author>
      <author><first>Orhan</first><last>Firat</last></author>
      <author><first>Jinlan</first><last>Fu</last></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <author><first>Junjie</first><last>Hu</last></author>
      <author><first>Dan</first><last>Garrette</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Melvin</first><last>Johnson</last></author>
      <pages>10215–10245</pages>
      <abstract>Machine learning has brought striking advances in multilingual natural language processing capabilities over the past year. For example, the latest techniques have improved the state-of-the-art performance on the XTREME multilingual benchmark by more than 13 points. While a sizeable gap to human-level performance remains, improvements have been easier to achieve in some tasks than in others. This paper analyzes the current state of cross-lingual transfer learning and summarizes some lessons learned. In order to catalyze meaningful progress, we extend XTREME to XTREME-R, which consists of an improved set of ten natural language understanding tasks, including challenging language-agnostic retrieval tasks, and covers 50 typologically diverse languages. In addition, we provide a massively multilingual diagnostic suite and fine-grained multi-dataset evaluation capabilities through an interactive public leaderboard to gain a better understanding of such models.</abstract>
      <url hash="1ba70470">2021.emnlp-main.802</url>
      <bibkey>ruder-etal-2021-xtreme</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.802</doi>
      <video href="2021.emnlp-main.802.mp4"/>
      <pwccode url="https://github.com/google-research/xtreme" additional="false">google-research/xtreme</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad-r">LAReQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqa">MLQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mewsli-9">Mewsli-9</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tatoeba">Tatoeba</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydi-qa">TyDi QA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/tydiqa-goldp">TyDiQA-GoldP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xcopa">XCOPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xquad">XQuAD</pwcdataset>
    </paper>
    <paper id="803">
      <title>Contrastive Conditioning for Assessing Disambiguation in <fixed-case>MT</fixed-case>: <fixed-case>A</fixed-case> Case Study of Distilled Bias</title>
      <author><first>Jannis</first><last>Vamvas</last></author>
      <author><first>Rico</first><last>Sennrich</last></author>
      <pages>10246–10265</pages>
      <abstract>Lexical disambiguation is a major challenge for machine translation systems, especially if some senses of a word are trained less often than others. Identifying patterns of overgeneralization requires evaluation methods that are both reliable and scalable. We propose contrastive conditioning as a reference-free black-box method for detecting disambiguation errors. Specifically, we score the quality of a translation by conditioning on variants of the source that provide contrastive disambiguation cues. After validating our method, we apply it in a case study to perform a targeted evaluation of sequence-level knowledge distillation. By probing word sense disambiguation and translation of gendered occupation names, we show that distillation-trained models tend to overgeneralize more than other models with a comparable BLEU score. Contrastive conditioning thus highlights a side effect of distillation that is not fully captured by standard evaluation metrics. Code and data to reproduce our findings are publicly available.</abstract>
      <url hash="386824f8">2021.emnlp-main.803</url>
      <bibkey>vamvas-sennrich-2021-contrastive</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.803</doi>
      <video href="2021.emnlp-main.803.mp4"/>
      <pwccode url="https://github.com/zurichnlp/contrastive-conditioning" additional="false">zurichnlp/contrastive-conditioning</pwccode>
    </paper>
    <paper id="804">
      <title><fixed-case>M</fixed-case>easuring Association Between Labels and Free-Text Rationales</title>
      <author><first>Sarah</first><last>Wiegreffe</last></author>
      <author><first>Ana</first><last>Marasović</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>10266–10284</pages>
      <abstract>In interpretable NLP, we require faithful rationales that reflect the model’s decision-making process for an explained instance. While prior work focuses on extractive rationales (a subset of the input words), we investigate their less-studied counterpart: free-text natural language rationales. We demonstrate that *pipelines*, models for faithful rationalization on information-extraction style tasks, do not work as well on “reasoning” tasks requiring free-text rationales. We turn to models that *jointly* predict and rationalize, a class of widely used high-performance models for free-text rationalization. We investigate the extent to which the labels and rationales predicted by these models are associated, a necessary property of faithful explanation. Via two tests, *robustness equivalence* and *feature importance agreement*, we find that state-of-the-art T5-based joint models exhibit desirable properties for explaining commonsense question-answering and natural language inference, indicating their potential for producing faithful free-text rationales.</abstract>
      <url hash="3d582512">2021.emnlp-main.804</url>
      <bibkey>wiegreffe-etal-2021-measuring</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.804</doi>
      <video href="2021.emnlp-main.804.mp4"/>
      <revision id="1" href="2021.emnlp-main.804v1" hash="4876a357"/>
      <revision id="2" href="2021.emnlp-main.804v2" hash="3d582512" date="2022-08-29">Updated simulatability relevant content.</revision>
      <pwccode url="https://github.com/allenai/label_rationale_association" additional="false">allenai/label_rationale_association</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cos-e">CoS-E</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/e-snli">e-SNLI</pwcdataset>
    </paper>
    <paper id="805">
      <title>Discretized Integrated Gradients for Explaining Language Models</title>
      <author><first>Soumya</first><last>Sanyal</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <pages>10285–10299</pages>
      <abstract>As a prominent attribution-based explanation algorithm, Integrated Gradients (IG) is widely adopted due to its desirable explanation axioms and the ease of gradient computation. It measures feature importance by averaging the model’s output gradient interpolated along a straight-line path in the input data space. However, such straight-line interpolated points are not representative of text data due to the inherent discreteness of the word embedding space. This questions the faithfulness of the gradients computed at the interpolated points and consequently, the quality of the generated explanations. Here we propose Discretized Integrated Gradients (DIG), which allows effective attribution along non-linear interpolation paths. We develop two interpolation strategies for the discrete word embedding space that generates interpolation points that lie close to actual words in the embedding space, yielding more faithful gradient computation. We demonstrate the effectiveness of DIG over IG through experimental and human evaluations on multiple sentiment classification datasets. We provide the source code of DIG to encourage reproducible research.</abstract>
      <url hash="7ad89942">2021.emnlp-main.805</url>
      <bibkey>sanyal-ren-2021-discretized</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.805</doi>
      <video href="2021.emnlp-main.805.mp4"/>
      <pwccode url="https://github.com/ink-usc/dig" additional="true">ink-usc/dig</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="806">
      <title>Putting Words in <fixed-case>BERT</fixed-case>’s Mouth: Navigating Contextualized Vector Spaces with Pseudowords</title>
      <author><first>Taelin</first><last>Karidi</last></author>
      <author><first>Yichu</first><last>Zhou</last></author>
      <author><first>Nathan</first><last>Schneider</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <author><first>Vivek</first><last>Srikumar</last></author>
      <pages>10300–10313</pages>
      <abstract>We present a method for exploring regions around individual points in a contextualized vector space (particularly, BERT space), as a way to investigate how these regions correspond to word senses. By inducing a contextualized “pseudoword” vector as a stand-in for a static embedding in the input layer, and then performing masked prediction of a word in the sentence, we are able to investigate the geometry of the BERT-space in a controlled manner around individual instances. Using our method on a set of carefully constructed sentences targeting highly ambiguous English words, we find substantial regularity in the contextualized space, with regions that correspond to distinct word senses; but between these regions there are occasionally “sense voids”—regions that do not correspond to any intelligible sense.</abstract>
      <url hash="23611c2b">2021.emnlp-main.806</url>
      <bibkey>karidi-etal-2021-putting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.806</doi>
      <video href="2021.emnlp-main.806.mp4"/>
      <pwccode url="https://github.com/tai314159/pwibm-putting-words-in-bert-s-mouth" additional="false">tai314159/pwibm-putting-words-in-bert-s-mouth</pwccode>
    </paper>
    <paper id="807">
      <title>Rationales for Sequential Predictions</title>
      <author><first>Keyon</first><last>Vafa</last></author>
      <author><first>Yuntian</first><last>Deng</last></author>
      <author><first>David</first><last>Blei</last></author>
      <author><first>Alexander</first><last>Rush</last></author>
      <pages>10314–10332</pages>
      <abstract>Sequence models are a critical component of modern NLP systems, but their predictions are difficult to explain. We consider model explanations though rationales, subsets of context that can explain individual model predictions. We find sequential rationales by solving a combinatorial optimization: the best rationale is the smallest subset of input tokens that would predict the same output as the full sequence. Enumerating all subsets is intractable, so we propose an efficient greedy algorithm to approximate this objective. The algorithm, which is called greedy rationalization, applies to any model. For this approach to be effective, the model should form compatible conditional distributions when making predictions on incomplete subsets of the context. This condition can be enforced with a short fine-tuning step. We study greedy rationalization on language modeling and machine translation. Compared to existing baselines, greedy rationalization is best at optimizing the sequential objective and provides the most faithful rationales. On a new dataset of annotated sequential rationales, greedy rationales are most similar to human rationales.</abstract>
      <url hash="41a62020">2021.emnlp-main.807</url>
      <bibkey>vafa-etal-2021-rationales</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.807</doi>
      <video href="2021.emnlp-main.807.mp4"/>
      <pwccode url="https://github.com/keyonvafa/sequential-rationales" additional="true">keyonvafa/sequential-rationales</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/lambada">LAMBADA</pwcdataset>
    </paper>
    <paper id="808">
      <title><fixed-case>F</fixed-case>ast<fixed-case>IF</fixed-case>: Scalable Influence Functions for Efficient Model Interpretation and Debugging</title>
      <author><first>Han</first><last>Guo</last></author>
      <author><first>Nazneen</first><last>Rajani</last></author>
      <author><first>Peter</first><last>Hase</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <pages>10333–10350</pages>
      <abstract>Influence functions approximate the “influences” of training data-points for test predictions and have a wide variety of applications. Despite the popularity, their computational cost does not scale well with model and training data size. We present FastIF, a set of simple modifications to influence functions that significantly improves their run-time. We use k-Nearest Neighbors (kNN) to narrow the search space down to a subset of good candidate data points, identify the configurations that best balance the speed-quality trade-off in estimating the inverse Hessian-vector product, and introduce a fast parallel variant. Our proposed method achieves about 80X speedup while being highly correlated with the original influence values. With the availability of the fast influence functions, we demonstrate their usefulness in four applications. First, we examine whether influential data-points can “explain” test time behavior using the framework of simulatability. Second, we visualize the influence interactions between training and test data-points. Third, we show that we can correct model errors by additional fine-tuning on certain influential data-points, improving the accuracy of a trained MultiNLI model by 2.5% on the HANS dataset. Finally, we experiment with a similar setup but fine-tuning on datapoints not seen during training, improving the model accuracy by 2.8% and 1.7% on HANS and ANLI datasets respectively. Overall, our fast influence functions can be efficiently applied to large models and datasets, and our experiments demonstrate the potential of influence functions in model interpretation and correcting model errors.</abstract>
      <url hash="fc43cb3a">2021.emnlp-main.808</url>
      <bibkey>guo-etal-2021-fastif</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.808</doi>
      <video href="2021.emnlp-main.808.mp4"/>
      <pwccode url="https://github.com/salesforce/fast-influence-functions" additional="false">salesforce/fast-influence-functions</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/anli">ANLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wilds">Wilds</pwcdataset>
    </paper>
    <paper id="809">
      <title>Studying word order through iterative shuffling</title>
      <author><first>Nikolay</first><last>Malkin</last></author>
      <author><first>Sameera</first><last>Lanka</last></author>
      <author><first>Pranav</first><last>Goel</last></author>
      <author><first>Nebojsa</first><last>Jojic</last></author>
      <pages>10351–10366</pages>
      <abstract>As neural language models approach human performance on NLP benchmark tasks, their advances are widely seen as evidence of an increasingly complex understanding of syntax. This view rests upon a hypothesis that has not yet been empirically tested: that word order encodes meaning essential to performing these tasks. We refute this hypothesis in many cases: in the GLUE suite and in various genres of English text, the words in a sentence or phrase can rarely be permuted to form a phrase carrying substantially different information. Our surprising result relies on inference by iterative shuffling (IBIS), a novel, efficient procedure that finds the ordering of a bag of words having the highest likelihood under a fixed language model. IBIS can use any black-box model without additional training and is superior to existing word ordering algorithms. Coalescing our findings, we discuss how shuffling inference procedures such as IBIS can benefit language modeling and constrained generation.</abstract>
      <url hash="c0bd06c9">2021.emnlp-main.809</url>
      <bibkey>malkin-etal-2021-studying</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.809</doi>
      <video href="2021.emnlp-main.809.mp4"/>
      <pwccode url="https://github.com/malkin1729/ibis" additional="false">malkin1729/ibis</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
    </paper>
    <paper id="810">
      <title>Distantly-Supervised Named Entity Recognition with Noise-Robust Learning and Language Model Augmented Self-Training</title>
      <author><first>Yu</first><last>Meng</last></author>
      <author><first>Yunyi</first><last>Zhang</last></author>
      <author><first>Jiaxin</first><last>Huang</last></author>
      <author><first>Xuan</first><last>Wang</last></author>
      <author><first>Yu</first><last>Zhang</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>10367–10378</pages>
      <abstract>We study the problem of training named entity recognition (NER) models using only distantly-labeled data, which can be automatically obtained by matching entity mentions in the raw text with entity types in a knowledge base. The biggest challenge of distantly-supervised NER is that the distant supervision may induce incomplete and noisy labels, rendering the straightforward application of supervised learning ineffective. In this paper, we propose (1) a noise-robust learning scheme comprised of a new loss function and a noisy label removal step, for training NER models on distantly-labeled data, and (2) a self-training method that uses contextualized augmentations created by pre-trained language models to improve the generalization ability of the NER model. On three benchmark datasets, our method achieves superior performance, outperforming existing distantly-supervised NER models by significant margins.</abstract>
      <url hash="ed4d4e85">2021.emnlp-main.810</url>
      <bibkey>meng-etal-2021-distantly</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.810</doi>
      <video href="2021.emnlp-main.810.mp4"/>
      <pwccode url="https://github.com/yumeng5/roster" additional="false">yumeng5/roster</pwccode>
    </paper>
    <paper id="811">
      <title>Open Knowledge Graphs Canonicalization using Variational Autoencoders</title>
      <author><first>Sarthak</first><last>Dash</last></author>
      <author><first>Gaetano</first><last>Rossiello</last></author>
      <author><first>Nandana</first><last>Mihindukulasooriya</last></author>
      <author><first>Sugato</first><last>Bagchi</last></author>
      <author><first>Alfio</first><last>Gliozzo</last></author>
      <pages>10379–10394</pages>
      <abstract>Noun phrases and Relation phrases in open knowledge graphs are not canonicalized, leading to an explosion of redundant and ambiguous subject-relation-object triples. Existing approaches to solve this problem take a two-step approach. First, they generate embedding representations for both noun and relation phrases, then a clustering algorithm is used to group them using the embeddings as features. In this work, we propose Canonicalizing Using Variational AutoEncoders and Side Information (CUVA), a joint model to learn both embeddings and cluster assignments in an end-to-end approach, which leads to a better vector representation for the noun and relation phrases. Our evaluation over multiple benchmarks shows that CUVA outperforms the existing state-of-the-art approaches. Moreover, we introduce CanonicNell, a novel dataset to evaluate entity canonicalization systems.</abstract>
      <url hash="339a40ac">2021.emnlp-main.811</url>
      <bibkey>dash-etal-2021-open</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.811</doi>
      <video href="2021.emnlp-main.811.mp4"/>
      <pwccode url="https://github.com/IBM/Open-KG-canonicalization" additional="false">IBM/Open-KG-canonicalization</pwccode>
    </paper>
    <paper id="812">
      <title><fixed-case>H</fixed-case>itt<fixed-case>ER</fixed-case>: Hierarchical Transformers for Knowledge Graph Embeddings</title>
      <author><first>Sanxing</first><last>Chen</last></author>
      <author><first>Xiaodong</first><last>Liu</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <author><first>Jian</first><last>Jiao</last></author>
      <author><first>Ruofei</first><last>Zhang</last></author>
      <author><first>Yangfeng</first><last>Ji</last></author>
      <pages>10395–10407</pages>
      <abstract>This paper examines the challenging problem of learning representations of entities and relations in a complex multi-relational knowledge graph. We propose HittER, a Hierarchical Transformer model to jointly learn Entity-relation composition and Relational contextualization based on a source entity’s neighborhood. Our proposed model consists of two different Transformer blocks: the bottom block extracts features of each entity-relation pair in the local neighborhood of the source entity and the top block aggregates the relational information from outputs of the bottom block. We further design a masked entity prediction task to balance information from the relational context and the source entity itself. Experimental results show that HittER achieves new state-of-the-art results on multiple link prediction datasets. We additionally propose a simple approach to integrate HittER into BERT and demonstrate its effectiveness on two Freebase factoid question answering datasets.</abstract>
      <url hash="0ac9e4fe">2021.emnlp-main.812</url>
      <bibkey>chen-etal-2021-hitter</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.812</doi>
      <video href="2021.emnlp-main.812.mp4"/>
      <pwccode url="" additional="true"/>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k">FB15k</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fb15k-237">FB15k-237</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wn18">WN18</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wn18rr">WN18RR</pwcdataset>
    </paper>
    <paper id="813">
      <title>Few-Shot Named Entity Recognition: An Empirical Baseline Study</title>
      <author><first>Jiaxin</first><last>Huang</last></author>
      <author><first>Chunyuan</first><last>Li</last></author>
      <author><first>Krishan</first><last>Subudhi</last></author>
      <author><first>Damien</first><last>Jose</last></author>
      <author><first>Shobana</first><last>Balakrishnan</last></author>
      <author><first>Weizhu</first><last>Chen</last></author>
      <author><first>Baolin</first><last>Peng</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>10408–10423</pages>
      <abstract>This paper presents an empirical study to efficiently build named entity recognition (NER) systems when a small amount of in-domain labeled data is available. Based upon recent Transformer-based self-supervised pre-trained language models (PLMs), we investigate three orthogonal schemes to improve model generalization ability in few-shot settings: (1) meta-learning to construct prototypes for different entity types, (2) task-specific supervised pre-training on noisy web data to extract entity-related representations and (3) self-training to leverage unlabeled in-domain data. On 10 public NER datasets, we perform extensive empirical comparisons over the proposed schemes and their combinations with various proportions of labeled data, our experiments show that (i)in the few-shot learning setting, the proposed NER schemes significantly improve or outperform the commonly used baseline, a PLM-based linear classifier fine-tuned using domain labels. (ii) We create new state-of-the-art results on both few-shot and training-free settings compared with existing methods.</abstract>
      <url hash="57d06afa">2021.emnlp-main.813</url>
      <bibkey>huang-etal-2021-shot</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.813</doi>
      <video href="2021.emnlp-main.813.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snips">SNIPS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wnut-2017-emerging-and-rare-entity">WNUT 2017</pwcdataset>
    </paper>
    <paper id="814">
      <title><fixed-case>XLE</fixed-case>nt: Mining a Large Cross-lingual Entity Dataset with Lexical-Semantic-Phonetic Word Alignment</title>
      <author><first>Ahmed</first><last>El-Kishky</last></author>
      <author><first>Adithya</first><last>Renduchintala</last></author>
      <author><first>James</first><last>Cross</last></author>
      <author><first>Francisco</first><last>Guzmán</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>10424–10430</pages>
      <abstract>Cross-lingual named-entity lexica are an important resource to multilingual NLP tasks such as machine translation and cross-lingual wikification. While knowledge bases contain a large number of entities in high-resource languages such as English and French, corresponding entities for lower-resource languages are often missing. To address this, we propose Lexical-Semantic-Phonetic Align (LSP-Align), a technique to automatically mine cross-lingual entity lexica from mined web data. We demonstrate LSP-Align outperforms baselines at extracting cross-lingual entity pairs and mine 164 million entity pairs from 120 different languages aligned with English. We release these cross-lingual entity pairs along with the massively multilingual tagged named entity corpus as a resource to the NLP community.</abstract>
      <url hash="26f17252">2021.emnlp-main.814</url>
      <bibkey>el-kishky-etal-2021-xlent</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.814</doi>
      <video href="2021.emnlp-main.814.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/xlent">XLEnt</pwcdataset>
    </paper>
    <paper id="815">
      <title>Utilizing Relative Event Time to Enhance Event-Event Temporal Relation Extraction</title>
      <author><first>Haoyang</first><last>Wen</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <pages>10431–10437</pages>
      <abstract>Event time is one of the most important features for event-event temporal relation extraction. However, explicit event time information in text is sparse. For example, only about 20% of event mentions in TimeBank-Dense have event-time links. In this paper, we propose a joint model for event-event temporal relation classification and an auxiliary task, relative event time prediction, which predicts the event time as real numbers. We adopt the Stack-Propagation framework to incorporate predicted relative event time for temporal relation classification and keep the differentiability. Our experiments on MATRES dataset show that our model can significantly improve the RoBERTa-based baseline and achieve state-of-the-art performance.</abstract>
      <url hash="39419d19">2021.emnlp-main.815</url>
      <bibkey>wen-ji-2021-utilizing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.815</doi>
      <video href="2021.emnlp-main.815.mp4"/>
      <pwccode url="https://github.com/wenhycs/emnlp2021-utilizing-relative-event-time-to-enhance-event-event-temporal-relation-extraction" additional="false">wenhycs/emnlp2021-utilizing-relative-event-time-to-enhance-event-event-temporal-relation-extraction</pwccode>
    </paper>
    <paper id="816">
      <title>Separating Retention from Extraction in the Evaluation of End-to-end <fixed-case>R</fixed-case>elation <fixed-case>E</fixed-case>xtraction</title>
      <author><first>Bruno</first><last>Taillé</last></author>
      <author><first>Vincent</first><last>Guigue</last></author>
      <author><first>Geoffrey</first><last>Scoutheeten</last></author>
      <author><first>Patrick</first><last>Gallinari</last></author>
      <pages>10438–10449</pages>
      <abstract>State-of-the-art NLP models can adopt shallow heuristics that limit their generalization capability (McCoy et al., 2019). Such heuristics include lexical overlap with the training set in Named-Entity Recognition (Taille et al., 2020) and Event or Type heuristics in Relation Extraction (Rosenman et al., 2020). In the more realistic end-to-end RE setting, we can expect yet another heuristic: the mere retention of training relation triples. In this paper we propose two experiments confirming that retention of known facts is a key factor of performance on standard benchmarks. Furthermore, one experiment suggests that a pipeline model able to use intermediate type representations is less prone to over-rely on retention.</abstract>
      <url hash="5e46e050">2021.emnlp-main.816</url>
      <bibkey>taille-etal-2021-separating</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.816</doi>
      <video href="2021.emnlp-main.816.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/scierc">SciERC</pwcdataset>
    </paper>
    <paper id="817">
      <title>Automatic Text Evaluation through the Lens of <fixed-case>W</fixed-case>asserstein Barycenters</title>
      <author><first>Pierre</first><last>Colombo</last></author>
      <author><first>Guillaume</first><last>Staerman</last></author>
      <author><first>Chloé</first><last>Clavel</last></author>
      <author><first>Pablo</first><last>Piantanida</last></author>
      <pages>10450–10466</pages>
      <abstract>A new metric BaryScore to evaluate text generation based on deep contextualized embeddings (<i>e.g.</i>, BERT, Roberta, ELMo) is introduced. This metric is motivated by a new framework relying on optimal transport tools, <i>i.e.</i>, Wasserstein distance and barycenter. By modelling the layer output of deep contextualized embeddings as a probability distribution rather than by a vector embedding; this framework provides a natural way to aggregate the different outputs through the Wasserstein space topology. In addition, it provides theoretical grounds to our metric and offers an alternative to available solutions (<i>e.g.</i>, MoverScore and BertScore). Numerical evaluation is performed on four different tasks: machine translation, summarization, data2text generation and image captioning. Our results show that BaryScore outperforms other BERT based metrics and exhibits more consistent behaviour in particular for text summarization.</abstract>
      <url hash="db428beb">2021.emnlp-main.817</url>
      <bibkey>colombo-etal-2021-automatic</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.817</doi>
      <video href="2021.emnlp-main.817.mp4"/>
      <pwccode url="" additional="true"/>
      <pwcdataset url="https://paperswithcode.com/dataset/coco">COCO</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2016">WMT 2016</pwcdataset>
    </paper>
    <paper id="818">
      <title>Visually Grounded Reasoning across Languages and Cultures</title>
      <author><first>Fangyu</first><last>Liu</last></author>
      <author><first>Emanuele</first><last>Bugliarello</last></author>
      <author><first>Edoardo Maria</first><last>Ponti</last></author>
      <author><first>Siva</first><last>Reddy</last></author>
      <author><first>Nigel</first><last>Collier</last></author>
      <author><first>Desmond</first><last>Elliott</last></author>
      <pages>10467–10485</pages>
      <abstract>The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western European bias. Therefore, we devise a new protocol to construct an ImageNet-style hierarchy representative of more languages and cultures. In particular, we let the selection of both concepts and images be entirely driven by native speakers, rather than scraping them automatically. Specifically, we focus on a typologically diverse set of languages, namely, Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images obtained through this new protocol, we create a multilingual dataset for Multicultural Reasoning over Vision and Language (MaRVL) by eliciting statements from native speaker annotators about pairs of images. The task consists of discriminating whether each grounded statement is true or false. We establish a series of baselines using state-of-the-art models and find that their cross-lingual transfer performance lags dramatically behind supervised performance in English. These results invite us to reassess the robustness and accuracy of current state-of-the-art models beyond a narrow domain, but also open up new exciting challenges for the development of truly multilingual and multicultural systems.</abstract>
      <url hash="60cbe204">2021.emnlp-main.818</url>
      <bibkey>liu-etal-2021-visually</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.818</doi>
      <video href="2021.emnlp-main.818.mp4"/>
      <pwccode url="https://github.com/e-bug/volta" additional="true">e-bug/volta</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/marvl">MaRVL</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/iglue">IGLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/nlvr">NLVR</pwcdataset>
    </paper>
    <paper id="819">
      <title>Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the <fixed-case>W</fixed-case>inograd Schema</title>
      <author><first>Yanai</first><last>Elazar</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>10486–10500</pages>
      <abstract>The Winograd Schema (WS) has been proposed as a test for measuring commonsense capabilities of models. Recently, pre-trained language model-based approaches have boosted performance on some WS benchmarks but the source of improvement is still not clear. This paper suggests that the apparent progress on WS may not necessarily reflect progress in commonsense reasoning. To support this claim, we first show that the current evaluation method of WS is sub-optimal and propose a modification that uses twin sentences for evaluation. We also propose two new baselines that indicate the existence of artifacts in WS benchmarks. We then develop a method for evaluating WS-like sentences in a zero-shot setting to account for the commonsense reasoning abilities acquired during the pretraining and observe that popular language models perform randomly in this setting when using our more strict evaluation. We conclude that the observed progress is mostly due to the use of supervision in training WS models, which is not likely to successfully support all the required commonsense reasoning skills and knowledge.</abstract>
      <url hash="72210213">2021.emnlp-main.819</url>
      <bibkey>elazar-etal-2021-back</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.819</doi>
      <video href="2021.emnlp-main.819.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/wsc">WSC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winogrande">WinoGrande</pwcdataset>
    </paper>
    <paper id="820">
      <title>Robustness Evaluation of Entity Disambiguation Using Prior Probes: the Case of Entity Overshadowing</title>
      <author><first>Vera</first><last>Provatorova</last></author>
      <author><first>Samarth</first><last>Bhargav</last></author>
      <author><first>Svitlana</first><last>Vakulenko</last></author>
      <author><first>Evangelos</first><last>Kanoulas</last></author>
      <pages>10501–10510</pages>
      <abstract>Entity disambiguation (ED) is the last step of entity linking (EL), when candidate entities are reranked according to the context they appear in. All datasets for training and evaluating models for EL consist of convenience samples, such as news articles and tweets, that propagate the prior probability bias of the entity distribution towards more frequently occurring entities. It was shown that the performance of the EL systems on such datasets is overestimated since it is possible to obtain higher accuracy scores by merely learning the prior. To provide a more adequate evaluation benchmark, we introduce the ShadowLink dataset, which includes 16K short text snippets annotated with entity mentions. We evaluate and report the performance of popular EL systems on the ShadowLink benchmark. The results show a considerable difference in accuracy between more and less common entities for all of the EL systems under evaluation, demonstrating the effect of prior probability bias and entity overshadowing.</abstract>
      <url hash="e98cd7fe">2021.emnlp-main.820</url>
      <bibkey>provatorova-etal-2021-robustness</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.820</doi>
      <video href="2021.emnlp-main.820.mp4"/>
    </paper>
    <paper id="821">
      <title><fixed-case>I</fixed-case>ndo<fixed-case>NLI</fixed-case>: A Natural Language Inference Dataset for <fixed-case>I</fixed-case>ndonesian</title>
      <author><first>Rahmad</first><last>Mahendra</last></author>
      <author><first>Alham Fikri</first><last>Aji</last></author>
      <author><first>Samuel</first><last>Louvan</last></author>
      <author><first>Fahrurrozi</first><last>Rahman</last></author>
      <author><first>Clara</first><last>Vania</last></author>
      <pages>10511–10527</pages>
      <abstract>We present IndoNLI, the first human-elicited NLI dataset for Indonesian. We adapt the data collection protocol for MNLI and collect ~18K sentence pairs annotated by crowd workers and experts. The expert-annotated data is used exclusively as a test set. It is designed to provide a challenging test-bed for Indonesian NLI by explicitly incorporating various linguistic phenomena such as numerical reasoning, structural changes, idioms, or temporal and spatial reasoning. Experiment results show that XLM-R outperforms other pre-trained models in our data. The best performance on the expert-annotated data is still far below human performance (13.4% accuracy gap), suggesting that this test set is especially challenging. Furthermore, our analysis shows that our expert-annotated data is more diverse and contains fewer annotation artifacts than the crowd-annotated data. We hope this dataset can help accelerate progress in Indonesian NLP research.</abstract>
      <url hash="19c57970">2021.emnlp-main.821</url>
      <bibkey>mahendra-etal-2021-indonli</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.821</doi>
      <video href="2021.emnlp-main.821.mp4"/>
      <pwccode url="https://github.com/ir-nlp-csui/indonli" additional="false">ir-nlp-csui/indonli</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/indonli">IndoNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/indonlu-benchmark">IndoNLU Benchmark</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ocnli">OCNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="822">
      <title>Agreeing to Disagree: Annotating Offensive Language Datasets with Annotators’ Disagreement</title>
      <author><first>Elisa</first><last>Leonardelli</last></author>
      <author><first>Stefano</first><last>Menini</last></author>
      <author><first>Alessio</first><last>Palmero Aprosio</last></author>
      <author><first>Marco</first><last>Guerini</last></author>
      <author><first>Sara</first><last>Tonelli</last></author>
      <pages>10528–10539</pages>
      <abstract>Since state-of-the-art approaches to offensive language detection rely on supervised learning, it is crucial to quickly adapt them to the continuously evolving scenario of social media. While several approaches have been proposed to tackle the problem from an algorithmic perspective, so to reduce the need for annotated data, less attention has been paid to the quality of these data. Following a trend that has emerged recently, we focus on the level of agreement among annotators while selecting data to create offensive language datasets, a task involving a high level of subjectivity. Our study comprises the creation of three novel datasets of English tweets covering different topics and having five crowd-sourced judgments each. We also present an extensive set of experiments showing that selecting training and test data according to different levels of annotators’ agreement has a strong effect on classifiers performance and robustness. Our findings are further validated in cross-domain experiments and studied using a popular benchmark dataset. We show that such hard cases, where low agreement is present, are not necessarily due to poor-quality annotation and we advocate for a higher presence of ambiguous cases in future datasets, in order to train more robust systems and better account for the different points of view expressed online.</abstract>
      <url hash="b2672866">2021.emnlp-main.822</url>
      <bibkey>leonardelli-etal-2021-agreeing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.822</doi>
      <video href="2021.emnlp-main.822.mp4"/>
      <pwccode url="https://github.com/dhfbk/annotators-agreement-dataset" additional="false">dhfbk/annotators-agreement-dataset</pwccode>
    </paper>
    <paper id="823">
      <title>A Root of a Problem: Optimizing Single-Root Dependency Parsing</title>
      <author><first>Miloš</first><last>Stanojević</last></author>
      <author><first>Shay B.</first><last>Cohen</last></author>
      <pages>10540–10557</pages>
      <abstract>We describe two approaches to single-root dependency parsing that yield significant speed ups in such parsing. One approach has been previously used in dependency parsers in practice, but remains undocumented in the parsing literature, and is considered a heuristic. We show that this approach actually finds the optimal dependency tree. The second approach relies on simple reweighting of the inference graph being input to the dependency parser and has an optimal running time. Here, we again show that this approach is fully correct and identifies the highest-scoring parse tree. Our experiments demonstrate a manyfold speed up compared to a previous graph-based state-of-the-art parser without any loss in accuracy or optimality.</abstract>
      <url hash="75b9148b">2021.emnlp-main.823</url>
      <bibkey>stanojevic-cohen-2021-root</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.823</doi>
      <video href="2021.emnlp-main.823.mp4"/>
      <pwccode url="https://github.com/stanojevic/fast-mst-algorithm" additional="false">stanojevic/fast-mst-algorithm</pwccode>
    </paper>
    <paper id="824">
      <title>Efficient Sampling of Dependency Structure</title>
      <author><first>Ran</first><last>Zmigrod</last></author>
      <author><first>Tim</first><last>Vieira</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <pages>10558–10569</pages>
      <abstract>Probabilistic distributions over spanning trees in directed graphs are a fundamental model of dependency structure in natural language processing, syntactic dependency trees. In NLP, dependency trees often have an additional root constraint: only one edge may emanate from the root. However, no sampling algorithm has been presented in the literature to account for this additional constraint. In this paper, we adapt two spanning tree sampling algorithms to faithfully sample dependency trees from a graph subject to the root constraint. Wilson (1996(’s sampling algorithm has a running time of O(H) where H is the mean hitting time of the graph. Colbourn (1996)’s sampling algorithm has a running time of O(Nˆ3), which is often greater than the mean hitting time of a directed graph. Additionally, we build upon Colbourn’s algorithm and present a novel extension that can sample K trees without replacement in O(K Nˆ3 + Kˆ2 N) time. To the best of our knowledge, no algorithm has been given for sampling spanning trees without replacement from a directed graph.</abstract>
      <url hash="76763412">2021.emnlp-main.824</url>
      <bibkey>zmigrod-etal-2021-efficient</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.824</doi>
      <video href="2021.emnlp-main.824.mp4"/>
      <revision id="1" href="2021.emnlp-main.824v1" hash="2c5b6dd2"/>
      <revision id="2" href="2021.emnlp-main.824v2" hash="76763412" date="2022-07-08">Corrected Theorem 2.</revision>
      <pwccode url="https://github.com/rycolab/treesample" additional="false">rycolab/treesample</pwccode>
    </paper>
    <paper id="825">
      <title>Reducing Discontinuous to Continuous Parsing with Pointer Network Reordering</title>
      <author><first>Daniel</first><last>Fernández-González</last></author>
      <author><first>Carlos</first><last>Gómez-Rodríguez</last></author>
      <pages>10570–10578</pages>
      <abstract>Discontinuous constituent parsers have always lagged behind continuous approaches in terms of accuracy and speed, as the presence of constituents with discontinuous yield introduces extra complexity to the task. However, a discontinuous tree can be converted into a continuous variant by reordering tokens. Based on that, we propose to reduce discontinuous parsing to a continuous problem, which can then be directly solved by any off-the-shelf continuous parser. To that end, we develop a Pointer Network capable of accurately generating the continuous token arrangement for a given input sentence and define a bijective function to recover the original order. Experiments on the main benchmarks with two continuous parsers prove that our approach is on par in accuracy with purely discontinuous state-of-the-art algorithms, but considerably faster.</abstract>
      <url hash="dd393819">2021.emnlp-main.825</url>
      <bibkey>fernandez-gonzalez-gomez-rodriguez-2021-reducing</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.825</doi>
      <video href="2021.emnlp-main.825.mp4"/>
      <pwccode url="https://github.com/danifg/Pointer-Network-Reordering" additional="false">danifg/Pointer-Network-Reordering</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/penn-treebank">Penn Treebank</pwcdataset>
    </paper>
    <paper id="826">
      <title>A New Representation for Span-based <fixed-case>CCG</fixed-case> Parsing</title>
      <author><first>Yoshihide</first><last>Kato</last></author>
      <author><first>Shigeki</first><last>Matsubara</last></author>
      <pages>10579–10584</pages>
      <abstract>This paper proposes a new representation for CCG derivations. CCG derivations are represented as trees whose nodes are labeled with categories strictly restricted by CCG rule schemata. This characteristic is not suitable for span-based parsing models because they predict node labels independently. In other words, span-based models may generate invalid CCG derivations that violate the rule schemata. Our proposed representation decomposes CCG derivations into several independent pieces and prevents the span-based parsing models from violating the schemata. Our experimental result shows that an off-the-shelf span-based parser with our representation is comparable with previous CCG parsers.</abstract>
      <url hash="fc23b410">2021.emnlp-main.826</url>
      <bibkey>kato-matsubara-2021-new</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.826</doi>
      <video href="2021.emnlp-main.826.mp4"/>
      <pwccode url="https://github.com/yosihide/span-based-ccg-derivation" additional="false">yosihide/span-based-ccg-derivation</pwccode>
    </paper>
    <paper id="827">
      <title><fixed-case>W</fixed-case>hat to Pre-Train on? <fixed-case>E</fixed-case>fficient Intermediate Task Selection</title>
      <author><first>Clifton</first><last>Poth</last></author>
      <author><first>Jonas</first><last>Pfeiffer</last></author>
      <author><first>Andreas</first><last>Rücklé</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>10585–10605</pages>
      <abstract>Intermediate task fine-tuning has been shown to culminate in large transfer gains across many NLP tasks. With an abundance of candidate datasets as well as pre-trained language models, it has become infeasible to experiment with all combinations to find the best transfer setting. In this work, we provide a comprehensive comparison of different methods for efficiently identifying beneficial tasks for intermediate transfer learning. We focus on parameter and computationally efficient adapter settings, highlight different data-availability scenarios, and provide expense estimates for each method. We experiment with a diverse set of 42 intermediate and 11 target English classification, multiple choice, question answering, and sequence tagging tasks. Our results demonstrate that efficient embedding based methods, which rely solely on the respective datasets, outperform computational expensive few-shot fine-tuning approaches. Our best methods achieve an average Regret@3 of 1% across all target tasks, demonstrating that we are able to efficiently identify the best datasets for intermediate training.</abstract>
      <url hash="15ba0e3a">2021.emnlp-main.827</url>
      <bibkey>poth-etal-2021-pre</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.827</doi>
      <video href="2021.emnlp-main.827.mp4"/>
      <pwccode url="https://github.com/adapter-hub/efficient-task-transfer" additional="false">adapter-hub/efficient-task-transfer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/copa">COPA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/drop">DROP</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="828">
      <title><fixed-case>P</fixed-case>ermute<fixed-case>F</fixed-case>ormer: Efficient Relative Position Encoding for Long Sequences</title>
      <author><first>Peng</first><last>Chen</last></author>
      <pages>10606–10618</pages>
      <abstract>A recent variation of Transformer, Performer, scales Transformer to longer sequences with a linear attention mechanism. However, it is not compatible with relative position encoding, which has advantages over absolute position encoding. In this paper, we discuss possible ways to add relative position encoding to Performer. Based on the analysis, we propose PermuteFormer, a Performer-based model with relative position encoding that scales linearly on long sequences. PermuteFormer applies position-dependent transformation on queries and keys to encode positional information into the attention module. This transformation is carefully crafted so that the final output of self-attention is not affected by absolute positions of tokens. PermuteFormer introduces negligible computational overhead by design that it runs as fast as Performer. We evaluate PermuteFormer on Long-Range Arena, a dataset for long sequences, as well as WikiText-103, a language modeling dataset. The experiments show that PermuteFormer uniformly improves the performance of Performer with almost no computational overhead and outperforms vanilla Transformer on most of the tasks.</abstract>
      <url hash="04b30d1c">2021.emnlp-main.828</url>
      <bibkey>chen-2021-permuteformer</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.828</doi>
      <video href="2021.emnlp-main.828.mp4"/>
      <pwccode url="https://github.com/cpcp1998/permuteformer" additional="false">cpcp1998/permuteformer</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="829">
      <title>Block Pruning For Faster Transformers</title>
      <author><first>François</first><last>Lagunas</last></author>
      <author><first>Ella</first><last>Charlaix</last></author>
      <author><first>Victor</first><last>Sanh</last></author>
      <author><first>Alexander</first><last>Rush</last></author>
      <pages>10619–10629</pages>
      <abstract>Pre-training has improved model accuracy for both classification and generation tasks at the cost of introducing much larger and slower models. Pruning methods have proven to be an effective way of reducing model size, whereas distillation methods are proven for speeding up inference. We introduce a block pruning approach targeting both small and fast models. Our approach extends structured methods by considering blocks of any size and integrates this structure into the movement pruning paradigm for fine-tuning. We find that this approach learns to prune out full components of the underlying model, such as attention heads. Experiments consider classification and generation tasks, yielding among other results a pruned model that is a 2.4x faster, 74% smaller BERT on SQuAD v1, with a 1% drop on F1, competitive both with distilled models in speed and pruned models in size.</abstract>
      <url hash="38185732">2021.emnlp-main.829</url>
      <bibkey>lagunas-etal-2021-block</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.829</doi>
      <video href="2021.emnlp-main.829.mp4"/>
      <pwccode url="https://github.com/huggingface/nn_pruning" additional="false">huggingface/nn_pruning</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quora-question-pairs">Quora Question Pairs</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="830">
      <title>Finetuning Pretrained Transformers into <fixed-case>RNN</fixed-case>s</title>
      <author><first>Jungo</first><last>Kasai</last></author>
      <author><first>Hao</first><last>Peng</last></author>
      <author><first>Yizhe</first><last>Zhang</last></author>
      <author><first>Dani</first><last>Yogatama</last></author>
      <author><first>Gabriel</first><last>Ilharco</last></author>
      <author><first>Nikolaos</first><last>Pappas</last></author>
      <author><first>Yi</first><last>Mao</last></author>
      <author><first>Weizhu</first><last>Chen</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>10630–10643</pages>
      <abstract>Transformers have outperformed recurrent neural networks (RNNs) in natural language generation. But this comes with a signifi- cant computational cost, as the attention mechanism’s complexity scales quadratically with sequence length. Efficient transformer variants have received increasing interest in recent works. Among them, a linear-complexity recurrent variant has proven well suited for autoregressive generation. It approximates the softmax attention with randomized or heuristic feature maps, but can be difficult to train and may yield suboptimal accuracy. This work aims to convert a pretrained transformer into its efficient recurrent counterpart, improving efficiency while maintaining accuracy. Specifically, we propose a swap-then-finetune procedure: in an off-the-shelf pretrained transformer, we replace the softmax attention with its linear-complexity recurrent alternative and then finetune. With a learned feature map, our approach provides an improved tradeoff between efficiency and accuracy over the standard transformer and other recurrent variants. We also show that the finetuning process has lower training cost relative to training these recurrent variants from scratch. As many models for natural language tasks are increasingly dependent on large-scale pretrained transformers, this work presents a viable approach to improving inference efficiency without repeating the expensive pretraining process.</abstract>
      <url hash="20e04c33">2021.emnlp-main.830</url>
      <bibkey>kasai-etal-2021-finetuning</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.830</doi>
      <video href="2021.emnlp-main.830.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/wmt-2014">WMT 2014</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-103">WikiText-103</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wikitext-2">WikiText-2</pwcdataset>
    </paper>
    <paper id="831">
      <title>How to Train <fixed-case>BERT</fixed-case> with an Academic Budget</title>
      <author><first>Peter</first><last>Izsak</last></author>
      <author><first>Moshe</first><last>Berchansky</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <pages>10644–10652</pages>
      <abstract>While large language models a la BERT are used ubiquitously in NLP, pretraining them is considered a luxury that only a few well-funded industry labs can afford. How can one train such models with a more modest budget? We present a recipe for pretraining a masked language model in 24 hours using a single low-end deep learning server. We demonstrate that through a combination of software optimizations, design choices, and hyperparameter tuning, it is possible to produce models that are competitive with BERT-base on GLUE tasks at a fraction of the original pretraining cost.</abstract>
      <url hash="af151510">2021.emnlp-main.831</url>
      <bibkey>izsak-etal-2021-train</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.831</doi>
      <video href="2021.emnlp-main.831.mp4"/>
      <pwccode url="https://github.com/peteriz/academic-budget-bert" additional="true">peteriz/academic-budget-bert</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cola">CoLA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mrpc">MRPC</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/qnli">QNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quora-question-pairs">Quora Question Pairs</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rte">RTE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sts-benchmark">STS Benchmark</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/superglue">SuperGLUE</pwcdataset>
    </paper>
    <paper id="832">
      <title>Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of <fixed-case>BERT</fixed-case> Compression</title>
      <author><first>Canwen</first><last>Xu</last></author>
      <author><first>Wangchunshu</first><last>Zhou</last></author>
      <author><first>Tao</first><last>Ge</last></author>
      <author><first>Ke</first><last>Xu</last></author>
      <author><first>Julian</first><last>McAuley</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <pages>10653–10659</pages>
      <abstract>Recent studies on compression of pretrained language models (e.g., BERT) usually use preserved accuracy as the metric for evaluation. In this paper, we propose two new metrics, label loyalty and probability loyalty that measure how closely a compressed model (i.e., student) mimics the original model (i.e., teacher). We also explore the effect of compression with regard to robustness under adversarial attacks. We benchmark quantization, pruning, knowledge distillation and progressive module replacing with loyalty and robustness. By combining multiple compression techniques, we provide a practical strategy to achieve better accuracy, loyalty and robustness.</abstract>
      <url hash="c297e6dc">2021.emnlp-main.832</url>
      <bibkey>xu-etal-2021-beyond</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.832</doi>
      <video href="2021.emnlp-main.832.mp4"/>
      <pwccode url="https://github.com/jetrunner/beyond-preserved-accuracy" additional="false">jetrunner/beyond-preserved-accuracy</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="833">
      <title><fixed-case>I</fixed-case>ndo<fixed-case>BERT</fixed-case>weet: A Pretrained Language Model for <fixed-case>I</fixed-case>ndonesian <fixed-case>T</fixed-case>witter with Effective Domain-Specific Vocabulary Initialization</title>
      <author><first>Fajri</first><last>Koto</last></author>
      <author><first>Jey Han</first><last>Lau</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <pages>10660–10668</pages>
      <abstract>We present IndoBERTweet, the first large-scale pretrained model for Indonesian Twitter that is trained by extending a monolingually-trained Indonesian BERT model with additive domain-specific vocabulary. We focus in particular on efficient model adaptation under vocabulary mismatch, and benchmark different ways of initializing the BERT embedding layer for new word types. We find that initializing with the average BERT subword embedding makes pretraining five times faster, and is more effective than proposed methods for vocabulary adaptation in terms of extrinsic evaluation over seven Twitter-based datasets.</abstract>
      <url hash="6045b071">2021.emnlp-main.833</url>
      <bibkey>koto-etal-2021-indobertweet</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.833</doi>
      <video href="2021.emnlp-main.833.mp4"/>
      <pwccode url="https://github.com/indolem/indobertweet" additional="false">indolem/indobertweet</pwccode>
    </paper>
    <paper id="834">
      <title>Pushing on Text Readability Assessment: A Transformer Meets Handcrafted Linguistic Features</title>
      <author><first>Bruce W.</first><last>Lee</last></author>
      <author><first>Yoo Sung</first><last>Jang</last></author>
      <author><first>Jason</first><last>Lee</last></author>
      <pages>10669–10686</pages>
      <abstract>We report two essential improvements in readability assessment: 1. three novel features in advanced semantics and 2. the timely evidence that traditional ML models (e.g. Random Forest, using handcrafted features) can combine with transformers (e.g. RoBERTa) to augment model performance. First, we explore suitable transformers and traditional ML models. Then, we extract 255 handcrafted linguistic features using self-developed extraction software. Finally, we assemble those to create several hybrid models, achieving state-of-the-art (SOTA) accuracy on popular datasets in readability assessment. The use of handcrafted features help model performance on smaller datasets. Notably, our RoBERTA-RF-T1 hybrid achieves the near-perfect classification accuracy of 99%, a 20.3% increase from the previous SOTA.</abstract>
      <url hash="75ab197c">2021.emnlp-main.834</url>
      <bibkey>lee-etal-2021-pushing</bibkey>
      <revision id="1" href="2021.emnlp-main.834v1" hash="68ff98b2"/>
      <revision id="2" href="2021.emnlp-main.834v2" hash="75ab197c" date="2021-11-09">Corrected a typo.</revision>
      <doi>10.18653/v1/2021.emnlp-main.834</doi>
      <video href="2021.emnlp-main.834.mp4"/>
      <pwccode url="https://github.com/brucewlee/lingfeat" additional="false">brucewlee/lingfeat</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/onestopenglish">OneStopEnglish</pwcdataset>
    </paper>
    <paper id="835">
      <title>Types of Out-of-Distribution Texts and How to Detect Them</title>
      <author><first>Udit</first><last>Arora</last></author>
      <author><first>William</first><last>Huang</last></author>
      <author><first>He</first><last>He</last></author>
      <pages>10687–10701</pages>
      <abstract>Despite agreement on the importance of detecting out-of-distribution (OOD) examples, there is little consensus on the formal definition of the distribution shifts of OOD examples and how to best detect them. We categorize these examples as exhibiting a background shift or semantic shift, and find that the two major approaches to OOD detection, calibration and density estimation (language modeling for text), have distinct behavior on these types of OOD data. Across 14 pairs of in-distribution and OOD English natural language understanding datasets, we find that density estimation methods consistently beat calibration methods in background shift settings and perform worse in semantic shift settings. In addition, we find that both methods generally fail to detect examples from challenge data, indicating that these examples constitute a different type of OOD data. Overall, while the categorization we apply explains many of the differences between the two methods, our results call for a more explicit definition of OOD to create better benchmarks and build detectors that can target the type of OOD data expected at test time.</abstract>
      <url hash="098c434c">2021.emnlp-main.835</url>
      <bibkey>arora-etal-2021-types</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.835</doi>
      <video href="2021.emnlp-main.835.mp4"/>
      <pwccode url="https://github.com/uditarora/ood-text-emnlp" additional="false">uditarora/ood-text-emnlp</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/dbpedia">DBpedia</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rte">RTE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/snli">SNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/sst">SST</pwcdataset>
    </paper>
    <paper id="836">
      <title>Self-training with Few-shot Rationalization</title>
      <author><first>Meghana Moorthy</first><last>Bhat</last></author>
      <author><first>Alessandro</first><last>Sordoni</last></author>
      <author><first>Subhabrata</first><last>Mukherjee</last></author>
      <pages>10702–10712</pages>
      <abstract>While pre-trained language models have obtained state-of-the-art performance for several natural language understanding tasks, they are quite opaque in terms of their decision-making process. While some recent works focus on rationalizing neural predictions by highlighting salient concepts in the text as justifications or rationales, they rely on thousands of labeled training examples for both task labels as well as annotated rationales for every instance. Such extensive large-scale annotations are infeasible to obtain for many tasks. To this end, we develop a multi-task teacher-student framework based on self-training pre-trained language models with limited task-specific labels and rationales and judicious sample selection to learn from informative pseudo-labeled examples. We study several characteristics of what constitutes a good rationale and demonstrate that the neural model performance can be significantly improved by making it aware of its rationalized predictions, particularly in low-resource settings. Extensive experiments in several benchmark datasets demonstrate the effectiveness of our approach.</abstract>
      <url hash="4c038751">2021.emnlp-main.836</url>
      <bibkey>bhat-etal-2021-self</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.836</doi>
      <video href="2021.emnlp-main.836.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/boolq">BoolQ</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/fever">FEVER</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/e-snli">e-SNLI</pwcdataset>
    </paper>
    <paper id="837">
      <title><fixed-case>MTA</fixed-case>dam: Automatic Balancing of Multiple Training Loss Terms</title>
      <author><first>Itzik</first><last>Malkiel</last></author>
      <author><first>Lior</first><last>Wolf</last></author>
      <pages>10713–10729</pages>
      <abstract>When training neural models, it is common to combine multiple loss terms. The balancing of these terms requires considerable human effort and is computationally demanding. Moreover, the optimal trade-off between the loss terms can change as training progresses, e.g., for adversarial terms. In this work, we generalize the Adam optimization algorithm to handle multiple loss terms. The guiding principle is that for every layer, the gradient magnitude of the terms should be balanced. To this end, the Multi-Term Adam (MTAdam) computes the derivative of each loss term separately, infers the first and second moments per parameter and loss term, and calculates a first moment for the magnitude per layer of the gradients arising from each loss. This magnitude is used to continuously balance the gradients across all layers, in a manner that both varies from one layer to the next and dynamically changes over time. Our results show that training with the new method leads to fast recovery from suboptimal initial loss weighting and to training outcomes that match or improve conventional training with the prescribed hyperparameters of each method.</abstract>
      <url hash="1b91b669">2021.emnlp-main.837</url>
      <bibkey>malkiel-wolf-2021-mtadam</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.837</doi>
      <video href="2021.emnlp-main.837.mp4"/>
      <pwccode url="https://github.com/ItzikMalkiel/MTAdam" additional="false">ItzikMalkiel/MTAdam</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/bsd">BSD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/set14">Set14</pwcdataset>
    </paper>
    <paper id="838">
      <title>Softmax Tree: An Accurate, Fast Classifier When the Number of Classes Is Large</title>
      <author><first>Arman</first><last>Zharmagambetov</last></author>
      <author><first>Magzhan</first><last>Gabidolla</last></author>
      <author><first>Miguel A.</first><last>Carreira-Perpinan</last></author>
      <pages>10730–10745</pages>
      <abstract>Classification problems having thousands or more classes naturally occur in NLP, for example language models or document classification. A softmax or one-vs-all classifier naturally handles many classes, but it is very slow at inference time, because every class score must be calculated to find the top class. We propose the “softmax tree”, consisting of a binary tree having sparse hyperplanes at the decision nodes (which make hard, not soft, decisions) and small softmax classifiers at the leaves. This is much faster at inference because the input instance follows a single path to a leaf (whose length is logarithmic on the number of leaves) and the softmax classifier at each leaf operates on a small subset of the classes. Although learning accurate tree-based models has proven difficult in the past, we are able to overcome this by using a variation of a recent algorithm, tree alternating optimization (TAO). Compared to a softmax and other classifiers, the resulting softmax trees are both more accurate in prediction and faster in inference, as shown in NLP problems having from one thousand to one hundred thousand classes.</abstract>
      <url hash="479737b0">2021.emnlp-main.838</url>
      <bibkey>zharmagambetov-etal-2021-softmax</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.838</doi>
      <video href="2021.emnlp-main.838.mp4"/>
    </paper>
    <paper id="839">
      <title>Improving Distantly-Supervised Named Entity Recognition with Self-Collaborative Denoising Learning</title>
      <author><first>Xinghua</first><last>Zhang</last></author>
      <author><first>Bowen</first><last>Yu</last></author>
      <author><first>Tingwen</first><last>Liu</last></author>
      <author><first>Zhenyu</first><last>Zhang</last></author>
      <author><first>Jiawei</first><last>Sheng</last></author>
      <author><first>Xue</first><last>Mengge</last></author>
      <author><first>Hongbo</first><last>Xu</last></author>
      <pages>10746–10757</pages>
      <abstract>Distantly supervised named entity recognition (DS-NER) efficiently reduces labor costs but meanwhile intrinsically suffers from the label noise due to the strong assumption of distant supervision. Typically, the wrongly labeled instances comprise numbers of incomplete and inaccurate annotations, while most prior denoising works are only concerned with one kind of noise and fail to fully explore useful information in the training set. To address this issue, we propose a robust learning paradigm named Self-Collaborative Denoising Learning (SCDL), which jointly trains two teacher-student networks in a mutually-beneficial manner to iteratively perform noisy label refinery. Each network is designed to exploit reliable labels via self denoising, and two networks communicate with each other to explore unreliable annotations by collaborative denoising. Extensive experimental results on five real-world datasets demonstrate that SCDL is superior to state-of-the-art DS-NER denoising methods.</abstract>
      <url hash="808956ac">2021.emnlp-main.839</url>
      <bibkey>zhang-etal-2021-improving-distantly-supervised</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.839</doi>
      <pwccode url="https://github.com/airobotzhang/scdl" additional="false">airobotzhang/scdl</pwccode>
    </paper>
    <paper id="840">
      <title>Multivalent Entailment Graphs for Question Answering</title>
      <author><first>Nick</first><last>McKenna</last></author>
      <author><first>Liane</first><last>Guillou</last></author>
      <author><first>Mohammad Javad</first><last>Hosseini</last></author>
      <author><first>Sander</first><last>Bijl de Vroe</last></author>
      <author><first>Mark</first><last>Johnson</last></author>
      <author><first>Mark</first><last>Steedman</last></author>
      <pages>10758–10768</pages>
      <abstract>Drawing inferences between open-domain natural language predicates is a necessity for true language understanding. There has been much progress in unsupervised learning of entailment graphs for this purpose. We make three contributions: (1) we reinterpret the Distributional Inclusion Hypothesis to model entailment between predicates of different valencies, like DEFEAT(Biden, Trump) entails WIN(Biden); (2) we actualize this theory by learning unsupervised Multivalent Entailment Graphs of open-domain predicates; and (3) we demonstrate the capabilities of these graphs on a novel question answering task. We show that directional entailment is more helpful for inference than non-directional similarity on questions of fine-grained semantics. We also show that drawing on evidence across valencies answers more questions than by using only the same valency evidence.</abstract>
      <url hash="15f92c08">2021.emnlp-main.840</url>
      <bibkey>mckenna-etal-2021-multivalent</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.840</doi>
      <video href="2021.emnlp-main.840.mp4"/>
    </paper>
    <paper id="841">
      <title>Is Everything in Order? A Simple Way to Order Sentences</title>
      <author><first>Somnath</first><last>Basu Roy Chowdhury</last></author>
      <author><first>Faeze</first><last>Brahman</last></author>
      <author><first>Snigdha</first><last>Chaturvedi</last></author>
      <pages>10769–10779</pages>
      <abstract>The task of organizing a shuffled set of sentences into a coherent text has been used to evaluate a machine’s understanding of causal and temporal relations. We formulate the sentence ordering task as a conditional text-to-marker generation problem. We present Reorder-BART (Re-BART) that leverages a pre-trained Transformer-based model to identify a coherent order for a given set of shuffled sentences. The model takes a set of shuffled sentences with sentence-specific markers as input and generates a sequence of position markers of the sentences in the ordered text. Re-BART achieves the state-of-the-art performance across 7 datasets in Perfect Match Ratio (PMR) and Kendall’s tau. We perform evaluations in a zero-shot setting, showcasing that our model is able to generalize well across other datasets. We additionally perform several experiments to understand the functioning and limitations of our framework.</abstract>
      <url hash="6336b7c2">2021.emnlp-main.841</url>
      <attachment type="Software" hash="e2591cf4">2021.emnlp-main.841.Software.zip</attachment>
      <bibkey>basu-roy-chowdhury-etal-2021-everything</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.841</doi>
      <video href="2021.emnlp-main.841.mp4"/>
      <pwccode url="https://github.com/fabrahman/rebart" additional="false">fabrahman/rebart</pwccode>
    </paper>
    <paper id="842">
      <title><fixed-case>V</fixed-case>ee<fixed-case>A</fixed-case>lign: Multifaceted Context Representation Using Dual Attention for Ontology Alignment</title>
      <author><first>Vivek</first><last>Iyer</last></author>
      <author><first>Arvind</first><last>Agarwal</last></author>
      <author id="harshit-kumar"><first>Harshit</first><last>Kumar</last></author>
      <pages>10780–10792</pages>
      <abstract>Ontology Alignment is an important research problem applied to various fields such as data integration, data transfer, data preparation, etc. State-of-the-art (SOTA) Ontology Alignment systems typically use naive domain-dependent approaches with handcrafted rules or domain-specific architectures, making them unscalable and inefficient. In this work, we propose VeeAlign, a Deep Learning based model that uses a novel dual-attention mechanism to compute the contextualized representation of a concept which, in turn, is used to discover alignments. By doing this, not only is our approach able to exploit both syntactic and semantic information encoded in ontologies, it is also, by design, flexible and scalable to different domains with minimal effort. We evaluate our model on four different datasets from different domains and languages, and establish its superiority through these results as well as detailed ablation studies. The code and datasets used are available at https://github.com/Remorax/VeeAlign.</abstract>
      <url hash="592d6a76">2021.emnlp-main.842</url>
      <attachment type="Software" hash="cb52034e">2021.emnlp-main.842.Software.zip</attachment>
      <bibkey>iyer-etal-2021-veealign</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.842</doi>
      <video href="2021.emnlp-main.842.mp4"/>
      <pwccode url="https://github.com/remorax/veealign" additional="false">remorax/veealign</pwccode>
    </paper>
    <paper id="843">
      <title>Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization</title>
      <author><first>Inbar</first><last>Oren</last></author>
      <author><first>Jonathan</first><last>Herzig</last></author>
      <author><first>Jonathan</first><last>Berant</last></author>
      <pages>10793–10809</pages>
      <abstract>Modern semantic parsers suffer from two principal limitations. First, training requires expensive collection of utterance-program pairs. Second, semantic parsers fail to generalize at test time to new compositions/structures that have not been observed during training. Recent research has shown that automatic generation of synthetic utterance-program pairs can alleviate the first problem, but its potential for the second has thus far been under-explored. In this work, we investigate automatic generation of synthetic utterance-program pairs for improving compositional generalization in semantic parsing. Given a small training set of annotated examples and an “infinite” pool of synthetic examples, we select a subset of synthetic examples that are structurally-diverse and use them to improve compositional generalization. We evaluate our approach on a new split of the schema2QA dataset, and show that it leads to dramatic improvements in compositional generalization as well as moderate improvements in the traditional i.i.d setup. Moreover, structurally-diverse sampling achieves these improvements with as few as 5K examples, compared to 1M examples when sampling uniformly at random – a 200x improvement in data efficiency.</abstract>
      <url hash="a0fa7271">2021.emnlp-main.843</url>
      <bibkey>oren-etal-2021-finding</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.843</doi>
      <video href="2021.emnlp-main.843.mp4"/>
      <pwccode url="https://github.com/inbaroren/scfg-sampling-for-comp-gen" additional="false">inbaroren/scfg-sampling-for-comp-gen</pwccode>
    </paper>
    <paper id="844">
      <title><fixed-case>G</fixed-case>ene<fixed-case>S</fixed-case>is: <fixed-case>A</fixed-case> <fixed-case>G</fixed-case>enerative <fixed-case>A</fixed-case>pproach to <fixed-case>S</fixed-case>ubstitutes in <fixed-case>C</fixed-case>ontext</title>
      <author><first>Caterina</first><last>Lacerra</last></author>
      <author><first>Rocco</first><last>Tripodi</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>10810–10823</pages>
      <abstract>The lexical substitution task aims at generating a list of suitable replacements for a target word in context, ideally keeping the meaning of the modified text unchanged. While its usage has increased in recent years, the paucity of annotated data prevents the finetuning of neural models on the task, hindering the full fruition of recently introduced powerful architectures such as language models. Furthermore, lexical substitution is usually evaluated in a framework that is strictly bound to a limited vocabulary, making it impossible to credit appropriate, but out-of-vocabulary, substitutes. To assess these issues, we proposed GeneSis (Generating Substitutes in contexts), the first generative approach to lexical substitution. Thanks to a seq2seq model, we generate substitutes for a word according to the context it appears in, attaining state-of-the-art results on different benchmarks. Moreover, our approach allows silver data to be produced for further improving the performances of lexical substitution systems. Along with an extensive analysis of GeneSis results, we also present a human evaluation of the generated substitutes in order to assess their quality. We release the fine-tuned models, the generated datasets, and the code to reproduce the experiments at https://github.com/SapienzaNLP/genesis.</abstract>
      <url hash="f3fb91e0">2021.emnlp-main.844</url>
      <bibkey>lacerra-etal-2021-genesis</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.844</doi>
      <video href="2021.emnlp-main.844.mp4"/>
      <pwccode url="https://github.com/sapienzanlp/genesis" additional="false">sapienzanlp/genesis</pwccode>
    </paper>
    <paper id="845">
      <title>Semi-Supervised Exaggeration Detection of Health Science Press Releases</title>
      <author><first>Dustin</first><last>Wright</last></author>
      <author><first>Isabelle</first><last>Augenstein</last></author>
      <pages>10824–10836</pages>
      <abstract>Public trust in science depends on honest and factual communication of scientific papers. However, recent studies have demonstrated a tendency of news media to misrepresent scientific papers by exaggerating their findings. Given this, we present a formalization of and study into the problem of exaggeration detection in science communication. While there are an abundance of scientific papers and popular media articles written about them, very rarely do the articles include a direct link to the original paper, making data collection challenging, and necessitating the need for few-shot learning. We address this by curating a set of labeled press release/abstract pairs from existing expert annotated studies on exaggeration in press releases of scientific papers suitable for benchmarking the performance of machine learning models on the task. Using limited data from this and previous studies on exaggeration detection in science, we introduce MT-PET, a multi-task version of Pattern Exploiting Training (PET), which leverages knowledge from complementary cloze-style QA tasks to improve few-shot learning. We demonstrate that MT-PET outperforms PET and supervised learning both when data is limited, as well as when there is an abundance of data for the main task.</abstract>
      <url hash="7e0fa396">2021.emnlp-main.845</url>
      <bibkey>wright-augenstein-2021-semi</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.845</doi>
      <video href="2021.emnlp-main.845.mp4"/>
      <pwccode url="https://github.com/copenlu/scientific-exaggeration-detection" additional="false">copenlu/scientific-exaggeration-detection</pwccode>
    </paper>
    <paper id="846">
      <title>Phrase-<fixed-case>BERT</fixed-case>: Improved Phrase Embeddings from <fixed-case>BERT</fixed-case> with an Application to Corpus Exploration</title>
      <author><first>Shufan</first><last>Wang</last></author>
      <author><first>Laure</first><last>Thompson</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <pages>10837–10851</pages>
      <abstract>Phrase representations derived from BERT often do not exhibit complex phrasal compositionality, as the model relies instead on lexical similarity to determine semantic relatedness. In this paper, we propose a contrastive fine-tuning objective that enables BERT to produce more powerful phrase embeddings. Our approach (Phrase-BERT) relies on a dataset of diverse phrasal paraphrases, which is automatically generated using a paraphrase generation model, as well as a large-scale dataset of phrases in context mined from the Books3 corpus. Phrase-BERT outperforms baselines across a variety of phrase-level similarity tasks, while also demonstrating increased lexical diversity between nearest neighbors in the vector space. Finally, as a case study, we show that Phrase-BERT embeddings can be easily integrated with a simple autoencoder to build a phrase-based neural topic model that interprets topics as mixtures of words and phrases by performing a nearest neighbor search in the embedding space. Crowdsourced evaluations demonstrate that this phrase-based topic model produces more coherent and meaningful topics than baseline word and phrase-level topic models, further validating the utility of Phrase-BERT.</abstract>
      <url hash="cbf3d88a">2021.emnlp-main.846</url>
      <bibkey>wang-etal-2021-phrase</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.846</doi>
      <video href="2021.emnlp-main.846.mp4"/>
      <pwccode url="https://github.com/sf-wa-326/phrase-bert-topic-model" additional="false">sf-wa-326/phrase-bert-topic-model</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/big-bird">BiRD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/paws">PAWS</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/the-pile">The Pile</pwcdataset>
    </paper>
    <paper id="847">
      <title>Detecting Contact-Induced Semantic Shifts: <fixed-case>W</fixed-case>hat Can Embedding-Based Methods Do in Practice?</title>
      <author><first>Filip</first><last>Miletic</last></author>
      <author><first>Anne</first><last>Przewozny-Desriaux</last></author>
      <author><first>Ludovic</first><last>Tanguy</last></author>
      <pages>10852–10865</pages>
      <abstract>This study investigates the applicability of semantic change detection methods in descriptively oriented linguistic research. It specifically focuses on contact-induced semantic shifts in Quebec English. We contrast synchronic data from different regions in order to identify the meanings that are specific to Quebec and potentially related to language contact. Type-level embeddings are used to detect new semantic shifts, and token-level embeddings to isolate regionally specific occurrences. We introduce a new 80-item test set and conduct both quantitative and qualitative evaluations. We demonstrate that diachronic word embedding methods can be applied to contact-induced semantic shifts observed in synchrony, obtaining results comparable to the state of the art on similar tasks in diachrony. However, we show that encouraging evaluation results do not translate to practical value in detecting new semantic shifts. Finally, our application of token-level embeddings accelerates manual data exploration and provides an efficient way of scaling up sociolinguistic analyses.</abstract>
      <url hash="336fccdb">2021.emnlp-main.847</url>
      <bibkey>miletic-etal-2021-detecting</bibkey>
      <doi>10.18653/v1/2021.emnlp-main.847</doi>
      <video href="2021.emnlp-main.847.mp4"/>
    </paper>
  </volume>
  <volume id="demo" ingest-date="2021-11-01">
    <meta>
      <booktitle>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</booktitle>
      <editor><first>Heike</first><last>Adel</last></editor>
      <editor><first>Shuming</first><last>Shi</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online and Punta Cana, Dominican Republic</address>
      <month>November</month>
      <year>2021</year>
      <url hash="cb091bdb">2021.emnlp-demo</url>
      <venue>emnlp</venue>
    </meta>
    <frontmatter>
      <url hash="724a2c21">2021.emnlp-demo.0</url>
      <bibkey>emnlp-2021-demo</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>M</fixed-case>i<fixed-case>SS</fixed-case>: An Assistant for Multi-Style Simultaneous Translation</title>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Kevin</first><last>Parnow</last></author>
      <author><first>Masao</first><last>Utiyama</last></author>
      <author><first>Eiichiro</first><last>Sumita</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <pages>1–10</pages>
      <abstract>In this paper, we present <b>MiSS</b>, an assistant for multi-style simultaneous translation. Our proposed translation system has five key features: highly accurate translation, simultaneous translation, translation for multiple text styles, back-translation for translation quality evaluation, and grammatical error correction. With this system, we aim to provide a complete translation experience for machine translation users. Our design goals are high translation accuracy, real-time translation, flexibility, and measurable translation quality. Compared with the free commercial translation systems commonly used, our translation assistance system regards the machine translation application as a more complete and fully-featured tool for users. By incorporating additional features and giving the user better control over their experience, we improve translation efficiency and performance. Additionally, our assistant system combines machine translation, grammatical error correction, and interactive edits, and uses a crowdsourcing mode to collect more data for further training to improve both the machine translation and grammatical error correction models. A short video demonstrating our system is available at <url>https://www.youtube.com/watch?v=ZGCo7KtRKd8</url>.</abstract>
      <url hash="17e47fa3">2021.emnlp-demo.1</url>
      <bibkey>li-etal-2021-miss</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.1</doi>
      <video href="2021.emnlp-demo.1.mp4"/>
    </paper>
    <paper id="2">
      <title>Automatic Construction of Enterprise Knowledge Base</title>
      <author><first>Junyi</first><last>Chai</last></author>
      <author><first>Yujie</first><last>He</last></author>
      <author><first>Homa</first><last>Hashemi</last></author>
      <author><first>Bing</first><last>Li</last></author>
      <author><first>Daraksha</first><last>Parveen</last></author>
      <author><first>Ranganath</first><last>Kondapally</last></author>
      <author><first>Wenjin</first><last>Xu</last></author>
      <pages>11–19</pages>
      <abstract>In this paper, we present an automatic knowledge base construction system from large scale enterprise documents with minimal efforts of human intervention. In the design and deployment of such a knowledge mining system for enterprise, we faced several challenges including data distributional shift, performance evaluation, compliance requirements and other practical issues. We leveraged state-of-the-art deep learning models to extract information (named entities and definitions) at per document level, then further applied classical machine learning techniques to process global statistical information to improve the knowledge base. Experimental results are reported on actual enterprise documents. This system is currently serving as part of a Microsoft 365 service.</abstract>
      <url hash="6508e321">2021.emnlp-demo.2</url>
      <bibkey>chai-etal-2021-automatic</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.2</doi>
    </paper>
    <paper id="3">
      <title><fixed-case>L</fixed-case>ight<fixed-case>T</fixed-case>ag: Text Annotation Platform</title>
      <author><first>Tal</first><last>Perry</last></author>
      <pages>20–27</pages>
      <abstract>Text annotation tools assume that their user’s goal is to create a labeled corpus. However,users view annotation as a necessary evil on the way to deliver business value through NLP.Thus an annotation tool should optimize for the throughput of the global NLP process, not only the productivity of individual annotators. LightTag is a text annotation tool designed and built on that principle. This paper shares our design rationale, data modeling choices, and user interface decisions then illustrates how those choices serve the full NLP lifecycle.</abstract>
      <url hash="465d459b">2021.emnlp-demo.3</url>
      <bibkey>perry-2021-lighttag</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.3</doi>
      <video href="2021.emnlp-demo.3.mp4"/>
    </paper>
    <paper id="4">
      <title><fixed-case>T</fixed-case>rans<fixed-case>I</fixed-case>ns: Document Translation with Markup Reinsertion</title>
      <author><first>Jörg</first><last>Steffen</last></author>
      <author><first>Josef</first><last>van Genabith</last></author>
      <pages>28–34</pages>
      <abstract>For many use cases, it is required that MT does not just translate raw text, but complex formatted documents (e.g. websites, slides, spreadsheets) and the result of the translation should reflect the formatting. This is challenging, as markup can be nested, apply to spans contiguous in source but non-contiguous in target etc. Here we present TransIns, a system for non-plain text document translation that builds on the Okapi framework and MT models trained with Marian NMT. We develop, implement and evaluate different strategies for reinserting markup into translated sentences using token alignments between source and target sentences. We propose a simple and effective strategy that compiles down all markup to single source tokens and transfers them to aligned target tokens. A first evaluation shows that this strategy yields highly accurate markup in the translated documents that outperforms the markup quality found in documents translated with popular translation services. We release TransIns under the MIT License as open-source software on https://github.com/DFKI-MLT/TransIns. An online demonstrator is available at https://transins.dfki.de.</abstract>
      <url hash="e85bc09f">2021.emnlp-demo.4</url>
      <bibkey>steffen-van-genabith-2021-transins</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.4</doi>
      <video href="2021.emnlp-demo.4.mp4"/>
      <pwccode url="https://github.com/dfki-mlt/transins" additional="false">dfki-mlt/transins</pwccode>
    </paper>
    <paper id="5">
      <title><fixed-case>ET</fixed-case>: A Workstation for Querying, Editing and Evaluating Annotated Corpora</title>
      <author><first>Elvis</first><last>de Souza</last></author>
      <author><first>Cláudia</first><last>Freitas</last></author>
      <pages>35–41</pages>
      <abstract>In this paper we explore the functionalities of ET, a suite designed to support linguistic research and natural language processing tasks using corpora annotated in the CoNLL-U format. These goals are achieved by two integrated environments – Interrogatório, an environment for querying and editing annotated corpora, and Julgamento, an environment for assessing their quality. ET is open-source, built on different Python Web technologies and has Web demonstrations available on-line. ET has been intensively used in our research group for over two years, being the chosen framework for several linguistic and NLP-related studies conducted by its researchers.</abstract>
      <url hash="07ec1285">2021.emnlp-demo.5</url>
      <bibkey>de-souza-freitas-2021-et</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.5</doi>
      <video href="2021.emnlp-demo.5.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="6">
      <title>N-<fixed-case>LTP</fixed-case>: An Open-source Neural Language Technology Platform for <fixed-case>C</fixed-case>hinese</title>
      <author><first>Wanxiang</first><last>Che</last></author>
      <author><first>Yunlong</first><last>Feng</last></author>
      <author><first>Libo</first><last>Qin</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <pages>42–49</pages>
      <abstract>We introduce N-LTP, an open-source neural language technology platform supporting six fundamental Chinese NLP tasks: lexical analysis (Chinese word segmentation, part-of-speech tagging, and named entity recognition), syntactic parsing (dependency parsing), and semantic parsing (semantic dependency parsing and semantic role labeling). Unlike the existing state-of-the-art toolkits, such as Stanza, that adopt an independent model for each task, N-LTP adopts the multi-task framework by using a shared pre-trained model, which has the advantage of capturing the shared knowledge across relevant Chinese tasks. In addition, a knowledge distillation method (Clark et al., 2019) where the single-task model teaches the multi-task model is further introduced to encourage the multi-task model to surpass its single-task teacher. Finally, we provide a collection of easy-to-use APIs and a visualization tool to make users to use and view the processing results more easily and directly. To the best of our knowledge, this is the first toolkit to support six Chinese NLP fundamental tasks. Source code, documentation, and pre-trained models are available at https://github.com/HIT-SCIR/ltp.</abstract>
      <url hash="76117ec7">2021.emnlp-demo.6</url>
      <bibkey>che-etal-2021-n</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.6</doi>
      <video href="2021.emnlp-demo.6.mp4"/>
      <pwccode url="https://github.com/HIT-SCIR/ltp" additional="false">HIT-SCIR/ltp</pwccode>
    </paper>
    <paper id="7">
      <title><fixed-case>COMBO</fixed-case>: State-of-the-Art Morphosyntactic Analysis</title>
      <author><first>Mateusz</first><last>Klimaszewski</last></author>
      <author><first>Alina</first><last>Wróblewska</last></author>
      <pages>50–62</pages>
      <abstract>We introduce COMBO – a fully neural NLP system for accurate part-of-speech tagging, morphological analysis, lemmatisation, and (enhanced) dependency parsing. It predicts categorical morphosyntactic features whilst also exposes their vector representations, extracted from hidden layers. COMBO is an easy to install Python package with automatically downloadable pre-trained models for over 40 languages. It maintains a balance between efficiency and quality. As it is an end-to-end system and its modules are jointly trained, its training is competitively fast. As its models are optimised for accuracy, they achieve often better prediction quality than SOTA. The COMBO library is available at: https://gitlab.clarin-pl.eu/syntactic-tools/combo.</abstract>
      <url hash="689b8b62">2021.emnlp-demo.7</url>
      <bibkey>klimaszewski-wroblewska-2021-combo-state</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.7</doi>
      <video href="2021.emnlp-demo.7.mp4"/>
    </paper>
    <paper id="8">
      <title><fixed-case>E</fixed-case>xcavator<fixed-case>C</fixed-case>ovid: Extracting Events and Relations from Text Corpora for Temporal and Causal Analysis for <fixed-case>COVID</fixed-case>-19</title>
      <author><first>Bonan</first><last>Min</last></author>
      <author><first>Benjamin</first><last>Rozonoyer</last></author>
      <author><first>Haoling</first><last>Qiu</last></author>
      <author><first>Alexander</first><last>Zamanian</last></author>
      <author><first>Nianwen</first><last>Xue</last></author>
      <author><first>Jessica</first><last>MacBride</last></author>
      <pages>63–71</pages>
      <abstract>Timely responses from policy makers to mitigate the impact of the COVID-19 pandemic rely on a comprehensive grasp of events, their causes, and their impacts. These events are reported at such a speed and scale as to be overwhelming. In this paper, we present ExcavatorCovid, a machine reading system that ingests open-source text documents (e.g., news and scientific publications), extracts COVID-19 related events and relations between them, and builds a Temporal and Causal Analysis Graph (TCAG). Excavator will help government agencies alleviate the information overload, understand likely downstream effects of political and economic decisions and events related to the pandemic, and respond in a timely manner to mitigate the impact of COVID-19. We expect the utility of Excavator to outlive the COVID-19 pandemic: analysts and decision makers will be empowered by Excavator to better understand and solve complex problems in the future. A demonstration video is available at https://vimeo.com/528619007.</abstract>
      <url hash="5a8bb868">2021.emnlp-demo.8</url>
      <bibkey>min-etal-2021-excavatorcovid</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.8</doi>
      <video href="2021.emnlp-demo.8.mp4"/>
    </paper>
    <paper id="9">
      <title><fixed-case>KOAS</fixed-case>: <fixed-case>K</fixed-case>orean Text Offensiveness Analysis System</title>
      <author><first>San-Hee</first><last>Park</last></author>
      <author><first>Kang-Min</first><last>Kim</last></author>
      <author><first>Seonhee</first><last>Cho</last></author>
      <author><first>Jun-Hyung</first><last>Park</last></author>
      <author><first>Hyuntae</first><last>Park</last></author>
      <author><first>Hyuna</first><last>Kim</last></author>
      <author><first>Seongwon</first><last>Chung</last></author>
      <author><first>SangKeun</first><last>Lee</last></author>
      <pages>72–78</pages>
      <abstract>Warning: This manuscript contains a certain level of offensive expression. As communication through social media platforms has grown immensely, the increasing prevalence of offensive language online has become a critical problem. Notably in Korea, one of the countries with the highest Internet usage, automatic detection of offensive expressions has recently been brought to attention. However, morphological richness and complex syntax of Korean causes difficulties in neural model training. Furthermore, most of previous studies mainly focus on the detection of abusive language, disregarding implicit offensiveness and underestimating a different degree of intensity. To tackle these problems, we present KOAS, a system that fully exploits both contextual and linguistic features and estimates an offensiveness score for a text. We carefully designed KOAS with a multi-task learning framework and constructed a Korean dataset for offensive analysis from various domains. Refer for a detailed demonstration.</abstract>
      <url hash="540d419f">2021.emnlp-demo.9</url>
      <bibkey>park-etal-2021-koas</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.9</doi>
      <video href="2021.emnlp-demo.9.mp4"/>
    </paper>
    <paper id="10">
      <title><fixed-case>R</fixed-case>ep<fixed-case>G</fixed-case>raph: Visualising and Analysing Meaning Representation Graphs</title>
      <author><first>Jaron</first><last>Cohen</last></author>
      <author><first>Roy</first><last>Cohen</last></author>
      <author><first>Edan</first><last>Toledo</last></author>
      <author><first>Jan</first><last>Buys</last></author>
      <pages>79–86</pages>
      <abstract>We present RepGraph, an open source visualisation and analysis tool for meaning representation graphs. Graph-based meaning representations provide rich semantic annotations, but visualising them clearly is more challenging than for fully lexicalized representations. Our application provides a seamless, unifying interface with which to visualise, manipulate and analyse semantically parsed graph data represented in a JSON-based serialisation format. RepGraph visualises graphs in multiple formats, with an emphasis on showing the relation between nodes and their corresponding token spans, whilst keeping the representation compact. Additionally, the web-based tool provides NLP researchers with a clear, visually intuitive way of interacting with these graphs, and includes a number of graph analysis features. The tool currently supports the DMRS, EDS, PTG, UCCA, and AMR semantic frameworks. A live demo is available at https://repgraph.vercel.app/.</abstract>
      <url hash="12007af4">2021.emnlp-demo.10</url>
      <bibkey>cohen-etal-2021-repgraph</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.10</doi>
      <video href="2021.emnlp-demo.10.mp4"/>
    </paper>
    <paper id="11">
      <title>Thermostat: A Large Collection of <fixed-case>NLP</fixed-case> Model Explanations and Analysis Tools</title>
      <author><first>Nils</first><last>Feldhus</last></author>
      <author><first>Robert</first><last>Schwarzenberg</last></author>
      <author><first>Sebastian</first><last>Möller</last></author>
      <pages>87–95</pages>
      <abstract>In the language domain, as in other domains, neural explainability takes an ever more important role, with feature attribution methods on the forefront. Many such methods require considerable computational resources and expert knowledge about implementation details and parameter choices. To facilitate research, we present Thermostat which consists of a large collection of model explanations and accompanying analysis tools. Thermostat allows easy access to over 200k explanations for the decisions of prominent state-of-the-art models spanning across different NLP tasks, generated with multiple explainers. The dataset took over 10k GPU hours (&gt; one year) to compile; compute time that the community now saves. The accompanying software tools allow to analyse explanations instance-wise but also accumulatively on corpus level. Users can investigate and compare models, datasets and explainers without the need to orchestrate implementation details. Thermostat is fully open source, democratizes explainability research in the language domain, circumvents redundant computations and increases comparability and replicability.</abstract>
      <url hash="fc87dae8">2021.emnlp-demo.11</url>
      <attachment type="Software" hash="1983c5a4">2021.emnlp-demo.11.Software.zip</attachment>
      <bibkey>feldhus-etal-2021-thermostat</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.11</doi>
      <video href="2021.emnlp-demo.11.mp4"/>
      <pwccode url="https://github.com/dfki-nlp/thermostat" additional="true">dfki-nlp/thermostat</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ag-news">AG News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/imdb-movie-reviews">IMDb Movie Reviews</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/xnli">XNLI</pwcdataset>
    </paper>
    <paper id="12">
      <title><fixed-case>LM</fixed-case>diff: A Visual Diff Tool to Compare Language Models</title>
      <author><first>Hendrik</first><last>Strobelt</last></author>
      <author><first>Benjamin</first><last>Hoover</last></author>
      <author><first>Arvind</first><last>Satyanaryan</last></author>
      <author><first>Sebastian</first><last>Gehrmann</last></author>
      <pages>96–105</pages>
      <abstract>While different language models are ubiquitous in NLP, it is hard to contrast their outputs and identify which contexts one can handle better than the other. To address this question, we introduce LMdiff, a tool that visually compares probability distributions of two models that differ, e.g., through finetuning, distillation, or simply training with different parameter sizes. LMdiff allows the generation of hypotheses about model behavior by investigating text instances token by token and further assists in choosing these interesting text instances by identifying the most interesting phrases from large corpora. We showcase the applicability of LMdiff for hypothesis generation across multiple case studies. A demo is available at http://lmdiff.net .</abstract>
      <url hash="2bac39a1">2021.emnlp-demo.12</url>
      <bibkey>strobelt-etal-2021-lmdiff</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.12</doi>
      <video href="2021.emnlp-demo.12.mp4"/>
      <pwccode url="https://github.com/hendrikstrobelt/lmdiff" additional="false">hendrikstrobelt/lmdiff</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/commonsenseqa">CommonsenseQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/winobias">WinoBias</pwcdataset>
    </paper>
    <paper id="13">
      <title>Semantic Context Path Labeling for Semantic Exploration of User Reviews</title>
      <author><first>Salah</first><last>Aït-Mokhtar</last></author>
      <author><first>Caroline</first><last>Brun</last></author>
      <author><first>Yves</first><last>Hoppenot</last></author>
      <author><first>Agnes</first><last>Sandor</last></author>
      <pages>106–113</pages>
      <abstract>In this paper we present a prototype demonstrator showcasing a novel method to perform semantic exploration of user reviews. The system enables effective navigation in a rich contextual semantic schema with a large number of structured classes indicating relevant information. In order to identify instances of the structured classes in the reviews, we defined a new Information Extraction task called Semantic Context Path (SCP) labeling, which simultaneously assigns types and semantic roles to entity mentions. Reviews can rapidly be explored based on the fine-grained and structured semantic classes. As a proof-of-concept, we have implemented this system for reviews on Points-of-Interest, in English and Korean.</abstract>
      <url hash="6329344c">2021.emnlp-demo.13</url>
      <bibkey>ait-mokhtar-etal-2021-semantic</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.13</doi>
      <video href="2021.emnlp-demo.13.mp4"/>
    </paper>
    <paper id="14">
      <title>Beyond Accuracy: A Consolidated Tool for Visual Question Answering Benchmarking</title>
      <author><first>Dirk</first><last>Väth</last></author>
      <author><first>Pascal</first><last>Tilli</last></author>
      <author><first>Ngoc Thang</first><last>Vu</last></author>
      <pages>114–123</pages>
      <abstract>On the way towards general Visual Question Answering (VQA) systems that are able to answer arbitrary questions, the need arises for evaluation beyond single-metric leaderboards for specific datasets. To this end, we propose a browser-based benchmarking tool for researchers and challenge organizers, with an API for easy integration of new models and datasets to keep up with the fast-changing landscape of VQA. Our tool helps test generalization capabilities of models across multiple datasets, evaluating not just accuracy, but also performance in more realistic real-world scenarios such as robustness to input noise. Additionally, we include metrics that measure biases and uncertainty, to further explain model behavior. Interactive filtering facilitates discovery of problematic behavior, down to the data sample level. As proof of concept, we perform a case study on four models. We find that state-of-the-art VQA models are optimized for specific tasks or datasets, but fail to generalize even to other in-domain test sets, for example they can not recognize text in images. Our metrics allow us to quantify which image and question embeddings provide most robustness to a model. All code s publicly available.</abstract>
      <url hash="d19ba3fb">2021.emnlp-demo.14</url>
      <bibkey>vath-etal-2021-beyond</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.14</doi>
      <video href="2021.emnlp-demo.14.mp4"/>
      <pwccode url="https://github.com/patilli/vqa_benchmarking" additional="false">patilli/vqa_benchmarking</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/clevr">CLEVR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/gqa">GQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/ok-vqa">OK-VQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/textvqa">TextVQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/visual-question-answering">Visual Question Answering</pwcdataset>
    </paper>
    <paper id="15">
      <title>Athena 2.0: Contextualized Dialogue Management for an <fixed-case>A</fixed-case>lexa <fixed-case>P</fixed-case>rize <fixed-case>S</fixed-case>ocial<fixed-case>B</fixed-case>ot</title>
      <author><first>Juraj</first><last>Juraska</last></author>
      <author><first>Kevin</first><last>Bowden</last></author>
      <author><first>Lena</first><last>Reed</last></author>
      <author><first>Vrindavan</first><last>Harrison</last></author>
      <author><first>Wen</first><last>Cui</last></author>
      <author><first>Omkar</first><last>Patil</last></author>
      <author><first>Rishi</first><last>Rajasekaran</last></author>
      <author><first>Angela</first><last>Ramirez</last></author>
      <author><first>Cecilia</first><last>Li</last></author>
      <author><first>Eduardo</first><last>Zamora</last></author>
      <author><first>Phillip</first><last>Lee</last></author>
      <author><first>Jeshwanth</first><last>Bheemanpally</last></author>
      <author><first>Rohan</first><last>Pandey</last></author>
      <author><first>Adwait</first><last>Ratnaparkhi</last></author>
      <author><first>Marilyn</first><last>Walker</last></author>
      <pages>124–133</pages>
      <abstract>Athena 2.0 is an Alexa Prize SocialBot that has been a finalist in the last two Alexa Prize Grand Challenges. One reason for Athena’s success is its novel dialogue management strategy, which allows it to dynamically construct dialogues and responses from component modules, leading to novel conversations with every interaction. Here we describe Athena’s system design and performance in the Alexa Prize during the 20/21 competition. A live demo of Athena as well as video recordings will provoke discussion on the state of the art in conversational AI.</abstract>
      <url hash="3c5b8a47">2021.emnlp-demo.15</url>
      <bibkey>walker-etal-2021-athena</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.15</doi>
      <video href="2021.emnlp-demo.15.mp4"/>
    </paper>
    <paper id="16">
      <title><fixed-case>SPRING</fixed-case> <fixed-case>G</fixed-case>oes <fixed-case>O</fixed-case>nline: <fixed-case>E</fixed-case>nd-to-<fixed-case>E</fixed-case>nd <fixed-case>AMR</fixed-case> <fixed-case>P</fixed-case>arsing and <fixed-case>G</fixed-case>eneration</title>
      <author><first>Rexhina</first><last>Blloshmi</last></author>
      <author><first>Michele</first><last>Bevilacqua</last></author>
      <author><first>Edoardo</first><last>Fabiano</last></author>
      <author><first>Valentina</first><last>Caruso</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>134–142</pages>
      <abstract>In this paper we present SPRING Online Services, a Web interface and RESTful APIs for our state-of-the-art AMR parsing and generation system, SPRING (Symmetric PaRsIng aNd Generation). The Web interface has been developed to be easily used by the Natural Language Processing community, as well as by the general public. It provides, among other things, a highly interactive visualization platform and a feedback mechanism to obtain user suggestions for further improvements of the system’s output. Moreover, our RESTful APIs enable easy integration of SPRING in downstream applications where AMR structures are needed. Finally, we make SPRING Online Services freely available at http://nlp.uniroma1.it/spring and, in addition, we release extra model checkpoints to be used with the original SPRING Python code.</abstract>
      <url hash="086e7258">2021.emnlp-demo.16</url>
      <bibkey>blloshmi-etal-2021-spring</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.16</doi>
      <video href="2021.emnlp-demo.16.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/bio">Bio</pwcdataset>
    </paper>
    <paper id="17">
      <title>fairseq Sˆ2: A Scalable and Integrable Speech Synthesis Toolkit</title>
      <author><first>Changhan</first><last>Wang</last></author>
      <author><first>Wei-Ning</first><last>Hsu</last></author>
      <author><first>Yossi</first><last>Adi</last></author>
      <author><first>Adam</first><last>Polyak</last></author>
      <author><first>Ann</first><last>Lee</last></author>
      <author><first>Peng-Jen</first><last>Chen</last></author>
      <author><first>Jiatao</first><last>Gu</last></author>
      <author><first>Juan</first><last>Pino</last></author>
      <pages>143–152</pages>
      <abstract>This paper presents fairseq Sˆ2, a fairseq extension for speech synthesis. We implement a number of autoregressive (AR) and non-AR text-to-speech models, and their multi-speaker variants. To enable training speech synthesis models with less curated data, a number of preprocessing tools are built and their importance is shown empirically. To facilitate faster iteration of development and analysis, a suite of automatic metrics is included. Apart from the features added specifically for this extension, fairseq Sˆ2 also benefits from the scalability offered by fairseq and can be easily integrated with other state-of-the-art systems provided in this framework. The code, documentation, and pre-trained models will be made available at https://github.com/pytorch/fairseq/tree/master/examples/speech_synthesis.</abstract>
      <url hash="956fb583">2021.emnlp-demo.17</url>
      <bibkey>wang-etal-2021-fairseq</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.17</doi>
      <pwccode url="https://github.com/pytorch/fairseq" additional="false">pytorch/fairseq</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ljspeech">LJSpeech</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/vctk">VCTK</pwcdataset>
    </paper>
    <paper id="18">
      <title>Press Freedom Monitor: Detection of Reported Press and Media Freedom Violations in <fixed-case>T</fixed-case>witter and News Articles</title>
      <author><first>Tariq</first><last>Yousef</last></author>
      <author><first>Antje</first><last>Schlaf</last></author>
      <author><first>Janos</first><last>Borst</last></author>
      <author><first>Andreas</first><last>Niekler</last></author>
      <author><first>Gerhard</first><last>Heyer</last></author>
      <pages>153–159</pages>
      <abstract>Freedom of the press and media is of vital importance for democratically organised states and open societies. We introduce the Press Freedom Monitor, a tool that aims to detect reported press and media freedom violations in news articles and tweets. It is used by press and media freedom organisations to support their daily monitoring and to trigger rapid response actions. The Press Freedom Monitor enables the monitoring experts to get a fast overview over recently reported incidents and it has shown an impressive performance in this regard. This paper presents our work on the tool, starting with the training phase, which comprises defining the topic-related keywords to be used for querying APIs for news and Twitter content and evaluating different machine learning models based on a training dataset specifically created for our use case. Then, we describe the components of the production pipeline, including data gathering, duplicates removal, country mapping, case mapping and the user interface. We also conducted a usability study to evaluate the effectiveness of the user interface, and describe improvement plans for future work.</abstract>
      <url hash="5b2a7234">2021.emnlp-demo.18</url>
      <bibkey>yousef-etal-2021-press</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.18</doi>
      <video href="2021.emnlp-demo.18.mp4"/>
    </paper>
    <paper id="19">
      <title><fixed-case>UMR</fixed-case>-Writer: A Web Application for Annotating Uniform Meaning Representations</title>
      <author><first>Jin</first><last>Zhao</last></author>
      <author><first>Nianwen</first><last>Xue</last></author>
      <author><first>Jens</first><last>Van Gysel</last></author>
      <author><first>Jinho D.</first><last>Choi</last></author>
      <pages>160–167</pages>
      <abstract>We present UMR-Writer, a web-based application for annotating Uniform Meaning Representations (UMR), a graph-based, cross-linguistically applicable semantic representation developed recently to support the development of interpretable natural language applications that require deep semantic analysis of texts. We present the functionalities of UMR-Writer and discuss the challenges in developing such a tool and how they are addressed.</abstract>
      <url hash="db7c4db2">2021.emnlp-demo.19</url>
      <bibkey>zhao-etal-2021-umr</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.19</doi>
      <video href="2021.emnlp-demo.19.mp4"/>
    </paper>
    <paper id="20">
      <title><fixed-case>T</fixed-case>ranslate<fixed-case>L</fixed-case>ocally: Blazing-fast translation running on the local <fixed-case>CPU</fixed-case></title>
      <author><first>Nikolay</first><last>Bogoychev</last></author>
      <author><first>Jelmer</first><last>Van der Linde</last></author>
      <author><first>Kenneth</first><last>Heafield</last></author>
      <pages>168–174</pages>
      <abstract>Every day, millions of people sacrifice their privacy and browsing habits in exchange for online machine translation. Companies and governments with confidentiality requirements often ban online translation or pay a premium to disable logging. To bring control back to the end user and demonstrate speed, we developed translateLocally. Running locally on a desktop or laptop CPU, translateLocally delivers cloud-like translation speed and quality even on 10 year old hardware. The open-source software is based on Marian and runs on Linux, Windows, and macOS.</abstract>
      <url hash="cd51f6e4">2021.emnlp-demo.20</url>
      <bibkey>bogoychev-etal-2021-translatelocally</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.20</doi>
      <video href="2021.emnlp-demo.20.mp4"/>
    </paper>
    <paper id="21">
      <title>Datasets: A Community Library for Natural Language Processing</title>
      <author><first>Quentin</first><last>Lhoest</last></author>
      <author><first>Albert</first><last>Villanova del Moral</last></author>
      <author><first>Yacine</first><last>Jernite</last></author>
      <author><first>Abhishek</first><last>Thakur</last></author>
      <author><first>Patrick</first><last>von Platen</last></author>
      <author><first>Suraj</first><last>Patil</last></author>
      <author><first>Julien</first><last>Chaumond</last></author>
      <author><first>Mariama</first><last>Drame</last></author>
      <author><first>Julien</first><last>Plu</last></author>
      <author><first>Lewis</first><last>Tunstall</last></author>
      <author><first>Joe</first><last>Davison</last></author>
      <author><first>Mario</first><last>Šaško</last></author>
      <author><first>Gunjan</first><last>Chhablani</last></author>
      <author><first>Bhavitvya</first><last>Malik</last></author>
      <author><first>Simon</first><last>Brandeis</last></author>
      <author><first>Teven</first><last>Le Scao</last></author>
      <author><first>Victor</first><last>Sanh</last></author>
      <author><first>Canwen</first><last>Xu</last></author>
      <author><first>Nicolas</first><last>Patry</last></author>
      <author><first>Angelina</first><last>McMillan-Major</last></author>
      <author><first>Philipp</first><last>Schmid</last></author>
      <author><first>Sylvain</first><last>Gugger</last></author>
      <author><first>Clément</first><last>Delangue</last></author>
      <author><first>Théo</first><last>Matussière</last></author>
      <author><first>Lysandre</first><last>Debut</last></author>
      <author><first>Stas</first><last>Bekman</last></author>
      <author><first>Pierric</first><last>Cistac</last></author>
      <author><first>Thibault</first><last>Goehringer</last></author>
      <author><first>Victor</first><last>Mustar</last></author>
      <author><first>François</first><last>Lagunas</last></author>
      <author><first>Alexander</first><last>Rush</last></author>
      <author><first>Thomas</first><last>Wolf</last></author>
      <pages>175–184</pages>
      <abstract>The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora. The design of the library incorporates a distributed, community-driven approach to adding datasets and documenting usage. After a year of development, the library now includes more than 650 unique datasets, has more than 250 contributors, and has helped support a variety of novel cross-dataset research projects and shared tasks. The library is available at https://github.com/huggingface/datasets.</abstract>
      <url hash="e870681f">2021.emnlp-demo.21</url>
      <bibkey>lhoest-etal-2021-datasets</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.21</doi>
      <video href="2021.emnlp-demo.21.mp4"/>
      <pwccode url="https://github.com/huggingface/datasets" additional="false">huggingface/datasets</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/glue">GLUE</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/universal-dependencies">Universal Dependencies</pwcdataset>
    </paper>
    <paper id="22">
      <title>Summary Explorer: Visualizing the State of the Art in Text Summarization</title>
      <author><first>Shahbaz</first><last>Syed</last></author>
      <author><first>Tariq</first><last>Yousef</last></author>
      <author><first>Khalid</first><last>Al Khatib</last></author>
      <author><first>Stefan</first><last>Jänicke</last></author>
      <author><first>Martin</first><last>Potthast</last></author>
      <pages>185–194</pages>
      <abstract>This paper introduces Summary Explorer, a new tool to support the manual inspection of text summarization systems by compiling the outputs of 55 state-of-the-art single document summarization approaches on three benchmark datasets, and visually exploring them during a qualitative assessment. The underlying design of the tool considers three well-known summary quality criteria (coverage, faithfulness, and position bias), encapsulated in a guided assessment based on tailored visualizations. The tool complements existing approaches for locally debugging summarization models and improves upon them. The tool is available at https://tldr.webis.de/</abstract>
      <url hash="f2634d04">2021.emnlp-demo.22</url>
      <bibkey>syed-etal-2021-summary</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.22</doi>
      <video href="2021.emnlp-demo.22.mp4"/>
      <pwccode url="https://github.com/webis-de/summary-explorer" additional="false">webis-de/summary-explorer</pwccode>
    </paper>
    <paper id="23">
      <title><fixed-case>M</fixed-case>eet<fixed-case>D</fixed-case>ot: Videoconferencing with Live Translation Captions</title>
      <author><first>Arkady</first><last>Arkhangorodsky</last></author>
      <author><first>Christopher</first><last>Chu</last></author>
      <author><first>Scot</first><last>Fang</last></author>
      <author><first>Yiqi</first><last>Huang</last></author>
      <author><first>Denglin</first><last>Jiang</last></author>
      <author><first>Ajay</first><last>Nagesh</last></author>
      <author><first>Boliang</first><last>Zhang</last></author>
      <author><first>Kevin</first><last>Knight</last></author>
      <pages>195–202</pages>
      <abstract>We present MeetDot, a videoconferencing system with live translation captions overlaid on screen. The system aims to facilitate conversation between people who speak different languages, thereby reducing communication barriers between multilingual participants. Currently, our system supports speech and captions in 4 languages and combines automatic speech recognition (ASR) and machine translation (MT) in a cascade. We use the re-translation strategy to translate the streamed speech, resulting in caption flicker. Additionally, our system has very strict latency requirements to have acceptable call quality. We implement several features to enhance user experience and reduce their cognitive load, such as smooth scrolling captions and reducing caption flicker. The modular architecture allows us to integrate different ASR and MT services in our backend. Our system provides an integrated evaluation suite to optimize key intrinsic evaluation metrics such as accuracy, latency and erasure. Finally, we present an innovative cross-lingual word-guessing game as an extrinsic evaluation metric to measure end-to-end system performance. We plan to make our system open-source for research purposes.</abstract>
      <url hash="b6a7f620">2021.emnlp-demo.23</url>
      <bibkey>arkhangorodsky-etal-2021-meetdot</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.23</doi>
      <video href="2021.emnlp-demo.23.mp4"/>
    </paper>
    <paper id="24">
      <title>Box Embeddings: An open-source library for representation learning using geometric structures</title>
      <author><first>Tejas</first><last>Chheda</last></author>
      <author><first>Purujit</first><last>Goyal</last></author>
      <author><first>Trang</first><last>Tran</last></author>
      <author><first>Dhruvesh</first><last>Patel</last></author>
      <author><first>Michael</first><last>Boratko</last></author>
      <author><first>Shib Sankar</first><last>Dasgupta</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>203–211</pages>
      <abstract>A fundamental component to the success of modern representation learning is the ease of performing various vector operations. Recently, objects with more geometric structure (eg. distributions, complex or hyperbolic vectors, or regions such as cones, disks, or boxes) have been explored for their alternative inductive biases and additional representational capacity. In this work, we introduce Box Embeddings, a Python library that enables researchers to easily apply and extend probabilistic box embeddings. Fundamental geometric operations on boxes are implemented in a numerically stable way, as are modern approaches to training boxes which mitigate gradient sparsity. The library is fully open source, and compatible with both PyTorch and TensorFlow, which allows existing neural network layers to be replaced with or transformed into boxes easily. In this work, we present the implementation details of the fundamental components of the library, and the concepts required to use box representations alongside existing neural network architectures.</abstract>
      <url hash="2eaeb3bf">2021.emnlp-demo.24</url>
      <bibkey>chheda-etal-2021-box</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.24</doi>
      <video href="2021.emnlp-demo.24.mp4"/>
      <pwccode url="https://github.com/iesl/box-embeddings" additional="false">iesl/box-embeddings</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
    </paper>
    <paper id="25">
      <title><fixed-case>L</fixed-case>exi<fixed-case>C</fixed-case>lean: An annotation tool for rapid multi-task lexical normalisation</title>
      <author><first>Tyler</first><last>Bikaun</last></author>
      <author><first>Tim</first><last>French</last></author>
      <author><first>Melinda</first><last>Hodkiewicz</last></author>
      <author><first>Michael</first><last>Stewart</last></author>
      <author><first>Wei</first><last>Liu</last></author>
      <pages>212–219</pages>
      <abstract>NLP systems are often challenged by difficulties arising from noisy, non-standard, and domain specific corpora. The task of lexical normalisation aims to standardise such corpora, but currently lacks suitable tools to acquire high-quality annotated data to support deep learning based approaches. In this paper, we present LexiClean, the first open-source web-based annotation tool for multi-task lexical normalisation. LexiClean’s main contribution is support for simultaneous in situ token-level modification and annotation that can be rapidly applied corpus wide. We demonstrate the usefulness of our tool through a case study on two sets of noisy corpora derived from the specialised-domain of industrial mining. We show that LexiClean allows for the rapid and efficient development of high-quality parallel corpora. A demo of our system is available at: https://youtu.be/P7_ooKrQPDU.</abstract>
      <url hash="d0a345c6">2021.emnlp-demo.25</url>
      <bibkey>bikaun-etal-2021-lexiclean</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.25</doi>
      <video href="2021.emnlp-demo.25.mp4"/>
      <pwccode url="https://github.com/nlp-tlp/lexiclean" additional="false">nlp-tlp/lexiclean</pwccode>
    </paper>
    <paper id="26">
      <title>T3-Vis: visual analytic for Training and fine-Tuning Transformers in <fixed-case>NLP</fixed-case></title>
      <author><first>Raymond</first><last>Li</last></author>
      <author><first>Wen</first><last>Xiao</last></author>
      <author><first>Lanjun</first><last>Wang</last></author>
      <author><first>Hyeju</first><last>Jang</last></author>
      <author><first>Giuseppe</first><last>Carenini</last></author>
      <pages>220–230</pages>
      <abstract>Transformers are the dominant architecture in NLP, but their training and fine-tuning is still very challenging. In this paper, we present the design and implementation of a visual analytic framework for assisting researchers in such process, by providing them with valuable insights about the model’s intrinsic properties and behaviours. Our framework offers an intuitive overview that allows the user to explore different facets of the model (e.g., hidden states, attention) through interactive visualization, and allows a suite of built-in algorithms that compute the importance of model components and different parts of the input sequence. Case studies and feedback from a user focus group indicate that the framework is useful, and suggest several improvements. Our framework is available at: https://github.com/raymondzmc/T3-Vis.</abstract>
      <url hash="acef0e4c">2021.emnlp-demo.26</url>
      <bibkey>li-etal-2021-t3</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.26</doi>
      <video href="2021.emnlp-demo.26.mp4"/>
      <pwccode url="https://github.com/raymondzmc/t3-vis" additional="false">raymondzmc/t3-vis</pwccode>
    </paper>
    <paper id="27">
      <title><fixed-case>D</fixed-case>omi<fixed-case>K</fixed-case>now<fixed-case>S</fixed-case>: A Library for Integration of Symbolic Domain Knowledge in Deep Learning</title>
      <author><first>Hossein</first><last>Rajaby Faghihi</last></author>
      <author><first>Quan</first><last>Guo</last></author>
      <author><first>Andrzej</first><last>Uszok</last></author>
      <author><first>Aliakbar</first><last>Nafar</last></author>
      <author><first>Parisa</first><last>Kordjamshidi</last></author>
      <pages>231–241</pages>
      <abstract>We demonstrate a library for the integration of domain knowledge in deep learning architectures. Using this library, the structure of the data is expressed symbolically via graph declarations and the logical constraints over outputs or latent variables can be seamlessly added to the deep models. The domain knowledge can be defined explicitly, which improves the explainability of the models in addition to their performance and generalizability in the low-data regime. Several approaches for such integration of symbolic and sub-symbolic models have been introduced; however, there is no library to facilitate the programming for such integration in a generic way while various underlying algorithms can be used. Our library aims to simplify programming for such integration in both training and inference phases while separating the knowledge representation from learning algorithms. We showcase various NLP benchmark tasks and beyond. The framework is publicly available at Github(https://github.com/HLR/DomiKnowS).</abstract>
      <url hash="aea8384f">2021.emnlp-demo.27</url>
      <bibkey>rajaby-faghihi-etal-2021-domiknows</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.27</doi>
      <video href="2021.emnlp-demo.27.mp4"/>
      <pwccode url="https://github.com/hlr/domiknows" additional="false">hlr/domiknows</pwccode>
    </paper>
    <paper id="28">
      <title><fixed-case>O</fixed-case>pen<fixed-case>F</fixed-case>raming: Open-sourced Tool for Computational Framing Analysis of Multilingual Data</title>
      <author><first>Vibhu</first><last>Bhatia</last></author>
      <author><first>Vidya Prasad</first><last>Akavoor</last></author>
      <author><first>Sejin</first><last>Paik</last></author>
      <author><first>Lei</first><last>Guo</last></author>
      <author><first>Mona</first><last>Jalal</last></author>
      <author><first>Alyssa</first><last>Smith</last></author>
      <author><first>David Assefa</first><last>Tofu</last></author>
      <author><first>Edward Edberg</first><last>Halim</last></author>
      <author><first>Yimeng</first><last>Sun</last></author>
      <author><first>Margrit</first><last>Betke</last></author>
      <author><first>Prakash</first><last>Ishwar</last></author>
      <author><first>Derry Tanti</first><last>Wijaya</last></author>
      <pages>242–250</pages>
      <abstract>When journalists cover a news story, they can cover the story from multiple angles or perspectives. These perspectives are called “frames,” and usage of one frame or another may influence public perception and opinion of the issue at hand. We develop a web-based system for analyzing frames in multilingual text documents. We propose and guide users through a five-step end-to-end computational framing analysis framework grounded in media framing theory in communication research. Users can use the framework to analyze multilingual text data, starting from the exploration of frames in user’s corpora and through review of previous framing literature (step 1-3) to frame classification (step 4) and prediction (step 5). The framework combines unsupervised and supervised machine learning and leverages a state-of-the-art (SoTA) multilingual language model, which can significantly enhance frame prediction performance while requiring a considerably small sample of manual annotations. Through the interactive website, anyone can perform the proposed computational framing analysis, making advanced computational analysis available to researchers without a programming background and bridging the digital divide within the communication research discipline in particular and the academic community in general. The system is available online at http://www.openframing.org, via an API http://www.openframing.org:5000/docs/, or through our GitHub page https://github.com/vibss2397/openFraming.</abstract>
      <url hash="42c2be30">2021.emnlp-demo.28</url>
      <bibkey>bhatia-etal-2021-openframing</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.28</doi>
      <video href="2021.emnlp-demo.28.mp4"/>
      <pwccode url="https://github.com/vibss2397/openframing" additional="false">vibss2397/openframing</pwccode>
    </paper>
    <paper id="29">
      <title><fixed-case>I</fixed-case>r<fixed-case>E</fixed-case>ne-viz: Visualizing Energy Consumption of Transformer Models</title>
      <author><first>Yash Kumar</first><last>Lal</last></author>
      <author><first>Reetu</first><last>Singh</last></author>
      <author><first>Harsh</first><last>Trivedi</last></author>
      <author><first>Qingqing</first><last>Cao</last></author>
      <author><first>Aruna</first><last>Balasubramanian</last></author>
      <author><first>Niranjan</first><last>Balasubramanian</last></author>
      <pages>251–258</pages>
      <abstract>IrEne is an energy prediction system that accurately predicts the interpretable inference energy consumption of a wide range of Transformer-based NLP models. We present the IrEne-viz tool, an online platform for visualizing and exploring energy consumption of various Transformer-based models easily. Additionally, we release a public API that can be used to access granular information about energy consumption of transformer models and their components. The live demo is available at http://stonybrooknlp.github.io/irene/demo/.</abstract>
      <url hash="09f47612">2021.emnlp-demo.29</url>
      <bibkey>lal-etal-2021-irene</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.29</doi>
      <video href="2021.emnlp-demo.29.mp4"/>
    </paper>
    <paper id="30">
      <title>Open-<fixed-case>D</fixed-case>omain Question-<fixed-case>A</fixed-case>nswering for <fixed-case>COVID</fixed-case>-19 and Other Emergent Domains</title>
      <author><first>Sharon</first><last>Levy</last></author>
      <author><first>Kevin</first><last>Mo</last></author>
      <author><first>Wenhan</first><last>Xiong</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>259–266</pages>
      <abstract>Since late 2019, COVID-19 has quickly emerged as the newest biomedical domain, resulting in a surge of new information. As with other emergent domains, the discussion surrounding the topic has been rapidly changing, leading to the spread of misinformation. This has created the need for a public space for users to ask questions and receive credible, scientific answers. To fulfill this need, we turn to the task of open-domain question-answering, which we can use to efficiently find answers to free-text questions from a large set of documents. In this work, we present such a system for the emergent domain of COVID-19. Despite the small data size available, we are able to successfully train the system to retrieve answers from a large-scale corpus of published COVID-19 scientific papers. Furthermore, we incorporate effective re-ranking and question-answering techniques, such as document diversity and multiple answer spans. Our open-domain question-answering system can further act as a model for the quick development of similar systems that can be adapted and modified for other developing emergent domains.</abstract>
      <url hash="2ca70ecf">2021.emnlp-demo.30</url>
      <bibkey>levy-etal-2021-open</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.30</doi>
      <video href="2021.emnlp-demo.30.mp4"/>
      <pwccode url="https://github.com/sharonlevy/open_domain_covidqa" additional="false">sharonlevy/open_domain_covidqa</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cord-19">CORD-19</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/covidqa">CovidQA</pwcdataset>
    </paper>
    <paper id="31">
      <title>Project <fixed-case>D</fixed-case>ebater <fixed-case>API</fixed-case>s: <fixed-case>D</fixed-case>ecomposing the <fixed-case>AI</fixed-case> Grand Challenge</title>
      <author><first>Roy</first><last>Bar-Haim</last></author>
      <author><first>Yoav</first><last>Kantor</last></author>
      <author><first>Elad</first><last>Venezian</last></author>
      <author><first>Yoav</first><last>Katz</last></author>
      <author><first>Noam</first><last>Slonim</last></author>
      <pages>267–274</pages>
      <abstract>Project Debater was revealed in 2019 as the first AI system that can debate human experts on complex topics. Engaging in a live debate requires a diverse set of skills, and Project Debater has been developed accordingly as a collection of components, each designed to perform a specific subtask. Project Debater APIs provide access to many of these capabilities, as well as to more recently developed ones. This diverse set of web services, publicly available for academic use, includes core NLP services, argument mining and analysis capabilities, and higher-level services for content summarization. We describe these APIs and their performance, and demonstrate how they can be used for building practical solutions. In particular, we will focus on Key Point Analysis, a novel technology that identifies the main points and their prevalence in a collection of texts such as survey responses and user reviews.</abstract>
      <url hash="51d3d1b2">2021.emnlp-demo.31</url>
      <bibkey>bar-haim-etal-2021-project</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.31</doi>
      <video href="2021.emnlp-demo.31.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/ibm-rank-30k">IBM-Rank-30k</pwcdataset>
    </paper>
    <paper id="32">
      <title><fixed-case>C</fixed-case>ro<fixed-case>A</fixed-case>no : A Crowd Annotation Platform for Improving Label Consistency of <fixed-case>C</fixed-case>hinese <fixed-case>NER</fixed-case> Dataset</title>
      <author><first>Baoli</first><last>Zhang</last></author>
      <author><first>Zhucong</first><last>Li</last></author>
      <author><first>Zhen</first><last>Gan</last></author>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Jing</first><last>Wan</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <author><first>Shengping</first><last>Liu</last></author>
      <author><first>Yafei</first><last>Shi</last></author>
      <pages>275–282</pages>
      <abstract>In this paper, we introduce CroAno, a web-based crowd annotation platform for the Chinese named entity recognition (NER). Besides some basic features for crowd annotation like fast tagging and data management, CroAno provides a systematic solution for improving label consistency of Chinese NER dataset. 1) Disagreement Adjudicator: CroAno uses a multi-dimensional highlight mode to visualize instance-level inconsistent entities and makes the revision process user-friendly. 2) Inconsistency Detector: CroAno employs a detector to locate corpus-level label inconsistency and provides users an interface to correct inconsistent entities in batches. 3) Prediction Error Analyzer: We deconstruct the entity prediction error of the model to six fine-grained entity error types. Users can employ this error system to detect corpus-level inconsistency from a model perspective. To validate the effectiveness of our platform, we use CroAno to revise two public datasets. In the two revised datasets, we get an improvement of +1.96% and +2.57% F1 respectively in model performance.</abstract>
      <url hash="5ef1cf2b">2021.emnlp-demo.32</url>
      <bibkey>zhang-etal-2021-croano</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.32</doi>
      <video href="2021.emnlp-demo.32.mp4"/>
    </paper>
    <paper id="33">
      <title>i<fixed-case>F</fixed-case>acet<fixed-case>S</fixed-case>um: Coreference-based Interactive Faceted Summarization for Multi-Document Exploration</title>
      <author><first>Eran</first><last>Hirsch</last></author>
      <author><first>Alon</first><last>Eirew</last></author>
      <author><first>Ori</first><last>Shapira</last></author>
      <author><first>Avi</first><last>Caciularu</last></author>
      <author><first>Arie</first><last>Cattan</last></author>
      <author><first>Ori</first><last>Ernst</last></author>
      <author><first>Ramakanth</first><last>Pasunuru</last></author>
      <author><first>Hadar</first><last>Ronen</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <pages>283–297</pages>
      <abstract>We introduce iFᴀᴄᴇᴛSᴜᴍ, a web application for exploring topical document collections. iFᴀᴄᴇᴛSᴜᴍ integrates interactive summarization together with faceted search, by providing a novel faceted navigation scheme that yields abstractive summaries for the user’s selections. This approach offers both a comprehensive overview as well as particular details regard-ing subtopics of choice. The facets are automatically produced based on cross-document coreference pipelines, rendering generic concepts, entities and statements surfacing in the source texts. We analyze the effectiveness of our application through small-scale user studies that suggest the usefulness of our tool.</abstract>
      <url hash="46621a34">2021.emnlp-demo.33</url>
      <bibkey>hirsch-etal-2021-ifacetsum</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.33</doi>
      <video href="2021.emnlp-demo.33.mp4"/>
      <pwccode url="https://github.com/biu-nlp/ifacetsum" additional="false">biu-nlp/ifacetsum</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/ecb">ECB+</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/wec-eng">WEC-Eng</pwcdataset>
    </paper>
    <paper id="34">
      <title><fixed-case>AMuSE-WSD</fixed-case>: <fixed-case>A</fixed-case>n All-in-one Multilingual System for Easy <fixed-case>W</fixed-case>ord <fixed-case>S</fixed-case>ense <fixed-case>D</fixed-case>isambiguation</title>
      <author><first>Riccardo</first><last>Orlando</last></author>
      <author><first>Simone</first><last>Conia</last></author>
      <author><first>Fabrizio</first><last>Brignone</last></author>
      <author><first>Francesco</first><last>Cecconi</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>298–307</pages>
      <abstract>Over the past few years, Word Sense Disambiguation (WSD) has received renewed interest: recently proposed systems have shown the remarkable effectiveness of deep learning techniques in this task, especially when aided by modern pretrained language models. Unfortunately, such systems are still not available as ready-to-use end-to-end packages, making it difficult for researchers to take advantage of their performance. The only alternative for a user interested in applying WSD to downstream tasks is to rely on currently available end-to-end WSD systems, which, however, still rely on graph-based heuristics or non-neural machine learning algorithms. In this paper, we fill this gap and propose AMuSE-WSD, the first end-to-end system to offer high-quality sense information in 40 languages through a state-of-the-art neural model for WSD. We hope that AMuSE-WSD will provide a stepping stone for the integration of meaning into real-world applications and encourage further studies in lexical semantics. AMuSE-WSD is available online at http://nlp.uniroma1.it/amuse-wsd.</abstract>
      <url hash="b66d4835">2021.emnlp-demo.34</url>
      <bibkey>orlando-etal-2021-amuse</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.34</doi>
      <video href="2021.emnlp-demo.34.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/word-sense-disambiguation-a-unified">Word Sense Disambiguation: a Unified Evaluation Framework and Empirical Comparison</pwcdataset>
    </paper>
    <paper id="35">
      <title><fixed-case>S</fixed-case>eq<fixed-case>A</fixed-case>ttack: <fixed-case>O</fixed-case>n Adversarial Attacks for Named Entity Recognition</title>
      <author><first>Walter</first><last>Simoncini</last></author>
      <author><first>Gerasimos</first><last>Spanakis</last></author>
      <pages>308–318</pages>
      <abstract>Named Entity Recognition is a fundamental task in information extraction and is an essential element for various Natural Language Processing pipelines. Adversarial attacks have been shown to greatly affect the performance of text classification systems but knowledge about their effectiveness against named entity recognition models is limited. This paper investigates the effectiveness and portability of adversarial attacks from text classification to named entity recognition and the ability of adversarial training to counteract these attacks. We find that character-level and word-level attacks are the most effective, but adversarial training can grant significant protection at little to no expense of standard performance. Alongside our results, we also release SeqAttack, a framework to conduct adversarial attacks against token classification models (used in this work for named entity recognition) and a companion web application to inspect and cherry pick adversarial examples.</abstract>
      <url hash="874d5009">2021.emnlp-demo.35</url>
      <bibkey>simoncini-spanakis-2021-seqattack</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.35</doi>
      <video href="2021.emnlp-demo.35.mp4"/>
      <pwcdataset url="https://paperswithcode.com/dataset/conll-2003">CoNLL-2003</pwcdataset>
    </paper>
    <paper id="36">
      <title><fixed-case>InVeRo-XL</fixed-case>: <fixed-case>M</fixed-case>aking Cross-Lingual <fixed-case>S</fixed-case>emantic <fixed-case>R</fixed-case>ole <fixed-case>L</fixed-case>abeling Accessible with Intelligible Verbs and Roles</title>
      <author><first>Simone</first><last>Conia</last></author>
      <author><first>Riccardo</first><last>Orlando</last></author>
      <author><first>Fabrizio</first><last>Brignone</last></author>
      <author><first>Francesco</first><last>Cecconi</last></author>
      <author><first>Roberto</first><last>Navigli</last></author>
      <pages>319–328</pages>
      <abstract>Notwithstanding the growing interest in cross-lingual techniques for Natural Language Processing, there has been a surprisingly small number of efforts aimed at the development of easy-to-use tools for cross-lingual Semantic Role Labeling. In this paper, we fill this gap and present InVeRo-XL, an off-the-shelf state-of-the-art system capable of annotating text with predicate sense and semantic role labels from 7 predicate-argument structure inventories in more than 40 languages. We hope that our system – with its easy-to-use RESTful API and Web interface – will become a valuable tool for the research community, encouraging the integration of sentence-level semantics into cross-lingual downstream tasks. InVeRo-XL is available online at http://nlp.uniroma1.it/invero.</abstract>
      <url hash="c251eec5">2021.emnlp-demo.36</url>
      <bibkey>conia-etal-2021-invero</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.36</doi>
      <video href="2021.emnlp-demo.36.mp4"/>
    </paper>
    <paper id="37">
      <title><fixed-case>S</fixed-case>ummer<fixed-case>T</fixed-case>ime: Text Summarization Toolkit for Non-experts</title>
      <author><first>Ansong</first><last>Ni</last></author>
      <author><first>Zhangir</first><last>Azerbayev</last></author>
      <author><first>Mutethia</first><last>Mutuma</last></author>
      <author><first>Troy</first><last>Feng</last></author>
      <author><first>Yusen</first><last>Zhang</last></author>
      <author><first>Tao</first><last>Yu</last></author>
      <author><first>Ahmed Hassan</first><last>Awadallah</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <pages>329–338</pages>
      <abstract>Recent advances in summarization provide models that can generate summaries of higher quality. Such models now exist for a number of summarization tasks, including query-based summarization, dialogue summarization, and multi-document summarization. While such models and tasks are rapidly growing in the research field, it has also become challenging for non-experts to keep track of them. To make summarization methods more accessible to a wider audience, we develop SummerTime by rethinking the summarization task from the perspective of an NLP non-expert. SummerTime is a complete toolkit for text summarization, including various models, datasets, and evaluation metrics, for a full spectrum of summarization-related tasks. SummerTime integrates with libraries designed for NLP researchers, and enables users with easy-to-use APIs. With SummerTime, users can locate pipeline solutions and search for the best model with their own data, and visualize the differences, all with a few lines of code. We also provide explanations for models and evaluation metrics to help users understand the model behaviors and select models that best suit their needs. Our library, along with a notebook demo, is available at https://github.com/Yale-LILY/SummerTime.</abstract>
      <url hash="46160b2b">2021.emnlp-demo.37</url>
      <bibkey>ni-etal-2021-summertime</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.37</doi>
      <video href="2021.emnlp-demo.37.mp4"/>
      <pwccode url="https://github.com/Yale-LILY/SummerTime" additional="false">Yale-LILY/SummerTime</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/cnn-daily-mail-1">CNN/Daily Mail</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/mlsum">MLSUM</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/multi-news">Multi-News</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/pubmedqa">PubMedQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/samsum-corpus">SAMSum Corpus</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/scisummnet">ScisummNet</pwcdataset>
    </paper>
    <paper id="38">
      <title>Chandler: An Explainable Sarcastic Response Generator</title>
      <author><first>Silviu</first><last>Oprea</last></author>
      <author><first>Steven</first><last>Wilson</last></author>
      <author><first>Walid</first><last>Magdy</last></author>
      <pages>339–349</pages>
      <abstract>We introduce Chandler, a system that generates sarcastic responses to a given utterance. Previous sarcasm generators assume the intended meaning that sarcasm conceals is the opposite of the literal meaning. We argue that this traditional theory of sarcasm provides a grounding that is neither necessary, nor sufficient, for sarcasm to occur. Instead, we ground our generation process on a formal theory that specifies conditions that unambiguously differentiate sarcasm from non-sarcasm. Furthermore, Chandler not only generates sarcastic responses, but also explanations for why each response is sarcastic. This provides accountability, crucial for avoiding miscommunication between humans and conversational agents, particularly considering that sarcastic communication can be offensive. In human evaluation, Chandler achieves comparable or higher sarcasm scores, compared to state-of-the-art generators, while generating more diverse responses, that are more specific and more coherent to the input.</abstract>
      <url hash="75e675dc">2021.emnlp-demo.38</url>
      <bibkey>oprea-etal-2021-chandler</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.38</doi>
      <video href="2021.emnlp-demo.38.mp4"/>
    </paper>
    <paper id="39">
      <title><fixed-case>T</fixed-case>ab<fixed-case>P</fixed-case>ert : An Effective Platform for Tabular Perturbation</title>
      <author><first>Nupur</first><last>Jain</last></author>
      <author><first>Vivek</first><last>Gupta</last></author>
      <author><first>Anshul</first><last>Rai</last></author>
      <author><first>Gaurav</first><last>Kumar</last></author>
      <pages>350–360</pages>
      <abstract>To grasp the true reasoning ability, the Natural Language Inference model should be evaluated on counterfactual data. TabPert facilitates this by generation of such counterfactual data for assessing model tabular reasoning issues. TabPert allows the user to update a table, change the hypothesis, change the labels, and highlight rows that are important for hypothesis classification. TabPert also details the technique used to automatically produce the table, as well as the strategies employed to generate the challenging hypothesis. These counterfactual tables and hypotheses, as well as the metadata, is then used to explore the existing model’s shortcomings methodically and quantitatively.</abstract>
      <url hash="ccbfb956">2021.emnlp-demo.39</url>
      <bibkey>jain-etal-2021-tabpert</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.39</doi>
      <video href="2021.emnlp-demo.39.mp4"/>
      <pwccode url="https://github.com/utahnlp/tabpert" additional="false">utahnlp/tabpert</pwccode>
    </paper>
    <paper id="40">
      <title><fixed-case>DRIFT</fixed-case>: A Toolkit for Diachronic Analysis of Scientific Literature</title>
      <author><first>Abheesht</first><last>Sharma</last></author>
      <author><first>Gunjan</first><last>Chhablani</last></author>
      <author><first>Harshit</first><last>Pandey</last></author>
      <author><first>Rajaswa</first><last>Patil</last></author>
      <pages>361–371</pages>
      <abstract>In this work, we present to the NLP community, and to the wider research community as a whole, an application for the diachronic analysis of research corpora. We open source an easy-to-use tool coined DRIFT, which allows researchers to track research trends and development over the years. The analysis methods are collated from well-cited research works, with a few of our own methods added for good measure. Succinctly put, some of the analysis methods are: keyword extraction, word clouds, predicting declining/stagnant/growing trends using Productivity, tracking bi-grams using Acceleration plots, finding the Semantic Drift of words, tracking trends using similarity, etc. To demonstrate the utility and efficacy of our tool, we perform a case study on the cs.CL corpus of the arXiv repository and draw inferences from the analysis methods. The toolkit and the associated code are available here: https://github.com/rajaswa/DRIFT.</abstract>
      <url hash="46f3ab48">2021.emnlp-demo.40</url>
      <attachment type="Software" hash="1aa6ffe5">2021.emnlp-demo.40.Software.zip</attachment>
      <bibkey>sharma-etal-2021-drift</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.40</doi>
      <video href="2021.emnlp-demo.40.mp4"/>
      <pwccode url="https://github.com/rajaswa/DRIFT" additional="false">rajaswa/DRIFT</pwccode>
    </paper>
    <paper id="41">
      <title><fixed-case>FAST</fixed-case>: <fixed-case>F</fixed-case>ast <fixed-case>A</fixed-case>nnotation tool for <fixed-case>S</fixed-case>mar<fixed-case>T</fixed-case> devices</title>
      <author><first>Shunyo</first><last>Kawamoto</last></author>
      <author><first>Yu</first><last>Sawai</last></author>
      <author><first>Kohei</first><last>Wakimoto</last></author>
      <author><first>Peinan</first><last>Zhang</last></author>
      <pages>372–381</pages>
      <abstract>Working with a wide range of annotators with the same attributes is crucial, as in real-world applications. Although such application cases often use crowd-sourcing mechanisms to gather a variety of annotators, most real-world users use mobile devices. In this paper, we propose “FAST,” an annotation tool for application tasks that focuses on the user experience of mobile devices, which has not yet been focused on thus far. We designed FAST as a web application for use on any device with a flexible interface that can be customized to fit various tasks. In our experiments, we conducted crowd-sourced annotation for a sentiment analysis task with several annotators and evaluated annotation metrics such as speed, quality, and ease of use from the tool’s logs and user surveys. Based on the results of our experiments, we conclude that our system can annotate faster than existing methods while maintaining the annotation quality.</abstract>
      <url hash="533ea0db">2021.emnlp-demo.41</url>
      <attachment type="Software" hash="d6af4d7e">2021.emnlp-demo.41.Software.zip</attachment>
      <bibkey>kawamoto-etal-2021-fast</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.41</doi>
      <video href="2021.emnlp-demo.41.mp4"/>
      <pwccode url="https://github.com/cyberagent/fast-annotation-tool" additional="false">cyberagent/fast-annotation-tool</pwccode>
    </paper>
    <paper id="42">
      <title>deep<fixed-case>Q</fixed-case>uest-py: <fixed-case>L</fixed-case>arge and Distilled Models for Quality Estimation</title>
      <author><first>Fernando</first><last>Alva-Manchego</last></author>
      <author><first>Abiola</first><last>Obamuyide</last></author>
      <author><first>Amit</first><last>Gajbhiye</last></author>
      <author><first>Frédéric</first><last>Blain</last></author>
      <author><first>Marina</first><last>Fomicheva</last></author>
      <author><first>Lucia</first><last>Specia</last></author>
      <pages>382–389</pages>
      <abstract>We introduce deepQuest-py, a framework for training and evaluation of large and light-weight models for Quality Estimation (QE). deepQuest-py provides access to (1) state-of-the-art models based on pre-trained Transformers for sentence-level and word-level QE; (2) light-weight and efficient sentence-level models implemented via knowledge distillation; and (3) a web interface for testing models and visualising their predictions. deepQuest-py is available at <url>https://github.com/sheffieldnlp/deepQuest-py</url> under a CC BY-NC-SA licence.</abstract>
      <url hash="71248731">2021.emnlp-demo.42</url>
      <bibkey>alva-manchego-etal-2021-deepquest</bibkey>
      <doi>10.18653/v1/2021.emnlp-demo.42</doi>
      <video href="2021.emnlp-demo.42.mp4"/>
      <pwccode url="https://github.com/sheffieldnlp/deepQuest-py" additional="false">sheffieldnlp/deepQuest-py</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/mlqe">MLQE</pwcdataset>
    </paper>
  </volume>
  <volume id="tutorials" ingest-date="2021-11-04">
    <meta>
      <booktitle>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts</booktitle>
      <editor><first>Jing</first><last>Jiang</last></editor>
      <editor><first>Ivan</first><last>Vulić</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Punta Cana, Dominican Republic &amp; Online</address>
      <month>November</month>
      <year>2021</year>
      <url hash="61ad21e0">2021.emnlp-tutorials</url>
      <venue>emnlp</venue>
    </meta>
    <frontmatter>
      <url hash="02a2aa34">2021.emnlp-tutorials.0</url>
      <bibkey>emnlp-2021-tutorials</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Crowdsourcing Beyond Annotation: Case Studies in Benchmark Data Collection</title>
      <author><first>Alane</first><last>Suhr</last></author>
      <author><first>Clara</first><last>Vania</last></author>
      <author><first>Nikita</first><last>Nangia</last></author>
      <author><first>Maarten</first><last>Sap</last></author>
      <author><first>Mark</first><last>Yatskar</last></author>
      <author><first>Samuel R.</first><last>Bowman</last></author>
      <author><first>Yoav</first><last>Artzi</last></author>
      <pages>1–6</pages>
      <abstract>Crowdsourcing from non-experts is one of the most common approaches to collecting data and annotations in NLP. Even though it is such a fundamental tool in NLP, crowdsourcing use is largely guided by common practices and the personal experience of researchers. Developing a theory of crowdsourcing use for practical language problems remains an open challenge. However, there are various principles and practices that have proven effective in generating high quality and diverse data. This tutorial exposes NLP researchers to such data collection crowdsourcing methods and principles through a detailed discussion of a diverse set of case studies. The selection of case studies focuses on challenging settings where crowdworkers are asked to write original text or otherwise perform relatively unconstrained work. Through these case studies, we discuss in detail processes that were carefully designed to achieve data with specific properties, for example to require logical inference, grounded reasoning or conversational understanding. Each case study focuses on data collection crowdsourcing protocol details that often receive limited attention in research presentations, for example in conferences, but are critical for research success.</abstract>
      <url hash="c4206406">2021.emnlp-tutorials.1</url>
      <bibkey>suhr-etal-2021-crowdsourcing</bibkey>
      <doi>10.18653/v1/2021.emnlp-tutorials.1</doi>
      <pwcdataset url="https://paperswithcode.com/dataset/multinli">MultiNLI</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/nlvr">NLVR</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/quac">QuAC</pwcdataset>
    </paper>
    <paper id="2">
      <title>Financial Opinion Mining</title>
      <author><first>Chung-Chi</first><last>Chen</last></author>
      <author><first>Hen-Hsen</first><last>Huang</last></author>
      <author><first>Hsin-Hsi</first><last>Chen</last></author>
      <pages>7–10</pages>
      <abstract>In this tutorial, we will show where we are and where we will be to those researchers interested in this topic. We divide this tutorial into three parts, including coarse-grained financial opinion mining, fine-grained financial opinion mining, and possible research directions. This tutorial starts by introducing the components in a financial opinion proposed in our research agenda and summarizes their related studies. We also highlight the task of mining customers’ opinions toward financial services in the FinTech industry, and compare them with usual opinions. Several potential research questions will be addressed. We hope the audiences of this tutorial will gain an overview of financial opinion mining and figure out their research directions.</abstract>
      <url hash="da3e881c">2021.emnlp-tutorials.2</url>
      <bibkey>chen-etal-2021-financial</bibkey>
      <doi>10.18653/v1/2021.emnlp-tutorials.2</doi>
    </paper>
    <paper id="3">
      <title>Knowledge-Enriched Natural Language Generation</title>
      <author><first>Wenhao</first><last>Yu</last></author>
      <author><first>Meng</first><last>Jiang</last></author>
      <author><first>Zhiting</first><last>Hu</last></author>
      <author><first>Qingyun</first><last>Wang</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Nazneen</first><last>Rajani</last></author>
      <pages>11–16</pages>
      <abstract>Knowledge-enriched text generation poses unique challenges in modeling and learning, driving active research in several core directions, ranging from integrated modeling of neural representations and symbolic information in the sequential/hierarchical/graphical structures, learning without direct supervisions due to the cost of structured annotation, efficient optimization and inference with massive and global constraints, to language grounding on multiple modalities, and generative reasoning with implicit commonsense knowledge and background knowledge. In this tutorial we will present a roadmap to line up the state-of-the-art methods to tackle these challenges on this cutting-edge problem. We will dive deep into various technical components: how to represent knowledge, how to feed knowledge into a generation model, how to evaluate generation results, and what are the remaining challenges?</abstract>
      <url hash="089c4029">2021.emnlp-tutorials.3</url>
      <bibkey>yu-etal-2021-knowledge</bibkey>
      <doi>10.18653/v1/2021.emnlp-tutorials.3</doi>
      <pwccode url="https://github.com/wyu97/KENLG-Reading" additional="false">wyu97/KENLG-Reading</pwccode>
    </paper>
    <paper id="4">
      <title>Multi-Domain Multilingual Question Answering</title>
      <author><first>Sebastian</first><last>Ruder</last></author>
      <author><first>Avi</first><last>Sil</last></author>
      <pages>17–21</pages>
      <abstract>Question answering (QA) is one of the most challenging and impactful tasks in natural language processing. Most research in QA, however, has focused on the open-domain or monolingual setting while most real-world applications deal with specific domains or languages. In this tutorial, we attempt to bridge this gap. Firstly, we introduce standard benchmarks in multi-domain and multilingual QA. In both scenarios, we discuss state-of-the-art approaches that achieve impressive performance, ranging from zero-shot transfer learning to out-of-the-box training with open-domain QA systems. Finally, we will present open research problems that this new research agenda poses such as multi-task learning, cross-lingual transfer learning, domain adaptation and training large scale pre-trained multilingual language models.</abstract>
      <url hash="906ab71f">2021.emnlp-tutorials.4</url>
      <bibkey>ruder-sil-2021-multi</bibkey>
      <doi>10.18653/v1/2021.emnlp-tutorials.4</doi>
      <pwccode url="https://github.com/sebastianruder/emnlp2021-multiqa-tutorial" additional="false">sebastianruder/emnlp2021-multiqa-tutorial</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/doqa">DoQA</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/natural-questions">Natural Questions</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/squad">SQuAD</pwcdataset>
    </paper>
    <paper id="5">
      <title>Robustness and Adversarial Examples in Natural Language Processing</title>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>He</first><last>He</last></author>
      <author><first>Robin</first><last>Jia</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <pages>22–26</pages>
      <abstract>Recent studies show that many NLP systems are sensitive and vulnerable to a small perturbation of inputs and do not generalize well across different datasets. This lack of robustness derails the use of NLP systems in real-world applications. This tutorial aims at bringing awareness of practical concerns about NLP robustness. It targets NLP researchers and practitioners who are interested in building reliable NLP systems. In particular, we will review recent studies on analyzing the weakness of NLP systems when facing adversarial inputs and data with a distribution shift. We will provide the audience with a holistic view of 1) how to use adversarial examples to examine the weakness of NLP models and facilitate debugging; 2) how to enhance the robustness of existing NLP models and defense against adversarial inputs; and 3) how the consideration of robustness affects the real-world NLP applications used in our daily lives. We will conclude the tutorial by outlining future research directions in this area.</abstract>
      <url hash="32de49cb">2021.emnlp-tutorials.5</url>
      <bibkey>chang-etal-2021-robustness</bibkey>
      <doi>10.18653/v1/2021.emnlp-tutorials.5</doi>
    </paper>
    <paper id="6">
      <title>Syntax in End-to-End Natural Language Processing</title>
      <author><first>Hai</first><last>Zhao</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Kehai</first><last>Chen</last></author>
      <pages>27–33</pages>
      <abstract>This tutorial surveys the latest technical progress of syntactic parsing and the role of syntax in end-to-end natural language processing (NLP) tasks, in which semantic role labeling (SRL) and machine translation (MT) are the representative NLP tasks that have always been beneficial from informative syntactic clues since a long time ago, though the advance from end-to-end deep learning models shows new results. In this tutorial, we will first introduce the background and the latest progress of syntactic parsing and SRL/NMT. Then, we will summarize the key evidence about the syntactic impacts over these two concerning tasks, and explore the behind reasons from both computational and linguistic backgrounds.</abstract>
      <url hash="8fb22971">2021.emnlp-tutorials.6</url>
      <bibkey>zhao-etal-2021-syntax</bibkey>
      <doi>10.18653/v1/2021.emnlp-tutorials.6</doi>
    </paper>
  </volume>
  <event id="emnlp-2021">
    <colocated>
      <volume-id>2021.findings-emnlp</volume-id>
      <volume-id>2021.codi-main</volume-id>
      <volume-id>2021.codi-sharedtask</volume-id>
      <volume-id>2021.argmining-1</volume-id>
      <volume-id>2021.blackboxnlp-1</volume-id>
      <volume-id>2021.cinlp-1</volume-id>
      <volume-id>2021.crac-1</volume-id>
      <volume-id>2021.conll-1</volume-id>
      <volume-id>2021.disrpt-1</volume-id>
      <volume-id>2021.eancs-1</volume-id>
      <volume-id>2021.econlp-1</volume-id>
      <volume-id>2021.eval4nlp-1</volume-id>
      <volume-id>2021.fever-1</volume-id>
      <volume-id>2021.latechclfl-1</volume-id>
      <volume-id>2021.law-1</volume-id>
      <volume-id>2021.mrl-1</volume-id>
      <volume-id>2021.mrqa-1</volume-id>
      <volume-id>2021.nllp-1</volume-id>
      <volume-id>2021.nlp4convai-1</volume-id>
      <volume-id>2021.sustainlp-1</volume-id>
      <volume-id>2021.newsum-1</volume-id>
      <volume-id>2021.insights-1</volume-id>
      <volume-id>2021.wnut-1</volume-id>
      <volume-id>2021.wmt-1</volume-id>
    </colocated>
  </event>
</collection>
