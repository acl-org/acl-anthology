<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.blackboxnlp">
  <volume id="1" ingest-date="2024-11-05" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP</booktitle>
      <editor><first>Yonatan</first><last>Belinkov</last></editor>
      <editor><first>Najoung</first><last>Kim</last></editor>
      <editor><first>Jaap</first><last>Jumelet</last></editor>
      <editor><first>Hosein</first><last>Mohebbi</last></editor>
      <editor><first>Aaron</first><last>Mueller</last></editor>
      <editor><first>Hanjie</first><last>Chen</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Miami, Florida, US</address>
      <month>November</month>
      <year>2024</year>
      <url hash="790d9f4a">2024.blackboxnlp-1</url>
      <venue>blackboxnlp</venue>
    </meta>
    <frontmatter>
      <url hash="74cf782f">2024.blackboxnlp-1.0</url>
      <bibkey>blackboxnlp-2024-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Optimal and efficient text counterfactuals using Graph Neural Networks</title>
      <author><first>Dimitris</first><last>Lymperopoulos</last><affiliation>National Technical University of Athens</affiliation></author>
      <author><first>Maria</first><last>Lymperaiou</last></author>
      <author><first>Giorgos</first><last>Filandrianos</last><affiliation>National Technical University of Athens</affiliation></author>
      <author><first>Giorgos</first><last>Stamou</last><affiliation>National Technical University of Athens</affiliation></author>
      <pages>1-14</pages>
      <abstract>As NLP models become increasingly integral to decision-making processes, the need for explainability and interpretability has become paramount. In this work, we propose a framework that achieves the aforementioned by generating semantically edited inputs, known as counterfactual interventions, which change the model prediction, thus providing a form of counterfactual explanations for the model. We frame the search for optimal counterfactual interventions as a graph assignment problem and employ a GNN to solve it, thus achieving high efficiency. We test our framework on two NLP tasks - binary sentiment classification and topic classification - and show that the generated edits are contrastive, fluent and minimal, while the whole process remains significantly faster than other state-of-the-art counterfactual editors.</abstract>
      <url hash="87c7725b">2024.blackboxnlp-1.1</url>
      <bibkey>lymperopoulos-etal-2024-optimal</bibkey>
    </paper>
    <paper id="2">
      <title>Routing in Sparsely-gated Language Models responds to Context</title>
      <author><first>Stefan</first><last>Arnold</last></author>
      <author><first>Marian</first><last>Fietta</last><affiliation>Friedrich-Alexander Universität Erlangen-Nürnberg</affiliation></author>
      <author><first>Dilara</first><last>Yesilbas</last></author>
      <pages>15-22</pages>
      <abstract>Language Models (LMs) recently incorporate mixture-of-experts layers consisting of a router and a collection of experts to scale up their parameter count given a fixed computational budget. Building on previous efforts indicating that token-expert assignments are predominantly influenced by token identities and positions, we trace routing decisions of similarity-annotated text pairs to evaluate the context sensitivity of learned token-expert assignments. We observe that routing in encoder layers mainly depends on (semantic) associations, but contextual cues provide an additional layer of refinement. Conversely, routing in decoder layers is more variable and markedly less sensitive to context.</abstract>
      <url hash="799783ab">2024.blackboxnlp-1.2</url>
      <bibkey>arnold-etal-2024-routing</bibkey>
    </paper>
    <paper id="3">
      <title>Are there identifiable structural parts in the sentence embedding whole?</title>
      <author><first>Vivi</first><last>Nastase</last><affiliation>University of Geneva</affiliation></author>
      <author><first>Paola</first><last>Merlo</last><affiliation>Idiap Research Institute and University of Geneva, Switzerland</affiliation></author>
      <pages>23-42</pages>
      <abstract>Sentence embeddings from transformer models encode much linguistic information in a fixed-length vector. We investigate whether structural information – specifically, information about chunks and their structural and semantic properties – can be detected in these representations. We use a dataset consisting of sentences with known chunk structure, and two linguistic intelligence datasets, whose solution relies on detecting chunks and their grammatical number, and respectively, their semantic roles. Through an approach involving indirect supervision, and through analyses of the performance on the tasks and of the internal representations built during learning, we show that information about chunks and their properties can be obtained from sentence embeddings.</abstract>
      <url hash="f5c25188">2024.blackboxnlp-1.3</url>
      <bibkey>nastase-merlo-2024-identifiable</bibkey>
    </paper>
    <paper id="4">
      <title>Learning, Forgetting, Remembering: Insights From Tracking <fixed-case>LLM</fixed-case> Memorization During Training</title>
      <author><first>Danny D.</first><last>Leybzon</last></author>
      <author><first>Corentin</first><last>Kervadec</last><affiliation>Universitat Pompeu Fabra</affiliation></author>
      <pages>43-57</pages>
      <abstract>Large language models memorize portions of their training data verbatim. Our findings indicate that models exhibit higher memorization rates both early on and at the very end of their training, with the lowest rates occurring midway through the process. This phenomenon can be attributed to the models retaining most of the examples memorized early on, while forgetting many more examples as training progresses. Interestingly, these forgotten examples are sometimes re-memorized later on, often undergoing cycles of forgetting and re-memorization. Notably, examples memorized early in training are more likely to remain consistently retained, suggesting that they become more firmly ’crystallized’ in the model’s representation. Based on these insights, we tentatively recommend placing data that is more likely to be sensitive in the middle stages of the training process.</abstract>
      <url hash="090002df">2024.blackboxnlp-1.4</url>
      <bibkey>leybzon-kervadec-2024-learning</bibkey>
    </paper>
    <paper id="5">
      <title>Language Models Linearly Represent Sentiment</title>
      <author><first>Oskar John</first><last>Hollinsworth</last><affiliation>FAR AI</affiliation></author>
      <author><first>Curt</first><last>Tigges</last><affiliation>EleutherAI Institute</affiliation></author>
      <author><first>Atticus</first><last>Geiger</last><affiliation>Pr(Ai)²R Group</affiliation></author>
      <author><first>Neel</first><last>Nanda</last><affiliation>Google DeepMind</affiliation></author>
      <pages>58-87</pages>
      <abstract>Sentiment is a pervasive feature in natural language text, yet it is an open question how sentiment is represented within Large Language Models (LLMs). In this study, we reveal that across a range of models, sentiment is represented linearly: a single direction in activation space mostly captures the feature across a range of tasks with one extreme for positive and the other for negative. In a causal analysis, we isolate this direction using interventions and show it is causal in both toy tasks and real world datasets such as Stanford Sentiment Treebank. We analyze the mechanisms that involve this direction and discover a phenomenon which we term the summarization motif: sentiment is not just represented on valenced words, but is also summarized at intermediate positions without inherent sentiment, such as punctuation and names. We show that in SST classification, ablating the sentiment direction across all tokens results in a drop in accuracy from 100% to 62% (vs. 50% random baseline), while ablating the summarized sentiment direction at comma positions alone produces close to half this result (reducing accuracy to 82%).</abstract>
      <url hash="a843ea94">2024.blackboxnlp-1.5</url>
      <bibkey>hollinsworth-etal-2024-language</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>LLM</fixed-case> Internal States Reveal Hallucination Risk Faced With a Query</title>
      <author><first>Ziwei</first><last>Ji</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Delong</first><last>Chen</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Etsuko</first><last>Ishii</last><affiliation>Amazon</affiliation></author>
      <author><first>Samuel</first><last>Cahyawijaya</last></author>
      <author><first>Yejin</first><last>Bang</last></author>
      <author><first>Bryan</first><last>Wilie</last></author>
      <author><first>Pascale</first><last>Fung</last><affiliation>HKUST</affiliation></author>
      <pages>88-104</pages>
      <abstract>The hallucination problem of Large Language Models (LLMs) significantly limits their reliability and trustworthiness. Humans have a self-awareness process that allows us to recognize what we don’t know when faced with queries. Inspired by this, our paper investigates whether LLMs can estimate their own hallucination risk before response generation. We analyze the internal mechanisms of LLMs broadly both in terms of training data sources and across 15 diverse Natural Language Generation (NLG) tasks, spanning over 700 datasets. Our empirical analysis reveals two key insights: (1) LLM internal states indicate whether they have seen the query in training data or not; and (2) LLM internal states show they are likely to hallucinate or not regarding the query. Our study explores particular neurons, activation layers, and tokens that play a crucial role in the LLM perception of uncertainty and hallucination risk. By a probing estimator, we leverage LLM self-assessment, achieving an average hallucination estimation accuracy of 84.32% at run time.</abstract>
      <url hash="66d6f28c">2024.blackboxnlp-1.6</url>
      <bibkey>ji-etal-2024-llm</bibkey>
    </paper>
    <paper id="7">
      <title>Enhancing adversarial robustness in Natural Language Inference using explanations</title>
      <author><first>Alexandros</first><last>Koulakos</last></author>
      <author><first>Maria</first><last>Lymperaiou</last></author>
      <author><first>Giorgos</first><last>Filandrianos</last><affiliation>National Technical University of Athens</affiliation></author>
      <author><first>Giorgos</first><last>Stamou</last><affiliation>National Technical University of Athens</affiliation></author>
      <pages>105-117</pages>
      <abstract>The surge of state-of-the-art transformer-based models has undoubtedly pushed the limits of NLP model performance, excelling in a variety of tasks. We cast the spotlight on the underexplored task of Natural Language Inference (NLI), since models trained on popular well-suited datasets are susceptible to adversarial attacks, allowing subtle input interventions to mislead the model. In this work, we validate the usage of natural language explanation as a model-agnostic defence strategy through extensive experimentation: only by fine-tuning a classifier on the explanation rather than premise-hypothesis inputs, robustness under various adversarial attacks is achieved in comparison to explanation-free baselines. Moreover, since there is no standard strategy for testing the semantic validity of the generated explanations, we research the correlation of widely used language generation metrics with human perception, in order for them to serve as a proxy towards robust NLI models. Our approach is resource-efficient and reproducible without significant computational limitations.</abstract>
      <url hash="fbba30f0">2024.blackboxnlp-1.7</url>
      <bibkey>koulakos-etal-2024-enhancing</bibkey>
    </paper>
    <paper id="8">
      <title><fixed-case>M</fixed-case>ulti<fixed-case>C</fixed-case>ontrievers: Analysis of Dense Retrieval Representations</title>
      <author><first>Seraphina</first><last>Goldfarb-Tarrant</last></author>
      <author><first>Pedro</first><last>Rodriguez</last><affiliation>Meta FAIR</affiliation></author>
      <author><first>Jane</first><last>Dwivedi-Yu</last><affiliation>Meta AI</affiliation></author>
      <author><first>Patrick</first><last>Lewis</last></author>
      <pages>118-139</pages>
      <abstract>Dense retrievers compress source documents into (possibly lossy) vector representations, yet there is little analysis of what information is lost versus preserved, and how it affects downstream tasks. We conduct the first analysis of the information captured by dense retrievers compared to the language models they are based on (e.g., BERT versus Contriever). We use 25 MultiBert checkpoints as randomized initialisations to train MultiContrievers, a set of 25 contriever models. We test whether specific pieces of information—such as genderand occupation—can be extracted from contriever vectors of wikipedia-like documents. We measure this extractability via information theoretic probing. We then examine the relationship of extractability to performance and gender bias, as well as the sensitivity of these results to many random initialisations and data shuffles. We find that (1) contriever models have significantly increased extractability, but extractability usually correlates poorly with benchmark performance 2) gender bias is present, but is not caused by the contriever representations 3) there is high sensitivity to both random initialisation and to data shuffle, suggesting that future retrieval research should test across a wider spread of both.</abstract>
      <url hash="be145a35">2024.blackboxnlp-1.8</url>
      <bibkey>goldfarb-tarrant-etal-2024-multicontrievers</bibkey>
    </paper>
    <paper id="9">
      <title>Can We Statically Locate Knowledge in Large Language Models? Financial Domain and Toxicity Reduction Case Studies</title>
      <author><first>Jordi</first><last>Armengol-Estapé</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Lingyu</first><last>Li</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Sebastian</first><last>Gehrmann</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Achintya</first><last>Gopal</last></author>
      <author><first>David S</first><last>Rosenberg</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Gideon S.</first><last>Mann</last></author>
      <author><first>Mark</first><last>Dredze</last><affiliation>Department of Computer Science, Whiting School of Engineering</affiliation></author>
      <pages>140-176</pages>
      <abstract>Current large language model (LLM) evaluations rely on benchmarks to assess model capabilities and their encoded knowledge. However, these evaluations cannot reveal where a model encodes its knowledge, and thus little is known about which weights contain specific information. We propose a method to statically (without forward or backward passes) locate topical knowledge in the weight space of an LLM, building on a prior insight that parameters can be decoded into interpretable tokens. If parameters can be mapped into the embedding space, it should be possible to directly search for knowledge via embedding similarity. We study the validity of this assumption across several LLMs for a variety of concepts in the financial domain and a toxicity detection setup. Our analysis yields an improved understanding of the promises and limitations of static knowledge location in real-world scenarios.</abstract>
      <url hash="8c3b7582">2024.blackboxnlp-1.9</url>
      <bibkey>armengol-estape-etal-2024-statically</bibkey>
    </paper>
    <paper id="10">
      <title>Attend First, Consolidate Later: On the Importance of Attention in Different <fixed-case>LLM</fixed-case> Layers</title>
      <author><first>Amit Ben</first><last>Artzy</last></author>
      <author><first>Roy</first><last>Schwartz</last><affiliation>Hebrew University, Hebrew University of Jerusalem</affiliation></author>
      <pages>177-184</pages>
      <abstract>In decoder-based LLMs, the representation of a given layer serves two purposes: as input to the next layer during the computation of the current token; and as input to the attention mechanism of future tokens. In this work, we show that the importance of the latter role might be overestimated. To show that, we start by manipulating the representations of previous tokens; e.g. by replacing the hidden states at some layer <tex-math>k</tex-math> with random vectors.Our experimenting with four LLMs and four tasks show that this operation often leads to small to negligible drop in performance. Importantly, this happens if the manipulation occurs in the top part of the model—<tex-math>k</tex-math> is in the final 30–50% of the layers. In contrast, doing the same manipulation in earlier layers might lead to chance level performance.We continue by switching the hidden state of certain tokens with hidden states of other tokens from another prompt; e.g., replacing the word “Italy” with “France” in “What is the capital of Italy?”. We find that when applying this switch in the top 1/3 of the model, the model ignores it (answering “Rome”). However if we apply it before, the model conforms to the switch (“Paris”).Our results hint at a two stage process in transformer-based LLMs: the first part gathers input from previous tokens, while the second mainly processes that information internally.</abstract>
      <url hash="72085541">2024.blackboxnlp-1.10</url>
      <bibkey>artzy-schwartz-2024-attend</bibkey>
    </paper>
    <paper id="11">
      <title>Enhancing Question Answering on Charts Through Effective Pre-training Tasks</title>
      <author><first>Ashim</first><last>Gupta</last></author>
      <author><first>Vivek</first><last>Gupta</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Shuo</first><last>Zhang</last></author>
      <author><first>Yujie</first><last>He</last><affiliation>Bloomberg L.P.</affiliation></author>
      <author><first>Ning</first><last>Zhang</last><affiliation>Bloomberg</affiliation></author>
      <author><first>Shalin</first><last>Shah</last><affiliation>Bloomberg</affiliation></author>
      <pages>185-192</pages>
      <abstract>To completely understand a document, the use of textual information is not enough. Understanding visual cues, such as layouts and charts, is also required. While the current state-of-the-art approaches for document understanding (both OCR-based and OCR-free) work well, a thorough analysis of their capabilities and limitations has not yet been performed. Therefore, in this work, we addresses the limitation of current VisualQA models when applied to charts and plots. To investigate shortcomings of the state-of-the-art models, we conduct a comprehensive behavioral analysis, using ChartQA as a case study. Our findings indicate that existing models particularly underperform in answering questions related to the chart’s structural and visual context, as well as numerical information. To address these issues, we propose three simple pre-training tasks that enforce the existing model in terms of both structural-visual knowledge, as well as its understanding of numerical questions. We evaluate our pre-trained model (called MatCha-v2) on three chart datasets - both extractive and abstractive question datasets - and observe that it achieves an average improvement of 1.7 % over the baseline model.</abstract>
      <url hash="607c97d7">2024.blackboxnlp-1.11</url>
      <bibkey>gupta-etal-2024-enhancing</bibkey>
    </paper>
    <paper id="12">
      <title>Faithfulness and the Notion of Adversarial Sensitivity in <fixed-case>NLP</fixed-case> Explanations</title>
      <author><first>Supriya</first><last>Manna</last></author>
      <author><first>Niladri</first><last>Sett</last><affiliation>SRM University</affiliation></author>
      <pages>193-206</pages>
      <abstract>Faithfulness is arguably the most critical metric to assess the reliability of explainable AI. In NLP, current methods for faithfulness evaluation are fraught with discrepancies and biases, often failing to capture the true reasoning of models. We introduce Adversarial Sensitivity as a novel approach to faithfulness evaluation, focusing on the explainer’s response when the model is under adversarial attack. Our method accounts for the faithfulness of explainers by capturing sensitivity to adversarial input changes. This work addresses significant limitations in existing evaluation techniques, and furthermore, quantifies faithfulness from a crucial yet underexplored paradigm.</abstract>
      <url hash="e8a96f75">2024.blackboxnlp-1.12</url>
      <bibkey>manna-sett-2024-faithfulness</bibkey>
    </paper>
    <paper id="13">
      <title>Transformers Learn Transition Dynamics when Trained to Predict <fixed-case>M</fixed-case>arkov Decision Processes</title>
      <author><first>Yuxi</first><last>Chen</last></author>
      <author><first>Suwei</first><last>Ma</last></author>
      <author><first>Tony</first><last>Dear</last><affiliation>Columbia University</affiliation></author>
      <author><first>Xu</first><last>Chen</last></author>
      <pages>207-216</pages>
      <abstract>Language models have displayed a wide array of capabilities, but the reason for their performance remains a topic of heated debate and investigation. Do these models simply recite the observed training data, or are they able to abstract away surface statistics and learn the underlying processes from which the data was generated? To investigate this question, we explore the capabilities of a GPT model in the context of Markov Decision Processes (MDPs), where the underlying transition dynamics and policies are not directly observed. The model is trained to predict the next state or action without any initial knowledge of the MDPs or the players’ policies. Despite this, we present evidence that the model develops emergent representations of the underlying parameters governing the MDPs.</abstract>
      <url hash="d9385db1">2024.blackboxnlp-1.13</url>
      <bibkey>chen-etal-2024-transformers</bibkey>
    </paper>
    <paper id="14">
      <title>On the alignment of <fixed-case>LM</fixed-case> language generation and human language comprehension</title>
      <author><first>Lena Sophia</first><last>Bolliger</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Patrick</first><last>Haller</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Lena Ann</first><last>Jäger</last><affiliation>University of Zurich and Universität Potsdam</affiliation></author>
      <pages>217-231</pages>
      <abstract>Previous research on the predictive power (PP) of surprisal and entropy has focused on determining which language models (LMs) generate estimates with the highest PP on reading times, and examining for which populations the PP is strongest. In this study, we leverage eye movement data on texts that were generated using a range of decoding strategies with different LMs. We then extract the transition scores that reflect the models’ production rather than comprehension effort. This allows us to investigate the alignment of LM language production and human language comprehension. Our findings reveal that there are differences in the strength of the alignment between reading behavior and certain LM decoding strategies and that this alignment further reflects different stages of language understanding (early, late, or global processes). Although we find lower PP of transition-based measures compared to surprisal and entropy for most decoding strategies, our results provide valuable insights into which decoding strategies impose less processing effort for readers. Our code is available via https://github.com/DiLi-Lab/LM-human-alignment.</abstract>
      <url hash="44c89ed7">2024.blackboxnlp-1.14</url>
      <bibkey>bolliger-etal-2024-alignment</bibkey>
    </paper>
    <paper id="15">
      <title>An Adversarial Example for Direct Logit Attribution: Memory Management in <fixed-case>GELU</fixed-case>-4<fixed-case>L</fixed-case></title>
      <author><first>Jett</first><last>Janiak</last><affiliation>AI Safety Camp</affiliation></author>
      <author><first>Can</first><last>Rager</last></author>
      <author><first>James</first><last>Dao</last></author>
      <author><first>Yeu-Tong</first><last>Lau</last></author>
      <pages>232-237</pages>
      <abstract>Prior work suggests that language models manage the limited bandwidth of the residual stream through a “memory management” mechanism, where certain attention heads and MLP layers clear residual stream directions set by earlier layers. Our study provides concrete evidence for this erasure phenomenon in a 4-layer transformer, identifying heads that consistently remove the output of earlier heads. We further demonstrate that direct logit attribution (DLA), a common technique for interpreting the output of intermediate transformer layers, can show misleading results by not accounting for erasure.</abstract>
      <url hash="16c50e3c">2024.blackboxnlp-1.15</url>
      <bibkey>janiak-etal-2024-adversarial</bibkey>
    </paper>
    <paper id="16">
      <title>Uncovering Syllable Constituents in the Self-Attention-Based Speech Representations of Whisper</title>
      <author><first>Erfan</first><last>A Shams</last><affiliation>University College Dublin</affiliation></author>
      <author><first>Iona</first><last>Gessinger</last></author>
      <author><first>Julie</first><last>Carson-Berndsen</last><affiliation>University College Dublin</affiliation></author>
      <pages>238-247</pages>
      <abstract>As intuitive units of speech, syllables have been widely studied in linguistics. A syllable can be defined as a three-constituent unit with a vocalic centre surrounded by two (in some languages optional) consonant clusters. Syllables are also used to design automatic speech recognition (ASR) models. The significance of knowledge-driven syllable-based tokenisation in ASR over data-driven byte-pair encoding has often been debated. However, the emergence of transformer-based ASR models employing self-attention (SA) overshadowed this debate. These models learn the nuances of speech from large corpora without prior knowledge of the domain; yet, they are not interpretable by design. Consequently, it is not clear if the recent performance improvements are related to the extraction of human-interpretable knowledge. We probe such models for syllable constituents and use an SA head pruning method to assess the relevance of the SA weights. We also investigate the role of vowel identification in syllable constituent probing. Our findings show that the general features of syllable constituents are extracted in the earlier layers of the model and the syllable-related features mostly depend on the temporal knowledge incorporated in specific SA heads rather than on vowel identification.</abstract>
      <url hash="631a5569">2024.blackboxnlp-1.16</url>
      <bibkey>a-shams-etal-2024-uncovering</bibkey>
    </paper>
    <paper id="17">
      <title>Recurrent Neural Networks Learn to Store and Generate Sequences using Non-Linear Representations</title>
      <author><first>Róbert</first><last>Csordás</last><affiliation>Stanford University</affiliation></author>
      <author><first>Christopher</first><last>Potts</last><affiliation>Stanford University</affiliation></author>
      <author><first>Christopher D</first><last>Manning</last><affiliation>Computer Science Department, Stanford University</affiliation></author>
      <author><first>Atticus</first><last>Geiger</last><affiliation>Pr(Ai)²R Group</affiliation></author>
      <pages>248-262</pages>
      <abstract>The Linear Representation Hypothesis (LRH) states that neural networks learn to encode concepts as directions in activation space, and a strong version of the LRH states that models learn only such encodings. In this paper, we present a counterexample to this strong LRH: when trained to repeat an input token sequence, gated recurrent neural networks (RNNs) learn to represent the token at each position with a particular order of magnitude, rather than a direction. These representations have layered features that are impossible to locate in distinct linear subspaces. To show this, we train interventions to predict and manipulate tokens by learning the scaling factor corresponding to each sequence position. These interventions indicate that the smallest RNNs find only this magnitude-based solution, while larger RNNs have linear representations. These findings strongly indicate that interpretability research should not be confined by the LRH.</abstract>
      <url hash="06d97eec">2024.blackboxnlp-1.17</url>
      <bibkey>csordas-etal-2024-recurrent</bibkey>
    </paper>
    <paper id="18">
      <title>Log Probabilities Are a Reliable Estimate of Semantic Plausibility in Base and Instruction-Tuned Language Models</title>
      <author><first>Carina</first><last>Kauf</last></author>
      <author><first>Emmanuele</first><last>Chersoni</last><affiliation>The Hong Kong Polytechnic University</affiliation></author>
      <author><first>Alessandro</first><last>Lenci</last><affiliation>University of Pisa</affiliation></author>
      <author><first>Evelina</first><last>Fedorenko</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Anna A</first><last>Ivanova</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>263-277</pages>
      <abstract>Semantic plausibility (e.g. knowing that “the actor won the award” is more likely than “the actor won the battle”) serves as an effective proxy for general world knowledge. Language models (LMs) capture vast amounts of world knowledge by learning distributional patterns in text, accessible via log probabilities (LogProbs) they assign to plausible vs. implausible outputs. The new generation of instruction-tuned LMs can now also provide explicit estimates of plausibility via prompting. Here, we evaluate the effectiveness of LogProbs and basic prompting to measure semantic plausibility, both in single-sentence minimal pairs (Experiment 1) and short context-dependent scenarios (Experiment 2). We find that (i) in both base and instruction-tuned LMs, LogProbs offers a more reliable measure of semantic plausibility than direct zero-shot prompting, which yields inconsistent and often poor results; (ii) instruction-tuning generally does not alter the sensitivity of LogProbs to semantic plausibility (although sometimes decreases it); (iii) across models, context mostly modulates LogProbs in expected ways, as measured by three novel metrics of context-sensitive plausibility and their match to explicit human plausibility judgments. We conclude that, even in the era of prompt-based evaluations, LogProbs constitute a useful metric of semantic plausibility, both in base and instruction-tuned LMs.</abstract>
      <url hash="21fc08d3">2024.blackboxnlp-1.18</url>
      <bibkey>kauf-etal-2024-log</bibkey>
    </paper>
    <paper id="19">
      <title>Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2</title>
      <author><first>Tom</first><last>Lieberum</last><affiliation>Google</affiliation></author>
      <author><first>Senthooran</first><last>Rajamanoharan</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Arthur</first><last>Conmy</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Lewis</first><last>Smith</last><affiliation>Google</affiliation></author>
      <author><first>Nicolas</first><last>Sonnerat</last><affiliation>DeepMind</affiliation></author>
      <author><first>Vikrant</first><last>Varma</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Janos</first><last>Kramar</last><affiliation>DeepMind</affiliation></author>
      <author><first>Anca</first><last>Dragan</last><affiliation>University of California Berkeley</affiliation></author>
      <author><first>Rohin</first><last>Shah</last><affiliation>DeepMind</affiliation></author>
      <author><first>Neel</first><last>Nanda</last><affiliation>Google DeepMind</affiliation></author>
      <pages>278-300</pages>
      <abstract>Sparse autoencoders (SAEs) are an unsupervised method for learning a sparse decomposition of a neural network’s latent representations into seemingly interpretable features.Despite recent excitement about their potential, research applications outside of industry are limited by the high cost of training a comprehensive suite of SAEs.In this work, we introduce Gemma Scope, an open suite of JumpReLU SAEs trained on all layers and sub-layers of Gemma 2 2B and 9B and select layers of Gemma 2 27B base models.We primarily train SAEs on the Gemma 2 pre-trained models, but additionally release SAEs trained on instruction-tuned Gemma 2 9B for comparison.We evaluate the quality of each SAE on standard metrics and release these results.We hope that by releasing these SAE weights, we can help make more ambitious safety and interpretability research easier for the community. Weights and a tutorial can be found at <url>https://huggingface.co/google/gemma-scope</url> and an interactive demo can be found at <url>https://neuronpedia.org/gemma-scope</url>.</abstract>
      <url hash="75eb68f9">2024.blackboxnlp-1.19</url>
      <bibkey>lieberum-etal-2024-gemma</bibkey>
    </paper>
    <paper id="20">
      <title>Self-Assessment Tests are Unreliable Measures of <fixed-case>LLM</fixed-case> Personality</title>
      <author><first>Akshat</first><last>Gupta</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Xiaoyang</first><last>Song</last><affiliation>University of Michigan - Ann Arbor</affiliation></author>
      <author><first>Gopala</first><last>Anumanchipalli</last><affiliation>University of California, Berkeley</affiliation></author>
      <pages>301-314</pages>
      <abstract>As large language models (LLM) evolve in their capabilities, various recent studies have tried to quantify their behavior using psychological tools created to study human behavior. One such example is the measurement of “personality” of LLMs using self-assessment personality tests developed to measure human personality. Yet almost none of these works verify the applicability of these tests on LLMs. In this paper, we analyze the reliability of LLM personality scores obtained from self-assessment personality tests using two simple experiments. We first introduce the property of <i>prompt sensitivity</i>, where three semantically equivalent prompts representing three intuitive ways of administering self-assessment tests on LLMs are used to measure the personality of the same LLM. We find that all three prompts lead to very different personality scores, a difference that is statistically significant for all traits in a large majority of scenarios. We then introduce the property of <i>option-order symmetry</i> for personality measurement of LLMs. Since most of the self-assessment tests exist in the form of multiple choice question (MCQ) questions, we argue that the scores should also be robust to not just the prompt template but also the order in which the options are presented. This test unsurprisingly reveals that the self-assessment test scores are not robust to the order of the options. These simple tests, done on ChatGPT and three Llama2 models of different sizes, show that self-assessment personality tests created for humans are unreliable measures of personality in LLMs.</abstract>
      <url hash="91303c89">2024.blackboxnlp-1.20</url>
      <bibkey>gupta-etal-2024-self</bibkey>
    </paper>
    <paper id="21">
      <title>How Language Models Prioritize Contextual Grammatical Cues?</title>
      <author><first>Hamidreza</first><last>Amirzadeh</last></author>
      <author><first>Afra</first><last>Alishahi</last><affiliation>Tilburg University</affiliation></author>
      <author><first>Hosein</first><last>Mohebbi</last></author>
      <pages>315-336</pages>
      <abstract>Transformer-based language models have shown an excellent ability to effectively capture and utilize contextual information. Although various analysis techniques have been used to quantify and trace the contribution of single contextual cues to a target task such as subject-verb agreement or coreference resolution, scenarios in which multiple relevant cues are available in the context remain underexplored.In this paper, we investigate how language models handle gender agreement when multiple gender cue words are present, each capable of independently disambiguating a target gender pronoun. We analyze two widely used Transformer-based models: BERT, an encoder-based, and GPT-2, a decoder-based model.Our analysis employs two complementary approaches: context mixing analysis, which tracks information flow within the model, and a variant of activation patching, which measures the impact of cues on the model’s prediction. We find that BERT tends to prioritize the first cue in the context to form both the target word representations and the model’s prediction, while GPT-2 relies more on the final cue. Our findings reveal striking differences in how encoder-based and decoder-based models prioritize and use contextual information for their predictions.</abstract>
      <url hash="f475a580">2024.blackboxnlp-1.21</url>
      <bibkey>amirzadeh-etal-2024-language</bibkey>
    </paper>
    <paper id="22">
      <title>Copy Suppression: Comprehensively Understanding a Motif in Language Model Attention Heads</title>
      <author><first>Callum Stuart</first><last>McDougall</last></author>
      <author><first>Arthur</first><last>Conmy</last><affiliation>Google DeepMind</affiliation></author>
      <author><first>Cody</first><last>Rushing</last><affiliation>University of Texas at Austin</affiliation></author>
      <author><first>Thomas</first><last>McGrath</last><affiliation>Google</affiliation></author>
      <author><first>Neel</first><last>Nanda</last><affiliation>Google DeepMind</affiliation></author>
      <pages>337-363</pages>
      <abstract>We present the copy suppression motif: an algorithm implemented by attention heads in large language models that reduces loss.If i) language model components in earlier layers predict a certain token, ii) this token appears earlier in the context and iii) later attention heads in the model suppress prediction of the token, then this is copy suppression. To show the importance of copy suppression, we focus on reverse-engineering attention head 10.7 (L10H7) in GPT-2 Small. This head suppresses naive copying behavior which improves overall model calibration, which explains why multiple prior works studying certain narrow tasks found negative heads that systematically favored the wrong answer. We uncover the mechanism that the negative heads use for copy suppression with weights-based evidence and are able to explain 76.9% of the impact of L10H7 in GPT-2 Small, by this motif alone.To the best of our knowledge, this is the most comprehensive description of the complete role of a component in a language model to date. One major effect of copy suppression is its role in self-repair. Self-repair refers to how ablating crucial model components results in downstream neural network parts compensating for this ablation. Copy suppression leads to self-repair: if an initial overconfident copier is ablated, then there is nothing to suppress. We show that self-repair is implemented by several mechanisms, one of which is copy suppression, which explains 39% of the behavior in a narrow task. Interactive visualizations of the copy suppression phenomena may be seen at our web app https://copy-suppression.streamlit.app/.</abstract>
      <url hash="307f4989">2024.blackboxnlp-1.22</url>
      <bibkey>mcdougall-etal-2024-copy</bibkey>
    </paper>
    <paper id="23">
      <title><fixed-case>W</fixed-case>ell<fixed-case>D</fixed-case>unn: On the Robustness and Explainability of Language Models and Large Language Models in Identifying Wellness Dimensions</title>
      <author><first>Seyedali</first><last>Mohammadi</last><affiliation>University of Maryland, Baltimore County</affiliation></author>
      <author><first>Edward</first><last>Raff</last><affiliation>University of Maryland, Baltimore County and Booz Allen Hamilton</affiliation></author>
      <author><first>Jinendra</first><last>Malekar</last></author>
      <author><first>Vedant</first><last>Palit</last><affiliation>Indian Institute of Technology, Kharagpur</affiliation></author>
      <author><first>Francis</first><last>Ferraro</last><affiliation>University of Maryland, Baltimore County</affiliation></author>
      <author><first>Manas</first><last>Gaur</last><affiliation>University of Maryland Baltimore County</affiliation></author>
      <pages>364-388</pages>
      <abstract>Language Models (LMs) are being proposed for mental health applications where the heightened risk of adverse outcomes means predictive performance may not be a sufficient litmus test of a model’s utility in clinical practice. A model that can be trusted for practice should have a correspondence between explanation and clinical determination, yet no prior research has examined the attention fidelity of these models and their effect on ground truth explanations. We introduce an evaluation design that focuses on the robustness and explainability of LMs in identifying Wellness Dimensions (WDs). We focus on two existing mental health and well-being datasets: (a) Multi-label Classification-based MultiWD, and (b) WellXplain for evaluating attention mechanism veracity against expert-labeled explanations. The labels are based on Halbert Dunn’s theory of wellness, which gives grounding to our evaluation. We reveal four surprising results about LMs/LLMs: (1) Despite their human-like capabilities, GPT-3.5/4 lag behind RoBERTa, and MedAlpaca, a fine-tuned LLM on WellXplain fails to deliver any remarkable improvements in performance or explanations. (2) Re-examining LMs’ predictions based on a confidence-oriented loss function reveals a significant performance drop. (3) Across all LMs/LLMs, the alignment between attention and explanations remains low, with LLMs scoring a dismal 0.0. (4) Most mental health-specific LMs/LLMs overlook domain-specific knowledge and undervalue explanations, causing these discrepancies. This study highlights the need for further research into their consistency and explanations in mental health and well-being.</abstract>
      <url hash="7047917d">2024.blackboxnlp-1.23</url>
      <bibkey>mohammadi-etal-2024-welldunn</bibkey>
    </paper>
    <paper id="24">
      <title>Do Metadata and Appearance of the Retrieved Webpages Affect <fixed-case>LLM</fixed-case>’s Reasoning in Retrieval-Augmented Generation?</title>
      <author><first>Cheng-Han</first><last>Chiang</last></author>
      <author><first>Hung-yi</first><last>Lee</last><affiliation>National Taiwan University</affiliation></author>
      <pages>389-406</pages>
      <abstract>Large language models (LLMs) answering questions with retrieval-augmented generation (RAG) can face conflicting evidence in the retrieved documents. While prior works study how textual features like perplexity and readability influence the persuasiveness of evidence, humans consider more than textual content when evaluating conflicting information on the web. In this paper, we focus on the following question: When two webpages contain conflicting information to answer a question, does non-textual information affect the LLM’s reasoning and answer? We consider three types of non-textual information: (1) the webpage’s publication time, (2) the source where the webpage is from, and (3) the appearance of the webpage. We give the LLM a Yes/No question and two conflicting webpages that support yes and no, respectively. We exchange the non-textual information in the two webpages to see if the LLMs tend to use the information from a newer, more reliable, and more visually appealing webpage. We find that changing the publication time of the webpage can change the answer for most LLMs, but changing the webpage’s source merely affects the LLM’s answer. We also reveal that the webpage’s appearance has a strong causal effect on Claude-3’s answers.The codes and datasets used in the paper are available at https://github.com/d223302/rag-metadata.</abstract>
      <url hash="23b0cdf2">2024.blackboxnlp-1.24</url>
      <bibkey>chiang-lee-2024-metadata</bibkey>
    </paper>
    <paper id="25">
      <title>Attribution Patching Outperforms Automated Circuit Discovery</title>
      <author><first>Aaquib</first><last>Syed</last></author>
      <author><first>Can</first><last>Rager</last></author>
      <author><first>Arthur</first><last>Conmy</last><affiliation>Google DeepMind</affiliation></author>
      <pages>407-416</pages>
      <abstract>Automated interpretability research has recently attracted attention as a potential research direction that could scale explanations of neural network behavior to large models. Existing automated circuit discovery work applies activation patching to identify subnetworks responsible for solving specific tasks (circuits). In this work, we show that a simple method based on attribution patching outperforms all existing methods while requiring just two forward passes and a backward pass. We apply a linear approximation to activation patching to estimate the importance of each edge in the computational subgraph. Using this approximation, we prune the least important edges of the network. We survey the performance and limitations of this method, finding that averaged over all tasks our method has greater AUC from circuit recovery than other methods.</abstract>
      <url hash="80a20540">2024.blackboxnlp-1.25</url>
      <bibkey>syed-etal-2024-attribution</bibkey>
    </paper>
    <paper id="26">
      <title>Pruning for Protection: Increasing Jailbreak Resistance in Aligned <fixed-case>LLM</fixed-case>s Without Fine-Tuning</title>
      <author><first>Adib</first><last>Hasan</last><affiliation>Massachusetts Institute of Technology</affiliation></author>
      <author><first>Ileana</first><last>Rugina</last></author>
      <author><first>Alex</first><last>Wang</last></author>
      <pages>417-430</pages>
      <abstract>This paper investigates the impact of model compression on the way Large Language Models (LLMs) process prompts, particularly concerning jailbreak resistance. We show that moderate WANDA pruning can enhance resistance to jailbreaking attacks without fine-tuning, while maintaining performance on standard benchmarks. To systematically evaluate this safety enhancement, we introduce a dataset of 225 harmful tasks across five categories. Our analysis of LLaMA-2 Chat, Vicuna 1.3, and Mistral Instruct v0.2 reveals that pruning benefits correlate with initial model safety levels. We interpret these results by examining changes in attention patterns and perplexity shifts, demonstrating that pruned models exhibit sharper attention and increased sensitivity to artificial jailbreak constructs. We extend our evaluation to the AdvBench harmful behavior tasks and the GCG attack method. We find that LLaMA-2 is much safer on AdvBench prompts than on our dataset when evaluated with manual jailbreak attempts, and that pruning is effective against both automated attacks and manual jailbreaking on Advbench.</abstract>
      <url hash="9a4b051a">2024.blackboxnlp-1.26</url>
      <bibkey>hasan-etal-2024-pruning</bibkey>
    </paper>
    <paper id="27">
      <title><fixed-case>I</fixed-case>v<fixed-case>RA</fixed-case>: A Framework to Enhance Attention-Based Explanations for Language Models with Interpretability-Driven Training</title>
      <author><first>Sean</first><last>Xie</last></author>
      <author><first>Soroush</first><last>Vosoughi</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Saeed</first><last>Hassanpour</last><affiliation>Dartmouth College</affiliation></author>
      <pages>431-451</pages>
      <abstract>Attention has long served as a foundational technique for generating explanations. With the recent developments made in Explainable AI (XAI), the multi-faceted nature of interpretability has become more apparent. Can attention, as an explanation method, be adapted to meet the diverse needs that our expanded understanding of interpretability demands? In this work, we aim to address this question by introducing IvRA, a framework designed to directly train a language model’s attention distribution through regularization to produce attribution explanations that align with interpretability criteria such as simulatability, faithfulness, and consistency. Our extensive experimental analysis demonstrates that IvRA outperforms existing methods in guiding language models to generate explanations that are simulatable, faithful, and consistent, in tandem with their predictions. Furthermore, we perform ablation studies to verify the robustness of IvRA across various experimental settings and to shed light on the interactions among different interpretability criteria.</abstract>
      <url hash="581524d7">2024.blackboxnlp-1.27</url>
      <bibkey>xie-etal-2024-ivra</bibkey>
    </paper>
    <paper id="28">
      <title>Counterfactuals As a Means for Evaluating Faithfulness of Attribution Methods in Autoregressive Language Models</title>
      <author><first>Sepehr</first><last>Kamahi</last></author>
      <author><first>Yadollah</first><last>Yaghoobzadeh</last><affiliation>University of Tehran</affiliation></author>
      <pages>452-468</pages>
      <abstract>Despite the widespread adoption of autoregressive language models, explainability evaluation research has predominantly focused on span infilling and masked language models. Evaluating the faithfulness of an explanation method—how accurately it explains the inner workings and decision-making of the model—is challenging because it is difficult to separate the model from its explanation. Most faithfulness evaluation techniques corrupt or remove input tokens deemed important by a particular attribution (feature importance) method and observe the resulting change in the model’s output. However, for autoregressive language models, this approach creates out-of-distribution inputs due to their next-token prediction training objective. In this study, we propose a technique that leverages counterfactual generation to evaluate the faithfulness of attribution methods for autoregressive language models. Our technique generates fluent, in-distribution counterfactuals, making the evaluation protocol more reliable.</abstract>
      <url hash="9f01cd03">2024.blackboxnlp-1.28</url>
      <bibkey>kamahi-yaghoobzadeh-2024-counterfactuals</bibkey>
    </paper>
    <paper id="29">
      <title>Investigating Layer Importance in Large Language Models</title>
      <author><first>Yang</first><last>Zhang</last></author>
      <author><first>Yanfei</first><last>Dong</last><affiliation>PayPal Inc. and national university of singaore, National University of Singapore</affiliation></author>
      <author><first>Kenji</first><last>Kawaguchi</last><affiliation>National University of Singapore</affiliation></author>
      <pages>469-479</pages>
      <abstract>Large language models (LLMs) have gained increasing attention due to their prominent ability to understand and process texts. Nevertheless, LLMs largely remain opaque. The lack of understanding of LLMs has obstructed the deployment in safety-critical scenarios and hindered the development of better models. In this study, we advance the understanding of LLM by investigating the significance of individual layers in LLMs. We propose an efficient sampling method to faithfully evaluate the importance of layers using Shapley values, a widely used explanation framework in feature attribution and data valuation. In addition, we conduct layer ablation experiments to assess the performance degradation resulting from the exclusion of specific layers. Our findings reveal the existence of cornerstone layers, wherein certain early layers can exhibit a dominant contribution over others. Removing one cornerstone layer leads to a drastic collapse of the model performance, often reducing it to random guessing. Conversely, removing non-cornerstone layers results in only marginal performance changes. This study identifies cornerstone layers in LLMs and underscores their critical role for future research.</abstract>
      <url hash="22202972">2024.blackboxnlp-1.29</url>
      <bibkey>zhang-etal-2024-investigating</bibkey>
    </paper>
    <paper id="30">
      <title>Mechanistic?</title>
      <author><first>Naomi</first><last>Saphra</last><affiliation>Harvard University</affiliation></author>
      <author><first>Sarah</first><last>Wiegreffe</last><affiliation>Allen Institute for Artificial Intelligence and University of Washington</affiliation></author>
      <pages>480-498</pages>
      <abstract>The rise of the term “mechanistic interpretability” has accompanied increasing interest in understanding neural models—particularly language models. However, this jargon has also led to a fair amount of confusion. So, what does it mean to be mechanistic? We describe four uses of the term in interpretability research. The most narrow technical definition requires a claim of causality, while a broader technical definition allows for any exploration of a model’s internals. However, the term also has a narrow cultural definition describing a cultural movement. To understand this semantic drift, we present a history of the NLP interpretability community and the formation of the separate, parallel mechanistic interpretability community. Finally, we discuss the broad cultural definition—encompassing the entire field of interpretability—and why the traditional NLP interpretability community has come to embrace it. We argue that the polysemy of “mechanistic” is the product of a critical divide within the interpretability community.</abstract>
      <url hash="42772d31">2024.blackboxnlp-1.30</url>
      <bibkey>saphra-wiegreffe-2024-mechanistic</bibkey>
    </paper>
    <paper id="31">
      <title>Toward the Evaluation of Large Language Models Considering Score Variance across Instruction Templates</title>
      <author><first>Yusuke</first><last>Sakai</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Adam</first><last>Nohejl</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <author><first>Jiangnan</first><last>Hang</last></author>
      <author><first>Hidetaka</first><last>Kamigaito</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology, Japan</affiliation></author>
      <pages>499-529</pages>
      <abstract>The natural language understanding (NLU) performance of large language models (LLMs) has been evaluated across various tasks and datasets. The existing evaluation methods, however, do not take into account the variance in scores due to differences in prompts, which leads to unfair evaluation and comparison of NLU performance. Moreover, evaluation designed for specific prompts is inappropriate for instruction tuning, which aims to perform well with any prompt. It is therefore necessary to find a way to measure NLU performance in a fair manner, considering score variance between different instruction templates. In this study, we provide English and Japanese cross-lingual datasets for evaluating the NLU performance of LLMs, which include multiple instruction templates for fair evaluation of each task, along with regular expressions to constrain the output format. Furthermore, we propose the Sharpe score as an evaluation metric that takes into account the variance in scores between templates. Comprehensive analysis of English and Japanese LLMs reveals that the high variance among templates has a significant impact on the fair evaluation of LLMs.</abstract>
      <url hash="8d0f757d">2024.blackboxnlp-1.31</url>
      <bibkey>sakai-etal-2024-toward</bibkey>
    </paper>
    <paper id="32">
      <title>Accelerating Sparse Autoencoder Training via Layer-Wise Transfer Learning in Large Language Models</title>
      <author><first>Davide</first><last>Ghilardi</last></author>
      <author><first>Federico</first><last>Belotti</last></author>
      <author><first>Marco</first><last>Molinari</last><affiliation>LSE.AI</affiliation></author>
      <author><first>Jaehyuk</first><last>Lim</last></author>
      <pages>530-550</pages>
      <abstract>Sparse AutoEncoders (SAEs) have gained popularity as a tool for enhancing the interpretability of Large Language Models (LLMs). However, training SAEs can be computationally intensive, especially as model complexity grows. In this study, the potential of transfer learning to accelerate SAEs training is explored by capitalizing on the shared representations found across adjacent layers of LLMs. Our experimental results demonstrate that fine-tuning SAEs using pre-trained models from nearby layers not only maintains but often improves the quality of learned representations, while significantly accelerating convergence. These findings indicate that the strategic reuse of pretrained SAEs is a promising approach, particularly in settings where computational resources are constrained.</abstract>
      <url hash="7df7d2db">2024.blackboxnlp-1.32</url>
      <bibkey>ghilardi-etal-2024-accelerating</bibkey>
    </paper>
    <paper id="33">
      <title>Wrapper Boxes for Faithful Attribution of Model Predictions to Training Data</title>
      <author><first>Yiheng</first><last>Su</last></author>
      <author><first>Junyi Jessy</first><last>Li</last><affiliation>University of Texas, Austin</affiliation></author>
      <author><first>Matthew</first><last>Lease</last><affiliation>Amazon and University of Texas at Austin</affiliation></author>
      <pages>551-576</pages>
      <abstract>Can we preserve the accuracy of neural models while also providing faithful explanations of model decisions to training data? We propose a “wrapper box” pipeline: training a neural model as usual and then using its learned feature representation in classic, interpretable models to perform prediction. Across seven language models of varying sizes, including four large language models (LLMs), two datasets at different scales, three classic models, and four evaluation metrics, we first show that the predictive performance of wrapper classic models is largely comparable to the original neural models. Because classic models are transparent, each model decision is determined by a known set of training examples that can be directly shown to users. Our pipeline thus preserves the predictive performance of neural language models while faithfully attributing classic model decisions to training data. Among other use cases, such attribution enables model decisions to be contested based on responsible training instances. Compared to prior work, our approach achieves higher coverage and correctness in identifying which training data to remove to change a model decision. To reproduce findings, our source code is online at: https://github.com/SamSoup/WrapperBox.</abstract>
      <url hash="6cb012cd">2024.blackboxnlp-1.33</url>
      <bibkey>su-etal-2024-wrapper</bibkey>
    </paper>
    <paper id="34">
      <title>Multi-property Steering of Large Language Models with Dynamic Activation Composition</title>
      <author><first>Daniel</first><last>Scalena</last><affiliation>University of Milan - Bicocca and University of Groningen</affiliation></author>
      <author><first>Gabriele</first><last>Sarti</last><affiliation>University of Groningen</affiliation></author>
      <author><first>Malvina</first><last>Nissim</last><affiliation>University of Groningen</affiliation></author>
      <pages>577-603</pages>
      <abstract>Activation steering methods were shown to be effective in conditioning language model generation by additively intervening over models’ intermediate representations. However, the evaluation of these techniques has so far been limited to single conditioning properties and synthetic settings. In this work, we conduct a comprehensive evaluation of various activation steering strategies, highlighting the property-dependent nature of optimal parameters to ensure a robust effect throughout generation. To address this issue, we propose Dynamic Activation Composition, an information-theoretic approach to modulate the steering intensity of one or more properties throughout generation. Our experiments on multi-property steering show that our method successfully maintains high conditioning while minimizing the impact of conditioning on generation fluency.</abstract>
      <url hash="8ffd1cb2">2024.blackboxnlp-1.34</url>
      <bibkey>scalena-etal-2024-multi</bibkey>
    </paper>
    <paper id="35">
      <title>Probing Language Models on Their Knowledge Source</title>
      <author><first>Zineddine</first><last>Tighidet</last></author>
      <author><first>Jiali</first><last>Mei</last><affiliation>BNP Paribas</affiliation></author>
      <author><first>Benjamin</first><last>Piwowarski</last><affiliation>CNRS / ISIR, Sorbonne Université and CNRS</affiliation></author>
      <author><first>Patrick</first><last>Gallinari</last><affiliation>Criteo AI Lab and Sorbonne Universite</affiliation></author>
      <pages>604-614</pages>
      <abstract>Large Language Models (LLMs) often encounter conflicts between their learned, internal (parametric knowledge, PK) and external knowledge provided during inference (contextual knowledge, CK). Understanding how LLMs models prioritize one knowledge source over the other remains a challenge. In this paper, we propose a novel probing framework to explore the mechanisms governing the selection between PK and CK in LLMs. Using controlled prompts designed to contradict the model’s PK, we demonstrate that specific model activations are indicative of the knowledge source employed. We evaluate this framework on various LLMs of different sizes and demonstrate that mid-layer activations, particularly those related to relations in the input, are crucial in predicting knowledge source selection, paving the way for more reliable models capable of handling knowledge conflicts effectively.</abstract>
      <url hash="3cb42948">2024.blackboxnlp-1.35</url>
      <bibkey>tighidet-etal-2024-probing</bibkey>
    </paper>
  </volume>
</collection>
