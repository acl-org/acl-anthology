<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.nejlt">
  <volume id="1" ingest-date="2024-08-06" type="proceedings">
    <meta>
      <booktitle>Northern European Journal of Language Technology, Volume 9</booktitle>
      <editor><first>Leon</first><last>Derczynski</last></editor>
      <publisher>Linköping University Electronic Press</publisher>
      <address>Linköping, Sweden</address>
      <doi>https://doi.org/10.3384/nejlt.2000-1533.9.1</doi>
      <year>2023</year>
      <url hash="9c75cf6f">2023.nejlt-1</url>
      <venue>nejlt</venue>
    </meta>
    <paper id="1">
      <title>Resource papers as registered reports: a proposal</title>
      <author><first>Emiel</first><last>van Miltenburg</last></author>
      <abstract>This is a proposal for publishing resource papers as registered reports in the Northern European Journal of Language Technology. The idea is that authors write a data collection plan with a full data statement, to the extent that it can be written before data collection starts. Once the proposal is approved, publication of the final resource paper is guaranteed, as long as the data collection plan is followed (modulo reasonable changes due to unforeseen circumstances). This proposal changes the reviewing process from an antagonistic to a collaborative enterprise, and hopefully encourages NLP resources to develop and publish more high-quality datasets. The key advantage of this proposal is that it helps to promote responsible resource development (through constructive peer review) and to avoid research waste.</abstract>
      <url hash="083f4d3e">2023.nejlt-1.1</url>
      <doi>https://doi.org/10.3384/nejlt.2000-1533.2023.4884</doi>
      <bibkey>van-miltenburg-2023-resource</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>PARSEME</fixed-case> Meets <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependencies: Getting on the Same Page in Representing Multiword Expressions</title>
      <author><first>Agata</first><last>Savary</last></author>
      <author><first>Sara</first><last>Stymne</last></author>
      <author><first>Verginica Barbu</first><last>Mititelu</last></author>
      <author><first>Nathan</first><last>Schneider</last></author>
      <author><first>Carlos</first><last>Ramisch</last></author>
      <author><first>Joakim</first><last>Nivre</last></author>
      <abstract>Multiword expressions (MWEs) are challenging and pervasive phenomena whose idiosyncratic properties show notably at the levels of lexicon, morphology, and syntax. Thus, they should best be annotated jointly with morphosyntax. We discuss two multilingual initiatives, Universal Dependencies and PARSEME, addressing these annotation layers in cross-lingually unified ways. We compare the annotation principles of these initiatives with respect to MWEs, and we put forward a roadmap towards their gradual unification. The expected outcomes are more consistent treebanking and higher universality in modeling idiosyncrasy.</abstract>
      <url hash="85bcb4cc">2023.nejlt-1.2</url>
      <doi>https://doi.org/10.3384/nejlt.2000-1533.2023.4453</doi>
      <bibkey>savary-etal-2023-parseme-meets</bibkey>
    </paper>
    <paper id="3">
      <title>Barriers and enabling factors for error analysis in <fixed-case>NLG</fixed-case> research</title>
      <author><first>Emiel</first><last>van Miltenburg</last></author>
      <author><first>Miruna</first><last>Clinciu</last></author>
      <author><first>Ondřej</first><last>Dušek</last></author>
      <author><first>Dimitra</first><last>Gkatzia</last></author>
      <author><first>Stephanie</first><last>Inglis</last></author>
      <author><first>Leo</first><last>Leppänen</last></author>
      <author><first>Saad</first><last>Mahamood</last></author>
      <author><first>Stephanie</first><last>Schoch</last></author>
      <author><first>Craig</first><last>Thomson</last></author>
      <author><first>Luou</first><last>Wen</last></author>
      <abstract>Earlier research has shown that few studies in Natural Language Generation (NLG) evaluate their system outputs using an error analysis, despite known limitations of automatic evaluation metrics and human ratings. This position paper takes the stance that error analyses should be encouraged, and discusses several ways to do so. This paper is based on our shared experience as authors as well as a survey we distributed as a means of public consultation. We provide an overview of existing barriers to carrying out error analyses, and propose changes to improve error reporting in the NLG literature.</abstract>
      <url hash="327030d4">2023.nejlt-1.3</url>
      <doi>https://doi.org/10.3384/nejlt.2000-1533.2023.4529</doi>
      <bibkey>van-miltenburg-etal-2023-barriers</bibkey>
    </paper>
    <paper id="4">
      <title>Benchmark for Evaluation of <fixed-case>D</fixed-case>anish Clinical Word Embeddings</title>
      <author><first>Martin Sundahl</first><last>Laursen</last></author>
      <author><first>Jannik Skyttegaard</first><last>Pedersen</last></author>
      <author><first>Pernille Just</first><last>Vinholt</last></author>
      <author><first>Rasmus Søgaard</first><last>Hansen</last></author>
      <author><first>Thiusius Rajeeth</first><last>Savarimuthu</last></author>
      <abstract>In natural language processing, benchmarks are used to track progress and identify useful models. Currently, no benchmark for Danish clinical word embeddings exists. This paper describes the development of a Danish benchmark for clinical word embeddings. The clinical benchmark consists of ten datasets: eight intrinsic and two extrinsic. Moreover, we evaluate word embeddings trained on text from the clinical domain, general practitioner domain and general domain on the established benchmark. All the intrinsic tasks of the benchmark are publicly available.</abstract>
      <url hash="37b6c8b5">2023.nejlt-1.4</url>
      <doi>https://doi.org/10.3384/nejlt.2000-1533.2023.4132</doi>
      <bibkey>laursen-etal-2023-benchmark</bibkey>
    </paper>
    <paper id="5">
      <title><fixed-case>NL</fixed-case>-Augmenter: A Framework for Task-Sensitive Natural Language Augmentation</title>
      <author><first>Kaustubh</first><last>Dhole</last></author>
      <author><first>Varun</first><last>Gangal</last></author>
      <author><first>Sebastian</first><last>Gehrmann</last></author>
      <author><first>Aadesh</first><last>Gupta</last></author>
      <author><first>Zhenhao</first><last>Li</last></author>
      <author><first>Saad</first><last>Mahamood</last></author>
      <author><first>Abinaya</first><last>Mahadiran</last></author>
      <author><first>Simon</first><last>Mille</last></author>
      <author><first>Ashish</first><last>Shrivastava</last></author>
      <author><first>Samson</first><last>Tan</last></author>
      <author><first>Tongshang</first><last>Wu</last></author>
      <author><first>Jascha</first><last>Sohl-Dickstein</last></author>
      <author><first>Jinho</first><last>Choi</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <author><first>Ondřej</first><last>Dušek</last></author>
      <author><first>Sebastian</first><last>Ruder</last></author>
      <author><first>Sajant</first><last>Anand</last></author>
      <author><first>Nagender</first><last>Aneja</last></author>
      <author><first>Rabin</first><last>Banjade</last></author>
      <author><first>Lisa</first><last>Barthe</last></author>
      <author><first>Hanna</first><last>Behnke</last></author>
      <author><first>Ian</first><last>Berlot-Attwell</last></author>
      <author><first>Connor</first><last>Boyle</last></author>
      <author><first>Caroline</first><last>Brun</last></author>
      <author><first>Marco Antonio Sobrevilla</first><last>Cabezudo</last></author>
      <author><first>Samuel</first><last>Cahyawijaya</last></author>
      <author><first>Emile</first><last>Chapuis</last></author>
      <author><first>Wanxiang</first><last>Che</last></author>
      <author><first>Mukund</first><last>Choudhary</last></author>
      <author><first>Christian</first><last>Clauss</last></author>
      <author><first>Pierre</first><last>Colombo</last></author>
      <author><first>Filip</first><last>Cornell</last></author>
      <author><first>Gautier</first><last>Dagan</last></author>
      <author><first>Mayukh</first><last>Das</last></author>
      <author><first>Tanay</first><last>Dixit</last></author>
      <author><first>Thomas</first><last>Dopierre</last></author>
      <author><first>Paul-Alexis</first><last>Dray</last></author>
      <author><first>Suchitra</first><last>Dubey</last></author>
      <author><first>Tatiana</first><last>Ekeinhor</last></author>
      <author><first>Marco Di</first><last>Giovanni</last></author>
      <author><first>Tanya</first><last>Goyal</last></author>
      <author><first>Rishabh</first><last>Gupta</last></author>
      <author><first>Louanes</first><last>Hamla</last></author>
      <author><first>Sang</first><last>Han</last></author>
      <author><first>Fabrice</first><last>Harel-Canada</last></author>
      <author><first>Antoine</first><last>Honoré</last></author>
      <author><first>Ishan</first><last>Jindal</last></author>
      <author><first>Przemysław</first><last>Joniak</last></author>
      <author><first>Denis</first><last>Kleyko</last></author>
      <author><first>Venelin</first><last>Kovatchev</last></author>
      <author><first>Kalpesh</first><last>Krishna</last></author>
      <author><first>Ashutosh</first><last>Kumar</last></author>
      <author><first>Stefan</first><last>Langer</last></author>
      <author><first>Seungjae Ryan</first><last>Lee</last></author>
      <author><first>Corey James</first><last>Levinson</last></author>
      <author><first>Hualou</first><last>Liang</last></author>
      <author><first>Kaizhao</first><last>Liang</last></author>
      <author><first>Zhexiong</first><last>Liu</last></author>
      <author><first>Andrey</first><last>Lukyanenko</last></author>
      <author><first>Vukosi</first><last>Marivate</last></author>
      <author><first>Gerard</first><last>de Melo</last></author>
      <author><first>Simon</first><last>Meoni</last></author>
      <author><first>Maxine</first><last>Meyer</last></author>
      <author><first>Afnan</first><last>Mir</last></author>
      <author><first>Nafise Sadat</first><last>Moosavi</last></author>
      <author><first>Niklas</first><last>Meunnighoff</last></author>
      <author><first>Timothy Sum Hon</first><last>Mun</last></author>
      <author><first>Kenton</first><last>Murray</last></author>
      <author><first>Marcin</first><last>Namysl</last></author>
      <author><first>Maria</first><last>Obedkova</last></author>
      <author><first>Priti</first><last>Oli</last></author>
      <author><first>Nivranshu</first><last>Pasricha</last></author>
      <author><first>Jan</first><last>Pfister</last></author>
      <author><first>Richard</first><last>Plant</last></author>
      <author><first>Vinay</first><last>Prabhu</last></author>
      <author><first>Vasile</first><last>Pais</last></author>
      <author><first>Libo</first><last>Qin</last></author>
      <author><first>Shahab</first><last>Raji</last></author>
      <author><first>Pawan Kumar</first><last>Rajpoot</last></author>
      <author><first>Vikas</first><last>Raunak</last></author>
      <author><first>Roy</first><last>Rinberg</last></author>
      <author><first>Nicholas</first><last>Roberts</last></author>
      <author><first>Juan Diego</first><last>Rodriguez</last></author>
      <author><first>Claude</first><last>Roux</last></author>
      <author><first>Vasconcellos</first><last>Samus</last></author>
      <author><first>Ananya</first><last>Sai</last></author>
      <author><first>Robin</first><last>Schmidt</last></author>
      <author><first>Thomas</first><last>Scialom</last></author>
      <author><first>Tshephisho</first><last>Sefara</last></author>
      <author><first>Saqib</first><last>Shamsi</last></author>
      <author><first>Xudong</first><last>Shen</last></author>
      <author><first>Yiwen</first><last>Shi</last></author>
      <author><first>Haoyue</first><last>Shi</last></author>
      <author><first>Anna</first><last>Shvets</last></author>
      <author><first>Nick</first><last>Siegel</last></author>
      <author><first>Damien</first><last>Sileo</last></author>
      <author><first>Jamie</first><last>Simon</last></author>
      <author><first>Chandan</first><last>Singh</last></author>
      <author><first>Roman</first><last>Sitelew</last></author>
      <author><first>Priyank</first><last>Soni</last></author>
      <author><first>Taylor</first><last>Sorensen</last></author>
      <author><first>William</first><last>Soto</last></author>
      <author><first>Aman</first><last>Srivastava</last></author>
      <author><first>Aditya</first><last>Srivatsa</last></author>
      <author><first>Tony</first><last>Sun</last></author>
      <author><first>Mukund</first><last>Varma</last></author>
      <author><first>A</first><last>Tabassum</last></author>
      <author><first>Fiona</first><last>Tan</last></author>
      <author><first>Ryan</first><last>Teehan</last></author>
      <author><first>Mo</first><last>Tiwari</last></author>
      <author><first>Marie</first><last>Tolkiehn</last></author>
      <author><first>Athena</first><last>Wang</last></author>
      <author><first>Zijian</first><last>Wang</last></author>
      <author><first>Zijie</first><last>Wang</last></author>
      <author><first>Gloria</first><last>Wang</last></author>
      <author><first>Fuxuan</first><last>Wei</last></author>
      <author><first>Bryan</first><last>Wilie</last></author>
      <author><first>Genta Indra</first><last>Winata</last></author>
      <author><first>Xinyu</first><last>Wu</last></author>
      <author><first>Witold</first><last>Wydmanski</last></author>
      <author><first>Tianbao</first><last>Xie</last></author>
      <author><first>Usama</first><last>Yaseen</last></author>
      <author><first>Michael</first><last>Yee</last></author>
      <author><first>Jing</first><last>Zhang</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <abstract>Data augmentation is an important method for evaluating the robustness of and enhancing the diversity of training data for natural language processing (NLP) models. In this paper, we present NL-Augmenter, a new participatory Python-based natural language (NL) augmentation framework which supports the creation of transformations (modifications to the data) and filters (data splits according to specific features). We describe the framework and an initial set of 117 transformations and 23 filters for a variety of NL tasks annotated with noisy descriptive tags. The transformations incorporate noise, intentional and accidental human mistakes, socio-linguistic variation, semantically-valid style, syntax changes, as well as artificial constructs that are unambiguous to humans. We demonstrate the efficacy of NL-Augmenter by using its transformations to analyze the robustness of popular language models. We find different models to be differently challenged on different tasks, with quasi-systematic score decreases. The infrastructure, datacards, and robustness evaluation results are publicly available on GitHub for the benefit of researchers working on paraphrase generation, robustness analysis, and low-resource NLP.</abstract>
      <url hash="d6cd4f93">2023.nejlt-1.5</url>
      <doi>https://doi.org/10.3384/nejlt.2000-1533.2023.4725</doi>
      <bibkey>dhole-etal-2023-nl</bibkey>
    </paper>
    <paper id="6">
      <title>On the Relationship between Frames and Emotionality in Text</title>
      <author><first>Enrica</first><last>Troiano</last></author>
      <author><first>Roman</first><last>Klinger</last></author>
      <author><first>Sebastian</first><last>Padó</last></author>
      <abstract>Emotions, which are responses to salient events, can be realized in text implicitly, for instance with mere references to facts (e.g., “That was the beginning of a long war”). Interpreting affective meanings thus relies on the readers background knowledge, but that is hardly modeled in computational emotion analysis. Much work in the field is focused on the word level and treats individual lexical units as the fundamental emotion cues in written communication. We shift our attention to word relations. We leverage Frame Semantics, a prominent theory for the description of predicate-argument structures, which matches the study of emotions: frames build on a “semantics of understanding” whose assumptions rely precisely on peoples world knowledge. Our overarching question is whether and to what extent the events that are represented by frames possess an emotion meaning. To carry out a large corpus-based correspondence analysis, we automatically annotate texts with emotions as well as with FrameNet frames and roles, and we analyze the correlations between them. Our main finding is that substantial groups of frames have an emotional import. With an extensive qualitative analysis, we show that they capture several properties of emotions that are purported by theories from psychology. These observations boost insights on the two strands of research that we bring together: emotion analysis can profit from the event-based perspective of frame semantics; in return, frame semantics gains a better grip of its position vis-à-vis emotions, an integral part of word meanings.</abstract>
      <url hash="84200429">2023.nejlt-1.6</url>
      <doi>https://doi.org/10.3384/nejlt.2000-1533.2023.4361</doi>
      <bibkey>troiano-etal-2023-relationship</bibkey>
    </paper>
    <paper id="7">
      <title>An Empirical Configuration Study of a Common Document Clustering Pipeline</title>
      <author><first>Anton</first><last>Eklund</last></author>
      <author><first>Mona</first><last>Forsman</last></author>
      <author><first>Frank</first><last>Drewes</last></author>
      <abstract>Document clustering is frequently used in applications of natural language processing, e.g. to classify news articles or creating topic models. In this paper, we study document clustering with the common clustering pipeline that includes vectorization with BERT or Doc2Vec, dimension reduction with PCA or UMAP, and clustering with K-Means or HDBSCAN. We discuss the inter- actions of the different components in the pipeline, parameter settings, and how to determine an appropriate number of dimensions. The results suggest that BERT embeddings combined with UMAP dimension reduction to no less than 15 dimensions provides a good basis for clustering, regardless of the specific clustering algorithm used. Moreover, while UMAP performed better than PCA in our experiments, tuning the UMAP settings showed little impact on the overall performance. Hence, we recommend configuring UMAP so as to optimize its time efficiency. According to our topic model evaluation, the combination of BERT and UMAP, also used in BERTopic, performs best. A topic model based on this pipeline typically benefits from a large number of clusters.</abstract>
      <url hash="e575bfd7">2023.nejlt-1.7</url>
      <doi>https://doi.org/10.3384/nejlt.2000-1533.2023.4396</doi>
      <bibkey>eklund-etal-2023-empirical</bibkey>
    </paper>
    <paper id="8">
      <title>Prevention or Promotion? Predicting Author’s Regulatory Focus</title>
      <author><first>Aswathy</first><last>Velutharambath</last></author>
      <author><first>Kai</first><last>Sassenberg</last></author>
      <author><first>Roman</first><last>Klinger</last></author>
      <abstract>People differ fundamentally in what motivates them to pursue a goal and how they approach it. For instance, some people seek growth and show eagerness, whereas others prefer security and are vigilant. The concept of regulatory focus is employed in psychology, to explain and predict this goal-directed behavior of humans underpinned by two unique motivational systems – the promotion and the prevention system. Traditionally, text analysis methods using closed-vocabularies are employed to assess the distinctive linguistic patterns associated with the two systems. From an NLP perspective, automatically detecting the regulatory focus of individuals from text provides valuable insights into the behavioral inclinations of the author, finding its applications in areas like marketing or health communication. However, the concept never made an impactful debut in computational linguistics research. To bridge this gap we introduce the novel task of regulatory focus classification from text and present two complementary German datasets – (1) experimentally generated event descriptions and (2) manually annotated short social media texts used for evaluating the generalizability of models on real-world data. First, we conduct a correlation analysis to verify if the linguistic footprints of regulatory focus reported in psychology studies are observable and to what extent in our datasets. For automatic classification, we compare closed-vocabulary-based analyses with a state-of-the-art BERT-based text classification model and observe that the latter outperforms lexicon-based approaches on experimental data and is notably better on out-of-domain Twitter data.</abstract>
      <url hash="22c11089">2023.nejlt-1.8</url>
      <doi>https://doi.org/10.3384/nejlt.2000-1533.2023.4561</doi>
      <bibkey>velutharambath-etal-2023-prevention</bibkey>
    </paper>
    <paper id="9">
      <title>Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis</title>
      <author><first>Jun-Min</first><last>Lee</last></author>
      <author><first>Tae-Bin</first><last>Ha</last></author>
      <abstract>Generative Adversarial Networks (GAN) is a model for data synthesis, which creates plausible data through the competition of generator and discriminator. Although GAN application to image synthesis is extensively studied, it has inherent limitations to natural language generation. Because natural language is composed of discrete tokens, a generator has difficulty updating its gradient through backpropagation; therefore, most text-GAN studies generate sentences starting with a random token based on a reward system. Thus, the generators of previous studies are pre-trained in an autoregressive way before adversarial training, causing data memorization that synthesized sentences reproduce the training data. In this paper, we synthesize sentences using a framework similar to the original GAN. More specifically, we propose Text Embedding Space Generative Adversarial Networks (TESGAN) which generate continuous text embedding spaces instead of discrete tokens to solve the gradient backpropagation problem. Furthermore, TESGAN conducts unsupervised learning which does not directly refer to the text of the training data to overcome the data memorization issue. By adopting this novel method, TESGAN can synthesize new sentences, showing the potential of unsupervised learning for text synthesis. We expect to see extended research combining Large Language Models with a new perspective of viewing text as an continuous space.</abstract>
      <url hash="7ea2dd46">2023.nejlt-1.9</url>
      <doi>https://doi.org/10.3384/nejlt.2000-1533.2023.4855</doi>
      <bibkey>lee-ha-2023-unsupervised</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>QUA</fixed-case>-<fixed-case>RC</fixed-case>: the semi-synthetic dataset of multiple choice questions for assessing reading comprehension in <fixed-case>U</fixed-case>krainian</title>
      <author><first>Mariia</first><last>Zyrianova</last></author>
      <author><first>Dmytro</first><last>Kalpakchi</last></author>
      <abstract>In this article we present the first dataset of multiple choice questions for assessing reading comprehension in Ukrainian. The dataset is based on the texts from the Ukrainian national tests for reading comprehension, and the MCQs themselves are created semi-automatically in three stages. The first stage was to use GPT-3 to generate the MCQs zero-shot, the second stage was to select MCQs of sufficient quality and revise the ones with minor errors, whereas the final stage was to expand the dataset with the MCQs written manually. The dataset is created by the Ukrainian language native speakers, one of whom is also a language teacher. The resulting corpus has slightly more than 900 MCQs, of which only 43 MCQs could be kept as they were generated by GPT-3.</abstract>
      <url hash="4c30412f">2023.nejlt-1.10</url>
      <doi>https://doi.org/10.3384/nejlt.2000-1533.2023.4939</doi>
      <bibkey>zyrianova-kalpakchi-2023-qua</bibkey>
    </paper>
  </volume>
</collection>
