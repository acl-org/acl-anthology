<?xml version='1.0' encoding='UTF-8'?>
<collection id="2007.jeptalnrecital">
  <volume id="long" ingest-date="2021-02-05" type="proceedings">
    <meta>
      <booktitle>Actes de la 14ème conférence sur le Traitement Automatique des Langues Naturelles. Articles longs</booktitle>
      <editor><first>Nabil</first><last>Hathout</last></editor>
      <editor><first>Philippe</first><last>Muller</last></editor>
      <publisher>ATALA</publisher>
      <address>Toulouse, France</address>
      <month>June</month>
      <year>2007</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="14368492">2007.jeptalnrecital-long.0</url>
      <bibkey>jep-taln-recital-2007-actes</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Exploiting structural meeting-specific features for topic segmentation</title>
      <author><first>Maria</first><last>Georgescul</last></author>
      <author><first>Alexander</first><last>Clarck</last></author>
      <author><first>Susan</first><last>Armstrong</last></author>
      <pages>15–24</pages>
      <abstract>In this article we address the task of automatic text structuring into linear and non-overlapping thematic episodes. Our investigation reports on the use of various lexical, acoustic and syntactic features, and makes a comparison of how these features influence performance of automatic topic segmentation. Using datasets containing multi-party meeting transcriptions, we base our experiments on a proven state-of-the-art approach using support vector classification.</abstract>
      <url hash="d465a7c6">2007.jeptalnrecital-long.1</url>
      <bibkey>georgescul-etal-2007-exploiting</bibkey>
    </paper>
    <paper id="2">
      <title>Énergie textuelle de mémoires associatives</title>
      <author><first>Silvia</first><last>Fernández</last></author>
      <author><first>Eric</first><last>Sanjuan</last></author>
      <author><first>Juan-Manuel</first><last>Torres-Moreno</last></author>
      <pages>25–34</pages>
      <abstract>Dans cet article, nous présentons une approche de réseaux de neurones inspirée de la physique statistique de systèmes magnétiques pour étudier des problèmes fondamentaux du Traitement Automatique de la Langue Naturelle. L’algorithme modélise un document comme un système de neurones où l’on déduit l’énergie textuelle. Nous avons appliqué cette approche aux problèmes de résumé automatique et de détection de frontières thématiques. Les résultats sont très encourageants.</abstract>
      <url hash="a7a3676e">2007.jeptalnrecital-long.2</url>
      <language>fra</language>
      <bibkey>fernandez-etal-2007-energie</bibkey>
    </paper>
    <paper id="3">
      <title>Une expérience d’extraction de relations sémantiques à partir de textes dans le domaine médical</title>
      <author><first>Mehdi</first><last>Embarek</last></author>
      <author><first>Olivier</first><last>Ferret</last></author>
      <pages>35–44</pages>
      <abstract>Dans cet article, nous présentons une méthode permettant d’extraire à partir de textes des relations sémantiques dans le domaine médical en utilisant des patrons linguistiques. La première partie de cette méthode consiste à identifier les entités entre lesquelles les relations visées interviennent, en l’occurrence les maladies, les examens, les médicaments et les symptômes. La présence d’une des relations sémantiques visées dans les phrases contenant un couple de ces entités est ensuite validée par l’application de patrons linguistiques préalablement appris de manière automatique à partir d’un corpus annoté. Nous rendons compte de l’évaluation de cette méthode sur un corpus en Français pour quatre relations.</abstract>
      <url hash="6dc4a022">2007.jeptalnrecital-long.3</url>
      <language>fra</language>
      <bibkey>embarek-ferret-2007-une</bibkey>
    </paper>
    <paper id="4">
      <title>Identifier les pronoms anaphoriques et trouver leurs antécédents : l’intérêt de la classification bayésienne</title>
      <author><first>Davy</first><last>Weissenbacher</last></author>
      <author><first>Adeline</first><last>Nazarenko</last></author>
      <pages>45–54</pages>
      <abstract>On oppose souvent en TAL les systèmes à base de connaissances linguistiques et ceux qui reposent sur des indices de surface. Chaque approche a ses limites et ses avantages. Nous proposons dans cet article une nouvelle approche qui repose sur les réseaux bayésiens et qui permet de combiner au sein d’une même représentation ces deux types d’informations hétérogènes et complémentaires. Nous justifions l’intérêt de notre approche en comparant les performances du réseau bayésien à celles des systèmes de l’état de l’art, sur un problème difficile du TAL, celui de la résolution d’anaphore.</abstract>
      <url hash="c9e0890d">2007.jeptalnrecital-long.4</url>
      <language>fra</language>
      <bibkey>weissenbacher-nazarenko-2007-identifier</bibkey>
    </paper>
    <paper id="5">
      <title>Régler les règles d’analyse morphologique</title>
      <author><first>Bruno</first><last>Cartoni</last></author>
      <pages>55–64</pages>
      <abstract>Dans cet article, nous présentons différentes contraintes mécaniques et linguistiques applicables à des règles d’analyse des mots inconnus afin d’améliorer la performance d’un analyseur morphologique de l’italien. Pour mesurer l’impact de ces contraintes, nous présentons les résultats d’une évaluation de chaque contrainte qui prend en compte les gains et les pertes qu’elle engendre. Nous discutons ainsi de la nécessaire évaluation de chaque réglage apporté aux règles afin d’en déterminer la pertinence.</abstract>
      <url hash="a1bb1854">2007.jeptalnrecital-long.5</url>
      <language>fra</language>
      <bibkey>cartoni-2007-regler</bibkey>
    </paper>
    <paper id="6">
      <title>Structures de traits typées et morphologie à partitions</title>
      <author><first>François</first><last>Barthélemy</last></author>
      <pages>65–74</pages>
      <abstract>Les structures de traits typées sont une façon abstraite et agréable de représenter une information partielle. Dans cet article, nous montrons comment la combinaison de deux techniques relativement classiques permet de définir une variante de morphologie à deux niveaux intégrant harmonieusement des structures de traits et se compilant en une machine finie. La première de ces techniques est la compilation de structure de traits en expressions régulières, la seconde est la morphologie à partition. Nous illustrons au moyen de deux exemples l’expressivité d’un formalisme qui rapproche les grammaires à deux niveaux des grammaires d’unification.</abstract>
      <url hash="63da3cdd">2007.jeptalnrecital-long.6</url>
      <language>fra</language>
      <bibkey>barthelemy-2007-structures</bibkey>
    </paper>
    <paper id="7">
      <title>Analyse morphosémantique des composés savants : transposition du français à l’anglais</title>
      <author><first>Louise</first><last>Deléger</last></author>
      <author><first>Fiammetta</first><last>Namer</last></author>
      <author><first>Pierre</first><last>Zweigenbaum</last></author>
      <pages>75–84</pages>
      <abstract>La plupart des vocabulaires spécialisés comprennent une part importante de lexèmes morphologiquement complexes, construits à partir de racines grecques et latines, qu’on appelle « composés savants ». Une analyse morphosémantique permet de décomposer et de donner des définitions à ces lexèmes, et semble pouvoir être appliquée de façon similaire aux composés de plusieurs langues. Cet article présente l’adaptation d’un analyseur morphosémantique, initialement dédié au français (DériF), à l’analyse de composés savants médicaux anglais, illustrant ainsi la similarité de structure de ces composés dans des langues européennes proches. Nous exposons les principes de cette transposition et ses performances. L’analyseur a été testé sur un ensemble de 1299 lexèmes extraits de la terminologie médicale WHO-ART : 859 ont pu être décomposés et définis, dont 675 avec succès. Outre une simple transposition d’une langue à l’autre, la méthode montre la potentialité d’un système multilingue.</abstract>
      <url hash="174c6a93">2007.jeptalnrecital-long.7</url>
      <language>fra</language>
      <bibkey>deleger-etal-2007-analyse</bibkey>
    </paper>
    <paper id="8">
      <title>A tool for detecting <fixed-case>F</fixed-case>rench-<fixed-case>E</fixed-case>nglish cognates and false friends</title>
      <author><first>Oana</first><last>Frunza</last></author>
      <author><first>Diana</first><last>Inkpen</last></author>
      <pages>85–94</pages>
      <abstract>Cognates are pairs of words in different languages similar in spelling and meaning. They can help a second-language learner on the tasks of vocabulary expansion and reading comprehension. False friends are pairs of words that have similar spelling but different meanings. Partial cognates are pairs of words in two languages that have the same meaning in some, but not all contexts. In this article we present a method to automatically classify a pair of words as cognates or false friends, by using several measures of orthographic similarity as features for classification. We use this method to create complete lists of cognates and false friends between two languages. We also disambiguate partial cognates in context. We applied all our methods to French and English, but they can be applied to other pairs of languages as well. We built a tool that takes the produced lists and annotates a French text with equivalent English cognates or false friends, in order to help second-language learners improve their reading comprehension skills and retention rate.</abstract>
      <url hash="5ecea481">2007.jeptalnrecital-long.8</url>
      <bibkey>frunza-inkpen-2007-tool</bibkey>
    </paper>
    <paper id="9">
      <title>Enrichissement d’un lexique bilingue par analogie</title>
      <author><first>Philippe</first><last>Langlais</last></author>
      <author><first>Alexandre</first><last>Patry</last></author>
      <pages>95–104</pages>
      <abstract>La présence de mots inconnus dans les applications langagières représente un défi de taille bien connu auquel n’échappe pas la traduction automatique. Les systèmes professionnels de traduction offrent à cet effet à leurs utilisateurs la possibilité d’enrichir un lexique de base avec de nouvelles entrées. Récemment, Stroppa et Yvon (2005) démontraient l’intérêt du raisonnement par analogie pour l’analyse morphologique d’une langue. Dans cette étude, nous montrons que le raisonnement par analogie offre également une réponse adaptée au problème de la traduction d’entrées lexicales inconnues.</abstract>
      <url hash="da3c4332">2007.jeptalnrecital-long.9</url>
      <language>fra</language>
      <bibkey>langlais-patry-2007-enrichissement</bibkey>
    </paper>
    <paper id="10">
      <title>Inférence de règles de réécriture pour la traduction de termes biomédicaux</title>
      <author><first>Vincent</first><last>Claveau</last></author>
      <pages>105–114</pages>
      <abstract>Dans le domaine biomédical, le caractère multilingue de l’accès à l’information est un problème d’importance. Dans cet article nous présentons une technique originale permettant de traduire des termes simples du domaine biomédical de et vers de nombreuses langues. Cette technique entièrement automatique repose sur l’apprentissage de règles de réécriture à partir d’exemples et l’utilisation de modèles de langues. Les évaluations présentées sont menées sur différentes paires de langues (français-anglais, espagnol-portugais, tchèque-anglais, russe-anglais...). Elles montrent que cette approche est très efficace et offre des performances variables selon les langues mais très bonnes dans l’ensemble et nettement supérieures à celles disponibles dans l’état de l’art. Les taux de précision de traductions s’étagent ainsi de 57.5% pour la paire russe-anglais jusqu’à 85% pour la paire espagnol-portugais et la paire françaisanglais.</abstract>
      <url hash="23f08c19">2007.jeptalnrecital-long.10</url>
      <language>fra</language>
      <bibkey>claveau-2007-inference</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>T</fixed-case>i<fixed-case>LT</fixed-case> correcteur de <fixed-case>SMS</fixed-case> : évaluation et bilan qualitatif</title>
      <author><first>Émilie</first><last>Guimier De Neef</last></author>
      <author><first>Arnaud</first><last>Debeurme</last></author>
      <author><first>Jungyeul</first><last>Park</last></author>
      <pages>115–124</pages>
      <abstract>Nous présentons le logiciel TiLT pour la correction des SMS et évaluons ses performances sur le corpus de SMS du DELIC. L’évaluation utilise la distance de Jaccard et la mesure BLEU. La présentation des résultats est suivie d’une analyse qualitative du système et de ses limites.</abstract>
      <url hash="205cb6dc">2007.jeptalnrecital-long.11</url>
      <language>fra</language>
      <bibkey>guimier-de-neef-etal-2007-tilt</bibkey>
    </paper>
    <paper id="12">
      <title>Vers un méta-<fixed-case>EDL</fixed-case> complet, puis un <fixed-case>EDL</fixed-case> universel pour la <fixed-case>TAO</fixed-case></title>
      <author><first>Hong-Thai</first><last>Nguyen</last></author>
      <author><first>Christian</first><last>Boitet</last></author>
      <pages>125–134</pages>
      <abstract>Un “méta-EDL” (méta-Environnement de Développement Linguiciel) pour la TAO permet de piloter à distance un ou plusieurs EDL pour construire des systèmes de TAO hétérogènes. Partant de CASH, un méta-EDL dédié à Ariane-G5, et de WICALE 1.0, un premier méta-EDL générique mais aux fonctionnalités minimales, nous dégageons les problèmes liés à l’ajout de fonctionnalités riches comme l’édition et la navigation en local, et donnons une solution implémentée dans WICALE 2.0. Nous y intégrons maintenant une base lexicale pour les systèmes à « pivot lexical », comme UNL/U++. Un but à plus long terme est de passer d’un tel méta-EDL générique multifonctionnel à un EDL « universel », ce qui suppose la réingénierie des compilateurs et des moteurs des langages spécialisés pour la programmation linguistique (LSPL) supportés par les divers EDL.</abstract>
      <url hash="ed024e73">2007.jeptalnrecital-long.12</url>
      <language>fra</language>
      <bibkey>nguyen-boitet-2007-vers</bibkey>
    </paper>
    <paper id="13">
      <title>Aides à la navigation dans un corpus de transcriptions d’oral</title>
      <author><first>Frederik</first><last>Cailliau</last></author>
      <author><first>Claude</first><last>De Loupy</last></author>
      <pages>135–144</pages>
      <abstract>Dans cet article, nous évaluons les performances de fonctionnalités d’aide à la navigation dans un contexte de recherche dans un corpus audio. Nous montrons que les particularités de la transcription et, en particulier les erreurs, conduisent à une dégradation parfois importante des performances des outils d’analyse. Si la navigation par concepts reste dans des niveaux d’erreur acceptables, la reconnaissance des entités nommées, utilisée pour l’aide à la lecture, voit ses performances fortement baisser. Notre remise en doute de la portabilité de ces fonctions à un corpus oral est néanmoins atténuée par la nature même du corpus qui incite à considérer que toute méthodes permettant de réduire le temps d’accès à l’information est pertinente, même si les outils utilisés sont imparfaits.</abstract>
      <url hash="e676e061">2007.jeptalnrecital-long.13</url>
      <language>fra</language>
      <bibkey>cailliau-de-loupy-2007-aides</bibkey>
    </paper>
    <paper id="14">
      <title>Une grammaire du français pour une théorie descriptive et formelle de la langue</title>
      <author><first>Marie-Laure</first><last>Guénot</last></author>
      <pages>145–154</pages>
      <abstract>Dans cet article, nous présentons une grammaire du français qui fait l’objet d’un modèle basé sur des descriptions linguistiques de corpus (provenant notamment des travaux de l’Approche Pronominale) et représentée selon le formalisme des Grammaires de Propriétés. Elle constitue une proposition nouvelle parmi les grammaires formelles du français, participant à la mise en convergence de la variété des travaux de description linguistique, et de la diversité des possibilités de représentation formelle. Cette grammaire est mise à disposition publique sur le Centre de Ressources pour la Description de l’Oral en tant que ressource pour la représentation et l’analyse.</abstract>
      <url hash="88f8fa07">2007.jeptalnrecital-long.14</url>
      <language>fra</language>
      <bibkey>guenot-2007-une</bibkey>
    </paper>
    <paper id="15">
      <title>Architecture compositionnelle pour les dépendances croisées</title>
      <author><first>Alexandre</first><last>Dikovsky</last></author>
      <pages>155–164</pages>
      <abstract>L’article présente les principes généraux sous-jacent aux grammaires catégorielles de dépendances : une classe de grammaires de types récemment proposée pour une description compositionnelle et uniforme des dépendances continues et discontinues. Ces grammaires très expressives et analysées en temps polynomial, adoptent naturellement l’architecture multimodale et expriment les dépendances croisées illimitées.</abstract>
      <url hash="ec438882">2007.jeptalnrecital-long.15</url>
      <language>fra</language>
      <bibkey>dikovsky-2007-architecture</bibkey>
    </paper>
    <paper id="16">
      <title><fixed-case>S</fixed-case>em<fixed-case>TAG</fixed-case>, une architecture pour le développement et l’utilisation de grammaires d’arbres adjoints à portée sémantique</title>
      <author><first>Claire</first><last>Gardent</last></author>
      <author><first>Yannick</first><last>Parmentier</last></author>
      <pages>165–174</pages>
      <abstract>Dans cet article, nous présentons une architecture logicielle libre et ouverte pour le développement de grammaires d’arbres adjoints à portée sémantique. Cette architecture utilise un compilateur de métagrammaires afin de faciliter l’extension et la maintenance de la grammaire, et intègre un module de construction sémantique permettant de vérifier la couverture aussi bien syntaxique que sémantique de la grammaire. Ce module utilise un analyseur syntaxique tabulaire généré automatiquement à partir de la grammaire par le système DyALog. Nous présentons également les résultats de l’évaluation d’une grammaire du français développée au moyen de cette architecture.</abstract>
      <url hash="ba251f8c">2007.jeptalnrecital-long.16</url>
      <language>fra</language>
      <bibkey>gardent-parmentier-2007-semtag-une</bibkey>
    </paper>
    <paper id="17">
      <title>Utiliser des classes de sélection distributionnelle pour désambiguïser les adjectifs</title>
      <author><first>Fabienne</first><last>Venant</last></author>
      <pages>175–184</pages>
      <abstract>La désambiguïsation lexicale présente un intérêt considérable pour un nombre important d’applications, en traitement automatique des langues comme en recherche d’information. Nous proposons un modèle d’un genre nouveau, fondé sur la théorie de la construction dynamique du sens (Victorri et Fuchs, 1996). Ce modèle donne une place centrale à la polysémie et propose une représentation géométrique du sens. Nous présentons ici une application de ce modèle à la désambiguïsation automatique des adjectifs. La méthode utilisée s’appuie sur une pré-désambiguïsation du nom régissant l’adjectif, par le biais de classes de sélection distributionnelle. Elle permet aussi de prendre en compte les positions relatives du nom et de l’adjectif (postpostion ou antéposition) dans le calcul du sens.</abstract>
      <url hash="aa7880a4">2007.jeptalnrecital-long.17</url>
      <language>fra</language>
      <bibkey>venant-2007-utiliser</bibkey>
    </paper>
    <paper id="18">
      <title>Disambiguating automatic semantic annotation based on a thesaurus structure</title>
      <author><first>Véronique</first><last>Malaisé</last></author>
      <author><first>Luit</first><last>Gazendam</last></author>
      <author><first>Hennie</first><last>Brugman</last></author>
      <pages>185–194</pages>
      <abstract>The use/use for relationship a thesaurus is usually more complex than the (para-) synonymy recommended in the ISO-2788 standard describing the content of these controlled vocabularies. The fact that a non preferred term can refer to multiple preferred terms (only the latter are relevant in controlled indexing) makes this relationship difficult to use in automatic annotation applications : it generates ambiguity cases. In this paper, we present the CARROT algorithm, meant to rank the output of our Information Extraction pipeline, and how this algorithm can be used to select the relevant preferred term out of different possibilities. This selection is meant to provide suggestions of keywords to human annotators, in order to ease and speed up their daily process and is based on the structure of their thesaurus. We achieve a 95 % success, and discuss these results along with perspectives for this experiment.</abstract>
      <url hash="76cac206">2007.jeptalnrecital-long.18</url>
      <bibkey>malaise-etal-2007-disambiguating</bibkey>
    </paper>
    <paper id="19">
      <title>Repérage de sens et désambiguïsation dans un contexte bilingue</title>
      <author><first>Marianna</first><last>Apidianaki</last></author>
      <pages>195–204</pages>
      <abstract>Les besoins de désambiguïsation varient dans les différentes applications du Traitement Automatique des Langues (TAL). Dans cet article, nous proposons une méthode de désambiguïsation lexicale opératoire dans un contexte bilingue et, par conséquent, adéquate pour la désambiguïsation au sein d’applications relatives à la traduction. Il s’agit d’une méthode contextuelle, qui combine des informations de cooccurrence avec des informations traductionnelles venant d’un bitexte. L’objectif est l’établissement de correspondances de traduction au niveau sémantique entre les mots de deux langues. Cette méthode étend les conséquences de l’hypothèse contextuelle du sens dans un contexte bilingue, tout en admettant l’existence d’une relation de similarité sémantique entre les mots de deux langues en relation de traduction. La modélisation de ces correspondances de granularité fine permet la désambiguïsation lexicale de nouvelles occurrences des mots polysémiques de la langue source ainsi que la prédiction de la traduction la plus adéquate pour ces occurrences.</abstract>
      <url hash="9ce49134">2007.jeptalnrecital-long.19</url>
      <language>fra</language>
      <bibkey>apidianaki-2007-reperage</bibkey>
    </paper>
    <paper id="20">
      <title><fixed-case>P</fixed-case>rep<fixed-case>L</fixed-case>ex : un lexique des prépositions du français pour l’analyse syntaxique</title>
      <author><first>Karën</first><last>Fort</last></author>
      <author><first>Bruno</first><last>Guillaume</last></author>
      <pages>205–214</pages>
      <abstract>PrepLex est un lexique des prépositions du français. Il contient les informations utiles à des systèmes d’analyse syntaxique. Il a été construit en comparant puis fusionnant différentes sources d’informations lexicales disponibles. Ce lexique met également en évidence les prépositions ou classes de prépositions qui apparaissent dans la définition des cadres de sous-catégorisation des ressources lexicales qui décrivent la valence des verbes.</abstract>
      <url hash="7113be1f">2007.jeptalnrecital-long.20</url>
      <language>fra</language>
      <bibkey>fort-guillaume-2007-preplex-un</bibkey>
    </paper>
    <paper id="21">
      <title>Comparaison du Lexique-Grammaire des verbes pleins et de <fixed-case>DICOVALENCE</fixed-case> : vers une intégration dans le Lefff</title>
      <author><first>Laurence</first><last>Danlos</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <pages>215–224</pages>
      <abstract>Cet article compare le Lexique-Grammaire des verbes pleins et DICOVALENCE, deux ressources lexicales syntaxiques pour le français développées par des linguistes depuis de nombreuses années. Nous étudions en particulier les divergences et les empiètements des modèles lexicaux sous-jacents. Puis nous présentons le Lefff , lexique syntaxique à grande échelle pour le TAL, et son propre modèle lexical. Nous montrons que ce modèle est à même d’intégrer les informations lexicales présentes dans le Lexique-Grammaire et dans DICOVALENCE. Nous présentons les résultats des premiers travaux effectués en ce sens, avec pour objectif à terme la constitution d’un lexique syntaxique de référence pour le TAL.</abstract>
      <url hash="adf20b11">2007.jeptalnrecital-long.21</url>
      <language>fra</language>
      <bibkey>danlos-sagot-2007-comparaison</bibkey>
    </paper>
    <paper id="22">
      <title>Dictionnaires électroniques et étiquetage syntactico-sémantique</title>
      <author><first>Pierre-André</first><last>Buvet</last></author>
      <author><first>Emmanuel</first><last>Cartier</last></author>
      <author><first>Fabrice</first><last>Issac</last></author>
      <author><first>Salah</first><last>Mejri</last></author>
      <pages>225–234</pages>
      <abstract>Nous présentons dans cet article le prototype d’un système d’étiquetage syntactico-sémantique des mots qui utilise comme principales ressources linguistiques différents dictionnaires du laboratoire Lexiques, Dictionnaires, Informatique (LDI). Dans un premier temps, nous mentionnons des travaux sur le même sujet. Dans un deuxième temps, nous faisons la présentation générale du système. Dans un troisième temps, nous exposons les principales caractéristiques des dictionnaires syntactico-sémantiques utilisés. Dans un quatrième temps, nous détaillons un exemple de traitement.</abstract>
      <url hash="78034283">2007.jeptalnrecital-long.22</url>
      <language>fra</language>
      <bibkey>buvet-etal-2007-dictionnaires</bibkey>
    </paper>
    <paper id="23">
      <title>Un analyseur hybride pour la détection et la correction des erreurs cachées sémantiques en langue arabe</title>
      <author><first>Chiraz</first><last>Ben Othmane Zribi</last></author>
      <author><first>Hanène</first><last>Mejri</last></author>
      <author><first>Mohamed</first><last>Ben Ahmed</last></author>
      <pages>235–244</pages>
      <abstract>Cet article s’intéresse au problème de la détection et de la correction des erreurs cachées sémantiques dans les textes arabes. Ce sont des erreurs orthographiques produisant des mots lexicalement valides mais invalides sémantiquement. Nous commençons par décrire le type d’erreur sémantique auquel nous nous intéressons. Nous exposons par la suite l’approche adoptée qui se base sur la combinaison de plusieurs méthodes, tout en décrivant chacune de ces méthodes. Puis, nous évoquons le contexte du travail qui nous a mené au choix de l’architecture multi-agent pour l’implémentation de notre système. Nous présentons et commentons vers la fin les résultats de l’évaluation dudit système.</abstract>
      <url hash="aa2bac3e">2007.jeptalnrecital-long.23</url>
      <language>fra</language>
      <bibkey>ben-othmane-zribi-etal-2007-un</bibkey>
    </paper>
    <paper id="24">
      <title>Résolution de la référence dans des dialogues homme-machine : évaluation sur corpus de deux approches symbolique et probabiliste</title>
      <author><first>Alexandre</first><last>Denis</last></author>
      <author><first>Frédéric</first><last>Béchet</last></author>
      <author><first>Matthieu</first><last>Quignard</last></author>
      <pages>245–254</pages>
      <abstract>Cet article décrit deux approches, l’une numérique, l’autre symbolique, traitant le problème de la résolution de la référence dans un cadre de dialogue homme-machine. L’analyse des résultats obtenus sur le corpus MEDIA montre la complémentarité des deux systèmes développés : robustesse aux erreurs et hypothèses multiples pour l’approche numérique ; modélisation de phénomènes complexes et interprétation complète pour l’approche symbolique.</abstract>
      <url hash="8d5184d1">2007.jeptalnrecital-long.24</url>
      <language>fra</language>
      <bibkey>denis-etal-2007-resolution</bibkey>
    </paper>
    <paper id="25">
      <title>Annotation précise du français en sémantique de rôles par projection cross-linguistique</title>
      <author><first>Sebastian</first><last>Padó</last></author>
      <author><first>Guillaume</first><last>Pitel</last></author>
      <pages>255–264</pages>
      <abstract>Dans le paradigme FrameNet, cet article aborde le problème de l’annotation précise et automatique de rôles sémantiques dans une langue sans lexique FrameNet existant. Nous évaluons la méthode proposée par Padó et Lapata (2005, 2006), fondée sur la projection de rôles et appliquée initialement à la paire anglais-allemand. Nous testons sa généralisabilité du point de vue (a) des langues, en l’appliquant à la paire (anglais-français) et (b) de la qualité de la source, en utilisant une annotation automatique du côté anglais. Les expériences montrent des résultats à la hauteur de ceux obtenus pour l’allemand, nous permettant de conclure que cette approche présente un grand potentiel pour réduire la quantité de travail nécessaire à la création de telles ressources dans de nombreuses langues.</abstract>
      <url hash="f07d9bd7">2007.jeptalnrecital-long.25</url>
      <language>fra</language>
      <bibkey>pado-pitel-2007-annotation</bibkey>
    </paper>
    <paper id="26">
      <title>Élaboration automatique d’un dictionnaire de cooccurrences grand public</title>
      <author><first>Simon</first><last>Charest</last></author>
      <author><first>Éric</first><last>Brunelle</last></author>
      <author><first>Jean</first><last>Fontaine</last></author>
      <author><first>Bertrand</first><last>Pelletier</last></author>
      <pages>265–274</pages>
      <abstract>Antidote RX, un logiciel d’aide à la rédaction grand public, comporte un nouveau dictionnaire de 800 000 cooccurrences, élaboré essentiellement automatiquement. Nous l’avons créé par l’analyse syntaxique détaillée d’un vaste corpus et par la sélection automatique des cooccurrences les plus pertinentes à l’aide d’un test statistique, le rapport de vraisemblance. Chaque cooccurrence est illustrée par des exemples de phrases également tirés du corpus automatiquement. Les cooccurrences et les exemples extraits ont été révisés par des linguistes. Nous examinons les choix d’interface que nous avons faits pour présenter ces données complexes à un public non spécialisé. Enfin, nous montrons comment nous avons intégré les cooccurrences au correcteur d’Antidote pour améliorer ses performances.</abstract>
      <url hash="9603761a">2007.jeptalnrecital-long.26</url>
      <language>fra</language>
      <bibkey>charest-etal-2007-elaboration</bibkey>
    </paper>
    <paper id="27">
      <title>Les vecteurs conceptuels, un outil complémentaire aux réseaux lexicaux</title>
      <author><first>Didier</first><last>Schwab</last></author>
      <author><first>Lian</first><last>Tze Lim</last></author>
      <author><first>Mathieu</first><last>Lafourcade</last></author>
      <pages>275–284</pages>
      <abstract>Fréquemment utilisés dans le Traitement Automatique des Langues Naturelles, les réseaux lexicaux font aujourd’hui l’objet de nombreuses recherches. La plupart d’entre eux, et en particulier le plus célèbre WordNet, souffrent du manque d’informations syntagmatiques mais aussi d’informations thématiques (« problème du tennis »). Cet article présente les vecteurs conceptuels qui permettent de représenter les idées contenues dans un segment textuel quelconque et permettent d’obtenir une vision continue des thématiques utilisées grâce aux distances calculables entre eux. Nous montrons leurs caractéristiques et en quoi ils sont complémentaires des réseaux lexico-sémantiques. Nous illustrons ce propos par l’enrichissement des données de WordNet par des vecteurs conceptuels construits par émergence.</abstract>
      <url hash="58c48ea9">2007.jeptalnrecital-long.27</url>
      <language>fra</language>
      <bibkey>schwab-etal-2007-les</bibkey>
    </paper>
    <paper id="28">
      <title>Alignements monolingues avec déplacements</title>
      <author><first>Julien</first><last>Bourdaillet</last></author>
      <author><first>Jean-Gabriel</first><last>Ganascia</last></author>
      <pages>285–294</pages>
      <abstract>Ce travail présente une application d’alignement monolingue qui répond à une problématique posée par la critique génétique textuelle, une école d’études littéraires qui s’intéresse à la genèse textuelle en comparant les différentes versions d’une oeuvre. Ceci nécessite l’identification des déplacements, cependant, le problème devient ainsi NP-complet. Notre algorithme heuristique est basé sur la reconnaissance des homologies entre séquences de caractères. Nous présentons une validation expérimentale et montrons que notre logiciel obtient de bons résultats ; il permet notamment l’alignement de livres entiers.</abstract>
      <url hash="0ec41ce4">2007.jeptalnrecital-long.28</url>
      <language>fra</language>
      <bibkey>bourdaillet-ganascia-2007-alignements</bibkey>
    </paper>
    <paper id="29">
      <title>Confondre le coupable : corrections d’un lexique suggérées par une grammaire</title>
      <author><first>Lionel</first><last>Nicolas</last></author>
      <author><first>Jacques</first><last>Farré</last></author>
      <author><first>Éric</first><last>Villemonte De La Clergerie</last></author>
      <pages>295–304</pages>
      <abstract>Le succès de l’analyse syntaxique d’une phrase dépend de la qualité de la grammaire sous-jacente mais aussi de celle du lexique utilisé. Une première étape dans l’amélioration des lexiques consiste à identifier les entrées lexicales potentiellement erronées, par exemple en utilisant des techniques de fouilles d’erreurs sur corpus (Sagot &amp; Villemonte de La Clergerie, 2006). Nous explorons ici l’étape suivante : la suggestion de corrections pour les entrées identifiées. Cet objectif est atteint au travers de réanalyses des phrases rejetées à l’étape précédente, après modification des informations portées par les entrées suspectées. Un calcul statistique sur les nouveaux résultats permet ensuite de mettre en valeur les corrections les plus pertinentes.</abstract>
      <url hash="29997096">2007.jeptalnrecital-long.29</url>
      <language>fra</language>
      <bibkey>nicolas-etal-2007-confondre</bibkey>
    </paper>
    <paper id="30">
      <title>Ambiguïté de portée et approche fonctionnelle des grammaires d’arbres adjoints</title>
      <author><first>Sylvain</first><last>Pogodalla</last></author>
      <pages>305–314</pages>
      <abstract>En s’appuyant sur la notion d’arbre de dérivation des Grammaires d’Arbres Adjoints (TAG), cet article propose deux objectifs : d’une part rendre l’interface entre syntaxe et sémantique indépendante du langage de représentation sémantique utilisé, et d’autre part offrir un noyau qui permette le traitement sémantique des ambiguïtés de portée de quantificateurs sans utiliser de langage de représentation sous-spécifiée.</abstract>
      <url hash="96d05eb0">2007.jeptalnrecital-long.30</url>
      <language>fra</language>
      <bibkey>pogodalla-2007-ambiguite</bibkey>
    </paper>
    <paper id="31">
      <title>Évaluer <fixed-case>SYNLEX</fixed-case></title>
      <author><first>Ingrid</first><last>Falk</last></author>
      <author><first>Gil</first><last>Francopoulo</last></author>
      <author><first>Claire</first><last>Gardent</last></author>
      <pages>315–324</pages>
      <abstract>SYNLEX est un lexique syntaxique extrait semi-automatiquement des tables du LADL. Comme les autres lexiques syntaxiques du français disponibles et utilisables pour le TAL (LEFFF, DICOVALENCE), il est incomplet et n’a pas fait l’objet d’une évaluation permettant de déterminer son rappel et sa précision par rapport à un lexique de référence. Nous présentons une approche qui permet de combler au moins partiellement ces lacunes. L’approche s’appuie sur les méthodes mises au point en acquisition automatique de lexique. Un lexique syntaxique distinct de SYNLEX est acquis à partir d’un corpus de 82 millions de mots puis utilisé pour valider et compléter SYNLEX. Le rappel et la précision de cette version améliorée de SYNLEX sont ensuite calculés par rapport à un lexique de référence extrait de DICOVALENCE.</abstract>
      <url hash="8d219bec">2007.jeptalnrecital-long.31</url>
      <language>fra</language>
      <bibkey>falk-etal-2007-evaluer</bibkey>
    </paper>
    <paper id="32">
      <title>Analyse automatique vs analyse interactive : un cercle vertueux pour la voyellation, l’étiquetage et la lemmatisation de l’arabe</title>
      <author><first>Fathi</first><last>Debili</last></author>
      <author><first>Zied</first><last>Ben Tahar</last></author>
      <author><first>Emna</first><last>Souissi</last></author>
      <pages>325–334</pages>
      <abstract>Comment produire de façon massive des textes annotés dans des conditions d’efficacité, de reproductibilité et de coût optimales ? Plutôt que de corriger les sorties d’analyse automatique moyennant des outils d’éditions éventuellement dédiés, ainsi qu’il estcommunément préconisé, nous proposons de recourir à des outils d’analyse interactive où la correction manuelle est au fur et à mesure prise en compte par l’analyse automatique. Posant le problème de l’évaluation de ces outils interactifs et du rendement de leur ergonomie linguistique, et proposant pour cela une métrique fondée sur le calcul du coût qu’exigent ces corrections exprimé en nombre de manipulations (frappe au clavier, clic de souris, etc.), nous montrons, au travers d’un protocole expérimental simple orienté vers la voyellation, l’étiquetage et la lemmatisation de l’arabe, que paradoxalement, les meilleures performances interactives d’un système ne sont pas toujours corrélées à ses meilleures performances automatiques. Autrement dit, que le comportement linguistique automatique le plus performant n’est pas toujours celui qui assure, dès lors qu’il y a contributions manuelles, le meilleur rendement interactif.</abstract>
      <url hash="7675a807">2007.jeptalnrecital-long.32</url>
      <language>fra</language>
      <bibkey>debili-etal-2007-analyse</bibkey>
    </paper>
    <paper id="33">
      <title>Évaluation des stades de développement en français langue étrangère</title>
      <author><first>Jonas</first><last>Granfeldt</last></author>
      <author><first>Pierre</first><last>Nugues</last></author>
      <pages>335–344</pages>
      <abstract>Cet article décrit un système pour définir et évaluer les stades de développement en français langue étrangère. L’évaluation de tels stades correspond à l’identification de la fréquence de certains phénomènes lexicaux et grammaticaux dans la production des apprenants et comment ces fréquences changent en fonction du temps. Les problèmes à résoudre dans cette démarche sont triples : identifier les attributs les plus révélateurs, décider des points de séparation entre les stades et évaluer le degré d’efficacité des attributs et de la classification dans son ensemble. Le système traite ces trois problèmes. Il se compose d’un analyseur morphosyntaxique, appelé Direkt Profil, auquel nous avons relié un module d’apprentissage automatique. Dans cet article, nous décrivons les idées qui ont conduit au développement du système et son intérêt. Nous présentons ensuite le corpus que nous avons utilisé pour développer notre analyseur morphosyntaxique. Enfin, nous présentons les résultats sensiblement améliorés des classificateurs comparé aux travaux précédents (Granfeldt et al., 2006). Nous présentons également une méthode de sélection de paramètres afin d’identifier les attributs grammaticaux les plus appropriés.</abstract>
      <url hash="14bc0e56">2007.jeptalnrecital-long.33</url>
      <language>fra</language>
      <bibkey>granfeldt-nugues-2007-evaluation</bibkey>
    </paper>
    <paper id="34">
      <title>Apprentissage non supervisé de familles morphologiques par classification ascendante hiérarchique</title>
      <author><first>Delphine</first><last>Bernhard</last></author>
      <pages>345–354</pages>
      <abstract>Cet article présente un système d’acquisition de familles morphologiques qui procède par apprentissage non supervisé à partir de listes de mots extraites de corpus de textes. L’approche consiste à former des familles par groupements successifs, similairement aux méthodes de classification ascendante hiérarchique. Les critères de regroupement reposent sur la similarité graphique des mots ainsi que sur des listes de préfixes et de paires de suffixes acquises automatiquement à partir des corpus traités. Les résultats obtenus pour des corpus de textes de spécialité en français et en anglais sont évalués à l’aide de la base CELEX et de listes de référence construites manuellement. L’évaluation démontre les bonnes performances du système, indépendamment de la langue, et ce malgré la technicité et la complexité morphologique du vocabulaire traité.</abstract>
      <url hash="82bfd90c">2007.jeptalnrecital-long.34</url>
      <language>fra</language>
      <bibkey>bernhard-2007-apprentissage</bibkey>
    </paper>
    <paper id="35">
      <title>Enchaînements verbaux – étude sur le temps et l’aspect utilisant des techniques d’apprentissage non supervisé</title>
      <author><first>Catherine</first><last>Recanati</last></author>
      <author><first>Nicoleta</first><last>Rogovschi</last></author>
      <pages>355–364</pages>
      <abstract>L’apprentissage non supervisé permet la découverte de catégories initialement inconnues. Les techniques actuelles permettent d’explorer des séquences de phénomènes alors qu’on a tendance à se focaliser sur l’analyse de phénomènes isolés ou sur la relation entre deux phénomènes. Elles offrent ainsi de précieux outils pour l’analyse de données organisées en séquences, et en particulier, pour la découverte de structures textuelles. Nous présentons ici les résultats d’une première tentative de les utiliser pour inspecter les suites de verbes provenant de phrases de récits d’accident de la route. Les verbes étaient encodés comme paires (cat, temps), où cat représente la catégorie aspectuelle d’un verbe, et temps son temps grammatical. L’analyse, basée sur une approche originale, a fourni une classification des enchaînements de deux verbes successifs en quatre groupes permettant de segmenter les textes. Nous donnons ici une interprétation de ces groupes à partir de statistiques sur des annotations sémantiques indépendantes.</abstract>
      <url hash="d503fecd">2007.jeptalnrecital-long.35</url>
      <language>fra</language>
      <bibkey>recanati-rogovschi-2007-enchainements</bibkey>
    </paper>
    <paper id="36">
      <title><fixed-case>D</fixed-case>-<fixed-case>STAG</fixed-case> : un formalisme pour le discours basé sur les <fixed-case>TAG</fixed-case> synchrones</title>
      <author><first>Laurence</first><last>Danlos</last></author>
      <pages>365–374</pages>
      <abstract>Nous proposons D-STAG, un formalisme pour le discours qui utilise les TAG synchrones. Les analyses sémantiques produites par D-STAG sont des structures de discours hiérarchiques annotées de relations de discours coordonnantes ou subordonnantes. Elles sont compatibles avec les structures de discours produites tant en RST qu’en SDRT. Les relations de discours coordonnantes et subordonnantes sont modélisées respectivement par les opérations de substitution et d’adjonction introduites en TAG.</abstract>
      <url hash="908bad1f">2007.jeptalnrecital-long.36</url>
      <language>fra</language>
      <bibkey>danlos-2007-stag</bibkey>
    </paper>
    <paper id="37">
      <title>Collocation translation based on sentence alignment and parsing</title>
      <author><first>Violeta</first><last>Seretan</last></author>
      <author><first>Éric</first><last>Wehrli</last></author>
      <pages>375–384</pages>
      <abstract>Bien que de nombreux efforts aient été déployés pour extraire des collocations à partir de corpus de textes, seule une minorité de travaux se préoccupent aussi de rendre le résultat de l’extraction prêt à être utilisé dans les applications TAL qui pourraient en bénéficier, telles que la traduction automatique. Cet article décrit une méthode précise d’identification de la traduction des collocations dans un corpus parallèle, qui présente les avantages suivants : elle peut traiter des collocation flexibles (et pas seulement figées) ; elle a besoin de ressources limitées et d’un pouvoir de calcul raisonnable (pas d’alignement complet, pas d’entraînement) ; elle peut être appliquée à plusieurs paires des langues et fonctionne même en l’absence de dictionnaires bilingues. La méthode est basée sur l’information syntaxique provenant du parseur multilingue Fips. L’évaluation effectuée sur 4000 collocations de type verbe-objet correspondant à plusieurs paires de langues a montré une précision moyenne de 89.8% et une couverture satisfaisante (70.9%). Ces résultats sont supérieurs à ceux enregistrés dans l’évaluation d’autres méthodes de traduction de collocations.</abstract>
      <url hash="ae226aab">2007.jeptalnrecital-long.37</url>
      <language>fra</language>
      <bibkey>seretan-wehrli-2007-collocation</bibkey>
    </paper>
    <paper id="38">
      <title>Utilisation d’une approche basée sur la recherche cross-lingue d’information pour l’alignement de phrases à partir de textes bilingues Arabe-Français</title>
      <author><first>Nasredine</first><last>Semmar</last></author>
      <author><first>Christian</first><last>Fluhr</last></author>
      <pages>385–394</pages>
      <abstract>L’alignement de phrases à partir de textes bilingues consiste à reconnaître les phrases qui sont traductions les unes des autres. Cet article présente une nouvelle approche pour aligner les phrases d’un corpus parallèle. Cette approche est basée sur la recherche crosslingue d’information et consiste à construire une base de données des phrases du texte cible et considérer chaque phrase du texte source comme une requête à cette base. La recherche crosslingue utilise un analyseur linguistique et un moteur de recherche. L’analyseur linguistique traite aussi bien les documents à indexer que les requêtes et produit un ensemble de lemmes normalisés, un ensemble d’entités nommées et un ensemble de mots composés avec leurs étiquettes morpho-syntaxiques. Le moteur de recherche construit les fichiers inversés des documents en se basant sur leur analyse linguistique et retrouve les documents pertinents à partir de leur indexes. L’aligneur de phrases a été évalué sur un corpus parallèle Arabe-Français et les résultats obtenus montrent que 97% des phrases ont été correctement alignées.</abstract>
      <url hash="aa1d3efe">2007.jeptalnrecital-long.38</url>
      <language>fra</language>
      <bibkey>semmar-fluhr-2007-utilisation</bibkey>
    </paper>
  </volume>
  <volume id="poster" ingest-date="2021-02-05" type="proceedings">
    <meta>
      <booktitle>Actes de la 14ème conférence sur le Traitement Automatique des Langues Naturelles. Posters</booktitle>
      <editor><first>Nabil</first><last>Hathout</last></editor>
      <editor><first>Philippe</first><last>Muller</last></editor>
      <publisher>ATALA</publisher>
      <address>Toulouse, France</address>
      <month>June</month>
      <year>2007</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="a564c332">2007.jeptalnrecital-poster.0</url>
      <bibkey>jep-taln-recital-2007-actes-de</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Désambiguïsation lexicale automatique : sélection automatique d’indices</title>
      <author><first>Laurent</first><last>Audibert</last></author>
      <pages>13–22</pages>
      <abstract>Nous exposons dans cet article une expérience de sélection automatique des indices du contexte pour la désambiguïsation lexicale automatique. Notre point de vue est qu’il est plus judicieux de privilégier la pertinence des indices du contexte plutôt que la sophistication des algorithmes de désambiguïsation utilisés. La sélection automatique des indices par le biais d’un algorithme génétique améliore significativement les résultats obtenus dans nos expériences précédentes tout en confortant des observations que nous avions faites sur la nature et la répartition des indices les plus pertinents.</abstract>
      <url hash="908f3ab7">2007.jeptalnrecital-poster.1</url>
      <language>fra</language>
      <bibkey>audibert-2007-desambiguisation</bibkey>
    </paper>
    <paper id="2">
      <title>Représenter la dynamique énonciative et modale de textes</title>
      <author><first>Delphine</first><last>Battistelli</last></author>
      <author><first>Marie</first><last>Chagnoux</last></author>
      <pages>23–32</pages>
      <abstract>Nous proposons d’exposer ici une méthodologie d’analyse et de représentation d’une des composantes de la structuration des textes, celle liée à la notion de prise en charge énonciative. Nous mettons l’accent sur la structure hiérarchisée des segments textuels qui en résulte ; nous la représentons d’une part sous forme d’arbre et d’autre part sous forme de graphe. Ce dernier permet d’appréhender la dynamique énonciative et modale de textes comme un cheminement qui s’opère entre différents niveaux de discours dans un texte au fur et à mesure de sa lecture syntagmatique.</abstract>
      <url hash="fb1cd7dc">2007.jeptalnrecital-poster.2</url>
      <language>fra</language>
      <bibkey>battistelli-chagnoux-2007-representer</bibkey>
    </paper>
    <paper id="3">
      <title>Segmentation en super-chunks</title>
      <author><first>Olivier</first><last>Blanc</last></author>
      <author><first>Matthieu</first><last>Constant</last></author>
      <author><first>Patrick</first><last>Watrin</last></author>
      <pages>33–42</pages>
      <abstract>Depuis l’analyseur développé par Harris à la fin des années 50, les unités polylexicales ont peu à peu été intégrées aux analyseurs syntaxiques. Cependant, pour la plupart, elles sont encore restreintes aux mots composés qui sont plus stables et moins nombreux. Toutefois, la langue est remplie d’expressions semi-figées qui forment également des unités sémantiques : les expressions adverbiales et les collocations. De même que pour les mots composés traditionnels, l’identification de ces structures limite la complexité combinatoire induite par l’ambiguïté lexicale. Dans cet article, nous détaillons une expérience qui intègre ces notions dans un processus de segmentation en super-chunks, préalable à l’analyse syntaxique. Nous montrons que notre chunker, développé pour le français, atteint une précision et un rappel de 92,9 % et 98,7 %, respectivement. Par ailleurs, les unités polylexicales réalisent 36,6 % des attachements internes aux constituants nominaux et prépositionnels.</abstract>
      <url hash="dd3c26ba">2007.jeptalnrecital-poster.3</url>
      <language>fra</language>
      <bibkey>blanc-etal-2007-segmentation</bibkey>
    </paper>
    <paper id="4">
      <title>Détection et prédiction de la satisfaction des usagers dans les dialogues Personne-Machine</title>
      <author><first>Narjès</first><last>Boufaden</last></author>
      <author><first>Truong</first><last>Le Hoang</last></author>
      <author><first>Pierre</first><last>Dumouchel</last></author>
      <pages>43–52</pages>
      <abstract>Nous étudions le rôle des entités nommées et marques discursives de rétroaction pour la tâche de classification et prédiction de la satisfaction usager à partir de dialogues. Les expériences menées sur 1027 dialogues Personne-Machine dans le domaine des agences de voyage montrent que les entités nommées et les marques discursives n’améliorent pas de manière significative le taux de classification des dialogues. Par contre, elles permettent une meilleure prédiction de la satisfaction usager à partir des premiers tours de parole usager.</abstract>
      <url hash="dde4d292">2007.jeptalnrecital-poster.4</url>
      <language>fra</language>
      <bibkey>boufaden-etal-2007-detection</bibkey>
    </paper>
    <paper id="5">
      <title>Les ellipses dans un système de traduction automatique de la parole</title>
      <author><first>Pierrette</first><last>Bouillon</last></author>
      <author><first>Manny</first><last>Rayner</last></author>
      <author><first>Marianne</first><last>Starlander</last></author>
      <author><first>Marianne</first><last>Santaholma</last></author>
      <pages>53–62</pages>
      <abstract>Dans tout dialogue, les phrases elliptiques sont très nombreuses. Dans cet article, nous évaluons leur impact sur la reconnaissance et la traduction dans le système de traduction automatique de la parole MedSLT. La résolution des ellipses y est effectuée par une méthode robuste et portable, empruntée aux systèmes de dialogue homme-machine. Cette dernière exploite une représentation sémantique plate et combine des techniques linguistiques (pour construire la représentation) et basées sur les exemples (pour apprendre sur la base d’un corpus ce qu’est une ellipse bien formée dans un sous-domaine donné et comment la résoudre).</abstract>
      <url hash="7a5c684e">2007.jeptalnrecital-poster.5</url>
      <language>fra</language>
      <bibkey>bouillon-etal-2007-les</bibkey>
    </paper>
    <paper id="6">
      <title>Analyse automatique de sondages téléphoniques d’opinion</title>
      <author><first>Nathalie</first><last>Camelin</last></author>
      <author><first>Frédéric</first><last>Béchet</last></author>
      <author><first>Géraldine</first><last>Damnati</last></author>
      <author><first>Renato</first><last>De Mori</last></author>
      <pages>63–72</pages>
      <abstract>Cette étude présente la problématique de l’analyse automatique de sondages téléphoniques d’opinion. Cette analyse se fait en deux étapes : tout d’abord extraire des messages oraux les expressions subjectives relatives aux opinions de utilisateurs sur une dimension particulière (efficacité, accueil, etc.) ; puis sélectionner les messages fiables, selon un ensemble de mesures de confiance, et estimer la distribution des diverses opinions sur le corpus de test. Le but est d’estimer une distribution aussi proche que possible de la distribution de référence. Cette étude est menée sur un corpus de messages provenant de vrais utilisateurs fournis par France Télécom R&amp;D.</abstract>
      <url hash="840377f8">2007.jeptalnrecital-poster.6</url>
      <language>fra</language>
      <bibkey>camelin-etal-2007-analyse</bibkey>
    </paper>
    <paper id="7">
      <title>Une réalisateur de surface basé sur une grammaire réversible</title>
      <author><first>Claire</first><last>Gardent</last></author>
      <author><first>Éric</first><last>Kow</last></author>
      <pages>73–82</pages>
      <abstract>En génération, un réalisateur de surface a pour fonction de produire, à partir d’une représentation conceptuelle donnée, une phrase grammaticale. Les réalisateur existants soit utilisent une grammaire réversible et des méthodes statistiques pour déterminer parmi l’ensemble des sorties produites la plus plausible ; soit utilisent des grammaires spécialisées pour la génération et des méthodes symboliques pour déterminer la paraphrase la plus appropriée à un contexte de génération donné. Dans cet article, nous présentons GENI, un réalisateur de surface basé sur une grammaire d’arbres adjoints pour le français qui réconcilie les deux approches en combinant une grammaire réversible avec une sélection symbolique des paraphrases.</abstract>
      <url hash="3e1f324a">2007.jeptalnrecital-poster.7</url>
      <language>fra</language>
      <bibkey>gardent-kow-2007-une</bibkey>
    </paper>
    <paper id="8">
      <title>Analyse des échecs d’une approche pour traiter les questions définitoires soumises à un système de questions/réponses</title>
      <author><first>Laurent</first><last>Gillard</last></author>
      <author><first>Patrice</first><last>Bellot</last></author>
      <author><first>Marc</first><last>El-Bèze</last></author>
      <pages>83–92</pages>
      <abstract>Cet article revient sur le type particulier des questions définitoires étudiées dans le cadre des campagnes d’évaluation des systèmes de Questions/Réponses. Nous présentons l’approche développée suite à notre participation à la campagne EQueR et son évaluation lors de QA@CLEF 2006. La réponse proposée est la plus représentative des expressions présentes en apposition avec l’objet à définir, sa sélection est faite depuis des indices dérivés de ces appositions. Environ 80% de bonnes réponses sont trouvées sur les questions définitoires des volets francophones de CLEF. Les cas d’erreurs rencontrés sont analysés et discutés en détail.</abstract>
      <url hash="b444541d">2007.jeptalnrecital-poster.8</url>
      <language>fra</language>
      <bibkey>gillard-etal-2007-analyse</bibkey>
    </paper>
    <paper id="9">
      <title>Caractérisation des discours scientifiques et vulgarisés en français, japonais et russe</title>
      <author><first>Lorraine</first><last>Goeuriot</last></author>
      <author><first>Natalia</first><last>Grabar</last></author>
      <author><first>Béatrice</first><last>Daille</last></author>
      <pages>93–102</pages>
      <abstract>L’objectif principal de notre travail consiste à étudier la notion de comparabilité des corpus, et nous abordons cette question dans un contexte monolingue en cherchant à distinguer les documents scientifiques et vulgarisés. Nous travaillons séparément sur des corpus composés de documents du domaine médical dans trois langues à forte distance linguistique (le français, le japonais et le russe). Dans notre approche, les documents sont caractérisés dans chaque langue selon leur thématique et une typologie discursive qui se situe à trois niveaux de l’analyse des documents : structurel, modal et lexical. Le typage des documents est implémenté avec deux algorithmes d’apprentissage (SVMlight et C4.5). L’évaluation des résultats montre que la typologie discursive proposée est portable d’une langue à l’autre car elle permet en effet de distinguer les deux discours. Nous constatons néanmoins des performances très variées selon les langues, les algorithmes et les types de caractéristiques discursives.</abstract>
      <url hash="06c5527b">2007.jeptalnrecital-poster.9</url>
      <language>fra</language>
      <bibkey>goeuriot-etal-2007-caracterisation</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>OGMIOS</fixed-case> : une plate-forme d’annotation linguistique de collection de documents issus du Web</title>
      <author><first>Thierry</first><last>Hamon</last></author>
      <author><first>Julien</first><last>Derivière</last></author>
      <author><first>Adeline</first><last>Nazarenko</last></author>
      <pages>103–112</pages>
      <abstract>L’un des objectifs du projet ALVIS est d’intégrer des informations linguistiques dans des moteurs de recherche spécialisés. Dans ce contexte, nous avons conçu une plate-forme d’enrichissement linguistique de documents issus du Web, OGMIOS, exploitant des outils de TAL existants. Les documents peuvent être en français ou en anglais. Cette architecture est distribuée, afin de répondre aux contraintes liées aux traitements de gros volumes de textes, et adaptable, pour permettre l’analyse de sous-langages. La plate-forme est développée en Perl et disponible sous forme de modules CPAN. C’est une structure modulaire dans lequel il est possible d’intégrer de nouvelles ressources ou de nouveaux outils de TAL. On peut ainsi définir des configuration différentes pour différents domaines et types de collections. Cette plateforme robuste permet d’analyser en masse des données issus du web qui sont par essence très hétérogènes. Nous avons évalué les performances de la plateforme sur plusieurs collections de documents. En distribuant les traitements sur vingt machines, une collection de 55 329 documents du domaine de la biologie (106 millions de mots) a été annotée en 35 heures tandis qu’une collection de 48 422 dépêches relatives aux moteurs de recherche (14 millions de mots) a été annotée en 3 heures et 15 minutes.</abstract>
      <url hash="968a84ad">2007.jeptalnrecital-poster.10</url>
      <language>fra</language>
      <bibkey>hamon-etal-2007-ogmios</bibkey>
    </paper>
    <paper id="11">
      <title>Les Lexiques-Miroirs. Du dictionnaire bilingue au graphe multilingue</title>
      <author><first>Sébastien</first><last>Haton</last></author>
      <author><first>Jean-Marie</first><last>Pierrel</last></author>
      <pages>113–122</pages>
      <abstract>On observe dans les dictionnaires bilingues une forte asymétrie entre les deux parties d’un même dictionnaire et l’existence de traductions et d’informations « cachées », i.e. pas directement visibles à l’entrée du mot à traduire. Nous proposons une méthodologie de récupération des données cachées ainsi que la « symétrisation » du dictionnaire grâce à un traitement automatique. L’étude d’un certain nombre de verbes et de leurs traductions en plusieurs langues a conduit à l’intégration de toutes les données, visibles ou cachées, au sein d’une base de données unique et multilingue. L’exploitation de la base de données a été rendue possible par l’écriture d’un algorithme de création de graphe synonymique qui lie dans un même espace les mots de langues différentes. Le programme qui en découle permettra de générer des dictionnaires paramétrables directement à partir du graphe.</abstract>
      <url hash="5acab7dc">2007.jeptalnrecital-poster.11</url>
      <language>fra</language>
      <bibkey>haton-pierrel-2007-les</bibkey>
    </paper>
    <paper id="12">
      <title>Traduction, restructurations syntaxiques et grammaires de correspondance</title>
      <author><first>Sylvain</first><last>Kahane</last></author>
      <pages>123–132</pages>
      <abstract>Cet article présente une nouvelle formalisation du modèle de traduction par transfert de la Théorie Sens-Texte. Notre modélisation utilise les grammaires de correspondance polarisées et fait une stricte séparation entre les modèles monolingues, un lexique bilingue minimal et des règles de restructuration universelles, directement associées aux fonctions lexicales syntaxiques.</abstract>
      <url hash="bc019351">2007.jeptalnrecital-poster.12</url>
      <language>fra</language>
      <bibkey>kahane-2007-traduction</bibkey>
    </paper>
    <paper id="13">
      <title>Modélisation des paradigmes de flexion des verbes arabes selon la norme <fixed-case>LMF</fixed-case> - <fixed-case>ISO</fixed-case> 24613</title>
      <author><first>Aïda</first><last>Khemakhem</last></author>
      <author><first>Bilel</first><last>Gargouri</last></author>
      <author><first>Abdelhamid</first><last>Abdelwahed</last></author>
      <author><first>Gil</first><last>Francopoulo</last></author>
      <pages>133–142</pages>
      <abstract>Dans cet article, nous spécifions les paradigmes de flexion des verbes arabes en respectant la version 9 de LMF (Lexical Markup Framework), future norme ISO 24613 qui traite de la standardisation des bases lexicales. La spécification de ces paradigmes se fonde sur une combinaison des racines et des schèmes. En particulier, nous mettons en relief les terminaisons de racines sensibles aux ajouts de suffixes et ce, afin de couvrir les situations non considérées dans les travaux existants. L’élaboration des paradigmes de flexion verbale que nous proposons est une description en intension d’ArabicLDB (Arabic Lexical DataBase) qui est une base lexicale normalisée pour la langue arabe. Nos travaux sont illustrés par la réalisation d’un conjugueur des verbes arabes à partir d’ArabicLDB.</abstract>
      <url hash="d94e66f0">2007.jeptalnrecital-poster.13</url>
      <language>fra</language>
      <bibkey>khemakhem-etal-2007-modelisation</bibkey>
    </paper>
    <paper id="14">
      <title>Du bruit, du silence et des ambiguïtés : que faire du <fixed-case>TAL</fixed-case> pour l’apprentissage des langues ?</title>
      <author><first>Olivier</first><last>Kraif</last></author>
      <author><first>Claude</first><last>Ponton</last></author>
      <pages>143–152</pages>
      <abstract>Nous proposons une nouvelle approche pour l’intégration du TAL dans les systèmes d’apprentissage des langues assisté par ordinateur (ALAO), la stratégie « moinsdisante ». Cette approche tire profit des technologies élémentaires mais fiables du TAL et insiste sur la nécessité de traitements modulaires et déclaratifs afin de faciliter la portabilité et la prise en main didactique des systèmes. Basé sur cette approche, ExoGen est un premier prototype pour la génération automatique d’activités lacunaires ou de lecture d’exemples. Il intègre un module de repérage et de description des réponses des apprenants fondé sur la comparaison entre réponse attendue et réponse donnée. L’analyse des différences graphiques, orthographiques et morphosyntaxiques permet un diagnostic des erreurs de type fautes d’orthographe, confusions, problèmes d’accord, de conjugaison, etc. La première évaluation d’ExoGen sur un extrait du corpus d’apprenants FRIDA produit des résultats prometteurs pour le développement de cette approche « moins-disante », et permet d’envisager un modèle d’analyse performant et généralisable à une grande variété d’activités.</abstract>
      <url hash="503e5734">2007.jeptalnrecital-poster.14</url>
      <language>fra</language>
      <bibkey>kraif-ponton-2007-du</bibkey>
    </paper>
    <paper id="15">
      <title>Extraction automatique de cadres de sous-catégorisation verbale pour le français à partir d’un corpus arboré</title>
      <author><first>Anna</first><last>Kupsc</last></author>
      <pages>153–162</pages>
      <abstract>Nous présentons une expérience d’extraction automatique des cadres de souscatégorisation pour 1362 verbes français. Nous exploitons un corpus journalistique richement annoté de 15 000 phrases dont nous extrayons 12 510 occurrences verbales. Nous évaluons dans un premier temps l’extraction des cadres basée sur la fonction des arguments, ce qui nous fournit 39 cadres différents avec une moyenne de 1.54 cadres par lemme. Ensuite, nous adoptons une approche mixte (fonction et catégorie syntaxique) qui nous fournit dans un premier temps 925 cadres différents, avec une moyenne de 3.44 cadres par lemme. Plusieurs méthodes de factorisation, neutralisant en particulier les variantes de réalisation avec le passif ou les pronoms clitiques, sont ensuite appliquées et nous permettent d’aboutir à 235 cadres différents avec une moyenne de 1.94 cadres par verbe. Nous comparons brièvement nos résultats avec les travaux existants pour le français et pour l’anglais.</abstract>
      <url hash="4d689376">2007.jeptalnrecital-poster.15</url>
      <language>fra</language>
      <bibkey>kupsc-2007-extraction</bibkey>
    </paper>
    <paper id="16">
      <title>Vers une formalisation des décompositions sémantiques dans la Grammaire d’Unification Sens-Texte</title>
      <author><first>François</first><last>Lareau</last></author>
      <pages>163–172</pages>
      <abstract>Nous proposons une formalisation de la décomposition du sens dans le cadre de la Grammaire d’Unification Sens-Texte. Cette formalisation vise une meilleure intégration des décompositions sémantiques dans un modèle global de la langue. Elle repose sur un jeu de saturation de polarités qui permet de contrôler la construction des représentations décomposées ainsi que leur mise en correspondance avec des arbres syntaxiques qui les expriment. Le formalisme proposé est illustré ici dans une perspective de synthèse, mais il s’applique également en analyse.</abstract>
      <url hash="8a011a17">2007.jeptalnrecital-poster.16</url>
      <language>fra</language>
      <bibkey>lareau-2007-vers</bibkey>
    </paper>
    <paper id="17">
      <title>Systèmes de questions-réponses : vers la validation automatique des réponses</title>
      <author><first>Anne-Laure</first><last>Ligozat</last></author>
      <author><first>Brigitte</first><last>Grau</last></author>
      <author><first>Isabelle</first><last>Robba</last></author>
      <author><first>Anne</first><last>Vilnat</last></author>
      <pages>173–182</pages>
      <abstract>Les systèmes de questions-réponses (SQR) ont pour but de trouver une information précise extraite d’une grande collection de documents comme le Web. Afin de pouvoir comparer les différentes stratégies possibles pour trouver une telle information, il est important d’évaluer ces systèmes. L’objectif d’une tâche de validation de réponses est d’estimer si une réponse donnée par un SQR est correcte ou non, en fonction du passage de texte donné comme justification. En 2006, nous avons participé à une tâche de validation de réponses, et dans cet article nous présentons la stratégie que nous avons utilisée. Celle-ci est fondée sur notre propre système de questions-réponses. Le principe est de comparer nos réponses avec les réponses à valider. Nous présentons les résultats obtenus et montrons les extensions possibles. À partir de quelques exemples, nous soulignons les difficultés que pose cette tâche.</abstract>
      <url hash="38e4eeb0">2007.jeptalnrecital-poster.17</url>
      <language>fra</language>
      <bibkey>ligozat-etal-2007-systemes</bibkey>
    </paper>
    <paper id="18">
      <title>Ressources lexicales chinoises pour le <fixed-case>TALN</fixed-case></title>
      <author><first>Huei-Chi</first><last>Lin</last></author>
      <author><first>Max</first><last>Silberztein</last></author>
      <pages>183–192</pages>
      <abstract>Nous voulons traiter des textes chinois automatiquement ; pour ce faire, nous formalisons le vocabulaire chinois, en utilisant principalement des dictionnaires et des grammaires morphologiques et syntaxiques formalisés avec le logiciel NooJ. Nous présentons ici les critères linguistiques qui nous ont permis de construire dictionnaires et grammaires, sachant que l’application envisagée (linguistique de corpus) nous impose certaines contraintes dans la formalisation des unités de la langue, en particulier des composés.</abstract>
      <url hash="151c2f8e">2007.jeptalnrecital-poster.18</url>
      <language>fra</language>
      <bibkey>lin-silberztein-2007-ressources</bibkey>
    </paper>
    <paper id="19">
      <title>Étiquetage morpho-syntaxique de textes kabyles</title>
      <author><first>Sinikka</first><last>Loikkanen</last></author>
      <pages>193–202</pages>
      <abstract>Cet article présente la construction d’un étiqueteur morpho-syntaxique développé pour annoter un corpus de textes kabyles (1 million de mots). Au sein de notre projet, un étiqueteur morpho-syntaxique a été développé et implémenté. Ceci inclut un analyseur morphologique ainsi que l’ensemble de règles de désambiguïsation qui se basent sur l’approche supervisée à base de règles. Pour effectuer le marquage, un jeu d’étiquettes morpho-syntaxiques pour le kabyle est proposé. Les résultats préliminaires sont très encourageants. Nous obtenons un taux d’étiquetage réussi autour de 97 % des textes en prose.</abstract>
      <url hash="c6ae8f86">2007.jeptalnrecital-poster.19</url>
      <language>fra</language>
      <bibkey>loikkanen-2007-etiquetage</bibkey>
    </paper>
    <paper id="20">
      <title>Analyse syntaxique et traitement automatique du syntagme nominal grec moderne</title>
      <author><first>Athina</first><last>Michou</last></author>
      <pages>203–212</pages>
      <abstract>Cet article décrit le traitement automatique du syntagme nominal en grec moderne par le modèle d’analyse syntaxique multilingue Fips. L’analyse syntaxique linguistique est focalisée sur les points principaux du DP grec : l’accord entre les constituants fléchis, l’ordre flexible des constituants, la cliticisation sur les noms et le phénomène de la polydéfinitude. Il est montré comment ces phénomènes sont traités et implémentés dans le cadre de l’analyseur syntaxique FipsGreek, qui met en oeuvre un formalisme inspiré de la grammaire générative chomskyenne.</abstract>
      <url hash="18c8bc93">2007.jeptalnrecital-poster.20</url>
      <language>fra</language>
      <bibkey>michou-2007-analyse</bibkey>
    </paper>
    <paper id="21">
      <title>Apprentissage symbolique de grammaires et traitement automatique des langues</title>
      <author><first>Erwan</first><last>Moreau</last></author>
      <pages>213–222</pages>
      <abstract>Le modèle de Gold formalise le processus d’apprentissage d’un langage. Nous présentons dans cet article les avantages et inconvénients de ce cadre théorique contraignant, dans la perspective d’applications en TAL. Nous décrivons brièvement les récentes avancées dans ce domaine, qui soulèvent selon nous certaines questions importantes.</abstract>
      <url hash="c77b1bfb">2007.jeptalnrecital-poster.21</url>
      <language>fra</language>
      <bibkey>moreau-2007-apprentissage</bibkey>
    </paper>
    <paper id="22">
      <title>Méthodes d’alignement des propositions : un défi aux traductions croisées</title>
      <author><first>Yayoi</first><last>Nakamura-Delloye</last></author>
      <pages>223–232</pages>
      <abstract>Le présent article décrit deux méthodes d’alignement des propositions : l’une basée sur les méthodes d’appariement des graphes et une autre inspirée de la classification ascendante hiérarchique (CAH). Les deux méthodes sont caractérisées par leur capacité d’alignement des traductions croisées, ce qui était impossible pour beaucoup de méthodes classiques d’alignement des phrases. Contrairement aux résultats obtenus avec l’approche spectrale qui nous paraissent non satisfaisants, l’alignement basé sur la méthode de classification ascendante hiérarchique est prometteur dans la mesure où cette technique supporte bien les traductions croisées.</abstract>
      <url hash="7da5ea40">2007.jeptalnrecital-poster.22</url>
      <language>fra</language>
      <bibkey>nakamura-delloye-2007-methodes</bibkey>
    </paper>
    <paper id="23">
      <title>Un Lexique Génératif de référence pour le français</title>
      <author><first>Fiammetta</first><last>Namer</last></author>
      <author><first>Pierrette</first><last>Bouillon</last></author>
      <author><first>Évelyne</first><last>Jacquey</last></author>
      <pages>233–242</pages>
      <abstract>Cet article propose une approche originale visant la construction d’un lexique sémantique de référence sur le français. Sa principale caractéristique est de pouvoir s’appuyer sur les propriétés morphologiques des lexèmes. La méthode combine en effet des résultats d’analyse morphologique (Namer, 2002;2003), à partir de ressources lexicales de grande taille (nomenclatures du TLF) et des méthodologies d’acquisition d’information lexicale déjà éprouvées (Namer 2005; Sébillot 2002). Le format de représentation choisi, dans le cadre du Lexique Génératif, se distingue par ses propriétés d’expressivité et d’économie. Cette approche permet donc d’envisager la construction d’un lexique de référence sur le français caractérisé par une forte homogénéité tout en garantissant une couverture large, tant du point de vue de la nomenclature que du point de vue des contenus sémantiques. Une première validation de la méthode fournit une projection quantitative et qualitative des résultats attendus.</abstract>
      <url hash="eb626bce">2007.jeptalnrecital-poster.23</url>
      <language>fra</language>
      <bibkey>namer-etal-2007-un</bibkey>
    </paper>
    <paper id="24">
      <title>Les résultats de la campagne <fixed-case>EASY</fixed-case> d’évaluation des analyseurs syntaxiques du français</title>
      <author><first>Patrick</first><last>Paroubek</last></author>
      <author><first>Anne</first><last>Vilnat</last></author>
      <author><first>Isabelle</first><last>Robba</last></author>
      <author><first>Christelle</first><last>Ayache</last></author>
      <pages>243–252</pages>
      <abstract>Dans cet article, nous présentons les résultats de la campagne d’évaluation EASY des analyseurs syntaxiques du français. EASY a été la toute première campagne d’évaluation comparative des analyseurs syntaxiques du français en mode boîte noire utilisant des mesures objectives quantitatives. EASY fait partie du programme TECHNOLANGUE du Ministère délégué à la Recherche et à l’Éducation, avec le soutien du ministère de délégué à l’industrie et du ministère de la culture et de la communication. Nous exposons tout d’abord la position de la campagne par rapport aux autres projets d’évaluation en analyse syntaxique, puis nous présentos son déroulement, et donnons les résultats des 15 analyseurs participants en fonction des différents types de corpus et des différentes annotations (constituants et relations). Nous proposons ensuite un ensemble de leçons à tirer de cette campagne, en particulier à propos du protocole d’évaluation, de la définition de la segmentation en unités linguistiques, du formalisme et des activités d’annotation, des critères de qualité des données, des annotations et des résultats, et finalement de la notion de référence en analyse syntaxique. Nous concluons en présentant comment les résultats d’EASY se prolongent dans le projet PASSAGE (ANR-06-MDCA-013) qui vient de débuter et dont l’objectif est d’étiqueter un grand corpus par plusieurs analyseurs en les combinant selon des paramètres issus de l’évaluation.</abstract>
      <url hash="aeff13fb">2007.jeptalnrecital-poster.24</url>
      <language>fra</language>
      <bibkey>paroubek-etal-2007-les</bibkey>
    </paper>
    <paper id="25">
      <title>Modèles statistiques enrichis par la syntaxe pour la traduction automatique</title>
      <author><first>Holger</first><last>Schwenk</last></author>
      <author><first>Daniel</first><last>Déchelotte</last></author>
      <author><first>Hélène</first><last>Bonneau-Maynard</last></author>
      <author><first>Alexandre</first><last>Allauzen</last></author>
      <pages>253–262</pages>
      <abstract>La traduction automatique statistique par séquences de mots est une voie prometteuse. Nous présentons dans cet article deux évolutions complémentaires. La première permet une modélisation de la langue cible dans un espace continu. La seconde intègre des catégories morpho-syntaxiques aux unités manipulées par le modèle de traduction. Ces deux approches sont évaluées sur la tâche Tc-Star. Les résultats les plus intéressants sont obtenus par la combinaison de ces deux méthodes.</abstract>
      <url hash="07b1bebc">2007.jeptalnrecital-poster.25</url>
      <language>fra</language>
      <bibkey>schwenk-etal-2007-modeles</bibkey>
    </paper>
    <paper id="26">
      <title>Traitements phrastiques phonétiques pour la réécriture de phrases dysorthographiées</title>
      <author><first>Laurianne</first><last>Sitbon</last></author>
      <author><first>Patrice</first><last>Bellot</last></author>
      <author><first>Philippe</first><last>Blache</last></author>
      <pages>263–272</pages>
      <abstract>Cet article décrit une méthode qui combine des hypothèses graphémiques et phonétiques au niveau de la phrase, à l’aide d’une réprésentation en automates à états finis et d’un modèle de langage, pour la réécriture de phrases tapées au clavier par des dysorthographiques. La particularité des écrits dysorthographiés qui empêche les correcteurs orthographiques d’être efficaces pour cette tâche est une segmentation en mots parfois incorrecte. La réécriture diffère de la correction en ce sens que les phrases réécrites ne sont pas à destination de l’utilisateur mais d’un système automatique, tel qu’un moteur de recherche. De ce fait l’évaluation est conduite sur des versions filtrées et lemmatisées des phrases. Le taux d’erreurs mots moyen passe de 51 % à 20 % avec notre méthode, et est de 0 % sur 43 % des phrases testées.</abstract>
      <url hash="6bedbe3c">2007.jeptalnrecital-poster.26</url>
      <language>fra</language>
      <bibkey>sitbon-etal-2007-traitements</bibkey>
    </paper>
    <paper id="27">
      <title>Vers une méthodologie générique de contrôle basée sur la combinaison de sources de jugement</title>
      <author><first>Grégory</first><last>Smits</last></author>
      <author><first>Christine</first><last>Chardenon</last></author>
      <pages>273–282</pages>
      <abstract>Le contrôle des hypothèses concurrentes générées par les différents modules qui peuvent intervenir dans des processus de TALN reste un enjeu important malgré de nombreuses avancées en terme de robustesse. Nous présentons dans cet article une méthodologie générique de contrôle exploitant des techniques issues de l’aide multicritère à la décision. À partir de l’ensemble des critères de comparaison disponibles et la formalisation des préférences d’un expert, l’approche proposée évalue la pertinence relative des différents objets linguistiques générés et conduit à la mise en place d’une action de contrôle appropriée telle que le filtrage, le classement, le tri ou la propagation.</abstract>
      <url hash="ebd3c44d">2007.jeptalnrecital-poster.27</url>
      <language>fra</language>
      <bibkey>smits-chardenon-2007-vers</bibkey>
    </paper>
    <paper id="28">
      <title>Traitement sémantique par analyse distributionnelle des noms transdisciplinaires des écrits scientifiques</title>
      <author><first>Agnès</first><last>Tutin</last></author>
      <pages>283–292</pages>
      <abstract>Dans cette étude sur le lexique transdisciplinaire des écrits scientifiques, nous souhaitons évaluer dans quelle mesure les méthodes distributionnelles de TAL peuvent faciliter la tâche du linguiste dans le traitement sémantique de ce lexique. Après avoir défini le champ lexical et les corpus exploités, nous testons plusieurs méthodes basées sur des dépendances syntaxiques et observons les proximités sémantiques et les classes établies. L’hypothèse que certaines relations syntaxiques - en particulier les relations de sous-catégorisation – sont plus appropriées pour établir des classements sémantiques n’apparaît qu’en partie vérifiée. Si les relations de sous-catégorisation génèrent des proximités sémantiques entre les mots de meilleure qualité, cela ne semble pas le cas pour la classification par voisinage.</abstract>
      <url hash="19482ad6">2007.jeptalnrecital-poster.28</url>
      <language>fra</language>
      <bibkey>tutin-2007-traitement</bibkey>
    </paper>
    <paper id="29">
      <title>Une expérience de compréhension en contexte de dialogue avec le système <fixed-case>LOGUS</fixed-case>, approche logique de la compréhension de la langue orale</title>
      <author><first>Jeanne</first><last>Villaneau</last></author>
      <pages>293–302</pages>
      <abstract>LOGUS est un système de compréhension de la langue orale dans le cadre d’un dialogue homme-machine finalisé. Il est la mise en oeuvre d’une approche logique qui utilise différents formalismes afin d’obtenir un système robuste mais néanmoins relativement extensible. Cet article décrit essentiellement l’étape de compréhension en contexte de dialogue implémentée sur LOGUS, développée et testée à partir d’un corpus de réservation hôtelière enregistré et annoté lors des travaux du groupe MEDIA du projet technolangue. Il décrit également les différentes interrogations et conclusions que peut susciter une telle expérience et les résultats obtenus par le système dans la résolution des références. Concernant l’approche elle-même, cette expérience semble montrer que le formalisme adopté pour la représentation sémantique des énoncés est bien adapté à la compréhension en contexte.</abstract>
      <url hash="efe42df0">2007.jeptalnrecital-poster.29</url>
      <language>fra</language>
      <bibkey>villaneau-2007-une</bibkey>
    </paper>
    <paper id="30">
      <title>Évaluation des performances d’un modèle de langage stochastique pour la compréhension de la parole arabe spontanée</title>
      <author><first>Anis</first><last>Zouaghi</last></author>
      <author><first>Mounir</first><last>Zrigui</last></author>
      <author><first>Mohamed</first><last>Ben Ahmed</last></author>
      <pages>303–312</pages>
      <abstract>Les modèles de Markov cachés (HMM : Hidden Markov Models) (Baum et al., 1970), sont très utilisés en reconnaissance de la parole et depuis quelques années en compréhension de la parole spontanée latine telle que le français ou l’anglais. Dans cet article, nous proposons d’utiliser et d’évaluer la performance de ce type de modèle pour l’interprétation sémantique de la parole arabe spontanée. Les résultats obtenus sont satisfaisants, nous avons atteint un taux d’erreur de l’ordre de 9,9% en employant un HMM à un seul niveau, avec des probabilités tri

_grammes de transitions.</abstract>
      <url hash="11baeabf">2007.jeptalnrecital-poster.30</url>
      <language>fra</language>
      <bibkey>zouaghi-etal-2007-evaluation</bibkey>
    </paper>
  </volume>
  <volume id="demonstration" ingest-date="2021-02-05" type="proceedings">
    <meta>
      <booktitle>Actes de la 14ème conférence sur le Traitement Automatique des Langues Naturelles. Démonstrations</booktitle>
      <editor><first>Nabil</first><last>Hathout</last></editor>
      <editor><first>Philippe</first><last>Muller</last></editor>
      <publisher>ATALA</publisher>
      <address>Toulouse, France</address>
      <month>June</month>
      <year>2007</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="d1875738">2007.jeptalnrecital-demonstration.0</url>
      <bibkey>jep-taln-recital-2007-actes-de-la</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Présentation du logiciel Antidote <fixed-case>RX</fixed-case></title>
      <author><first>Éric</first><last>Brunelle</last></author>
      <author><first>Simon</first><last>Charest</last></author>
      <pages>315–317</pages>
      <abstract>Antidote RX est la sixième édition d’Antidote, un logiciel d’aide à la rédaction développé et commercialisé par la société Druide informatique. Antidote RX comporte un correcteur grammatical avancé, dix dictionnaires de consultation et dix guides linguistiques. Il fonctionne sous les systèmes d’exploitation Windows, Mac OS X et Linux.</abstract>
      <url hash="2f424920">2007.jeptalnrecital-demonstration.1</url>
      <language>fra</language>
      <bibkey>brunelle-charest-2007-presentation</bibkey>
    </paper>
    <paper id="2">
      <title>Logiciel Cordial</title>
      <author><first>Dominique</first><last>Laurent</last></author>
      <author><first>Sophie</first><last>Nègre</last></author>
      <author><first>Patrick</first><last>Séguéla</last></author>
      <pages>318–320</pages>
      <abstract>Cordial est un correcteur efficace et discret enrichi d’un grand nombre de fonctions d’aide à la rédaction et d’analyse de documents. Très riche avec ces multiples dictionnaires et souvent pertinent dans ses propositions, Cordial est un compagnon précieux qui vous permet d’assurer la qualité de vos écrits. La version 2007 de Cordial s’intègre dans un vaste éventail de logiciels comme les traitements de texte (Word, Open Office, Word Perfect...), clients de messagerie (Outlook, Notes, Thunderbird, webmails...) ou navigateurs (Explorer, Mozilla).</abstract>
      <url hash="7e828944">2007.jeptalnrecital-demonstration.2</url>
      <language>fra</language>
      <bibkey>laurent-etal-2007-logiciel</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>T</fixed-case>rans<fixed-case>C</fixed-case>heck : un vérificateur automatique de traductions</title>
      <author><first>Elliott</first><last>Macklovitch</last></author>
      <author><first>Guy</first><last>Lapalme</last></author>
      <pages>321–323</pages>
      <abstract>Nous offrirons une démonstration de la dernière version de TransCheck, un vérificateur automatique de traductions que le RALI est en train de développer. TransCheck prend en entrée deux textes, un texte source dans une langue et sa traduction dans une autre, les aligne au niveau de la phrase et ensuite vérifie les régions alignées pour s’assurer de la présence de certains équivalents obligatoires (p. ex. la terminologie normalisée) et de l’absence de certaines interdictions de traduction (p. ex. des interférences de la langue source). Ainsi, TransCheck se veut un nouveau type d’outil d’aide à la traduction qui pourra à réduire le fardeau de la révision et diminuer le coût du contrôle de la qualité.</abstract>
      <url hash="61608f10">2007.jeptalnrecital-demonstration.3</url>
      <language>fra</language>
      <bibkey>macklovitch-lapalme-2007-transcheck</bibkey>
    </paper>
    <paper id="4">
      <title>Le <fixed-case>CNRTL</fixed-case>, Centre National de Ressources Textuelles et Lexicales, un outil de mutualisation de ressources linguistiques</title>
      <author><first>Jean-Marie</first><last>Pierrel</last></author>
      <author><first>Etienne</first><last>Petitjean</last></author>
      <pages>324–326</pages>
      <abstract>Créé en 2005 à l’initiative du Centre National de la Recherche Scientifique, le CNRTL propose une plate-forme unifiée pour l’accès aux ressources et documents électroniques destinés à l’étude et l’analyse de la langue française. Les services du CNRTL comprennent le recensement, la documentation (métadonnées), la normalisation, l’archivage, l’enrichissement et la diffusion des ressources. La pérennité du service et des données est garantie par le soutien institutionnel du CNRS, l’adossement à un laboratoire de recherche en linguistique et informatique du CNRS et de Nancy Université (ATILF – Analyse et Traitement Informatique de la Langue Française), ainsi que l’intégration dans le réseau européen CLARIN (common language resources and technology infrastructure european).</abstract>
      <url hash="fc3c7547">2007.jeptalnrecital-demonstration.4</url>
      <language>fra</language>
      <bibkey>pierrel-petitjean-2007-le</bibkey>
    </paper>
  </volume>
  <volume id="recital" ingest-date="2021-02-05" type="proceedings">
    <meta>
      <booktitle>Actes de la 14ème conférence sur le Traitement Automatique des Langues Naturelles. REncontres jeunes Chercheurs en Informatique pour le Traitement Automatique des Langues</booktitle>
      <editor><first>Farah</first><last>Benamara</last></editor>
      <editor><first>Sylwia</first><last>Ozdowska</last></editor>
      <publisher>ATALA</publisher>
      <address>Toulouse, France</address>
      <month>June</month>
      <year>2007</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="38711a0a">2007.jeptalnrecital-recital.0</url>
      <bibkey>jep-taln-recital-2007-actes-de-la-14eme</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Utilisation des ontologies pour la modélisation logique d’une commande en langue naturel</title>
      <author><first>Laurent</first><last>Mazuel</last></author>
      <pages>427–436</pages>
      <abstract>Dans cet article, nous nous intéressons à l’interprétation de commandes en langue naturelle pour un agent artificiel. Notre architecture repose sur une modélisation logique de la commande pour l’interprétation sémantique, qui permet de capturer la « structure fonctionnelle » de la phrase, c’est-à-dire les rôles des termes les uns par rapport aux autres. Cet article décrit une méthode d’analyse structurelle de surface qui s’appuie sur l’ontologie de l’agent pour construire cette modélisation logique. Nous définissons tout d’abord un algorithme d’ancrage des termes de la commande dans l’ontologie de l’agent puis nous montrons comment s’en servir pour l’analyse de surface. Enfin, nous expliquons brièvement comment notre modélisation peut être utilisée au moment de l’interprétation sémantique des commandes.</abstract>
      <url hash="7c693a3b">2007.jeptalnrecital-recital.1</url>
      <language>fra</language>
      <bibkey>mazuel-2007-utilisation</bibkey>
    </paper>
    <paper id="2">
      <title>L’analyse morphologique des réponses d’apprenants</title>
      <author><first>Alexia</first><last>Blanchard</last></author>
      <pages>437–446</pages>
      <abstract>Nous présentons une approche empirique de l’évaluation automatique des réponses d’apprenants au sein d’un système d’Apprentissage des Langues Assisté par Ordinateur (ALAO). Nous proposons la mise en place d’un module d’analyse d’erreurs attestées sur corpus qui s’appuie sur des techniques robustes de Traitement Automatique des Langues (TAL). Cet article montre la réalisation d’un module d’analyse de morphologie flexionnelle, en situation hors-contexte, à partir d’un modèle linguistique existant.</abstract>
      <url hash="712fbc6b">2007.jeptalnrecital-recital.2</url>
      <language>fra</language>
      <bibkey>blanchard-2007-lanalyse</bibkey>
    </paper>
    <paper id="3">
      <title>Repérage automatique de génériques dans les définitions terminographiques</title>
      <author><first>Selja</first><last>Seppälä</last></author>
      <pages>447–456</pages>
      <abstract>Cet article présente une procédure de repérage et de balisage de l’élément générique de la définition terminographique exploitant les caractéristiques formelles du sous-langage définitoire. La procédure, qui comporte quatre étapes, constitue l’une des sous-tâches d’un analyseur (semi-)automatique de la structure conceptuelle des définitions terminographiques, destiné à faciliter l’annotation d’un corpus en vue de l’étude de régularités dans cette structure. La tâche décrite consiste à mettre au point un système d’annotation automatique basé sur le repérage d’indices morphosyntaxiques, sans recourir à d’autres ressources linguistiques informatisées.</abstract>
      <url hash="dd4ce994">2007.jeptalnrecital-recital.3</url>
      <language>fra</language>
      <bibkey>seppala-2007-reperage</bibkey>
    </paper>
    <paper id="4">
      <title>Extraction de paraphrases désambiguïsées à partir d’un corpus d’articles encyclopédiques alignés automatiquement</title>
      <author><first>François-Régis</first><last>Chaumartin</last></author>
      <pages>457–466</pages>
      <abstract>Nous décrivons ici comment enrichir automatiquement WordNet en y important des articles encyclopédiques. Ce processus permet de créer des nouvelles entrées, en les rattachant au bon hyperonyme. Par ailleurs, les entrées préexistantes de WordNet peuvent être enrichies de descriptions complémentaires. La répétition de ce processus sur plusieurs encyclopédies permet de constituer un corpus d’articles comparables. On peut ensuite extraire automatiquement des paraphrases à partir des couples d’articles ainsi créés. Grâce à l’application d’une mesure de similarité, utilisant la hiérarchie de verbes de WordNet, les constituants de ces paraphrases peuvent être désambiguïsés.</abstract>
      <url hash="442854fb">2007.jeptalnrecital-recital.4</url>
      <language>fra</language>
      <bibkey>chaumartin-2007-extraction</bibkey>
    </paper>
    <paper id="5">
      <title>Extension de l’encodage formel des fonctions lexicales dans le cadre de la Lexicologie Explicative et Combinatoire</title>
      <author><first>Anne-Laure</first><last>Jousse</last></author>
      <pages>467–476</pages>
      <abstract>Dans les ressources dictionnairiques développées à partir du cadre théorique de la Lexicologie Explicative et Combinatoire telles que le DiCo, les relations sémanticolexicales sont modélisées au moyen de fonctions lexicales. Cependant, seulement la majorité d’entre elles (dites standard) répondent véritablement à un encodage formel. Les autres (dites non standard), représentant des relations plus spécifiques à certaines unités lexicales, sont écrites sous la forme d’un encodage hétérogène et très peu formalisé. Par conséquent, certaines relations ne peuvent entrer en ligne de compte dans les traitements automatiques. Nous proposons dans cet article une méthodologie pour la normalisation des fonctions lexicales non standard afin de les rendre exploitables dans des applications telles que l’analyse et la génération de texte. Pour ce faire, nous discutons certains principes théoriques associés à ce formalisme de description et esquissons des propositions pour un traitement global et homogène de l’ensemble des relations décrites dans le DiCo.</abstract>
      <url hash="c796f66c">2007.jeptalnrecital-recital.5</url>
      <language>fra</language>
      <bibkey>jousse-2007-extension</bibkey>
    </paper>
    <paper id="6">
      <title>Traitement de désignations orales dans un contexte visuel</title>
      <author><first>Ali</first><last>Choumane</last></author>
      <pages>477–486</pages>
      <abstract>Nous nous intéressons aux systèmes multimodaux qui utilisent les modes et modalités suivantes : l’oral (et le langage naturel) en entrée et en sortie, le geste en entrée et le visuel en sortie par affichage sur écran. L’usager échange avec le système par un geste et/ou un énoncé oral en langue naturelle. Dans cet échange, encodé sur les différentes modalités, se trouvent l’expression du but de l’usager et la désignation des objets (référents) nécessaires à la réalisation de ce but. Le système doit identifier de manière précise et non ambiguë les objets désignés par l’usager. Nous traitons plus spécialement dans cet article les désignations orales, sans geste, des objets dans le contexte visuel. En effet, l’ensemble du contexte multimodal, dont le mode visuel, influe sur la production de l’entrée de l’usager. Afin d’identifier une désignation produite en s’appuyant sur le contexte visuel, nous proposons un algorithme qui utilise des connaissances « classiques » linguistiques, des connaissances sur les objets manipulés, et des connaissances sur les aspects perceptifs (degré de saillance) associés à ces objets.</abstract>
      <url hash="987c7171">2007.jeptalnrecital-recital.6</url>
      <language>fra</language>
      <bibkey>choumane-2007-traitement</bibkey>
    </paper>
    <paper id="7">
      <title>Combinaison de ressources linguistiques pour l’aide à l’accès lexical : étude de faisabilité</title>
      <author><first>Laurianne</first><last>Sitbon</last></author>
      <pages>487–496</pages>
      <abstract>Cet article propose une évaluation combinée et comparative de 5 ressources (descriptive, paradigmatique et syntagmatiques) pour l’aide à l’accès lexical en situation de “mot sur le bout de la langue”, en vue de la création d’un outil utilisant la combinaison de ces ressources. En situation de “mot sur le bout de la langue”, l’utilisateur n’accède plus au mot qu’il veut dire ou écrire mais est capable d’en produire d’autres sémantiquement associés. L’évaluation se base sur un corpus de 20 mots “sur le bout de la langue” pour lesquels on dispose de 50 groupes de 5 associations sémantiques effectuées par des utilisateurs. Les résultats montrent que les ressources sont complémentaires et peu redondantes. De plus au moins une association proposée parmi les 5 permettrait de retrouver le mot “sur le bout de la langue” dans 79% des cas, à condition de le sélectionner parmi les 2500 mot potentiels. Enfin, les résultats montrent des disparités entre les utilisateurs, ce qui permettrait de définir des profils d’utilisateur pour une amélioration des performances.</abstract>
      <url hash="49bcc0f5">2007.jeptalnrecital-recital.7</url>
      <language>fra</language>
      <bibkey>sitbon-2007-combinaison</bibkey>
    </paper>
  </volume>
  <volume id="recitalposter" ingest-date="2021-02-05" type="proceedings">
    <meta>
      <booktitle>Actes de la 14ème conférence sur le Traitement Automatique des Langues Naturelles. REncontres jeunes Chercheurs en Informatique pour le Traitement Automatique des Langues (Posters)</booktitle>
      <editor><first>Farah</first><last>Benamara</last></editor>
      <editor><first>Sylwia</first><last>Ozdowska</last></editor>
      <publisher>ATALA</publisher>
      <address>Toulouse, France</address>
      <month>June</month>
      <year>2007</year>
      <venue>jeptalnrecital</venue>
    </meta>
    <frontmatter>
      <url hash="a0cfc16d">2007.jeptalnrecital-recitalposter.0</url>
      <bibkey>jep-taln-recital-2007-actes-de-la-14eme-sur</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Vers une nouvelle structuration de l’information extraite automatiquement</title>
      <author><first>Alejandro</first><last>Acosta</last></author>
      <pages>337–346</pages>
      <abstract>Les systèmes d’Extraction d’Information se contentent, le plus souvent, d’enrichir des bases de données plates avec les informations qu’ils extraient. Nous décrivons dans cet article un travail en cours sur l’utilisation de données extraites automatiquement pour la construction d’une structure de représentation plus complexe. Cette structure modélise un réseau social composé de relations entre les entités d’un corpus de biographies.</abstract>
      <url hash="419e90ba">2007.jeptalnrecital-recitalposter.1</url>
      <language>fra</language>
      <bibkey>acosta-2007-vers</bibkey>
    </paper>
    <paper id="2">
      <title>Vers une ressource prédicative pour l’extraction d’information</title>
      <author><first>Aurélien</first><last>Bossard</last></author>
      <pages>347–356</pages>
      <abstract>Cet article présente une méthode pour construire, à partir d’une ressource lexicale prédicative existante, une ressource enrichie pouvant servir à une tâche d’extraction. Nous montrons les points forts et les lacunes de deux ressources existantes pour le Français : les Tables du LADL et Volem. Après avoir montré pourquoi nous avons sélectionné Volem, nous listons les données nécessaires à la tâche d’extraction d’information. Nous présentons le processus d’enrichissement de la ressource initiale et une évaluation, à travers une tâche d’extraction d’information concernant des textes de rachats d’entreprise.</abstract>
      <url hash="73bf1d00">2007.jeptalnrecital-recitalposter.2</url>
      <language>fra</language>
      <bibkey>bossard-2007-vers</bibkey>
    </paper>
    <paper id="3">
      <title>Caractérisation d’un corpus de requêtes d’assistance</title>
      <author><first>François</first><last>Bouchet</last></author>
      <pages>357–366</pages>
      <abstract>Afin de concevoir un agent conversationnel logiciel capable d’assister des utilisateurs novices d’applications informatiques, nous avons été amenés à constituer un corpus spécifique de requêtes d’assistance en français, et à étudier ses caractéristiques. Nous montrons ici que les requêtes d’assistance se distinguent nettement de requêtes issues d’autres corpus disponibles dans des domaines proches. Nous mettons également en évidence le fait que ce corpus n’est pas homogène, mais contient au contraire plusieurs activités conversationnelles distinctes, dont l’assistance elle-même. Ces observations nous permettent de discuter de l’opportunité de considérer l’assistance comme un registre particulier de la langue générale.</abstract>
      <url hash="e2ab6218">2007.jeptalnrecital-recitalposter.3</url>
      <language>fra</language>
      <bibkey>bouchet-2007-caracterisation</bibkey>
    </paper>
    <paper id="4">
      <title>Extraction endogène d’une structure de document pour un alignement multilingue</title>
      <author><first>Romain</first><last>Brixtel</last></author>
      <pages>367–376</pages>
      <abstract>Pour des raisons variées, diverses communautés se sont intéressées aux corpus multilingues. Parmi ces corpus, les textes parallèles sont utilisés aussi bien en terminologie, lexicographie ou comme source d’informations pour les systèmes de traduction par l’exemple. L’Union Européenne, qui a entraîné la production de document législatif dans vingtaine de langues, est une des sources de ces textes parallèles. Aussi, avec le Web comme vecteur principal de diffusion de ces textes parallèles, cet objet d’étude est passé à un nouveau statut : celui de document. Cet article décrit un système d’alignement prenant en compte un grand nombre de langues simultanément (&gt; 2) et les caractéristiques structurelles des documents analysés.</abstract>
      <url hash="9b8c2a41">2007.jeptalnrecital-recitalposter.4</url>
      <language>fra</language>
      <bibkey>brixtel-2007-extraction</bibkey>
    </paper>
    <paper id="5">
      <title>Évaluation transparente de systèmes de questions-réponses : application au focus</title>
      <author><first>Sarra</first><last>El Ayari</last></author>
      <pages>377–386</pages>
      <abstract>Les campagnes d’évaluation ne tiennent compte que des résultats finaux obtenus par les systèmes de recherche d’informations (RI). Nous nous situons dans une perspective d’évaluation transparente d’un système de questions-réponses, où le traitement d’une question se fait grâce à plusieurs composants séquentiels. Dans cet article, nous nous intéressons à l’étude de l’élément de la question qui porte l’information qui se trouvera dans la phrase réponse à proximité de la réponse elle-même : le focus. Nous définissons ce concept, l’appliquons au système de questions-réponses QALC, et démontrons l’utilité d’évaluations des composants afin d’augmenter la performance globale du système.</abstract>
      <url hash="f464c5bb">2007.jeptalnrecital-recitalposter.5</url>
      <language>fra</language>
      <bibkey>el-ayari-2007-evaluation</bibkey>
    </paper>
    <paper id="6">
      <title>La segmentation thématique <fixed-case>T</fixed-case>ext<fixed-case>T</fixed-case>iling comme indice pour le repérage de segments d’information évolutive dans un corpus de textes encyclopédiques</title>
      <author><first>Marion</first><last>Laignelet</last></author>
      <author><first>Christophe</first><last>Pimm</last></author>
      <pages>387–396</pages>
      <abstract>Nous faisons l’hypothèse que les bornes délimitées par la méthode statistique TextTiling peuvent servir d’indices qui, cumulées à des indices de nature linguistique, permettront de repérer automatiquement des segments d’informations évolutives. Ce travail est développé dans le cadre d’un projet industriel plus général dont le but est le repérage automatique de zones textuelles contenant de l’information potentiellement évolutive.</abstract>
      <url hash="fe3549b3">2007.jeptalnrecital-recitalposter.6</url>
      <language>fra</language>
      <bibkey>laignelet-pimm-2007-la</bibkey>
    </paper>
    <paper id="7">
      <title>Annotation des disfluences dans les corpus oraux</title>
      <author><first>Marie</first><last>Piu</last></author>
      <author><first>Rémi</first><last>Bove</last></author>
      <pages>397–406</pages>
      <abstract>Les disfluences (répétitions, amorces, autocorrections, constructions inachevées, etc.) inhérentes à toute production orale spontanée constituent une réelle difficulté en termes d’annotation. En effet, l’annotation de ces phénomènes se révèle difficilement automatisable dans la mesure où leur étude réclame un jugement éminemment interprétatif. Dans cet article, nous présentons une méthodologie applicable à l’annotation des disfluences (ou « phénomènes de production ») que l’on rencontre fréquemment dans les corpus oraux. Le fait de constituer un tel corpus de données annotées, permet non seulement de représenter certains aspects pertinents de l’oral (de manière à servir de base aux observations et aux comparaisons avec d’autres données) mais aussi d’améliorer in fine le traitement automatique de l’oral (notamment l’analyse syntaxique automatique).</abstract>
      <url hash="2d928e1e">2007.jeptalnrecital-recitalposter.7</url>
      <language>fra</language>
      <bibkey>piu-bove-2007-annotation</bibkey>
    </paper>
    <paper id="8">
      <title>Architecture modulaire portable pour la génération du langage naturel en dialogue homme-machine</title>
      <author><first>Vladimir</first><last>Popescu</last></author>
      <pages>407–416</pages>
      <abstract>La génération du langage naturel pour le dialogue oral homme-machine pose des contraintes spécifiques, telles que la spontanéité et le caractère fragmenté des énoncés, les types des locuteurs ou les contraintes de temps de réponse de la part du système. Dans ce contexte, le problème d’une architecture rigoureusement spécifiée se pose, autant au niveau des étapes de traitement et des modules impliqués, qu’au niveau des interfaces entre ces modules. Afin de permettre une liberté quasi-totale à l’égard des démarches théoriques, une telle architecture doit être à la fois modulaire (c’est-à-dire, permettre l’indépendance des niveaux de traitement les uns des autres) et portable (c’est-à-dire, permettre l’interopérabilité avec des modules conçus selon des architectures standard en génération du langage naturel, telles que le modèle RAGS - « Reference Architecture for Generation Systems »). Ainsi, dans cet article on présente de manière concise l’architecture proposée, la comparant ensuite au modèle RAGS, pour argumenter les choix opérés en conception. Dans un second temps, la portabilité de l’architecture sera décrite à travers un exemple étendu, dont la généralité réside dans l’obtention d’un ensemble de règles permettant de plonger automatiquement les représentations des informations de notre architecture vers le format du modèle RAGS et inversement. Finalement, un ensemble de conclusions et perspectives clôturera l’article.</abstract>
      <url hash="f3b1fb9c">2007.jeptalnrecital-recitalposter.8</url>
      <language>fra</language>
      <bibkey>popescu-2007-architecture</bibkey>
    </paper>
    <paper id="9">
      <title>Résolution anaphorique intégrée à une analyse automatique de discours d’un corpus oral retranscrit</title>
      <author><first>Alain</first><last>Régnier</last></author>
      <pages>417–426</pages>
      <abstract>Nous présentons une résolution anaphorique intégrée à une analyse automatique de discours. Cette étude traite des anaphores pronominales et des anaphores zéro. Notre analyse est basée sur trois approches : une analyse basée sur les contraintes, une analyse fonctionnelle et une analyse dynamique. Pour évaluer la faisabilité et la fiabilité de notre approche, nous l’avons expérimentée sur un corpus de 97 histoires produites à l’oral par des enfants. Nous présentons le résultat de cette évaluation.</abstract>
      <url hash="2eba9159">2007.jeptalnrecital-recitalposter.9</url>
      <language>fra</language>
      <bibkey>regnier-2007-resolution</bibkey>
    </paper>
  </volume>
</collection>
