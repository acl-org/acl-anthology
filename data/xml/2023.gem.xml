<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.gem">
  <volume id="1" ingest-date="2024-03-27" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)</booktitle>
      <editor><first>Sebastian</first><last>Gehrmann</last></editor>
      <editor><first>Alex</first><last>Wang</last></editor>
      <editor><first>João</first><last>Sedoc</last></editor>
      <editor><first>Elizabeth</first><last>Clark</last></editor>
      <editor><first>Kaustubh</first><last>Dhole</last></editor>
      <editor><first>Khyathi Raghavi</first><last>Chandu</last></editor>
      <editor><first>Enrico</first><last>Santus</last></editor>
      <editor><first>Hooman</first><last>Sedghamiz</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Singapore</address>
      <month>December</month>
      <year>2023</year>
      <url hash="18c92b88">2023.gem-1</url>
      <venue>gem</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="c471f37d">2023.gem-1.0</url>
      <bibkey>gem-2023-natural</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Contextualizing the Limits of Model &amp; Evaluation Dataset Curation on Semantic Similarity Classification Tasks</title>
      <author><first>Daniel</first><last>Theron</last><affiliation>Google</affiliation></author>
      <pages>1-8</pages>
      <abstract>This paper demonstrates how the limitations of pre-trained models and open evaluation datasets factor into assessing the performance of binary semantic similarity classification tasks. As (1) end-user-facing documentation around the curation of these datasets and pre-trained model training regimes is often not easily accessible and (2) given the lower friction and higher demand to quickly deploy such systems in real-world contexts, our study reinforces prior work showing performance disparities across datasets, embedding techniques and distance metrics, while highlighting the importance of understanding how data is collected, curated and analyzed in semantic similarity classification.</abstract>
      <url hash="48e62485">2023.gem-1.1</url>
      <bibkey>theron-2023-contextualizing</bibkey>
    </paper>
    <paper id="2">
      <title>Dialogue Quality and Emotion Annotations for Customer Support Conversations</title>
      <author><first>John</first><last>Mendonca</last><affiliation>INESC-ID/Instituto Superior Técnico</affiliation></author>
      <author><first>Patrícia</first><last>Pereira</last><affiliation>Instituto Superior Técnico / INESC-ID</affiliation></author>
      <author><first>Miguel</first><last>Menezes</last><affiliation>Inesc-ID; Unbabel; Faculdade de Letras da Universidade de Lisboa</affiliation></author>
      <author><first>Vera</first><last>Cabarrão</last><affiliation>Unbabel</affiliation></author>
      <author><first>Ana C</first><last>Farinha</last><affiliation>Unbabel</affiliation></author>
      <author><first>Helena</first><last>Moniz</last><affiliation>INESC-ID, University of Lisbon, EAMT</affiliation></author>
      <author id="alon-lavie"><first>Alon</first><last>Lavie</last><affiliation>Unbabel/Carnegie Mellon University</affiliation></author>
      <author><first>Isabel</first><last>Trancoso</last><affiliation>INESC-ID / IST Univ. Lisbon</affiliation></author>
      <pages>9-21</pages>
      <abstract>Task-oriented conversational datasets often lack topic variability and linguistic diversity. However, with the advent of Large Language Models (LLMs) pretrained on extensive, multilingual and diverse text data, these limitations seem overcome. Nevertheless, their generalisability to different languages and domains in dialogue applications remains uncertain without benchmarking datasets. This paper presents a holistic annotation approach for emotion and conversational quality in the context of bilingual customer support conversations. By performing annotations that take into consideration the complete instances that compose a conversation, one can form a broader perspective of the dialogue as a whole. Furthermore, it provides a unique and valuable resource for the development of text classification models. To this end, we present benchmarks for Emotion Recognition and Dialogue Quality Estimation and show that further research is needed to leverage these models in a production setting.</abstract>
      <url hash="b385aea4">2023.gem-1.2</url>
      <attachment type="SupplementaryMaterial" hash="97390de9">2023.gem-1.2.SupplementaryMaterial.zip</attachment>
      <bibkey>mendonca-etal-2023-dialogue</bibkey>
    </paper>
    <paper id="3">
      <title>Formalizing content creation and evaluation methods for <fixed-case>AI</fixed-case>-generated social media content</title>
      <author><first>Christian</first><last>Jensen</last><affiliation>University of Copenhagen</affiliation></author>
      <author><first>Axel</first><last>Højmark</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>22-41</pages>
      <abstract>This study explores the use of large language models (LLMs), such as ChatGPT and GPT-4, in creating high-quality text-based social media content for businesses on LinkedIn. We introduce a novel architecture incorporating external knowledge bases and a multi-step writing approach, which extracts facts from company websites to form a knowledge graph. Our method’s efficacy is assessed using the “Long-LinkedIn” evaluation dataset designed for long-form post generation. Results indicate that our iterative refinement significantly improves content quality. However, knowledge-enhanced prompts occasionally reduced quality due to potential formulation issues. LLM-based evaluations, particularly using ChatGPT, showcased potential as a less resource-intensive alternative to human assessments, with a notable alignment between the two evaluation techniques.</abstract>
      <url hash="5d8d81aa">2023.gem-1.3</url>
      <attachment type="SupplementaryMaterial" hash="00000000">2023.gem-1.3.SupplementaryMaterial.zip</attachment>
      <bibkey>jensen-hojmark-2023-formalizing</bibkey>
    </paper>
    <paper id="4">
      <title>Automatic Evaluation of Generative Models with Instruction Tuning</title>
      <author><first>Shuhaib</first><last>Mehri</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Vered</first><last>Shwartz</last><affiliation>University of British Columbia</affiliation></author>
      <pages>42-52</pages>
      <abstract>Automatic evaluation of natural language generation has long been an elusive goal in NLP. A recent paradigm fine-tunes pre-trained language models to emulate human judgements for a particular task and evaluation criterion. Inspired by the generalization ability of instruction-tuned models, we propose a learned metric based on instruction tuning. To test our approach, we collected HEAP, a dataset of human judgements across various NLG tasks and evaluation criteria. Our findings demonstrate that instruction tuning language models on HEAP yields good performance on many evaluation tasks, though some criteria are less trivial to learn than others. Further, jointly training on multiple tasks can yield additional performance improvements, which can be beneficial for future tasks with little to no human annotated data.</abstract>
      <url hash="6ce1f6ea">2023.gem-1.4</url>
      <bibkey>mehri-shwartz-2023-automatic</bibkey>
    </paper>
    <paper id="5">
      <title>Effective Proxy for Human Labeling: Ensemble Disagreement Scores in Large Language Models for Industrial <fixed-case>NLP</fixed-case></title>
      <author><first>Wei</first><last>Du</last><affiliation>Applied Scientist</affiliation></author>
      <author><first>Laksh</first><last>Advani</last><affiliation>Qualtrics</affiliation></author>
      <author><first>Yashmeet</first><last>Gambhir</last><affiliation>Qualtrics</affiliation></author>
      <author><first>Daniel</first><last>Perry</last><affiliation>Qualtrics</affiliation></author>
      <author><first>Prashant</first><last>Shiralkar</last><affiliation>Qualtrics</affiliation></author>
      <author><first>Zhengzheng</first><last>Xing</last><affiliation>Qualtrics</affiliation></author>
      <author><first>Aaron</first><last>Colak</last><affiliation>Qualtrics</affiliation></author>
      <pages>53-61</pages>
      <abstract>Large language models (LLMs) have demonstrated significant capability to generalize across a large number of NLP tasks. For industry applications, it is imperative to assess the performance of the LLM on unlabeled production data from time to time to validate for a real-world setting. Human labeling to assess model error requires considerable expense and time delay. Here we demonstrate that ensemble disagreement scores work well as a proxy for human labeling for language models in zero-shot, few-shot, and fine-tuned settings, per our evaluation on keyphrase extraction (KPE) task. We measure fidelity of the results by comparing to true error measured from human labeled ground truth. We contrast with the alternative of using another LLM as a source of machine labels, or ‘silver labels’. Results across various languages and domains show disagreement scores provide a better estimation of model performance with mean average error (MAE) as low as 0.4% and on average 13.8% better than using silver labels.</abstract>
      <url hash="51884468">2023.gem-1.5</url>
      <bibkey>du-etal-2023-effective</bibkey>
    </paper>
    <paper id="6">
      <title>Automatic Reflection Generation for Peer-to-Peer Counseling</title>
      <author><first>Emma</first><last>O’neil</last><affiliation>University of Pennsylvania</affiliation></author>
      <author><first>João</first><last>Sedoc</last><affiliation>New York University</affiliation></author>
      <author><first>Diyi</first><last>Yang</last><affiliation>Stanford University</affiliation></author>
      <author><first>Haiyi</first><last>Zhu</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author id="lyle-ungar"><first>Lyle</first><last>Ungar</last><affiliation>University of Pennsylvania</affiliation></author>
      <pages>62-75</pages>
      <abstract>Online peer counseling platforms enable conversations between millions of people seeking and offering mental health support. Among counseling skills, reflective listening, i.e., capturing and returning to the client something the client has said, is important for positive therapeutic outcomes. We introduce a reflection generation system for online mental health support conversations leveraging GPT-3, a large language model. We compare few-shot learning against fine-tuning and assess the impact of the quality of training examples as measured by fluency, reflection resemblance, and overall preference. Fine-tuned GPT-3 generates responses that human evaluators rate as comparable in reflection quality to responses used for tuning. Models based on high-quality responses generate substantially better reflections than ones tuned on actual responses from a large online counseling service–and better reflections than the actual counselor responses. These results suggest the care needed in selecting examples for tuning generative models.</abstract>
      <url hash="c4df9163">2023.gem-1.6</url>
      <bibkey>oneil-etal-2023-automatic</bibkey>
    </paper>
    <paper id="7">
      <title>One-Shot and Few-Shot Exemplification Modeling</title>
      <author><first>John</first><last>Harvill</last><affiliation>University of Illinois at Urbana-Champaign</affiliation></author>
      <author><first>Hee Suk</first><last>Yoon</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Eunseop</first><last>Yoon</last><affiliation>Korea Advanced Institute of Science and Technology</affiliation></author>
      <author><first>Mark</first><last>Hasegawa-Johnson</last><affiliation>University of Illinois</affiliation></author>
      <author><first>Chang</first><last>Yoo</last><affiliation>kaist</affiliation></author>
      <pages>76-87</pages>
      <abstract>Exemplification modeling is a task where the goal is to produce a viable example sentence that uses a target word with a target definition. The task is non-trivial for polysemous words, and previous works have only explored settings where ample labeled training data is available. In this paper, we demonstrate that exemplification modeling can be performed without a large labeled training corpus by either changing the format of the task (one-shot) or prompting large language models (few-shot), and ablate key components of our proposed one-shot and few-shot systems. We provide extensive automatic and human evaluations of model performance and find that our proposed one-shot and few-shot approaches perform similarly to a fully supervised baseline. We compare and contrast each method in terms of labeled training dataset size, performance, and model size, and find that each technique has at least one tradeoff that another approach does not.</abstract>
      <url hash="bf43c364">2023.gem-1.7</url>
      <bibkey>harvill-etal-2023-one-shot</bibkey>
    </paper>
    <paper id="8">
      <title>Leveraging Large Language Models for Enhanced Product Descriptions in e<fixed-case>C</fixed-case>ommerce</title>
      <author><first>Jianghong</first><last>Zhou</last><affiliation>Walmart Global Tech</affiliation></author>
      <author><first>Bo</first><last>Liu</last><affiliation>Walmart Global Tech</affiliation></author>
      <author><first>Jhalak</first><last>Acharya</last><affiliation>Walmart Global Tech</affiliation></author>
      <author><first>Yao</first><last>Hong</last><affiliation>Walmart Global Tech</affiliation></author>
      <author><first>Kuang-Chih</first><last>Lee</last><affiliation>Walmart Global Tech</affiliation></author>
      <author><first>Musen</first><last>Wen</last><affiliation>Walmart Global Tech</affiliation></author>
      <pages>88-96</pages>
      <abstract>In the dynamic field of eCommerce, the quality and comprehensiveness of product descriptions are pivotal for enhancing search visibility and customer engagement. Effective product descriptions can address the ‘cold start’ problem, align with market trends, and ultimately lead to increased click-through rates. Traditional methods for crafting these descriptions often involve significant human effort and may lack both consistency and scalability. This paper introduces a novel methodology for automating product description generation using the LLAMA 2.0 7B language model. We train the model on a dataset of authentic product descriptions from Walmart, one of the largest eCommerce platforms. The model is then fine-tuned for domain-specific language features and eCommerce nuances to enhance its utility in sales and user engagement. We employ multiple evaluation metrics—including NDCG, customer click-through rates, and human assessments—to validate the effectiveness of our approach. Our findings reveal that the system is not only scalable but also significantly reduces the human workload involved in creating product descriptions. This study underscores the considerable potential of large language models like LLAMA 2.0 7B in automating and optimizing various facets of eCommerce platforms, offering significant business impact, including improved search functionality and increased sales.</abstract>
      <url hash="93cd6548">2023.gem-1.8</url>
      <bibkey>zhou-etal-2023-leveraging</bibkey>
      <video href="2023.gem-1.8.mp4"/>
    </paper>
    <paper id="9">
      <title><fixed-case>QAMPARI</fixed-case>: A Benchmark for Open-domain Questions with Many Answers</title>
      <author><first>Samuel</first><last>Amouyal</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Tomer</first><last>Wolfson</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Ohad</first><last>Rubin</last><affiliation>Tel Aviv University</affiliation></author>
      <author><first>Ori</first><last>Yoran</last><affiliation>Tel-Aviv University</affiliation></author>
      <author><first>Jonathan</first><last>Herzig</last><affiliation>Google Research</affiliation></author>
      <author><first>Jonathan</first><last>Berant</last><affiliation>Tel Aviv University and AI2</affiliation></author>
      <pages>97-110</pages>
      <abstract>Existing benchmarks for open-domain question answering (ODQA) typically focus on questions whose answers are all in a single paragraph. By contrast, many natural questions, such as “What players were drafted by the Brooklyn Nets?” have a long list of answers extracted from multiple paragraphs. Answering such questions requires retrieving and reading many passages from a large corpus. We introduce QAMPARI, an ODQA benchmark, where answers are lists of entities, spread across many paragraphs. We created QAMPARI by (a) generating questions with multiple answers from Wikipedia’s knowledge graph and tables, (b) automatically pairing answers with supporting evidence in Wikipedia paragraphs, and (c) manually paraphrasing questions and validating each answer. Across a wide range of ODQA models, we find that QAMPARI is challenging in terms of both passage retrieval and answer generation, with models reaching an F1 score of 32.8 at best. We view QAMPARI as a valuable resource for ODQA research, which will aid to develop models that handle a broad range of question types, including single and multi-answer questions.</abstract>
      <url hash="e6facdfe">2023.gem-1.9</url>
      <bibkey>amouyal-etal-2023-qampari</bibkey>
    </paper>
    <paper id="10">
      <title>Unveiling Safety Vulnerabilities of Large Language Models</title>
      <author><first>George</first><last>Kour</last><affiliation>IBM Research</affiliation></author>
      <author><first>Marcel</first><last>Zalmanovici</last><affiliation>IBM</affiliation></author>
      <author><first>Naama</first><last>Zwerdling</last><affiliation>IBM</affiliation></author>
      <author><first>Esther</first><last>Goldbraich</last><affiliation>IBM</affiliation></author>
      <author><first>Ora</first><last>Fandina</last><affiliation>IBM Research</affiliation></author>
      <author><first>Ateret</first><last>Anaby Tavor</last><affiliation>IBM Research AI</affiliation></author>
      <author><first>Orna</first><last>Raz</last><affiliation>IBM Research</affiliation></author>
      <author><first>Eitan</first><last>Farchi</last><affiliation>IBM research</affiliation></author>
      <pages>111-127</pages>
      <abstract>As large language models become more prevalent, their possible harmful or inappropriate responses are a cause for concern. This paper introduces a unique dataset containing adversarial examples in the form of questions, we call AttaQ, designed to provoke such harmful or inappropriate responses. We assess the efficacy of our dataset by analyzing the vulnerabilities of various models when subjected to it. Additionally, we introduce a novel automatic approach for identifying and naming vulnerable semantic regions — input semantic areas for which the model is likely to produce harmful outputs. This is achieved through the application of specialized clustering techniques that consider both the semantic similarity of the input attacks and the harmfulness of the model’s responses.Automatically identifying vulnerable semantic regions enhances the evaluation of model weaknesses, facilitating targeted improvements to its safety mechanisms and overall reliability.</abstract>
      <url hash="de9bb6ec">2023.gem-1.10</url>
      <bibkey>kour-etal-2023-unveiling</bibkey>
    </paper>
    <paper id="11">
      <title>Adapting Pre-trained Generative Models for Extractive Question Answering</title>
      <author><first>Prabir</first><last>Mallick</last><affiliation>TCS research</affiliation></author>
      <author id="tapas-nayak"><first>Tapas</first><last>Nayak</last><affiliation>TCS Research</affiliation></author>
      <author><first>Indrajit</first><last>Bhattacharya</last><affiliation>TCS Research</affiliation></author>
      <pages>128-137</pages>
      <abstract>Pre-trained Generative models such as BART, T5, etc. have gained prominence as a preferred method for text generation in various natural language processing tasks, including abstractive long-form question answering (QA) and summarization. However, the potential of generative models in extractive QA tasks, where discriminative models are commonly employed, remains largely unexplored. Discriminative models often encounter challenges associated with label sparsity, particularly when only a small portion of the context contains the answer. The challenge is more pronounced for multi-span answers. In this work, we introduce a novel approach that uses the power of pre-trained generative models to address extractive QA tasks by generating indexes corresponding to context tokens or sentences that form part of the answer. Through comprehensive evaluations on multiple extractive QA datasets, including MultiSpanQA, BioASQ, MASHQA, and WikiQA, we demonstrate the superior performance of our proposed approach compared to existing state-of-the-art models.</abstract>
      <url hash="70d9bf18">2023.gem-1.11</url>
      <bibkey>mallick-etal-2023-adapting</bibkey>
    </paper>
    <paper id="12">
      <title>Predicting Question-Answering Performance of Large Language Models through Semantic Consistency</title>
      <author><first>Ella</first><last>Rabinovich</last><affiliation>IBM Research</affiliation></author>
      <author><first>Samuel</first><last>Ackerman</last><affiliation>IBM Research</affiliation></author>
      <author><first>Orna</first><last>Raz</last><affiliation>IBM Research</affiliation></author>
      <author><first>Eitan</first><last>Farchi</last><affiliation>IBM research</affiliation></author>
      <author><first>Ateret</first><last>Anaby Tavor</last><affiliation>IBM Research AI</affiliation></author>
      <pages>138-154</pages>
      <abstract>Semantic consistency of a language model is broadly defined as the model’s ability to produce semantically-equivalent outputs, given semantically-equivalent inputs. We address the task of assessing question-answering (QA) semantic consistency of contemporary large language models (LLMs) by manually creating a benchmark dataset with high-quality paraphrases for factual questions, and release the dataset to the community.We further combine the semantic consistency metric with additional measurements suggested in prior work as correlating with LLM QA accuracy, for building and evaluating a framework for factual QA reference-less performance prediction – predicting the likelihood of a language model to accurately answer a question. Evaluating the framework on five contemporary LLMs, we demonstrate encouraging, significantly outperforming baselines, results.</abstract>
      <url hash="542b69e8">2023.gem-1.12</url>
      <bibkey>rabinovich-etal-2023-predicting</bibkey>
    </paper>
    <paper id="13">
      <title>Towards Effective Long-Form <fixed-case>QA</fixed-case> with Evidence Augmentation</title>
      <author><first>Mengxia</first><last>Yu</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>Sara</first><last>Rosenthal</last><affiliation>IBM Research</affiliation></author>
      <author><first>Mihaela</first><last>Bornea</last><affiliation>IBM Research</affiliation></author>
      <author id="avirup-sil"><first>Avi</first><last>Sil</last><affiliation>IBM Research AI</affiliation></author>
      <pages>155-164</pages>
      <abstract>In this study, we focus on the challenge of improving Long-form Question Answering (LFQA) by extracting and effectively utilizing knowledge from a large set of retrieved passages. We first demonstrate the importance of accurate evidence retrieval for LFQA, showing that optimal extracted knowledge from passages significantly benefits the generation. We also show that the choice of generative models impacts the system’s ability to leverage the evidence and produce answers that are grounded in the retrieved passages. We propose a Mixture of Experts (MoE) model as an alternative to the Fusion in Decoder (FiD) used in state-of-the-art LFQA systems and we compare these two models in our experiments.</abstract>
      <url hash="6b5b0fc2">2023.gem-1.13</url>
      <bibkey>yu-etal-2023-towards</bibkey>
    </paper>
    <paper id="14">
      <title>Harnessing the Plug-and-Play Controller by Prompting</title>
      <author><first>Hao</first><last>Wang</last><affiliation>BeiHang University</affiliation></author>
      <author><first>Lei</first><last>Sha</last><affiliation>Beihang University</affiliation></author>
      <pages>165-174</pages>
      <abstract>Controllable text generation is a growing field within natural language generation (NLG) that focuses on producing text that meets specific constraints in real-world applications. Previous approaches, such as plug-and-play controllers (PPCs), aimed to steer the properties of generated text in a flexible manner. However, these methods often compromised the integrity of the language model’s decoding process, resulting in less smooth text generation.Alternatively, other techniques utilized multiple attribute prompts to align the generated text with desired attributes, but this approach required prompt design for each attribute and was dependent on the size of the language model. This paper introduces a novel method for flexible attribute control in text generation using pre-trained language models (PLMs). The proposed approach aims to enhance the fluency of generated text by guiding the generation process with PPCs. The key idea is to dynamically adjust the distribution of generated text by modifying prompts, effectively constraining the output space of the language model and influencing the desired attribute. To enable smooth cooperation between the PLM and the PPC, our work innovativel proposes a new model fine-tuning method: Reinforcement Learning with Dynamic Adjust Feedback (RLDAF).This fine-tuning process adapts a small subset of the language model’s parameters based on the generating actions taken during the PPC control process. The resulting harmonious collaboration between the PLM and PPC leads to improved smoothness in text generation during inference. Extensive experiments were conducted on the SST2 dataset, and the proposed method outperformed previous approaches in various evaluation metrics, including text fluency and attribute consistency.</abstract>
      <url hash="3de795a8">2023.gem-1.14</url>
      <bibkey>wang-sha-2023-harnessing</bibkey>
    </paper>
    <paper id="15">
      <title>Context and Literacy Aware Learnable Metric for Text Simplification</title>
      <author><first>Jeongwon</first><last>Kwak</last><affiliation>Interdisciplinary Program for Bioengineering, Graduate School, Seoul National University</affiliation></author>
      <author><first>Hyeryun</first><last>Park</last><affiliation>Interdisciplinary Program for Bioengineering, Graduate School, Seoul National University</affiliation></author>
      <author><first>Kyungmo</first><last>Kim</last><affiliation>Interdisciplinary Program for Bioengineering, Graduate School, Seoul National University</affiliation></author>
      <author><first>Jinwook</first><last>Choi</last><affiliation>Seoul National University</affiliation></author>
      <pages>175-180</pages>
      <abstract>Automatic evaluation of text simplification is important; but assessing its transformation into simpler sentences can be challenging for various reasons. However, the most commonly used metric in text simplification, SARI, fails to capture the difficulty of generating words that are not present in the references, regardless of their meaning. We propose a new learnable evaluation metric that decomposes and reconstructs sentences to simultaneously measure the similarity and difficulty of sentences within a single system. Through experiments, we confirm that it exhibited the highest similarity in correlation with the human evaluation.</abstract>
      <url hash="3537970a">2023.gem-1.15</url>
      <bibkey>kwak-etal-2023-context</bibkey>
    </paper>
    <paper id="16">
      <title>Synthetic Dialogue Dataset Generation using <fixed-case>LLM</fixed-case> Agents</title>
      <author><first>Yelaman</first><last>Abdullin</last><affiliation>Macquarie University</affiliation></author>
      <author id="diego-molla"><first>Diego</first><last>Molla</last><affiliation>Macquarie University</affiliation></author>
      <author><first>Bahadorreza</first><last>Ofoghi</last><affiliation>Deakin University</affiliation></author>
      <author><first>John</first><last>Yearwood</last><affiliation>Deakin University</affiliation></author>
      <author><first>Qingyang</first><last>Li</last><affiliation>The University of Melbourne</affiliation></author>
      <pages>181-191</pages>
      <abstract>Linear programming (LP) problems are pervasive in real-life applications. However, despite their apparent simplicity, an untrained user may find it difficult to determine the linear model of their specific problem. We envisage the creation of a goal-oriented conversational agent that will engage in conversation with the user to elicit all information required so that a subsequent agent can generate the linear model. In this paper, we present an approach for the generation of sample dialogues that can be used to develop and train such a conversational agent. Using prompt engineering, we develop two agents that “talk” to each other, one acting as the conversational agent, and the other acting as the user. Using a set of text descriptions of linear problems from NL4Opt available to the user only, the agent and the user engage in conversation until the agent has retrieved all key information from the original problem description. We also propose an extrinsic evaluation of the dialogues by assessing how well the summaries generated by the dialogues match the original problem descriptions. We conduct human and automatic evaluations, including an evaluation approach that uses GPT-4 to mimic the human evaluation metrics. The evaluation results show an overall good quality of the dialogues, though research is still needed to improve the quality of the GPT-4 evaluation metrics. The resulting dialogues, including the human annotations of a subset, are available to the research community. The conversational agent used for the generation of the dialogues can be used as a baseline.</abstract>
      <url hash="c36e8371">2023.gem-1.16</url>
      <bibkey>abdullin-etal-2023-synthetic</bibkey>
    </paper>
    <paper id="17">
      <title>An Empirical <fixed-case>B</fixed-case>ayes Framework for Open-Domain Dialogue Generation</title>
      <author><first>Jing Yang</first><last>Lee</last><affiliation>Nanyang Technological University</affiliation></author>
      <author><first>Kong Aik</first><last>Lee</last><affiliation>Institute for Infocomm Research, A*STAR</affiliation></author>
      <author><first>Woon Seng</first><last>Gan</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>192-204</pages>
      <abstract>To engage human users in meaningful conversation, open-domain dialogue agents are required to generate diverse and contextually coherent dialogue. Despite recent advancements, which can be attributed to the usage of pretrained language models, the generation of diverse and coherent dialogue remains an open research problem. A popular approach to address this issue involves the adaptation of variational frameworks. However, while these approaches successfully improve diversity, they tend to compromise on contextual coherence. Hence, we propose the Bayesian Open-domain Dialogue with Empirical Bayes (BODEB) framework, an empirical bayes framework for constructing an Bayesian open-domain dialogue agent by leveraging pretrained parameters to inform the prior and posterior parameter distributions. Empirical results show that BODEB achieves better results in terms of both diversity and coherence compared to variational frameworks.</abstract>
      <url hash="9812a4e4">2023.gem-1.17</url>
      <bibkey>lee-etal-2023-empirical</bibkey>
    </paper>
    <paper id="18">
      <title>Flesch or Fumble? Evaluating Readability Standard Alignment of Instruction-Tuned Language Models</title>
      <author><first>Joseph Marvin</first><last>Imperial</last><affiliation>University of Bath</affiliation></author>
      <author><first>Harish</first><last>Tayyar Madabushi</last><affiliation>University of Bath</affiliation></author>
      <pages>205-223</pages>
      <abstract>Readability metrics and standards such as Flesch Kincaid Grade Level (FKGL) and the Common European Framework of Reference for Languages (CEFR) exist to guide teachers and educators to properly assess the complexity of educational materials before administering them for classroom use. In this study, we select a diverse set of open and closed-source instruction-tuned language models and investigate their performances in writing story completions and simplifying narratives—tasks that teachers perform—using standard-guided prompts controlling text readability. Our extensive findings provide empirical proof of how globally recognized models like ChatGPT may be considered less effective and may require more refined prompts for these generative tasks compared to other open-sourced models such as BLOOMZ and FlanT5—which have shown promising results.</abstract>
      <url hash="b2f1e994">2023.gem-1.18</url>
      <bibkey>imperial-tayyar-madabushi-2023-flesch</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> as a <fixed-case>J</fixed-case>ava Decompiler</title>
      <author><first>Bradley</first><last>Mcdanel</last><affiliation>Franklin and Marshall College</affiliation></author>
      <author><first>Zhanhao</first><last>Liu</last><affiliation>Franklin and Marshall College</affiliation></author>
      <pages>224-232</pages>
      <abstract>We propose a novel approach using instruction-tuned large language models (LLMs), such as ChatGPT, to automatically decompile entire Java classes. Our method relies only on a textual representation of the Java bytecode and corresponding unit tests generated from the bytecode. While no additional domain knowledge or fine-tuning is performed, we provide a single training example of this decompilation process in the model’s prompt. To overcome both compilation errors and test failures, we use an iterative prompting approach. We find that ChatGPT-4 is able to generate more human-readable output than existing software-based decompilers while achieving slightly lower pass rates on unit tests. Source code and datasets are available at <url>https://github.com/BradMcDanel/gpt-java-decompiler</url>.</abstract>
      <url hash="30806f02">2023.gem-1.19</url>
      <bibkey>mcdanel-liu-2023-chatgpt</bibkey>
    </paper>
    <paper id="20">
      <title>Multi-domain Summarization from Leaderboards to Practice: Re-examining Automatic and Human Evaluation</title>
      <author><first>David</first><last>Demeter</last><affiliation>Northwestern University</affiliation></author>
      <author><first>Oshin</first><last>Agarwal</last><affiliation>Bloomberg</affiliation></author>
      <author id="simon-benigeri"><first>Simon</first><last>Ben Igeri</last><affiliation>Northwestern University</affiliation></author>
      <author><first>Marko</first><last>Sterbentz</last><affiliation>Northwestern University</affiliation></author>
      <author><first>Neil</first><last>Molino</last><affiliation>IDA/CCS</affiliation></author>
      <author id="john-conroy"><first>John</first><last>Conroy</last><affiliation>IDA Center for Computing Sciences</affiliation></author>
      <author><first>Ani</first><last>Nenkova</last><affiliation>Adobe Research</affiliation></author>
      <pages>233-242</pages>
      <abstract>Existing literature does not give much guidance on how to build the best possible multi-domain summarization model from existing components. We present an extensive evaluation of popular pre-trained models on a wide range of datasets to inform the selection of both the model and the training data for robust summarization across several domains. We find that fine-tuned BART performs better than T5 and PEGASUS, both on in-domain and out-of-domain data, regardless of the dataset used for fine-tuning. While BART has the best performance, it does vary considerably across domains. A multi-domain summarizer that works well for all domains can be built by simply fine-tuning on diverse domains. It even performs better than an in-domain summarizer, even when using fewer total training examples. While the success of such a multi-domain summarization model is clear through automatic evaluation, by conducting a human evaluation, we find that there are variations that can not be captured by any of the automatic evaluation metrics and thus not reflected in standard leaderboards. Furthermore, we find that conducting reliable human evaluation can be complex as well. Even experienced summarization researchers can be inconsistent with one another in their assessment of the quality of a summary, and also with themselves when re-annotating the same summary. The findings of our study are two-fold. First, BART fine-tuned on heterogeneous domains is a great multi-domain summarizer for practical purposes. At the same time, we need to re-examine not just automatic evaluation metrics but also human evaluation methods to responsibly measure progress in summarization.</abstract>
      <url hash="d718d51d">2023.gem-1.20</url>
      <bibkey>demeter-etal-2023-multi</bibkey>
    </paper>
    <paper id="21">
      <title>Targeted Image Data Augmentation Increases Basic Skills Captioning Robustness</title>
      <author><first>Valentin</first><last>Barriere</last><affiliation>Joint Research Center</affiliation></author>
      <author><first>Felipe</first><last>Del Rio</last><affiliation>Department of Computer Science, Pontificia Universidad Catolica de Chile, Santiago, Chile</affiliation></author>
      <author><first>Andres</first><last>Carvallo</last><affiliation>CENIA</affiliation></author>
      <author><first>Carlos</first><last>Aspillaga</last><affiliation>CENIA</affiliation></author>
      <author><first>Eugenio</first><last>Herrera-Berg</last><affiliation>CENIA</affiliation></author>
      <author><first>Cristian</first><last>Buc</last><affiliation>CENIA</affiliation></author>
      <pages>243-257</pages>
      <abstract>Artificial neural networks typically struggle in generalizing to out-of-context examples. One reason for this limitation is caused by having datasets that incorporate only partial information regarding the potential correlational structure of the world. In this work, we propose TIDA (Targeted Image-editing Data Augmentation), a targeted data augmentation method focused on improving models’ human-like abilities (e.g., gender recognition) by filling the correlational structure gap using a text-to-image generative model. More specifically, TIDA identifies specific skills in captions describing images (e.g., the presence of a specific gender in the image), changes the caption (e.g., “woman” to “man”), and then uses a text-to-image model to edit the image in order to match the novel caption (e.g., uniquely changing a woman to a man while maintaining the context identical). Based on the Flickr30K benchmark, we show that, compared with the original data set, a TIDA-enhanced dataset related to gender, color, and counting abilities induces better performance in several image captioning metrics. Furthermore, on top of relying on the classical BLEU metric, we conduct a fine-grained analysis of the improvements of our models against the baseline in different ways. We compared text-to-image generative models and found different behaviors of the image captioning models in terms of encoding visual encoding and textual decoding.</abstract>
      <url hash="e2f1b88e">2023.gem-1.21</url>
      <bibkey>barriere-etal-2023-targeted</bibkey>
    </paper>
    <paper id="22">
      <title>Separating form and meaning: Using self-consistency to quantify task understanding across multiple senses</title>
      <author><first>Xenia</first><last>Ohmer</last><affiliation>Osnabrück University</affiliation></author>
      <author><first>Elia</first><last>Bruni</last><affiliation>University of Osnabrück</affiliation></author>
      <author><first>Dieuwke</first><last>Hupkes</last><affiliation>Facebook AI Research</affiliation></author>
      <pages>258-276</pages>
      <abstract>At the staggering pace with which the capabilities of large language models (LLMs) are increasing, creating future-proof evaluation sets to assess their understanding becomes more and more challenging. In this paper, we propose a novel paradigm for evaluating LLMs which leverages the idea that correct world understanding should be consistent across different (Fregean) senses of the same meaning. Accordingly, we measure understanding not in terms of correctness but by evaluating consistency across multiple senses that are generated by the model itself. We showcase our approach by instantiating a test where the different senses are different languages, hence using multilingual self-consistency as a litmus test for the model’s understanding and simultaneously addressing the important topic of multilingualism. Taking one of the latest versions of ChatGPT as our object of study, we evaluate multilingual consistency for two different tasks across three different languages. We show that its multilingual consistency is still lacking, and that its task and world understanding are thus not language-independent. As our approach does not require any static evaluation corpora in languages other than English, it can easily and cheaply be extended to different languages and tasks and could become an integral part of future benchmarking efforts.</abstract>
      <url hash="4538e4ea">2023.gem-1.22</url>
      <bibkey>ohmer-etal-2023-separating</bibkey>
      <video href="2023.gem-1.22.mp4"/>
    </paper>
    <paper id="23">
      <title>Text Encoders Lack Knowledge: Leveraging Generative <fixed-case>LLM</fixed-case>s for Domain-Specific Semantic Textual Similarity</title>
      <author><first>Joseph</first><last>Gatto</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Omar</first><last>Sharif</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Parker</first><last>Seegmiller</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Philip</first><last>Bohlman</last><affiliation>Dartmouth College</affiliation></author>
      <author id="sarah-masud-preum"><first>Sarah M.</first><last>Preum</last><affiliation>Dartmouth College</affiliation></author>
      <pages>277-288</pages>
      <abstract>Amidst the sharp rise in the evaluation of large language models (LLMs) on various tasks, we find that semantic textual similarity (STS) has been under-explored. In this study, we show that STS can be cast as a text generation problem while maintaining strong performance on multiple STS benchmarks. Additionally, we show generative LLMs significantly outperform existing encoder-based STS models when characterizing the semantic similarity between two texts with complex semantic relationships dependent on world knowledge. We validate this claim by evaluating both generative LLMs and existing encoder-based STS models on three newly-collected STS challenge sets which require world knowledge in the domains of Health, Politics, and Sports. All newly-collected data is sourced from social media content posted after May 2023 to ensure the performance of closed-source models like ChatGPT cannot be credited to memorization. Our results show that, on average, generative LLMs outperform the best encoder-only baselines by an average of 22.3% on STS tasks requiring world knowledge. Our results suggest generative language models with STS-specific prompting strategies achieve state-of-the-art performance in complex, domain-specific STS tasks.</abstract>
      <url hash="3106a585">2023.gem-1.23</url>
      <bibkey>gatto-etal-2023-text</bibkey>
      <video href="2023.gem-1.23.mp4"/>
    </paper>
    <paper id="24">
      <title>To Burst or Not to Burst: Generating and Quantifying Improbable Text</title>
      <author><first>Kuleen</first><last>Sasse</last><affiliation>Johns Hopkins University</affiliation></author>
      <author id="efsun-sarioglu-kayi"><first>Efsun</first><last>Sarioglu Kayi</last><affiliation>Johns Hopkins APL</affiliation></author>
      <author><first>Samuel</first><last>Barham</last><affiliation>University of Maryland</affiliation></author>
      <author><first>Edward</first><last>Staley</last><affiliation>JHUAPL</affiliation></author>
      <pages>289-309</pages>
      <abstract>While large language models (LLMs) are extremely capable at text generation, their outputs are still distinguishable from human-authored text. We explore this separation across many metrics over text, many sampling techniques, many types of text data, and across two popular LLMs, LLaMA and Vicuna. Along the way, we introduce a new metric, recoverability, to highlight differences between human and machine text; and we propose a new sampling technique, burst sampling, designed to close this gap. We find that LLaMA and Vicuna have distinct distributions under many of the metrics, and that this influences our results: Recoverability separates real from fake text better than any other metric when using LLaMA. When using Vicuna, burst sampling produces text which is distributionally closer to real text compared to other sampling techniques.</abstract>
      <url hash="12880778">2023.gem-1.24</url>
      <bibkey>sasse-etal-2023-burst</bibkey>
    </paper>
    <paper id="25">
      <title>Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of <fixed-case>LLM</fixed-case>s</title>
      <author><first>Xue-Yong</first><last>Fu</last><affiliation>Dialpad Inc</affiliation></author>
      <author><first>Md Tahmid Rahman</first><last>Laskar</last><affiliation>Dialpad Inc</affiliation></author>
      <author><first>Cheng</first><last>Chen</last><affiliation>Dialpad Inc</affiliation></author>
      <author><first>Shashi Bhushan</first><last>Tn</last><affiliation>Dialpad Inc</affiliation></author>
      <pages>310-316</pages>
      <abstract>In recent years, large language models (LLMs) have drawn significant attention due to their impressive emergent capabilities that were not observed in earlier language models. One emerging area where LLMs have been widely used in recent times is the utilization of LLMs as the evaluator of the texts generated by various generative models. In this paper, we also explore the possibility of whether LLMs are reliable in assessing the factual consistency of summaries generated by text generation models. We first propose a new approach to evaluate the factuality score using LLMs by utilizing the same LLM to perform all steps in the question-answering-based factuality scoring pipeline. Subsequently, we study the performance of various LLMs to directly score the factuality. Our evaluation is conducted in traditional benchmarks by comparing their correlation with human annotations. Contrary to expectations, our findings revealed that none of the factuality metrics showed any significant correlations (e.g., coefficient scores greater than 0.3) to human evaluations of factuality for GPT-4, PaLM-2, and Claude-2, with the only exception being GPT-3.5 in two subcategories of factuality. Nonetheless, our findings are consistent across almost all factual error types, suggesting a fundamental limitation in the ability of current LLMs to assess factuality.</abstract>
      <url hash="1d7843b4">2023.gem-1.25</url>
      <bibkey>fu-etal-2023-large</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>R</fixed-case>ank<fixed-case>A</fixed-case>ug: Augmented data ranking for text classification</title>
      <author><first>Tiasa</first><last>Roy</last><affiliation>Manipal Institute of Technology</affiliation></author>
      <author><first>Priyam</first><last>Basu</last><affiliation>Manipal Institute of Technology</affiliation></author>
      <pages>317-323</pages>
      <abstract>Research on data generation and augmentation has been focused majorly around enhancing generation models, leaving a notable gap in the exploration and refinement of methods for evaluating synthetic data. There are several text similarity metrics within the context of generated data filtering which can impact the performance of specific Natural Language Understanding (NLU) tasks, specifically focusing on intent and sentiment classification. In this study, we propose RankAug, a text-ranking approach that detects and filters out the top augmented texts in terms of being most similar in meaning with lexical and syntactical diversity. Through experiments conducted on multiple datasets, we demonstrate that the judicious selection of filtering techniques can yield a substantial improvement of up to 35% in classification accuracy for under-represented classes.</abstract>
      <url hash="29b7507b">2023.gem-1.26</url>
      <bibkey>roy-basu-2023-rankaug</bibkey>
    </paper>
    <paper id="27">
      <title>Separating the Wheat from the Chaff with <fixed-case>BREAD</fixed-case>: An open-source benchmark and metrics to detect redundancy in text</title>
      <author><first>Isaac</first><last>Caswell</last><affiliation>Google Research</affiliation></author>
      <author><first>Lisa</first><last>Wang</last><affiliation>DeepMind</affiliation></author>
      <author><first>Isabel</first><last>Papadimitriou</last><affiliation>Stanford University</affiliation></author>
      <pages>324-338</pages>
      <abstract>Data quality is a problem that perpetually resurfaces throughout the field of NLP, regardless of task, domain, or architecture, and remains especially severe for lower-resource languages. A typical and insidious issue, affecting both training data and model output, is data that is repetitive and dominated by linguistically uninteresting boilerplate, such as price catalogs or computer-generated log files. Though this problem permeates many web-scraped corpora, there has yet to be a benchmark to test against, or a systematic study to find simple metrics that generalize across languages and agree with human judgements of data quality. In the present work, we create and release BREAD, a human-labeled benchmark on repetitive boilerplate vs. plausible linguistic content, spanning 360 languages. We release several baseline CRED (Character REDundancy) scores along with it, and evaluate their effectiveness on BREAD. We hope that the community will use this resource to develop better filtering methods, and that our reference implementations of CRED scores can become standard corpus evaluation tools, driving the development of cleaner language modeling corpora, especially in low-resource languages.</abstract>
      <url hash="58a7da96">2023.gem-1.27</url>
      <bibkey>caswell-etal-2023-separating</bibkey>
      <video href="2023.gem-1.27.mp4"/>
    </paper>
    <paper id="28">
      <title>Elo Uncovered: Robustness and Best Practices in Language Model Evaluation</title>
      <author><first>Meriem</first><last>Boubdir</last><affiliation>Cohere For AI</affiliation></author>
      <author><first>Edward</first><last>Kim</last><affiliation>Cohere</affiliation></author>
      <author><first>Beyza</first><last>Ermis</last><affiliation>Cohere For AI</affiliation></author>
      <author><first>Sara</first><last>Hooker</last><affiliation>Cohere For AI</affiliation></author>
      <author><first>Marzieh</first><last>Fadaee</last><affiliation>Cohere for AI</affiliation></author>
      <pages>339-352</pages>
      <abstract>In Natural Language Processing (NLP), the Elo rating system, well-established for ranking dynamic competitors in games like chess, has seen increasing adoption for evaluating Large Language Models (LLMs) through “A vs B” paired comparisons. However, while popular, the system’s suitability for assessing entities with constant skill levels, such as LLMs, remains relatively unexplored. Our study investigates the sensitivity and reproducibility of Elo scores for LLMs, integrating both synthetic and human feedback. We show that Elo ratings for LLMs stabilize with 100 or more comparison permutations. A lower K-factor is preferable for closely matched models, whereas a higher K-factor better distinguishes models with clear performance differences. We also report that transitivity (A B and B C implies A C) does not consistently hold, particularly when models demonstrate similar performance. Our empirical findings provide guidelines for more reliable LLM evaluation.</abstract>
      <url hash="73e680dd">2023.gem-1.28</url>
      <bibkey>boubdir-etal-2023-elo</bibkey>
    </paper>
    <paper id="29">
      <title><fixed-case>P</fixed-case>ersonality<fixed-case>C</fixed-case>hat: Conversation Distillation for Personalized Dialog Modeling with Facts and Traits</title>
      <author><first>Ehsan</first><last>Lotfi</last><affiliation>University of Antwerp, CLiPS</affiliation></author>
      <author><first>Maxime</first><last>De Bruyn</last><affiliation>University of Antwerp</affiliation></author>
      <author><first>Jeska</first><last>Buhmann</last><affiliation>University of Antwerp</affiliation></author>
      <author id="walter-daelemans"><first>Walter</first><last>Daelemans</last><affiliation>University of Antwerp, CLiPS</affiliation></author>
      <pages>353-371</pages>
      <abstract>The new wave of Large Language Models (LLM) has offered an efficient tool to curate sizeable conversational datasets. So far studies have mainly focused on task-oriented or generic open-domain dialogs, and have not fully explored the ability of LLMs in following complicated prompts. In this work, we focus on personalization, and employ LLMs to curate a dataset which is difficult and costly to crowd-source: PersonalityChat is a synthetic conversational dataset based upon the popular PersonaChat dataset, but conditioned on both personas and (Big-5) personality traits. Evaluating models fine-tuned on this dataset, we show that the personality trait labels can be used for trait-based personalization of generative dialogue models. We also perform a head-to-head comparison between PersonalityChat and PersonaChat, and show that training on the distilled dataset results in more fluent and coherent dialog agents in the small-model regime.</abstract>
      <url hash="3fa92bfc">2023.gem-1.29</url>
      <bibkey>lotfi-etal-2023-personalitychat</bibkey>
    </paper>
    <paper id="30">
      <title>How well <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case> understand <fixed-case>M</fixed-case>alaysian <fixed-case>E</fixed-case>nglish? An Evaluation on Named Entity Recognition and Relation Extraction</title>
      <author><first>Mohanraj</first><last>Chanthran</last><affiliation>Monash University Malaysia</affiliation></author>
      <author><first>Lay-Ki</first><last>Soon</last><affiliation>Monash University Malaysia</affiliation></author>
      <author><first>Ong Huey</first><last>Fang</last><affiliation>Monash University Malaysia</affiliation></author>
      <author><first>Bhawani</first><last>Selvaretnam</last><affiliation>Valiantlytix Sdn. Bhd.</affiliation></author>
      <pages>372-397</pages>
      <abstract>Recently, ChatGPT has attracted a lot of interest from both researchers and the general public. While the performance of ChatGPT in Named Entity Recognition and Relation Extraction from Standard English texts is satisfactory, it remains to be seen if it can perform similarly for Malaysian English. Malaysian English is unique as it exhibits morphosyntactic and semantical adaptation from local contexts. In this study, we assess ChatGPT’s capability in extracting entities and relations from the Malaysian English News (MEN) dataset. We propose a three-step methodology referred to as <b>educate-predict-evaluate</b>. The performance of ChatGPT is assessed using F1-Score across 18 unique prompt settings, which were carefully engineered for a comprehensive review. From our evaluation, we found that ChatGPT does not perform well in extracting entities from Malaysian English news articles, with the highest F1-Score of 0.497. Further analysis shows that the morphosyntactic adaptation in Malaysian English caused the limitation. However, interestingly, this morphosyntactic adaptation does not impact the performance of ChatGPT for relation extraction.</abstract>
      <url hash="93c45e68">2023.gem-1.30</url>
      <bibkey>chanthran-etal-2023-well</bibkey>
    </paper>
    <paper id="31">
      <title>Post <fixed-case>T</fixed-case>uring: Mapping the landscape of <fixed-case>LLM</fixed-case> Evaluation</title>
      <author><first>Alexey</first><last>Tikhonov</last><affiliation>Inworld.AI</affiliation></author>
      <author><first>Ivan P.</first><last>Yamshchikov</last><affiliation>THWS</affiliation></author>
      <pages>398-412</pages>
      <abstract>In the rapidly evolving landscape of Large Language Models (LLMs), introduction of well-defined and standardized evaluation methodologies remains a crucial challenge. This paper traces the historical trajectory of LLM evaluations, from the foundational questions posed by Alan Turing to the modern era of AI research. We categorize the evolution of LLMs into distinct periods, each characterized by its unique benchmarks and evaluation criteria. As LLMs increasingly mimic human-like behaviors, traditional evaluation proxies, such as the Turing test, have become less reliable. We emphasize the pressing need for a unified evaluation system, given the broader societal implications of these models. Through an analysis of common evaluation methodologies, we advocate for a qualitative shift in assessment approaches, underscoring the importance of standardization and objective criteria. This work serves as a call for the AI community to collaboratively address the challenges of LLM evaluation, ensuring their reliability, fairness, and societal benefit.</abstract>
      <url hash="43a3904e">2023.gem-1.31</url>
      <bibkey>tikhonov-yamshchikov-2023-post</bibkey>
      <video href="2023.gem-1.31.mp4"/>
    </paper>
    <paper id="32">
      <title>A Simple yet Efficient Ensemble Approach for <fixed-case>AI</fixed-case>-generated Text Detection</title>
      <author><first>Harika</first><last>Abburi</last><affiliation>IIIT Hyderabad</affiliation></author>
      <author><first>Kalyani</first><last>Roy</last><affiliation>Deloitte &amp; Touche Assurance &amp; Enterprise Risk Services India Private Limited, India</affiliation></author>
      <author><first>Michael</first><last>Suesserman</last><affiliation>Deloitte &amp; Touche LLP, USA</affiliation></author>
      <author><first>Nirmala</first><last>Pudota</last><affiliation>Deloitte &amp; Touche Assurance &amp; Enterprise Risk Services India Private Limited, India</affiliation></author>
      <author><first>Balaji</first><last>Veeramani</last><affiliation>Deloitte &amp; Touche LLP, USA</affiliation></author>
      <author><first>Edward</first><last>Bowen</last><affiliation>Deloitte &amp; Touche LLP, USA</affiliation></author>
      <author><first>Sanmitra</first><last>Bhattacharya</last><affiliation>Deloitte &amp; Touche LLP, USA</affiliation></author>
      <pages>413-421</pages>
      <abstract>Recent Large Language Models (LLMs) have demonstrated remarkable capabilities in generating text that closely resembles human writing across wide range of styles and genres. However, such capabilities are prone to potential abuse, such as fake news generation, spam email creation, and misuse in academic assignments. Hence, it is essential to build automated approaches capable of distinguishing between artificially generated text and human-authored text. In this paper, we propose a simple yet efficient solution to this problem by ensembling predictions from multiple constituent LLMs. Compared to previous state-of-the-art approaches, which are perplexity-based or uses ensembles with a large number of LLMs, our condensed ensembling approach uses only two constituent LLMs to achieve comparable performance. Experiments conducted on four benchmark datasets for generative text classification show performance improvements in the range of 0.5 to 100% compared to previous state-of-the-art approaches. We also study that the influence the training data from individual LLMs have on model performance. We found that substituting commercially-restrictive Generative Pre-trained Transformer (GPT) data with data generated from other open language models such as Falcon, Large Language Model Meta AI (LLaMA2), and Mosaic Pretrained Transformers (MPT) is a feasible alternative when developing generative text detectors. Furthermore, to demonstrate zero-shot generalization, we experimented with an English essays dataset, and results suggest that our ensembling approach can handle new data effectively.</abstract>
      <url hash="9a9f4efd">2023.gem-1.32</url>
      <bibkey>abburi-etal-2023-simple</bibkey>
    </paper>
  </volume>
</collection>
