<?xml version='1.0' encoding='UTF-8'?>
<collection id="2023.eval4nlp">
  <volume id="1" ingest-date="2024-02-28" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems</booktitle>
      <editor><first>Daniel</first><last>Deutsch</last></editor>
      <editor><first>Rotem</first><last>Dror</last></editor>
      <editor><first>Steffen</first><last>Eger</last></editor>
      <editor><first>Yang</first><last>Gao</last></editor>
      <editor><first>Christoph</first><last>Leiter</last></editor>
      <editor><first>Juri</first><last>Opitz</last></editor>
      <editor><first>Andreas</first><last>Rücklé</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Bali, Indonesia</address>
      <month>November</month>
      <year>2023</year>
      <url hash="49e912cc">2023.eval4nlp-1</url>
      <venue>eval4nlp</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="f64c2d42">2023.eval4nlp-1.0</url>
      <bibkey>eval4nlp-2023-evaluation</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>WRF</fixed-case>: Weighted Rouge-F1 Metric for Entity Recognition</title>
      <author><first>Lukas</first><last>Weber</last></author>
      <author><first>Krishnan</first><last>Jothi Ramalingam</last></author>
      <author><first>Matthias</first><last>Beyer</last></author>
      <author><first>Axel</first><last>Zimmermann</last></author>
      <pages>1-11</pages>
      <abstract>The continuous progress in Named Entity Recognition allows the identification of complex entities in multiple domains. The traditionally used metrics like precision, recall, and F1-score can only reflect the classification quality of the underlying NER model to a limited extent. Existing metrics do not distinguish between a non-recognition of an entity and a misclassification of an entity. Additionally, the dealing with redundant entities remains unaddressed. We propose WRF, a Weighted Rouge F1 metric for Entity Recognition, to solve the mentioned gaps in currently available metrics. We successfully employ the WRF metric for automotive entity recognition, followed by a comprehensive qualitative and quantitative analysis of the obtained results.</abstract>
      <url hash="6cdeb2aa">2023.eval4nlp-1.1</url>
      <bibkey>weber-etal-2023-wrf</bibkey>
      <doi>10.18653/v1/2023.eval4nlp-1.1</doi>
    </paper>
    <paper id="2">
      <title>Assessing Distractors in Multiple-Choice Tests</title>
      <author><first>Vatsal</first><last>Raina</last></author>
      <author><first>Adian</first><last>Liusie</last><affiliation>University of Cambridge</affiliation></author>
      <author><first>Mark</first><last>Gales</last><affiliation>University of Cambridge</affiliation></author>
      <pages>12-22</pages>
      <abstract>Multiple-choice tests are a common approach for assessing candidates’ comprehension skills. Standard multiple-choice reading comprehension exams require candidates to select the correct answer option from a discrete set based on a question in relation to a contextual passage. For appropriate assessment, the distractor answer options must by definition be incorrect but plausible and diverse. However, generating good quality distractors satisfying these criteria is a challenging task for content creators. We propose automated assessment metrics for the quality of distractors in multiple-choice reading comprehension tests. Specifically, we define quality in terms of the incorrectness, plausibility and diversity of the distractor options. We assess incorrectness using the classification ability of a binary multiple-choice reading comprehension system. Plausibility is assessed by considering the distractor confidence - the probability mass associated with the distractor options for a standard multi-class multiple-choice reading comprehension system. Diversity is assessed by pairwise comparison of an embedding-based equivalence metric between the distractors of a question. To further validate the plausibility metric we compare against candidate distributions over multiple-choice questions and agreement with a ChatGPT model’s interpretation of distractor plausibility and diversity.</abstract>
      <url hash="99ec7c27">2023.eval4nlp-1.2</url>
      <bibkey>raina-etal-2023-assessing</bibkey>
      <doi>10.18653/v1/2023.eval4nlp-1.2</doi>
    </paper>
    <paper id="3">
      <title>Delving into Evaluation Metrics for Generation: A Thorough Assessment of How Metrics Generalize to Rephrasing Across Languages</title>
      <author><first>Yixuan</first><last>Wang</last></author>
      <author><first>Qingyan</first><last>Chen</last></author>
      <author><first>Duygu</first><last>Ataman</last><affiliation>New York University</affiliation></author>
      <pages>23-31</pages>
      <abstract>Language generation has been an important task in natural language processing (NLP) with increasing variety of applications especially in the recent years. The evaluation of generative language models typically rely on automatic heuristics which search for overlaps over word or phrase level patterns in generated outputs and traditionally some hand-crafted reference sentences in the given language ranging in the forms from sentences to entire documents. Language, on the other hand, is productive by nature, which means the same concept can be expressed potentially in many different lexical or phrasal forms, making the assessment of generated outputs a very difficult one. Many studies have indicated potential hazards related to the prominent choice of heuristics matching generated language to selected references and the limitations raised by this setting in developing robust generative models. This paper undertakes an in-depth analysis of evaluation metrics used for generative models, specifically investigating their responsiveness to various syntactic structures, and how these characteristics vary across languages with different morphosyntactic typologies. Preliminary findings indicate that while certain metrics exhibit robustness in particular linguistic contexts, a discernible variance emerges in their performance across distinct syntactic forms. Through this exploration, we highlight the imperative need for more nuanced and encompassing evaluation strategies in generative models, advocating for metrics that are sensitive to the multifaceted nature of languages.</abstract>
      <url hash="3db69826">2023.eval4nlp-1.3</url>
      <bibkey>wang-etal-2023-delving</bibkey>
      <doi>10.18653/v1/2023.eval4nlp-1.3</doi>
    </paper>
    <paper id="4">
      <title><fixed-case>E</fixed-case>du<fixed-case>Q</fixed-case>uick: A Dataset Toward Evaluating Summarization of Informal Educational Content for Social Media</title>
      <author><first>Zahra</first><last>Kolagar</last><affiliation>Fraunhofer IIS</affiliation></author>
      <author><first>Sebastian</first><last>Steindl</last><affiliation>Ostbayerische Technische Hochschule Amberg-Weiden</affiliation></author>
      <author><first>Alessandra</first><last>Zarcone</last><affiliation>Hochschule Augsburg</affiliation></author>
      <pages>32-48</pages>
      <abstract>This study explores the capacity of large language models (LLMs) to efficiently generate summaries of informal educational content tailored for platforms like TikTok. It also investigates how both humans and LLMs assess the quality of these summaries, based on a series of experiments, exploring the potential replacement of human evaluation with LLMs. Furthermore, the study delves into how experienced content creators perceive the utility of automatic summaries for TikTok videos. We employ strategic prompt selection techniques to guide LLMs in producing engaging summaries based on the characteristics of viral TikTok content, including hashtags, captivating hooks, storytelling, and user engagement. The study leverages OpenAI’s GPT-4 model to generate TikTok content summaries, aiming to align them with the essential features identified. By employing this model and incorporating human evaluation and expert assessment, this research endeavors to shed light on the intricate dynamics of modern content creation, where AI and human ingenuity converge. Ultimately, it seeks to enhance strategies for disseminating and evaluating educational information effectively in the realm of social media.</abstract>
      <url hash="5847e84b">2023.eval4nlp-1.4</url>
      <bibkey>kolagar-etal-2023-eduquick</bibkey>
      <doi>10.18653/v1/2023.eval4nlp-1.4</doi>
    </paper>
    <paper id="5">
      <title>Zero-shot Probing of Pretrained Language Models for Geography Knowledge</title>
      <author><first>Nitin</first><last>Ramrakhiyani</last><affiliation>International Institute of Information Technology, Hyderabad and Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Vasudeva</first><last>Varma</last><affiliation>International Institute of Information Technology Hyderabad, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <author><first>Girish</first><last>Palshikar</last></author>
      <author><first>Sachin</first><last>Pawar</last></author>
      <pages>49-61</pages>
      <abstract>Gauging the knowledge of Pretrained Language Models (PLMs) about facts in niche domains is an important step towards making them better in those domains. In this paper, we aim at evaluating multiple PLMs for their knowledge about world Geography. We contribute (i) a sufficiently sized dataset of masked Geography sentences to probe PLMs on masked token prediction and generation tasks, (ii) benchmark the performance of multiple PLMs on the dataset. We also provide a detailed analysis of the performance of the PLMs on different Geography facts.</abstract>
      <url hash="6928a02f">2023.eval4nlp-1.5</url>
      <bibkey>ramrakhiyani-etal-2023-zero</bibkey>
      <doi>10.18653/v1/2023.eval4nlp-1.5</doi>
    </paper>
    <paper id="6">
      <title>Transformers Go for the <fixed-case>LOL</fixed-case>s: Generating (Humourous) Titles from Scientific Abstracts End-to-End</title>
      <author><first>Yanran</first><last>Chen</last></author>
      <author><first>Steffen</first><last>Eger</last><affiliation>Universität Bielefeld</affiliation></author>
      <pages>62-84</pages>
      <abstract>We consider the end-to-end abstract-to-title generation problem, exploring seven recent transformer based models (including ChatGPT) fine-tuned on more than 30k abstract-title pairs from NLP and machine learning (ML) venues. As an extension, we also consider the harder problem of generating humorous paper titles. For the latter, we compile the first large-scale humor annotated dataset for scientific papers in the NLP/ML domains, comprising 2.6k titles. We evaluate all models using human and automatic metrics. Our human evaluation suggests that our best end-to-end system per-forms similarly to human authors (but arguably slightly worse). Generating funny titles is more difficult, however, and our automatic systems clearly underperform relative to humans and often learn dataset artefacts of humor. Finally, ChatGPT, without any fine-tuning, performs on the level of our best fine-tuned system.</abstract>
      <url hash="700be55e">2023.eval4nlp-1.6</url>
      <bibkey>chen-eger-2023-transformers</bibkey>
      <doi>10.18653/v1/2023.eval4nlp-1.6</doi>
    </paper>
    <paper id="7">
      <title>Summary Cycles: Exploring the Impact of Prompt Engineering on Large Language Models’ Interaction with Interaction Log Information</title>
      <author><first>Jeremy</first><last>Block</last></author>
      <author><first>Yu-Peng</first><last>Chen</last><affiliation>University of Florida</affiliation></author>
      <author><first>Abhilash</first><last>Budharapu</last></author>
      <author><first>Lisa</first><last>Anthony</last><affiliation>University of Florida</affiliation></author>
      <author><first>Bonnie</first><last>Dorr</last><affiliation>University of Florida</affiliation></author>
      <pages>85-99</pages>
      <abstract>With the aim of improving work efficiency, we examine how Large Language Models (LLMs) can better support the handoff of information by summarizing user interactions in collaborative intelligence analysis communication. We experiment with interaction logs, or a record of user interactions with a system. Inspired by chain-of-thought prompting, we describe a technique to avoid API token limits with recursive summarization requests. We then apply ChatGPT over multiple iterations to extract named entities, topics, and summaries, combined with interaction sequence sentences, to generate summaries of critical events and results of analysis sessions. We quantitatively evaluate the generated summaries against human-generated ones using common accuracy metrics (e.g., ROUGE-L, BLEU, BLEURT, and TER). We also report qualitative trends and the factuality of the output. We find that manipulating the audience feature or providing single-shot examples minimally influences the model’s accuracy. While our methodology successfully summarizes interaction logs, the lack of significant results raises questions about prompt engineering and summarization effectiveness generally. We call on explainable artificial intelligence research to better understand how terms and their placement may change LLM outputs, striving for more consistent prompt engineering guidelines.</abstract>
      <url hash="da75115c">2023.eval4nlp-1.7</url>
      <bibkey>block-etal-2023-summary</bibkey>
      <doi>10.18653/v1/2023.eval4nlp-1.7</doi>
    </paper>
    <paper id="8">
      <title>Large Language Models As Annotators: A Preliminary Evaluation For Annotating Low-Resource Language Content</title>
      <author><first>Savita</first><last>Bhat</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Vasudeva</first><last>Varma</last><affiliation>International Institute of Information Technology Hyderabad, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>100-107</pages>
      <abstract>The process of collecting human-generated annotations is time-consuming and resource-hungry. In the case of low-resource (LR) languages such as Indic languages, these efforts are more expensive due to the dearth of data and human experts. Considering their importance in solving downstream applications, there have been concentrated efforts exploring alternatives for human-generated annotations. To that extent, we seek to evaluate multilingual large language models (LLMs) for their potential to substitute or aid human-generated annotation efforts. We use LLMs to re-label publicly available datasets in LR languages for the tasks of natural language inference, sentiment analysis, and news classification. We compare these annotations with existing ground truth labels to analyze the efficacy of using LLMs for annotation tasks. We observe that the performance of these LLMs varies substantially across different tasks and languages. The results show that off-the-shelf use of multilingual LLMs is not appropriate and results in poor performance in two of the three tasks.</abstract>
      <url hash="9377f891">2023.eval4nlp-1.8</url>
      <bibkey>bhat-varma-2023-large</bibkey>
      <doi>10.18653/v1/2023.eval4nlp-1.8</doi>
    </paper>
    <paper id="9">
      <title>Can a Prediction’s Rank Offer a More Accurate Quantification of Bias? A Case Study Measuring Sexism in Debiased Language Models</title>
      <author><first>Jad</first><last>Doughman</last></author>
      <author><first>Shady</first><last>Shehata</last><affiliation>Mohamed bin Zayed University of Artificial Intelligence</affiliation></author>
      <author><first>Leen</first><last>Al Qadi</last></author>
      <author><first>Youssef</first><last>Nafea</last></author>
      <author><first>Fakhri</first><last>Karray</last><affiliation>University of Waterloo</affiliation></author>
      <pages>108-116</pages>
      <abstract>Pre-trained language models are known to inherit a plethora of contextual biases from their training data. These biases have proven to be projected onto a variety of downstream applications, making their detection and mitigation imminent. Limited research has been conducted to quantify specific bias types, such as benevolent sexism, which may be subtly present within the inferred connotations of a sentence. To this extent, our work aims to: (1) provide a benchmark of sexism sentences; (2) adapt two bias metrics: mean probability score and mean normalized rank; (3) conduct a case study to quantify and analyze sexism in base and de-biased masked language models. We find that debiasing, even in its most effective form (Auto-Debias), solely nullifies the probability score of biasing tokens, while retaining them in high ranks. Auto-Debias illustrates a 90%-96% reduction in mean probability scores from base to debiased models, while only a 3%-16% reduction in mean normalized ranks. Similar to the application of non-parametric statistical tests for data that does not follow a normal distribution, operating on the ranks of predictions rather than their probability scores offers a more representative bias measure.</abstract>
      <url hash="e756827b">2023.eval4nlp-1.9</url>
      <bibkey>doughman-etal-2023-predictions</bibkey>
      <doi>10.18653/v1/2023.eval4nlp-1.9</doi>
    </paper>
    <paper id="10">
      <title>The <fixed-case>E</fixed-case>val4<fixed-case>NLP</fixed-case> 2023 Shared Task on Prompting Large Language Models as Explainable Metrics</title>
      <author><first>Christoph</first><last>Leiter</last><affiliation>Universität Bielefeld</affiliation></author>
      <author><first>Juri</first><last>Opitz</last><affiliation>Ruprecht-Karls-Universität Heidelberg</affiliation></author>
      <author><first>Daniel</first><last>Deutsch</last><affiliation>Google</affiliation></author>
      <author><first>Yang</first><last>Gao</last><affiliation>Google</affiliation></author>
      <author><first>Rotem</first><last>Dror</last><affiliation>University of Haifa</affiliation></author>
      <author><first>Steffen</first><last>Eger</last><affiliation>Universität Bielefeld</affiliation></author>
      <pages>117-138</pages>
      <abstract>Generative large language models (LLMs) have seen many breakthroughs over the last year. With an increasing number of parameters and pre-training data, they have shown remarkable capabilities to solve tasks with minimal or no task-related examples. Notably, LLMs have been successfully employed as evaluation metrics in text generation tasks. Strategies employed in this context differ in the choice of input prompts, the selection of samples for demonstration, and the methodology used to construct scores grading the generations. Approaches often differ in the input prompts, the samples that are selected for demonstration and the construction process of scores from the output. Within this context, we introduce the Eval4NLP 2023 shared task that asks participants to explore such approaches for machine translation evaluation and summarization eval- uation. Specifically, we select a list of allowed LLMs and disallow fine-tuning to ensure a focus on prompting. We test the approaches of the participants on a new reference-free test-set spanning 3 language pairs for machine transla- tion as well as a summarization dataset. Further, we present an overview of the approaches taken by the participants, present their results on the test set and analyze paths for future work. Fi- nally, as a separate track, we perform a human evaluation of the plausibility of explanations given by the LLMs and its effect on model performance. We make parts of our code and datasets available.</abstract>
      <url hash="5ff11896">2023.eval4nlp-1.10</url>
      <bibkey>leiter-etal-2023-eval4nlp</bibkey>
      <doi>10.18653/v1/2023.eval4nlp-1.10</doi>
    </paper>
    <paper id="11">
      <title><fixed-case>HIT</fixed-case>-<fixed-case>MI</fixed-case>&amp;<fixed-case>T</fixed-case> Lab’s Submission to <fixed-case>E</fixed-case>val4<fixed-case>NLP</fixed-case> 2023 Shared Task</title>
      <author><first>Rui</first><last>Zhang</last></author>
      <author><first>Fuhai</first><last>Song</last></author>
      <author><first>Hui</first><last>Huang</last></author>
      <author><first>Jinghao</first><last>Yuan</last></author>
      <author><first>Muyun</first><last>Yang</last></author>
      <author><first>Tiejun</first><last>Zhao</last><affiliation>Harbin Institute of Technology</affiliation></author>
      <pages>139-148</pages>
      <abstract>Recently, Large Language Models (LLMs) have boosted the research in natural language processing and shown impressive capabilities across numerous domains, including machine translation evaluation. This paper presents our methods developed for the machine translation evaluation sub-task of the Eval4NLP 2023 Shared Task. Based on the provided LLMs, we propose a generation-based method as well as a probability-based method to perform evaluation, explore different strategies when selecting the demonstrations for in-context learning, and try different ensemble methods to further improve the evaluation accuracy. The experiment results on the development set and test set demonstrate the effectiveness of our proposed method.</abstract>
      <url hash="b245f666">2023.eval4nlp-1.11</url>
      <bibkey>zhang-etal-2023-hit</bibkey>
      <doi>10.18653/v1/2023.eval4nlp-1.11</doi>
    </paper>
    <paper id="12">
      <title>Understanding Large Language Model Based Metrics for Text Summarization</title>
      <author><first>Abhishek</first><last>Pradhan</last><affiliation>Incivus</affiliation></author>
      <author><first>Ketan</first><last>Todi</last><affiliation>Google</affiliation></author>
      <pages>149-155</pages>
      <abstract>This paper compares the two most widely used techniques for evaluating generative tasks with large language models (LLMs): prompt-based evaluation and log-likelihood evaluation as part of the Eval4NLP shared task. We focus on the summarization task and evaluate both small and large LLM models. We also study the impact of LLAMA and LLAMA 2 on summarization, using the same set of prompts and techniques. We used the Eval4NLP dataset for our comparison. This study provides evidence of the advantages of prompt-based evaluation techniques over log-likelihood based techniques, especially for large models and models with better reasoning power.</abstract>
      <url hash="61f21389">2023.eval4nlp-1.12</url>
      <bibkey>pradhan-todi-2023-understanding</bibkey>
      <doi>10.18653/v1/2023.eval4nlp-1.12</doi>
    </paper>
    <paper id="13">
      <title><fixed-case>LTRC</fixed-case>_<fixed-case>IIITH</fixed-case>’s 2023 Submission for Prompting Large Language Models as Explainable Metrics Task</title>
      <author><first>Pavan</first><last>Baswani</last></author>
      <author><first>Ananya</first><last>Mukherjee</last></author>
      <author><first>Manish</first><last>Shrivastava</last><affiliation>International Institute of Information Technology Hyderabad, India</affiliation></author>
      <pages>156-163</pages>
      <abstract>In this report, we share our contribution to the Eval4NLP Shared Task titled “Prompting Large Language Models as Explainable Metrics.” We build our prompts with a primary focus on effective prompting strategies, score-aggregation, and explainability for LLM-based metrics. We participated in the track for smaller models by submitting the scores along with their explanations. According to the Kendall correlation scores on the leaderboard, our MT evaluation submission ranks second-best, while our summarization evaluation submission ranks fourth, with only a 0.06 difference from the leading submission.</abstract>
      <url hash="2dc4f64a">2023.eval4nlp-1.13</url>
      <bibkey>baswani-etal-2023-ltrc-iiiths</bibkey>
      <doi>10.18653/v1/2023.eval4nlp-1.13</doi>
    </paper>
    <paper id="14">
      <title>Which is better? Exploring Prompting Strategy For <fixed-case>LLM</fixed-case>-based Metrics</title>
      <author><first>JoongHoon</first><last>Kim</last></author>
      <author><first>Sangmin</first><last>Lee</last><affiliation>Korea University</affiliation></author>
      <author><first>Seung</first><last>Hun Han</last></author>
      <author><first>Saeran</first><last>Park</last></author>
      <author><first>Jiyoon</first><last>Lee</last></author>
      <author><first>Kiyoon</first><last>Jeong</last><affiliation>Korea University</affiliation></author>
      <author><first>Pilsung</first><last>Kang</last><affiliation>Korea University</affiliation></author>
      <pages>164-183</pages>
      <abstract>This paper describes the DSBA submissions to the Prompting Large Language Models as Explainable Metrics shared task, where systems were submitted to two tracks: small and large summarization tracks. With advanced Large Language Models (LLMs) such as GPT-4, evaluating the quality of Natural Language Generation (NLG) has become increasingly paramount. Traditional similarity-based metrics such as BLEU and ROUGE have shown to misalign with human evaluation and are ill-suited for open-ended generation tasks. To address this issue, we explore the potential capability of LLM-based metrics, especially leveraging open-source LLMs. In this study, wide range of prompts and prompting techniques are systematically analyzed with three approaches: prompting strategy, score aggregation, and explainability. Our research focuses on formulating effective prompt templates, determining the granularity of NLG quality scores and assessing the impact of in-context examples on LLM-based evaluation. Furthermore, three aggregation strategies are compared to identify the most reliable method for aggregating NLG quality scores. To examine explainability, we devise a strategy that generates rationales for the scores and analyzes the characteristics of the explanation produced by the open-source LLMs. Extensive experiments provide insights regarding evaluation capabilities of open-source LLMs and suggest effective prompting strategies.</abstract>
      <url hash="b2f548b3">2023.eval4nlp-1.14</url>
      <bibkey>kim-etal-2023-better</bibkey>
      <doi>10.18653/v1/2023.eval4nlp-1.14</doi>
    </paper>
    <paper id="15">
      <title>Characterised <fixed-case>LLM</fixed-case>s Affect its Evaluation of Summary and Translation</title>
      <author><first>Yuan</first><last>Lu</last></author>
      <author><first>Yu-Ting</first><last>Lin</last><affiliation>Taipei Municipal Chenggong High School</affiliation></author>
      <pages>184-192</pages>
      <abstract>In today’s widespread use of Large Language Models (LLMs), there have been significant achievements in various text domains such as generating summaries and translations. However, there is still room for development and improvement in evaluating the outputs of LLMs. In this paper, we propose an innovative scoring system that assesses the quality of summaries and translations using multiple metrics, we also enhance LLM’s performance in scoring tasks by assigning it different roles, effectively making it act as an expert. We test four roles in the study: a teacher, a proofreader, a travel writer, and an internet troll, comparing the advantages and disadvantages of each role in the scoring task. Our research results demonstrate that emphasizing LLM’s multilingual capabilities and strict standards as its identity can effectively boost its performance. Additionally, imbuing LLM with a more critical thinking ability enhances its performance in translation tasks compared to a milder LLM identity. In summary, we show that assigning different identities to LLM can influence its performance in scoring tasks. We believe that this research will contribute to the use of LLMs for scoring purposes.</abstract>
      <url hash="2ec4b1e6">2023.eval4nlp-1.15</url>
      <bibkey>lu-lin-2023-characterised</bibkey>
      <doi>10.18653/v1/2023.eval4nlp-1.15</doi>
    </paper>
    <paper id="16">
      <title>Reference-Free Summarization Evaluation with Large Language Models</title>
      <author><first>Abbas</first><last>Akkasi</last><affiliation>Carleton University</affiliation></author>
      <author><first>Kathleen</first><last>Fraser</last><affiliation>National Research Council Canada</affiliation></author>
      <author><first>Majid</first><last>Komeili</last><affiliation>Carleton University</affiliation></author>
      <pages>193-201</pages>
      <abstract>With the continuous advancement in unsupervised learning methodologies, text generation has become increasingly pervasive. However, the evaluation of the quality of the generated text remains challenging. Human annotations are expensive and often show high levels of disagreement, in particular for certain tasks characterized by inherent subjectivity, such as translation and summarization.Consequently, the demand for automated metrics that can reliably assess the quality of such generative systems and their outputs has grown more pronounced than ever. In 2023, Eval4NLP organized a shared task dedicated to the automatic evaluation of outputs from two specific categories of generative systems: machine translation and summarization. This evaluation was achieved through the utilization of prompts with Large Language Models. Participating in the summarization evaluation track, we propose an approach that involves prompting LLMs to evaluate six different latent dimensions of summarization quality. In contrast to many previous approaches to summarization assessments, which emphasize lexical overlap with reference text, this method surfaces the importance of correct syntax in summarization evaluation. Our method resulted in the second-highest performance in this shared task, demonstrating its effectiveness as a reference-free evaluation.</abstract>
      <url hash="d6c2c2c9">2023.eval4nlp-1.16</url>
      <bibkey>akkasi-etal-2023-reference</bibkey>
      <doi>10.18653/v1/2023.eval4nlp-1.16</doi>
    </paper>
    <paper id="17">
      <title>Little Giants: Exploring the Potential of Small <fixed-case>LLM</fixed-case>s as Evaluation Metrics in Summarization in the <fixed-case>E</fixed-case>val4<fixed-case>NLP</fixed-case> 2023 Shared Task</title>
      <author><first>Neema</first><last>Kotonya</last></author>
      <author><first>Saran</first><last>Krishnasamy</last></author>
      <author><first>Joel</first><last>Tetreault</last></author>
      <author><first>Alejandro</first><last>Jaimes</last><affiliation>Dataminr</affiliation></author>
      <pages>202-218</pages>
      <abstract>This paper describes and analyzes our participation in the 2023 Eval4NLP shared task, which focuses on assessing the effectiveness of prompt-based techniques to empower Large Language Models to handle the task of quality estimation, particularly in the context of evaluating machine translations and summaries. We conducted systematic experiments with various prompting techniques, including standard prompting, prompts informed by annotator instructions, and innovative chain-of-thought prompting. In addition, we integrated these approaches with zero-shot and one-shot learning methods to maximize the efficacy of our evaluation procedures. Our work reveals that combining these approaches using a “small”, open source model (orca_mini_v3_7B) yields competitive results.</abstract>
      <url hash="3340844e">2023.eval4nlp-1.17</url>
      <bibkey>kotonya-etal-2023-little</bibkey>
      <doi>10.18653/v1/2023.eval4nlp-1.17</doi>
    </paper>
    <paper id="18">
      <title>Exploring Prompting Large Language Models as Explainable Metrics</title>
      <author><first>Ghazaleh</first><last>Mahmoudi</last></author>
      <pages>219-227</pages>
      <abstract>This paper describes the IUST NLP Lab submission to the Prompting Large Language Models as Explainable Metrics Shared Task at the Eval4NLP 2023 Workshop on Evaluation &amp; Comparison of NLP Systems. We have proposed a zero-shot prompt-based strategy for explainable evaluation of the summarization task using Large Language Models (LLMs). The conducted experiments demonstrate the promising potential of LLMs as evaluation metrics in Natural Language Processing (NLP), particularly in the field of summarization. Both few-shot and zero-shot approaches are employed in these experiments. The performance of our best provided prompts achieved a Kendall correlation of 0.477 with human evaluations in the text summarization task on the test data.</abstract>
      <url hash="d19950a0">2023.eval4nlp-1.18</url>
      <bibkey>mahmoudi-2023-exploring</bibkey>
      <doi>10.18653/v1/2023.eval4nlp-1.18</doi>
    </paper>
    <paper id="19">
      <title>Team <fixed-case>NLLG</fixed-case> submission for <fixed-case>E</fixed-case>val4<fixed-case>NLP</fixed-case> 2023 Shared Task: Retrieval-Augmented In-Context Learning for <fixed-case>NLG</fixed-case> Evaluation</title>
      <author><first>Daniil</first><last>Larionov</last><affiliation>Universität Bielefeld</affiliation></author>
      <author><first>Vasiliy</first><last>Viskov</last></author>
      <author><first>George</first><last>Kokush</last></author>
      <author><first>Alexander</first><last>Panchenko</last><affiliation>Skoltech</affiliation></author>
      <author><first>Steffen</first><last>Eger</last><affiliation>Universität Bielefeld</affiliation></author>
      <pages>228-234</pages>
      <abstract>In this paper, we propose a retrieval-augmented in-context learning for natural language generation (NLG) evaluation. This method allows practitioners to utilize large language models (LLMs) for various NLG evaluation tasks without any fine-tuning. We apply our approach to Eval4NLP 2023 Shared Task in translation evaluation and summarization evaluation subtasks. The findings suggest that retrieval-augmented in-context learning is a promising approach for creating LLM-based evaluation metrics for NLG. Further research directions include exploring the performance of various publicly available LLM models and identifying which LLM properties help boost the quality of the metric.</abstract>
      <url hash="946138d3">2023.eval4nlp-1.19</url>
      <bibkey>larionov-etal-2023-team</bibkey>
      <doi>10.18653/v1/2023.eval4nlp-1.19</doi>
    </paper>
  </volume>
</collection>
