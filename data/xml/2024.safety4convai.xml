<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.safety4convai">
  <volume id="1" ingest-date="2024-05-18" type="proceedings">
    <meta>
      <booktitle>Proceedings of Safety4ConvAI: The Third Workshop on Safety for Conversational AI @ LREC-COLING 2024</booktitle>
      <editor><first>Tanvi</first><last>Dinkar</last></editor>
      <editor><first>Giuseppe</first><last>Attanasio</last></editor>
      <editor><first>Amanda</first><last>Cercas Curry</last></editor>
      <editor><first>Ioannis</first><last>Konstas</last></editor>
      <editor><first>Dirk</first><last>Hovy</last></editor>
      <editor><first>Verena</first><last>Rieser</last></editor>
      <publisher>ELRA and ICCL</publisher>
      <address>Torino, Italia</address>
      <month>May</month>
      <year>2024</year>
      <url hash="d0c409c6">2024.safety4convai-1</url>
      <venue>safety4convai</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="370cd433">2024.safety4convai-1.0</url>
      <bibkey>safety4convai-2024-safety4convai</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Grounding <fixed-case>LLM</fixed-case>s to In-prompt Instructions: Reducing Hallucinations Caused by Static Pre-training Knowledge</title>
      <author><first>Angus</first><last>Addlesee</last></author>
      <pages>1–7</pages>
      <abstract>When deploying LLMs in certain commercial or research settings, domain specific knowledge must be explicitly provided within the prompt. This in-prompt knowledge can conflict with an LLM’s static world knowledge learned at pre-training, causing model hallucination (see examples in Table 1). In safety-critical settings, like healthcare and finance, these hallucinations can harm vulnerable users. We have curated a QA corpus containing information that LLMs could not have seen at pre-training. Using our corpus, we have probed various LLMs, manipulating both the prompt and the knowledge representation. We have found that our ‘Jodie’ prompt consistently improves the model’s textual grounding to the given knowledge, and in-turn the overall answer accuracy. This is true in both the healthcare and finance domains - improving accuracy by up to 28% (mean: 12%). We have also identified that hierarchical and direct node-property graph structures could lead to more interpretable and controllable systems that provide a natural language interface with real-time in-domain knowledge. Our corpus will enable further work on this critical challenge.</abstract>
      <url hash="28e11cfc">2024.safety4convai-1.1</url>
      <bibkey>addlesee-2024-grounding</bibkey>
    </paper>
    <paper id="2">
      <title>Diversity-Aware Annotation for Conversational <fixed-case>AI</fixed-case> Safety</title>
      <author><first>Alicia</first><last>Parrish</last></author>
      <author><first>Vinodkumar</first><last>Prabhakaran</last></author>
      <author><first>Lora</first><last>Aroyo</last></author>
      <author><first>Mark</first><last>Díaz</last></author>
      <author><first>Christopher M.</first><last>Homan</last></author>
      <author><first>Greg</first><last>Serapio-García</last></author>
      <author><first>Alex S.</first><last>Taylor</last></author>
      <author><first>Ding</first><last>Wang</last></author>
      <pages>8–15</pages>
      <abstract>How people interpret content is deeply influenced by their socio-cultural backgrounds and lived experiences. This is especially crucial when evaluating AI systems for safety, where accounting for such diversity in interpretations and potential impacts on human users will make them both more successful and inclusive. While recent work has demonstrated the importance of diversity in human ratings that underlie AI pipelines, effective and efficient ways to incorporate diverse perspectives in human data annotation pipelines is still largely elusive. In this paper, we discuss the primary challenges faced in incorporating diversity into model evaluations, and propose a practical diversity-aware annotation approach. Using an existing dataset with highly parallel safety annotations, we take as a test case a policy that prioritizes recall of safety issues, and demonstrate that our diversity-aware approach can efficiently obtain a higher recall of safety issues flagged by minoritized rater groups without hurting overall precision.</abstract>
      <url hash="714a8034">2024.safety4convai-1.2</url>
      <bibkey>parrish-etal-2024-diversity</bibkey>
    </paper>
    <paper id="3">
      <title>Using Information Retrieval Techniques to Automatically Repurpose Existing Dialogue Datasets for Safe Chatbot Development</title>
      <author><first>Tunde Oluwaseyi</first><last>Ajayi</last></author>
      <author><first>Gaurav</first><last>Negi</last></author>
      <author><first>Mihael</first><last>Arcan</last></author>
      <author><first>Paul</first><last>Buitelaar</last></author>
      <pages>16–27</pages>
      <abstract>There has been notable progress in the development of open-domain dialogue systems (chatbots) especially with the rapid advancement of the capabilities of Large Language Models. Chatbots excel at holding conversations in a manner that keeps a user interested and engaged. However, their responses can be unsafe, as they can respond in an offensive manner or offer harmful professional advice. As a way to mitigate this issue, recent work crowdsource datasets with exemplary responses or annotate dialogue safety datasets, which are relatively scarce compared to casual dialogues. Despite the quality of data obtained from crowdsourcing, it can be expensive and time consuming. This work proposes an effective pipeline, using information retrieval, to automatically repurpose existing dialogue datasets for safe chatbot development, as a way to address the aforementioned challenges. We select an existing dialogue dataset, revise its unsafe responses, as a way to obtain a dataset with safer responses to unsafe user inputs. We then fine-tune dialogue models on the original and revised datasets and generate responses to evaluate the safeness of the models.</abstract>
      <url hash="713bd26f">2024.safety4convai-1.3</url>
      <bibkey>ajayi-etal-2024-using</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>F</fixed-case>air<fixed-case>P</fixed-case>air: A Robust Evaluation of Biases in Language Models through Paired Perturbations</title>
      <author><first>Jane</first><last>Dwivedi-Yu</last></author>
      <pages>28–39</pages>
      <abstract>The accurate evaluation of differential treatment in language models to specific groups is critical to ensuring a positive and safe user experience. An ideal evaluation should have the properties of being robust, extendable to new groups or attributes, and being able to capture biases that appear in typical usage (rather than just extreme, rare cases). Relatedly, bias evaluation should surface not only egregious biases but also ones that are subtle and commonplace, such as a likelihood for talking about appearances with regard to women. We present FairPair, an evaluation framework for assessing differential treatment that occurs during ordinary usage. FairPair operates through counterfactual pairs, but crucially, the paired continuations are grounded in the same demographic group, which ensures equivalent comparison. Additionally, unlike prior work, our method factors in the inherent variability that comes from the generation process itself by measuring the sampling variability. We present an evaluation of several commonly used generative models and a qualitative analysis that indicates a preference for discussing family and hobbies with regard to women.</abstract>
      <url hash="efdbe264">2024.safety4convai-1.4</url>
      <bibkey>dwivedi-yu-2024-fairpair</bibkey>
    </paper>
    <paper id="5">
      <title>Learning To See But Forgetting To Follow: Visual Instruction Tuning Makes <fixed-case>LLM</fixed-case>s More Prone To Jailbreak Attacks</title>
      <author><first>Georgios</first><last>Pantazopoulos</last></author>
      <author><first>Amit</first><last>Parekh</last></author>
      <author><first>Malvina</first><last>Nikandrou</last></author>
      <author><first>Alessandro</first><last>Suglia</last></author>
      <pages>40–51</pages>
      <abstract>Augmenting Large Language Models (LLMs) with image-understanding capabilities has resulted in a boom of high-performing Vision-Language models (VLMs). While studying the alignment of LLMs to human values has received widespread attention, the safety of VLMs has not received the same attention. In this paper, we explore the impact of jailbreaking on three state-of-the-art VLMs, each using a distinct modeling approach. By comparing each VLM to their respective LLM backbone, we find that each VLM is more susceptible to jailbreaking. We consider this as an undesirable outcome from visual instruction-tuning, which imposes a forgetting effect on an LLM’s safety guardrails. Therefore, we provide recommendations for future work based on evaluation strategies that aim to highlight the weaknesses of a VLM, as well as take safety measures into account during visual instruction tuning.</abstract>
      <url hash="e60055c5">2024.safety4convai-1.5</url>
      <bibkey>pantazopoulos-etal-2024-learning</bibkey>
    </paper>
  </volume>
</collection>
