<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.sltat">
  <volume id="1" ingest-date="2022-09-28">
    <meta>
      <booktitle>Proceedings of the 7th International Workshop on Sign Language Translation and Avatar Technology: The Junction of the Visual and the Textual: Challenges and Perspectives</booktitle>
      <editor><first>Eleni</first><last>Efthimiou</last></editor>
      <editor><first>Stavroula-Evita</first><last>Fotinea</last></editor>
      <editor><first>Thomas</first><last>Hanke</last></editor>
      <editor><first>John C.</first><last>McDonald</last></editor>
      <editor><first>Dimitar</first><last>Shterionov</last></editor>
      <editor><first>Rosalee</first><last>Wolfe</last></editor>
      <publisher>European Language Resources Association</publisher>
      <address>Marseille, France</address>
      <month>June</month>
      <year>2022</year>
      <url hash="ad9c3ffe">2022.sltat-1</url>
      <venue>sltat</venue>
    </meta>
    <frontmatter>
      <url hash="3301e753">2022.sltat-1.0</url>
      <bibkey>sltat-2022-international</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Synthesis for the Kinematic Control of Identity in Sign Language</title>
      <author><first>Félix</first><last>Bigand</last></author>
      <author><first>Elise</first><last>Prigent</last></author>
      <author><first>Annelies</first><last>Braffort</last></author>
      <pages>1–6</pages>
      <abstract>Sign Language (SL) animations generated from motion capture (mocap) of real signers convey critical information about their identity. It has been suggested that this information is mostly carried by statistics of the movements kinematics. Manipulating these statistics in the generation of SL movements could allow controlling the identity of the signer, notably to preserve anonymity. This paper tests this hypothesis by presenting a novel synthesis algorithm that manipulates the identity-specific statistics of mocap recordings. The algorithm produced convincing new versions of French Sign Language discourses, which accurately modulated the identity prediction of a machine learning model. These results open up promising perspectives toward the automatic control of identity in the motion animation of virtual signers.</abstract>
      <url hash="cf2584a4">2022.sltat-1.1</url>
      <bibkey>bigand-etal-2022-synthesis</bibkey>
    </paper>
    <paper id="2">
      <title>Analysis of Torso Movement for Signing Avatar Using Deep Learning</title>
      <author><first>Shatabdi</first><last>Choudhury</last></author>
      <pages>7–12</pages>
      <abstract>Avatars are virtual or on-screen representations of a human used in various roles for sign language display, including translation and educational tools. Though the ability of avatars to portray acceptable sign language with believable human-like motion has improved in recent years, many still lack the naturalness and supporting motions of human signing. Such details are generally not included in the linguistic annotation. Nevertheless, these motions are highly essential to displaying lifelike and communicative animations. This paper presents a deep learning model for use in a signing avatar. The study focuses on coordinating torso movements and other human body parts. The proposed model will automatically compute the torso rotation based on the avatar’s wrist positions. The resulting motion can improve the user experience and engagement with the avatar.</abstract>
      <url hash="9f78d7a3">2022.sltat-1.2</url>
      <bibkey>choudhury-2022-analysis</bibkey>
    </paper>
    <paper id="3">
      <title>Isolated Sign Recognition using <fixed-case>ASL</fixed-case> Datasets with Consistent Text-based Gloss Labeling and Curriculum Learning</title>
      <author><first>Konstantinos M.</first><last>Dafnis</last></author>
      <author><first>Evgenia</first><last>Chroni</last></author>
      <author><first>Carol</first><last>Neidle</last></author>
      <author><first>Dimitri</first><last>Metaxas</last></author>
      <pages>13–20</pages>
      <abstract>We present a new approach for isolated sign recognition, which combines a spatial-temporal Graph Convolution Network (GCN) architecture for modeling human skeleton keypoints with late fusion of both the forward and backward video streams, and we explore the use of curriculum learning. We employ a type of curriculum learning that dynamically estimates, during training, the order of difficulty of each input video for sign recognition; this involves learning a new family of data parameters that are dynamically updated during training. The research makes use of a large combined video dataset for American Sign Language (ASL), including data from both the American Sign Language Lexicon Video Dataset (ASLLVD) and the Word-Level American Sign Language (WLASL) dataset, with modified gloss labeling of the latter—to ensure 1-1 correspondence between gloss labels and distinct sign productions, as well as consistency in gloss labeling across the two datasets. This is the first time that these two datasets have been used in combination for isolated sign recognition research. We also compare the sign recognition performance on several different subsets of the combined dataset, varying in, e.g., the minimum number of samples per sign (and therefore also in the total number of sign classes and video examples).</abstract>
      <url hash="3f615038">2022.sltat-1.3</url>
      <bibkey>dafnis-etal-2022-isolated</bibkey>
    </paper>
    <paper id="4">
      <title>Example-based Multilinear Sign Language Generation from a Hierarchical Representation</title>
      <author><first>Boris</first><last>Dauriac</last></author>
      <author><first>Annelies</first><last>Braffort</last></author>
      <author><first>Elise</first><last>Bertin-Lemée</last></author>
      <pages>21–28</pages>
      <abstract>This article presents an original method for automatic generation of sign language (SL) content by means of the animation of an avatar, with the aim of creating animations that respect as much as possible linguistic constraints while keeping bio-realistic properties. This method is based on the use of a domain-specific bilingual corpus richly annotated with timed alignments between SL motion capture data, text and hierarchical expressions from the framework called AZee at subsentential level. Animations representing new SL content are built from blocks of animations present in the corpus and adapted to the context if necessary. A smart blending approach has been designed that allows the concatenation, replacement and adaptation of original animation blocks. This approach has been tested on a tailored testset to show as a proof of concept its potential in comprehensibility and fluidity of the animation, as well as its current limits.</abstract>
      <url hash="0c8205d4">2022.sltat-1.4</url>
      <bibkey>dauriac-etal-2022-example</bibkey>
    </paper>
    <paper id="5">
      <title>Fine-tuning of Convolutional Neural Networks for the Recognition of Facial Expressions in Sign Language Video Samples</title>
      <author><first>Neha</first><last>Deshpande</last></author>
      <author><first>Fabrizio</first><last>Nunnari</last></author>
      <author><first>Eleftherios</first><last>Avramidis</last></author>
      <pages>29–38</pages>
      <abstract>In this paper, we investigate the capability of convolutional neural networks to recognize in sign language video frames the six basic Ekman facial expressions for ‘fear’, ‘disgust’, ‘surprise’, ‘sadness’, ‘happiness’, ‘anger’ along with the ‘neutral’ class. Given the limited amount of annotated facial expression data for the sign language domain, we started from a model pre-trained on general-purpose facial expression datasets and we applied various machine learning techniques such as fine-tuning, data augmentation, class balancing, as well as image preprocessing to reach a better accuracy. The models were evaluated using K-fold cross-validation to get more accurate conclusions. It is experimentally demonstrated that fine-tuning a pre-trained model along with data augmentation by horizontally flipping images and image normalization, helps in providing the best accuracy on the sign language dataset. The best setting achieves satisfactory classification accuracy, comparable to state-of-the-art systems in generic facial expression recognition. Experiments were performed using different combinations of the above-mentioned techniques based on two different architectures, namely MobileNet and EfficientNet, and is deemed that both architectures seem equally suitable for the purpose of fine-tuning, whereas class balancing is discouraged.</abstract>
      <url hash="22d0197a">2022.sltat-1.5</url>
      <bibkey>deshpande-etal-2022-fine</bibkey>
    </paper>
    <paper id="6">
      <title>Signing Avatar Performance Evaluation within <fixed-case>EASIER</fixed-case> Project</title>
      <author><first>Athanasia-Lida</first><last>Dimou</last></author>
      <author><first>Vassilis</first><last>Papavassiliou</last></author>
      <author><first>John</first><last>McDonald</last></author>
      <author><first>Theodore</first><last>Goulas</last></author>
      <author><first>Kyriaki</first><last>Vasilaki</last></author>
      <author><first>Anna</first><last>Vacalopoulou</last></author>
      <author><first>Stavroula-Evita</first><last>Fotinea</last></author>
      <author><first>Eleni</first><last>Efthimiou</last></author>
      <author><first>Rosalee</first><last>Wolfe</last></author>
      <pages>39–44</pages>
      <abstract>The direct involvement of deaf users in the development and evaluation of signing avatars is imperative to achieve legibility and raise trust among synthetic signing technology consumers. A paradigm of constructive cooperation between researchers and the deaf community is the EASIER project , where user driven design and technology development have already started producing results. One major goal of the project is the direct involvement of sign language (SL) users at every stage of development of the project’s signing avatar. As developers wished to consider every parameter of SL articulation including affect and prosody in developing the EASIER SL representation engine, it was necessary to develop a steady communication channel with a wide public of SL users who may act as evaluators and can provide guidance throughout research steps, both during the project’s end-user evaluation cycles and beyond. To this end, we have developed a questionnaire-based methodology, which enables researchers to reach signers of different SL communities on-line and collect their guidance and preferences on all aspects of SL avatar animation that are under study. In this paper, we report on the methodology behind the application of the EASIER evaluation framework for end-user guidance in signing avatar development as it is planned to address signers of four SLs -Greek Sign Language (GSL), French Sign Language (LSF), German Sign Language (DGS) and Swiss German Sign Language (DSGS)- during the first project evaluation cycle. We also briefly report on some interesting findings from the pilot implementation of the questionnaire with content from the Greek Sign Language (GSL).</abstract>
      <url hash="59c973f6">2022.sltat-1.6</url>
      <bibkey>dimou-etal-2022-signing</bibkey>
    </paper>
    <paper id="7">
      <title>Improving Signer Independent Sign Language Recognition for Low Resource Languages</title>
      <author><first>Ruth</first><last>Holmes</last></author>
      <author><first>Ellen</first><last>Rushe</last></author>
      <author><first>Frank</first><last>Fowley</last></author>
      <author><first>Anthony</first><last>Ventresque</last></author>
      <pages>45–52</pages>
      <abstract>The reliance of deep learning algorithms on large scale datasets represents a significant challenge when learning from low resource sign language datasets. This challenge is compounded when we consider that, for a model to be effective in the real world, it must not only learn the variations of a given sign, but also learn to be invariant to the person signing. In this paper, we first illustrate the performance gap between signer-independent and signer-dependent models on Irish Sign Language manual hand shape data. We then evaluate the effect of transfer learning, with different levels of fine-tuning, on the generalisation of signer independent models, and show the effects of different input representations, namely variations in image data and pose estimation. We go on to investigate the sensitivity of current pose estimation models in order to establish their limitations and areas in need of improvement. The results show that accurate pose estimation outperforms raw RGB image data, even when relying on pre-trained image models. Following on from this, we investigate image texture as a potential contributing factor to the gap in performance between signer-dependent and signer-independent models using counterfactual testing images and discuss potential ramifications for low-resource sign languages. Keywords: Sign language recognition, Transfer learning, Irish Sign Language, Low-resource languages</abstract>
      <url hash="b6796d07">2022.sltat-1.7</url>
      <bibkey>holmes-etal-2022-improving</bibkey>
    </paper>
    <paper id="8">
      <title>Improved Facial Realism through an Enhanced Representation of Anatomical Behavior in Sign Language Avatars</title>
      <author><first>Ronan</first><last>Johnson</last></author>
      <pages>53–58</pages>
      <abstract>Facial movements and expressions are critical features of signed languages, yet are some of the most challenging to reproduce on signing avatars. Due to the relative lack of research efforts in this area, the facial capabilities of such avatars have yet to receive the approval of those in the Deaf community. This paper revisits the representations of the human face in signed avatars, specifically those based on parameterized muscle simulation such as FACS and the MPEG-4 file definition. An improved framework based on rotational pivots and pre-defined movements is capable of reproducing realistic, natural gestures and mouthings on sign language avatars. The new approach is more harmonious with the underlying construction of signed avatars, generates improved results, and allows for a more intuitive workflow for the artists and animators who interact with the system.</abstract>
      <url hash="dc8b80aa">2022.sltat-1.8</url>
      <bibkey>johnson-2022-improved</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>K</fixed-case>o<fixed-case>S</fixed-case>ign Sign Language Translation Project: Introducing The <fixed-case>NIASL</fixed-case>2021 Dataset</title>
      <author><first>Mathew</first><last>Huerta-Enochian</last></author>
      <author><first>Du Hui</first><last>Lee</last></author>
      <author><first>Hye Jin</first><last>Myung</last></author>
      <author><first>Kang Suk</first><last>Byun</last></author>
      <author><first>Jun Woo</first><last>Lee</last></author>
      <pages>59–66</pages>
      <abstract>We introduce a new sign language production (SLP) and sign language translation (SLT) dataset, NIASL2021, consisting of 201,026 Korean-KSL data pairs. KSL translations of Korean source texts are represented in three formats: video recordings, keypoint position data, and time-aligned gloss annotations for each hand (using a 7,989 sign vocabulary) and for eight different non-manual signals (NMS). We evaluated our sign language elicitation methodology and found that text-based prompting had a negative effect on translation quality in terms of naturalness and comprehension. We recommend distilling text into a visual medium before translating into sign language or adding a prompt-blind review step to text-based translation methodologies.</abstract>
      <url hash="68fa6f9f">2022.sltat-1.9</url>
      <bibkey>huerta-enochian-etal-2022-kosign</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/how2sign">How2Sign</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rwth-phoenix-weather-2014-t">RWTH-PHOENIX-Weather 2014 T</pwcdataset>
    </paper>
    <paper id="10">
      <title>A Novel Approach to Managing Lower Face Complexity in Signing Avatars</title>
      <author><first>John</first><last>McDonald</last></author>
      <author><first>Ronan</first><last>Johnson</last></author>
      <author><first>Rosalee</first><last>Wolfe</last></author>
      <pages>67–72</pages>
      <abstract>An avatar that produces legible, easy-to-understand signing is one of the essential components to an effective automatic signed/spoken translation system. Facial nonmanual signals are essential to natural signing, but unfortunately signing avatars still do not produce acceptable facial expressions, particularly on the lower face. This paper reports on an innovative method to create more realistic lip postures. The approach manages the complexity of creating lip postures, thus making fewer demands on the artists making them. The method will be integral to our efforts to develop libraries containing lip postures to support the generation of facial expressions for several sign languages.</abstract>
      <url hash="16aef310">2022.sltat-1.10</url>
      <bibkey>mcdonald-etal-2022-novel</bibkey>
    </paper>
    <paper id="11">
      <title>A Software Toolkit for Pre-processing Sign Language Video Streams</title>
      <author><first>Fabrizio</first><last>Nunnari</last></author>
      <pages>73–78</pages>
      <abstract>We present the requirements, design guidelines, and the software architecture of an open-source toolkit dedicated to the pre-processing of sign language video material. The toolkit is a collection of functions and command-line tools designed to be integrated with build automation systems. Every pre-processing tool is dedicated to standard pre-processing operations (e.g., trimming, cropping, resizing) or feature extraction (e.g., identification of areas of interest, landmark detection) and can be used also as a standalone Python module. The UML diagrams of its architecture are presented together with a few working examples of its usage. The software is freely available with an open-source license on a public repository.</abstract>
      <url hash="ecfef6c8">2022.sltat-1.11</url>
      <bibkey>nunnari-2022-software</bibkey>
      <pwccode url="https://github.com/dfki-signlanguage/videoprocessingtools" additional="false">dfki-signlanguage/videoprocessingtools</pwccode>
    </paper>
    <paper id="12">
      <title><fixed-case>G</fixed-case>reek <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Recognition for the <fixed-case>SL</fixed-case>-<fixed-case>R</fixed-case>e<fixed-case>D</fixed-case>u Learning Platform</title>
      <author><first>Katerina</first><last>Papadimitriou</last></author>
      <author><first>Gerasimos</first><last>Potamianos</last></author>
      <author><first>Galini</first><last>Sapountzaki</last></author>
      <author><first>Theodore</first><last>Goulas</last></author>
      <author><first>Eleni</first><last>Efthimiou</last></author>
      <author><first>Stavroula-Evita</first><last>Fotinea</last></author>
      <author><first>Petros</first><last>Maragos</last></author>
      <pages>79–84</pages>
      <abstract>There has been increasing interest lately in developing education tools for sign language (SL) learning that enable self-assessment and objective evaluation of learners’ SL productions, assisting both students and their instructors. Crucially, such tools require the automatic recognition of SL videos, while operating in a signer-independent fashion and under realistic recording conditions. Here, we present an early version of a Greek Sign Language (GSL) recognizer that satisfies the above requirements, and integrate it within the SL-ReDu learning platform that constitutes a first in GSL with recognition functionality. We develop the recognition module incorporating state-of-the-art deep-learning based visual detection, feature extraction, and classification, designing it to accommodate a medium-size vocabulary of isolated signs and continuously fingerspelled letter sequences. We train the module on a specifically recorded GSL corpus of multiple signers by a web-cam in non-studio conditions, and conduct both multi-signer and signer-independent recognition experiments, reporting high accuracies. Finally, we let student users evaluate the learning platform during GSL production exercises, reporting very satisfactory objective and subjective assessments based on recognition performance and collected questionnaires, respectively.</abstract>
      <url hash="90195ded">2022.sltat-1.12</url>
      <bibkey>papadimitriou-etal-2022-greek</bibkey>
    </paper>
    <paper id="13">
      <title>Signing Avatars in a New Dimension: Challenges and Opportunities in Virtual Reality</title>
      <author><first>Lorna</first><last>Quandt</last></author>
      <author><first>Jason</first><last>Lamberton</last></author>
      <author><first>Carly</first><last>Leannah</last></author>
      <author><first>Athena</first><last>Willis</last></author>
      <author><first>Melissa</first><last>Malzkuhn</last></author>
      <pages>85–90</pages>
      <abstract>With improved and more easily accessible technology, immersive virtual reality (VR) head-mounted devices have become more ubiquitous. As signing avatar technology improves, virtual reality presents a new and relatively unexplored application for signing avatars. This paper discusses two primary ways that signed language can be represented in immersive virtual spaces: 1) Third-person, in which the VR user sees a character who communicates in signed language; and 2) First-person, in which the VR user produces signed content themselves, tracked by the head-mounted device and visible to the user herself (and/or to other users) in the virtual environment. We will discuss the unique affordances granted by virtual reality and how signing avatars might bring accessibility and new opportunities to virtual spaces. We will then discuss the limitations of signed con-tent in virtual reality concerning virtual signers shown from both third- and first-person perspectives.</abstract>
      <url hash="68e944c9">2022.sltat-1.13</url>
      <bibkey>quandt-etal-2022-signing</bibkey>
    </paper>
    <paper id="14">
      <title>Mouthing Recognition with <fixed-case>O</fixed-case>pen<fixed-case>P</fixed-case>ose in Sign Language</title>
      <author><first>Maria Del Carmen</first><last>Saenz</last></author>
      <pages>91–94</pages>
      <abstract>Many avatars focus on the hands and how they express sign language. However, sign language also uses mouth and face gestures to modify verbs, adjectives, or adverbs; these are known as non-manual components of the sign. To have a translation system that the Deaf community will accept, we need to include these non-manual signs. Just as machine learning is being used on generating hand signs, the work we are focusing on will be doing the same, but with mouthing and mouth gestures. We will be using data from The National Center for Sign Language and Gesture Resources. The data from the center are videos of native signers focusing on different areas of signer movement, gesturing, and mouthing, and are annotated specifically for mouthing studies. With this data, we will run a pre-trained Neural Network application called OpenPose. After running through OpenPose, further analysis of the data is conducted using a Random Forest Classifier. This research looks at how well an algorithm can be trained to spot certain mouthing points and output the mouth annotations with a high degree of accuracy. With this, the appropriate mouthing for animated signs can be easily applied to avatar technologies.</abstract>
      <url hash="e448a13e">2022.sltat-1.14</url>
      <bibkey>saenz-2022-mouthing</bibkey>
    </paper>
    <paper id="15">
      <title>Skeletal Graph Self-Attention: Embedding a Skeleton Inductive Bias into Sign Language Production</title>
      <author><first>Ben</first><last>Saunders</last></author>
      <author><first>Necati Cihan</first><last>Camgöz</last></author>
      <author><first>Richard</first><last>Bowden</last></author>
      <pages>95–102</pages>
      <abstract>Recent approaches to Sign Language Production (SLP) have adopted spoken language Neural Machine Translation (NMT) architectures, applied without sign-specific modifications. In addition, these works represent sign language as a sequence of skeleton pose vectors, projected to an abstract representation with no inherent skeletal structure. In this paper, we represent sign language sequences as a skeletal graph structure, with joints as nodes and both spatial and temporal connections as edges. To operate on this graphical structure, we propose Skeletal Graph Self-Attention (SGSA), a novel graphical attention layer that embeds a skeleton inductive bias into the SLP model. Retaining the skeletal feature representation throughout, we directly apply a spatio-temporal adjacency matrix into the self-attention formulation. This provides structure and context to each skeletal joint that is not possible when using a non-graphical abstract representation, enabling fluid and expressive sign language production. We evaluate our Skeletal Graph Self-Attention architecture on the challenging RWTH-PHOENIX-Weather-2014T (PHOENIX14T) dataset, achieving state-of-the-art back translation performance with an 8% and 7% improvement over competing methods for the dev and test sets.</abstract>
      <url hash="c99c065b">2022.sltat-1.15</url>
      <bibkey>saunders-etal-2022-skeletal</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/phoenix14t">PHOENIX14T</pwcdataset>
      <pwcdataset url="https://paperswithcode.com/dataset/rwth-phoenix-weather-2014-t">RWTH-PHOENIX-Weather 2014 T</pwcdataset>
    </paper>
    <paper id="16">
      <title>Multi-track Bottom-Up Synthesis from Non-Flattened <fixed-case>AZ</fixed-case>ee Scores</title>
      <author><first>Paritosh</first><last>Sharma</last></author>
      <author><first>Michael</first><last>Filhol</last></author>
      <pages>103–108</pages>
      <abstract>We present an algorithm to improve the pre-existing bottom-up animation system for AZee descriptions to synthesize sign language utterances. Our algorithm allows us to synthesize AZee descriptions by preserving the dynamics of underlying blocks. This bottom-up approach aims to deliver procedurally generated animations capable of generating any sign language utterance if an equivalent AZee description exists. The proposed algorithm is built upon the modules of an open-source animation toolkit and takes advantage of the integrated inverse kinematics solver and a non-linear editor.</abstract>
      <url hash="ab83dcc0">2022.sltat-1.16</url>
      <bibkey>sharma-filhol-2022-multi</bibkey>
    </paper>
    <paper id="17">
      <title>First Steps Towards a Signing Avatar for Railway Travel Announcements in the <fixed-case>N</fixed-case>etherlands</title>
      <author><first>Britt</first><last>Van Gemert</last></author>
      <author><first>Richard</first><last>Cokart</last></author>
      <author><first>Lyke</first><last>Esselink</last></author>
      <author><first>Maartje</first><last>De Meulder</last></author>
      <author><first>Nienke</first><last>Sijm</last></author>
      <author><first>Floris</first><last>Roelofsen</last></author>
      <pages>109–116</pages>
      <abstract>This paper presents first steps towards a sign language avatar for communicating railway travel announcements in Dutch Sign Language. Taking an interdisciplinary approach, it demonstrates effective ways to employ co-design and focus group methods in the context of developing sign language technology, and presents several concrete findings and results obtained through co-design and focus group sessions which have not only led to improvements of our own prototype but may also inform the development of signing avatars for other languages and in other application domains.</abstract>
      <url hash="692650c7">2022.sltat-1.17</url>
      <bibkey>van-gemert-etal-2022-first</bibkey>
    </paper>
    <paper id="18">
      <title>Changing the Representation: Examining Language Representation for Neural Sign Language Production</title>
      <author><first>Harry</first><last>Walsh</last></author>
      <author><first>Ben</first><last>Saunders</last></author>
      <author><first>Richard</first><last>Bowden</last></author>
      <pages>117–124</pages>
      <abstract>Neural Sign Language Production (SLP) aims to automatically translate from spoken language sentences to sign language videos. Historically the SLP task has been broken into two steps; Firstly, translating from a spoken language sentence to a gloss sequence and secondly, producing a sign language video given a sequence of glosses. In this paper we apply Natural Language Processing techniques to the first step of the SLP pipeline. We use language models such as BERT and Word2Vec to create better sentence level embeddings, and apply several tokenization techniques, demonstrating how these improve performance on the low resource translation task of Text to Gloss. We introduce Text to HamNoSys (T2H) translation, and show the advantages of using a phonetic representation for sign language translation rather than a sign level gloss representation. Furthermore, we use HamNoSys to extract the hand shape of a sign and use this as additional supervision during training, further increasing the performance on T2H. Assembling best practise, we achieve a BLEU-4 score of 26.99 on the MineDGS dataset and 25.09 on PHOENIX14T, two new state-of-the-art baselines.</abstract>
      <url hash="498919df">2022.sltat-1.18</url>
      <bibkey>walsh-etal-2022-changing</bibkey>
    </paper>
    <paper id="19">
      <title>Supporting Mouthing in Signed Languages: New innovations and a proposal for future corpus building</title>
      <author><first>Rosalee</first><last>Wolfe</last></author>
      <author><first>John</first><last>McDonald</last></author>
      <author><first>Ronan</first><last>Johnson</last></author>
      <author><first>Ben</first><last>Sturr</last></author>
      <author><first>Syd</first><last>Klinghoffer</last></author>
      <author><first>Anthony</first><last>Bonzani</last></author>
      <author><first>Andrew</first><last>Alexander</last></author>
      <author><first>Nicole</first><last>Barnekow</last></author>
      <pages>125–130</pages>
      <abstract>A recurring concern, oft repeated, regarding the quality of signing avatars is the lack of proper facial movements, particularly in actions that involve mouthing. An analysis uncovered three challenges contributing to the problem. The first is a difficulty in devising an algorithmic strategy for generating mouthing due to the rich variety of mouthings in sign language. For example, part or all of a spoken word may be mouthed depending on the sign language, the syllabic structure of the mouthed word, as well as the register of address and discourse setting. The second challenge was technological. Previous efforts to create avatar mouthing have failed to model the timing present in mouthing or have failed to properly model the mouth’s appearance. The third challenge is one of usability. Previous editing systems, when they existed, were time-consuming to use. This paper describes efforts to improve avatar mouthing by addressing these challenges, resulting in a new approach for mouthing animation. The paper concludes by proposing an experiment in corpus building using the new approach.</abstract>
      <url hash="e6fdac7b">2022.sltat-1.19</url>
      <bibkey>wolfe-etal-2022-supporting</bibkey>
    </paper>
  </volume>
</collection>
