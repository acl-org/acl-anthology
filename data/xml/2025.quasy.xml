<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.quasy">
  <volume id="1" ingest-date="2025-07-27" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Quantitative Syntax (QUASY, SyntaxFest 2025)</booktitle>
      <editor><first>Xinying</first><last>Chen</last></editor>
      <editor><first>Yaqin</first><last>Wang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Ljubljana, Slovenia</address>
      <month>August</month>
      <year>2025</year>
      <url hash="09f75c1a">2025.quasy-1</url>
      <venue>quasy</venue>
      <venue>ws</venue>
      <venue>syntaxfest</venue>
      <isbn>979-8-89176-293-0</isbn>
    </meta>
    <frontmatter>
      <url hash="b21bd1bd">2025.quasy-1.0</url>
      <bibkey>quasy-ws-syntaxfest-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Subject-Verb Agreement Alternations in <fixed-case>S</fixed-case>panish Pseudopartitive Constructions: A Corpus Study</title>
      <author><first>Marina</first><last>Cerebrinsky</last><affiliation>Bar-Ilan University</affiliation></author>
      <pages>1-8</pages>
      <abstract>Pseudopartitive constructions, following the format N1-of-N2 (such as a group of students), are known to feature alternations in their subject-verb agreement patterns, either with the N1 or the N2. Through a corpus analysis, this study investigates the possibility of a correlation between the choice of N1/N2 as an agreement trigger and the semantic type of the N1, as well as the animacy status of the N2. Although a positive correlation was found for N1 semantic type, no statistically significant results emerged for N2 animacy.</abstract>
      <url hash="31d71f99">2025.quasy-1.1</url>
      <bibkey>cerebrinsky-2025-subject</bibkey>
    </paper>
    <paper id="2">
      <title>Degree centrality as a measure of robustness of dependency structures of the sentences in a large-scale learner corpus of <fixed-case>E</fixed-case>nglish</title>
      <author><first>Masanori</first><last>Oya</last><affiliation>Meiji University</affiliation></author>
      <pages>9-16</pages>
      <abstract>This paper examines the differences in the robustness of syntactic dependency structures in written English produced by learners of varying proficiency levels and by native English speakers. The robustness of these dependency structures is represented by their degree centralities, and corpus-based investigation revealed that learners with higher proficiency levels tend to produce sentences with lower degree centralities. This means that they produce more robust, and more embedded sentences. It is also revealed that the sentences produced by native speakers of English tend to produce more embedded sentences than non-native speakers.</abstract>
      <url hash="7908414e">2025.quasy-1.2</url>
      <bibkey>oya-2025-degree</bibkey>
    </paper>
    <paper id="4">
      <title>Application of Existing Readability Methods to the <fixed-case>U</fixed-case>krainian Language: A Comprehensive Study</title>
      <author><first>Serhii D.</first><last>Prykhodchenko</last><affiliation>Dnipro University of Technology</affiliation></author>
      <author><first>Oksana Yu.</first><last>Prykhodchenko</last><affiliation>Dnipro University of Technology</affiliation></author>
      <pages>17-25</pages>
      <abstract>The Ukrainian language currently lacks a well-developed framework for assessing text readability. This study addresses this gap by focusing on three key contributions. First, we present the creation of UkrTB, a Ukrainian-language corpus of texts categorized by reader age. Second, we conduct a statistical analysis of the corpus, evaluating key linguistic features such as sentence length, word complexity, and part-of-speech distribution. Third, we systematically assess the applicability of existing readability formulas, including Flesch, Flesch-Kincaid, Matskovskii, Pisarek, and Solnyshkina et al., to Ukrainian texts. Our findings indicate that readability models developed for English and other Slavic languages exhibit significant limitations when applied to Ukrainian. While some methods demonstrate partial correlation with expected readability levels, others produce inconsistent results, underscoring the need for a specialized readability metric tailored to Ukrainian. This work lays the foundation for further research in Ukrainian readability assessment and the development of language-specific models</abstract>
      <url hash="6dc51ac8">2025.quasy-1.4</url>
      <bibkey>prykhodchenko-prykhodchenko-2025-application</bibkey>
    </paper>
    <paper id="5">
      <title>Extraction of Contrastive Rules from Syntactic Treebanks: A Case Study in <fixed-case>R</fixed-case>omance Languages</title>
      <author><first>Santiago</first><last>Herrera</last><affiliation>University of Paris Nanterre</affiliation></author>
      <author><first>Ioana-Madalina</first><last>Silai</last><affiliation>Université Paris Ouest Paris X Nanterre</affiliation></author>
      <author><first>Bruno</first><last>Guillaume</last><affiliation>INRIA</affiliation></author>
      <author><first>Sylvain</first><last>Kahane</last><affiliation>Université Paris Nanterre</affiliation></author>
      <pages>26-38</pages>
      <abstract>In this paper, we develop a data-driven contrastive framework to extract common and distinctive linguistic descriptions from syntactic treebanks. The extracted contrastive rules are defined by a statistically significant difference in precision and classified as common and distinctive rules across the set of treebanks. We illustrate our method by working on object word order using Universal Dependencies (UD) treebanks in 6 Romance languages: Brazilian Portuguese, Catalan, French, Italian, Romanian and Spanish. We discuss the limitations faced due to inconsistent annotation and the feasibility of conducting contrasting studies using the UD collection.</abstract>
      <url hash="374f331f">2025.quasy-1.5</url>
      <bibkey>herrera-etal-2025-extraction</bibkey>
    </paper>
    <paper id="6">
      <title>A Quantitative Study of Syntactic Complexity across Genres: Dependency Distance in <fixed-case>E</fixed-case>nglish and <fixed-case>C</fixed-case>hinese</title>
      <author><first>Yaqin</first><last>Wang</last><affiliation>Guangdong University of Foreign Studies</affiliation></author>
      <pages>39-46</pages>
      <abstract>This study investigates syntactic complexity in fiction and news genres by analyzing mean dependency distances (MDD) across controlled sentence lengths in English and Chinese corpora. Results show that English fiction exhibits greater MDD than news, while Chinese fiction shows the reverse. More complex syntactic structures, i.e., complex coordination structures, are found in English fiction texts than in news writing. In contrast, Chinese news writing relies more on nominal modification and prepositional phrases that create long-distance dependencies than fiction texts. These findings show deviations from uniform correlations between genre formality and syntactic complexity across languages.</abstract>
      <url hash="9cb1fe98">2025.quasy-1.6</url>
      <bibkey>wang-2025-quantitative</bibkey>
    </paper>
    <paper id="7">
      <title>Syntactic Complexity in <fixed-case>L</fixed-case>2 Reading: A Comparison of Adapted and Original <fixed-case>C</fixed-case>zech Texts</title>
      <author><first>Žaneta</first><last>Stiborská</last><affiliation>University of Ostrava</affiliation></author>
      <author><first>Michaela</first><last>Nogolová</last><affiliation>University of Ostrava</affiliation></author>
      <author><first>Xinying</first><last>Chen</last><affiliation>University of Ostrava</affiliation></author>
      <author><first>Miroslav</first><last>Kubát</last><affiliation>University of Ostrava</affiliation></author>
      <pages>47-55</pages>
      <abstract>This corpus-based study explores the syntactic complexity of adapted Czech texts designed for learners of Czech as a second language (L2). It investigates how syntactic complexity varies according to learner proficiency levels (A2, B1, B2) as defined by the Common European Framework of Reference for Languages (CEFR) and how these adapted texts differ from their original versions. Quantitative analyses using metrics such as average sentence length (ASL), average clause length (ACL), mean dependency distance (MDD), and mean hierarchical distance (MHD) demonstrate clear systematic simplifications in adapted texts at lower proficiency levels. At A2 and B1 levels, adapted texts were found to be significantly less syntactically complex compared to their original counterparts. However, these differences diminished notably at the B2 proficiency level, indicating a gradual alignment of adapted texts with native-level syntactic complexity as learner proficiency increased. These results underscore the importance of careful syntactic calibration in creating educational materials for language learners, highlighting implications for curriculum design, instructional methodologies, and materials development. The findings offer valuable insights for language educators and textbook authors aiming to optimize reading materials to support language acquisition effectively</abstract>
      <url hash="fa500f01">2025.quasy-1.7</url>
      <bibkey>stiborska-etal-2025-syntactic</bibkey>
    </paper>
    <paper id="8">
      <title>Modeling the Law of Abbreviation in Classical, Modern, and <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case>-Generated <fixed-case>C</fixed-case>hinese: A Power-Law Analysis of Structural Economy</title>
      <author><first>Jianwei</first><last>Yan</last><affiliation>Zhejiang University</affiliation></author>
      <author><first>Heng</first><last>Chen</last><affiliation>Guangdong University of Foreign Studies</affiliation></author>
      <pages>56-62</pages>
      <abstract>This study investigates the Law of Abbreviation—the inverse relationship between word length and frequency—across Classical, Modern, and ChatGPT-generated Chinese. Using a tri-partite parallel corpus and a power-law model y = a*x^(-b), we analyze the relationship between word length and the average usage frequency of words within a given word length category to assess structural economy. Results confirm consistent Zipfian distribution across all text types, with high R2 values indicating strong model fit. However, the parameter b varies significantly: Classical Chinese shows the steepest decline, suggesting strong pressure for brevity; Modern Chinese exhibits a moderated pattern; ChatGPT-generated texts display the weakest pressure, prioritizing fluency over compression. These differences reflect evolving communicative priorities and reveal that while AI models can mimic statistical distributions, they underrepresent deeper structural pressures found in natural language evolution. This study offers new insights into lexical optimization and the parameter b offers a useful metric for comparing structural efficiency across modalities. Implications are discussed in relation to language modeling, cognitive economy, and the evolution of linguistic structure.</abstract>
      <url hash="8801e062">2025.quasy-1.8</url>
      <bibkey>yan-chen-2025-modeling</bibkey>
    </paper>
    <paper id="9">
      <title>A Computational Method for Analyzing Syntactic Profiles: The Case of the <fixed-case>ELEXIS</fixed-case>-<fixed-case>WSD</fixed-case> Parallel Sense-Annotated Corpus</title>
      <author><first>Jaka</first><last>Čibej</last><affiliation>University of Ljubljana</affiliation></author>
      <pages>63-71</pages>
      <abstract>In the paper, we present an approach to comparing corpora annotated with dependency relations. The method relies on the compilation of syntactic profiles – numeric vectors representing the relative frequencies of different syntactic (sub)trees extracted automatically with the STARK 3.0 open-access dependency tree extraction tool. We perform the extraction on the ELEXIS-WSD Parallel Sense-Annotated Corpus, which has recently been published as version 1.2 with UD dependency relation annotations for 10 European languages. The corpus provides an additional resource for contrastive studies in quantitative syntax. In addition to presenting the corpus and conducting some proof-of-concept analyses, we discuss several other potential uses and improvements to the proposed approach.</abstract>
      <url hash="073f5778">2025.quasy-1.9</url>
      <bibkey>cibej-2025-computational</bibkey>
    </paper>
    <paper id="10">
      <title>The Interplay of Noun Phrase Complexity and Modification Type in Scientific Writing</title>
      <author><first>Isabell</first><last>Landwehr</last><affiliation>Universität des Saarlandes</affiliation></author>
      <pages>72-82</pages>
      <abstract>We investigate the interplay of noun phrase (NP) complexity and modification type, namely the choice between pre- and postmodification, using a corpus-based approach. Our dataset is the Royal Society Corpus (RSC, Fischer et al. 2020), a diachronic corpus of English scientific writing. We find that the number of dependents, length of the head noun and distance to the head noun’s own syntactic head (typically the main verb) affect the likelihood of pre- vs. postmodification: NPs with more dependents are more likely to be premodified, NPs with a longer head noun and a head noun closer to its own head are more likely to be postmodified. In addition, we find an effect of syntactic role and definiteness as well as time: The likelihood of premodification over postmodification increases with time and subject NPs as well as indefinite NPs are more likely to be premodified than NPs in other syntactic roles or definite NPs.</abstract>
      <url hash="4b9fa0f1">2025.quasy-1.10</url>
      <bibkey>landwehr-2025-interplay</bibkey>
    </paper>
    <paper id="11">
      <title>Predictability Effects of <fixed-case>S</fixed-case>panish-<fixed-case>E</fixed-case>nglish Code-Switching: A Directionality and Part of Speech Analysis</title>
      <author><first>Josh</first><last>Higdon</last><affiliation>University of Florida</affiliation></author>
      <author><first>Valeria</first><last>Pagliai</last><affiliation>University of Florida</affiliation></author>
      <author><first>Zoey</first><last>Liu</last><affiliation>University of Florida</affiliation></author>
      <pages>83-89</pages>
      <abstract>Research on code-switching (CS), the spontaneous alternation between two or more languages within a discourse, remains relatively new and often limited by the use of elicited production tasks, with some exceptions leveraging naturalistic corpora. This study analyses the effects of language directionality and part-of-speech (POS) tags on Spanish-English CS production between corpus modalities and speech communities. We use data from two spoken corpora: Miami Bangor Corpus (MBC; N = 261,711) and Spanish in Texas Corpus (STC; N = 416,784), as well as the written LinCE Corpus (N=278,093). Bootstrap analyses indicate that Spanish serves as the matrix language (i.e., the most used) for MBC and LinCE, while English is for STC. Logistic regression analyses show that the particle-coordinating conjunction combination was the strongest POS predictor of a CS. The results suggest that corpus modality and the speech community affect matrix language proportions and that both previously attested and unseen POS combinations modulate the production of Spanish-English CS.</abstract>
      <url hash="1192eb7c">2025.quasy-1.11</url>
      <bibkey>higdon-etal-2025-predictability</bibkey>
    </paper>
    <paper id="12">
      <title>On the Flatness, Non-linearity, and Branching Direction of Natural Language and Random Constituency Trees: Analyzing Structural Variation within and across Languages</title>
      <author><first>Taiga</first><last>Ishii</last><affiliation>The University of Tokyo, Tokyo Institute of Technology</affiliation></author>
      <author><first>Yusuke</first><last>Miyao</last><affiliation>The University of Tokyo</affiliation></author>
      <pages>90-104</pages>
      <abstract>Natural languages exhibit remarkable diversity in their syntactic structures. Previous research has investigated the cross-lingual differences in local structural features such as word order or dependency relations. However, considering structural variation within individual language, it remains unclear how such features influence the variation in the overall constituency tree structure and hence the structural variation across languages. To this end, we focus on the shape of constituency trees, analyzing the cross-lingual overlap in the distributions of flatness, non-linearity, and branching direction. While acknowledging that the findings may be influenced by the potential annotation idiosyncrasies across treebanks, the experiments quantitatively suggest that flatness and branching direction vary significantly across languages. As for non-linearity, the cross-lingual difference was relatively small, and the distributions tend to skew towards linear structures. Furthermore, comparison with randomly generated trees suggests that while phrase category and frequency information is crucial for reproducing the branching direction found in natural languages, non-linearity can be replicated reasonably well even without such information.</abstract>
      <url hash="b1ec46fb">2025.quasy-1.12</url>
      <bibkey>ishii-miyao-2025-flatness</bibkey>
    </paper>
    <paper id="13">
      <title>First Insights into the Syntax of <fixed-case>S</fixed-case>lovene Student Writing: A Statistical Analysis of Šolar 3.0 vs. Učbeniki 1.0</title>
      <author><first>Tina</first><last>Munda</last><affiliation>University of Primorska</affiliation></author>
      <author><first>Špela</first><last>Arhar Holdt</last><affiliation>University of Ljubljana</affiliation></author>
      <pages>105-114</pages>
      <abstract>This study investigates the syntactic features of Slovene student writing by comparing essays from the Šolar 3.0 corpus (ages 13–19; primary and secondary school levels) with textbook texts from the Učbeniki 1.0 corpus aligned to the same educational stages. We apply quantitative syntactic analysis at two complementary levels: clause-type frequency (coordination, parataxis, and four types of subordination) and tree-based syntactic complexity measures (number of clauses, clauses per T-unit, and maximum parse-tree depth). Results show that students heavily rely on coordination and specific subordinate clauses (especially object and adverbial), producing more clauses per sentence and per T-unit than textbooks. However, their sentences tend to exhibit flatter syntactic structures, with shallower embedding in primary school and only modest increases in tree depth by secondary school. These findings reveal a divergence between surface-level complexity and hierarchical depth, highlighting developmental trends and instructional targets in written syntactic maturity. We conclude by discussing implications for syntactic development and directions for future research.</abstract>
      <url hash="24ce209c">2025.quasy-1.13</url>
      <bibkey>munda-arhar-holdt-2025-first</bibkey>
    </paper>
    <paper id="14">
      <title>Syntactic units and their length distributions: A case study in <fixed-case>C</fixed-case>zech</title>
      <author><first>Michaela</first><last>Nogolová</last><affiliation>University of Ostrava</affiliation></author>
      <author><first>Michaela</first><last>Koščová</last><affiliation>Mathematical Institute, Slovak Academy of Sciences</affiliation></author>
      <author><first>Jan</first><last>Macutek</last><affiliation>Mathematical Institute, Slovak Academy of Sciences</affiliation></author>
      <author><first>Radek</first><last>Cech</last><affiliation>Masaryk University</affiliation></author>
      <pages>115-123</pages>
      <abstract>This study investigates the length distributions of syntactic units in Czech across multiple hierarchical levels: sentences, independent clauses, clauses, phrases, subphrases, and chunks. Using a diverse dataset – including Universal Dependency treebanks, presidential speeches, the Czech Bible, and random sample from corpora of modern Czech – the analysis examines whether lengths of these syntactic units follow consistent distributional patterns. Length is defined as the number of immediate subunits, and the distributions were modeled using the hyper-Poisson distribution. The results demonstrate that the hyper-Poisson model fits well distributions of length of all abovementioned syntactic units, pointing to a common principle underlying the organization of syntactic structure in Czech.</abstract>
      <url hash="4cb0a55d">2025.quasy-1.14</url>
      <bibkey>nogolova-etal-2025-syntactic</bibkey>
    </paper>
    <paper id="15">
      <title>Do Multilingual Transformers Encode <fixed-case>P</fixed-case>aninian Grammatical Relations? A Layer-wise Probing Study</title>
      <author><first>Akshit</first><last>Kumar</last><affiliation>International Institute of Information Technology, Hyderabad, International Institute of Information Technology Hyderabad</affiliation></author>
      <author id="dipti-misra-sharma"><first>Dipti</first><last>Sharma</last><affiliation>IIIT Hyderabad</affiliation></author>
      <author><first>Parameswari</first><last>Krishnamurthy</last><affiliation>International Institute of Information Technology Hyderabad, Dhirubhai Ambani Institute Of Information and Communication Technology</affiliation></author>
      <pages>124-130</pages>
      <abstract>Large multilingual transformers such as XLM-RoBERTa achieve impressive performance on diverse NLP benchmarks, but understanding how they internally encode grammatical information remains challenging. This study investigates the encoding of syntactic and morphological information derived from the Paninian grammatical framework—specifically designed for morphologically rich Indian languages—across model layers. Using diagnostic probing, we analyze the hidden representations of frozen XLM-RoBERTa-base, mBERT, and IndicBERT models across seven Indian languages (Hindi, Kannada, Malayalam, Marathi, Telugu, Urdu, Bengali). Probes are trained to predict Paninian dependency relations (by edge probing) and essential morphosyntactic features (UPOS tags, Vibhakti markers). We find that syntactic structure (dependencies) is primarily encoded in the middle-to-upper-middle layers (layers 6–9), while lexical features peak slightly earlier. Although the general layer-wise trends are shared across models, significant variations in absolute probing performance reflect differences in model capacity, pre-training data, and language-specific characteristics. These findings shed light on how theory-specific grammatical information emerges implicitly within multilingual transformer representations trained largely on unstructured raw text.</abstract>
      <url hash="52533399">2025.quasy-1.15</url>
      <bibkey>kumar-etal-2025-multilingual</bibkey>
    </paper>
  </volume>
</collection>
