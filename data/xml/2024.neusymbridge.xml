<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.neusymbridge">
  <volume id="1" ingest-date="2024-05-18" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Workshop: Bridging Neurons and Symbols for Natural Language Processing and Knowledge Graphs Reasoning (NeusymBridge) @ LREC-COLING-2024</booktitle>
      <editor><first>Tiansi</first><last>Dong</last></editor>
      <editor><first>Erhard</first><last>Hinrichs</last></editor>
      <editor><first>Zhen</first><last>Han</last></editor>
      <editor><first>Kang</first><last>Liu</last></editor>
      <editor><first>Yangqiu</first><last>Song</last></editor>
      <editor><first>Yixin</first><last>Cao</last></editor>
      <editor><first>Christian F.</first><last>Hempelmann</last></editor>
      <editor><first>Rafet</first><last>Sifa</last></editor>
      <publisher>ELRA and ICCL</publisher>
      <address>Torino, Italia</address>
      <month>May</month>
      <year>2024</year>
      <url hash="9b1db1c2">2024.neusymbridge-1</url>
      <venue>neusymbridge</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="8e807509">2024.neusymbridge-1.0</url>
      <bibkey>neusymbridge-ws-2024-bridging</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Probing Large Language Models from a Human Behavioral Perspective</title>
      <author><first>Xintong</first><last>Wang</last></author>
      <author><first>Xiaoyu</first><last>Li</last></author>
      <author><first>Xingshan</first><last>Li</last></author>
      <author><first>Chris</first><last>Biemann</last></author>
      <pages>1–7</pages>
      <abstract>Large Language Models (LLMs) have emerged as dominant foundational models in modern NLP. However, the understanding of their prediction processes and internal mechanisms, such as feed-forward networks (FFN) and multi-head self-attention (MHSA), remains largely unexplored. In this work, we probe LLMs from a human behavioral perspective, correlating values from LLMs with eye-tracking measures, which are widely recognized as meaningful indicators of human reading patterns. Our findings reveal that LLMs exhibit a similar prediction pattern with humans but distinct from that of Shallow Language Models (SLMs). Moreover, with the escalation of LLM layers from the middle layers, the correlation coefficients also increase in FFN and MHSA, indicating that the logits within FFN increasingly encapsulate word semantics suitable for predicting tokens from the vocabulary.</abstract>
      <url hash="def54e50">2024.neusymbridge-1.1</url>
      <bibkey>wang-etal-2024-probing</bibkey>
    </paper>
    <paper id="2">
      <title>The Semantic Relations in <fixed-case>LLM</fixed-case>s: An Information-theoretic Compression Approach</title>
      <author><first>Yu-Hsiang</first><last>Tseng</last></author>
      <author><first>Pin-Er</first><last>Chen</last></author>
      <author><first>Da-Chen</first><last>Lian</last></author>
      <author><first>Shu-Kai</first><last>Hsieh</last></author>
      <pages>8–21</pages>
      <abstract>Compressibility is closely related to the predictability of the texts from the information theory viewpoint. As large language models (LLMs) are trained to maximize the conditional probabilities of upcoming words, they may capture the subtlety and nuances of the semantic constraints underlying the texts, and texts aligning with the encoded semantic constraints are more compressible than those that do not. This paper systematically tests whether and how LLMs can act as compressors of semantic pairs. Using semantic relations from English and Chinese Wordnet, we empirically demonstrate that texts with correct semantic pairings are more compressible than incorrect ones, measured by the proposed compression advantages index. We also show that, with the Pythia model suite and a fine-tuned model on Chinese Wordnet, compression capacities are modulated by the model’s seen data. These findings are consistent with the view that LLMs encode the semantic knowledge as underlying constraints learned from texts and can act as compressors of semantic information or potentially other structured knowledge.</abstract>
      <url hash="e0e5af37">2024.neusymbridge-1.2</url>
      <bibkey>tseng-etal-2024-semantic</bibkey>
    </paper>
    <paper id="3">
      <title>Word Sense Disambiguation as a Game of Neurosymbolic Darts</title>
      <author><first>Tiansi</first><last>Dong</last></author>
      <author><first>Rafet</first><last>Sifa</last></author>
      <pages>22–32</pages>
      <abstract>Word Sense Disambiguation (WSD) is one of the hardest tasks in natural language understanding and knowledge engineering. The glass ceiling of the 80% F1 score is recently achieved through supervised learning, enriched by knowledge graphs. Here, we propose a novel neurosymbolic methodology that may push the F1 score above 90%. The core of our methodology is a neurosymbolic sense embedding, in terms of a configuration of nested n-dimensional balls. The central point of a ball well preserves pre-trained word embeddings learned from data, which partially fixes the locations of balls. Inclusion relations among balls precisely encode symbolic hypernym relations among senses, and enable simple logic deduction among sense embeddings. We trained a Transformer to learn the mapping from a contextualized word embedding to its sense ball embedding, just like playing the game of darts (a game of shooting darts into a dartboard). A series of experiments are carried out using pre-training n ball embeddings, which cover around 70% training data and 75% testing data in the benchmark WSD corpus. Euclidean distance and cosine similarity functions are used as objective functions, separately, and each reaches &gt;95.0% F1 score in the ALL-n-ball dataset. This substantially breaks the glass ceiling of deep learning methods. Future work is discussed to develop a full-fledged neurosymbolic WSD system that substantially outperforms deep learning approaches.</abstract>
      <url hash="027d22d7">2024.neusymbridge-1.3</url>
      <bibkey>dong-sifa-2024-word</bibkey>
    </paper>
    <paper id="4">
      <title>Open Event Causality Extraction by the Assistance of <fixed-case>LLM</fixed-case> in Task Annotation, Dataset, and Method</title>
      <author><first>Kun</first><last>Luo</last></author>
      <author><first>Tong</first><last>Zhou</last></author>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <pages>33–44</pages>
      <abstract>Event Causality Extraction (ECE) aims to extract explicit causal relations between event pairs from the text. However, the event boundary deviation and the causal event pair mismatching are two crucial challenges that remain unaddressed. To address the above issues, we propose a paradigm to utilize LLM to optimize the task definition, evolve the datasets, and strengthen our proposed customized Contextual Highlighting Event Causality Extraction framework (CHECE). Specifically in CHECE, we propose an Event Highlighter and an Event Concretization Module, guiding the model to represent the event by a higher-level cluster and consider its causal counterpart in event boundary prediction to deal with event boundary deviation. And we propose a Contextual Event Causality Matching mechanism, meanwhile, applying LLM to diversify the content templates to force the model to learn causality from context to targeting on causal event pair mismatching. Experimental results on two ECE datasets demonstrate the effectiveness of our method.</abstract>
      <url hash="bcdb7585">2024.neusymbridge-1.4</url>
      <bibkey>luo-etal-2024-open</bibkey>
    </paper>
    <paper id="5">
      <title>The Need for Grounding in <fixed-case>LLM</fixed-case>-based Dialogue Systems</title>
      <author><first>Kristiina</first><last>Jokinen</last></author>
      <pages>45–52</pages>
      <abstract>Grounding is a pertinent part of the design of LLM-based dialogue systems. Although research on grounding has a long tradition, the paradigm shift caused by LLMs has brought the concept onto the foreground, in particular in the context of cognitive robotics. To avoid generation of irrelevant or false information, the system needs to ground its utterances into real-world events, and to avoid the statistical parrot effect, the system needs to construct shared understanding of the dialogue context and of the partner’s intents. Grounding and construction of the shared context enables cooperation between the participants, and thus supports trustworthy interaction. This paper discusses grounding using neural LLM technology. It aims to bridge neural and symbolic computing on the cognitive architecture level, so as to contribute to a better understanding of how conversational reasoning and collaboration can be linked to LLM implementations to support trustworthy and flexible interaction.</abstract>
      <url hash="6ac82679">2024.neusymbridge-1.5</url>
      <bibkey>jokinen-2024-need</bibkey>
    </paper>
  </volume>
</collection>
