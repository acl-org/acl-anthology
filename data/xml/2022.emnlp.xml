<?xml version='1.0' encoding='UTF-8'?>
<collection id="2022.emnlp">
  <volume id="main" ingest-date="2022-12-08">
    <meta>
      <booktitle>Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</booktitle>
      <editor><first>Yoav</first><last>Goldberg</last></editor>
      <editor><first>Zornitsa</first><last>Kozareva</last></editor>
      <editor><first>Yue</first><last>Zhang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Abu Dhabi, United Arab Emirates</address>
      <month>December</month>
      <year>2022</year>
      <venue>emnlp</venue>
    </meta>
    <paper id="1">
      <title>Generative Knowledge Graph Construction: A Review</title>
      <author><first>Hongbin</first><last>Ye</last></author>
      <author><first>Ningyu</first><last>Zhang</last></author>
      <author><first>Hui</first><last>Chen</last></author>
      <author><first>Huajun</first><last>Chen</last></author>
      <pages>1-17</pages>
      <abstract>Generative Knowledge Graph Construction (KGC) refers to those methods that leverage the sequence-to-sequence framework for building knowledge graphs, which is flexible and can be adapted to widespread tasks. In this study, we summarize the recent compelling progress in generative knowledge graph construction. We present the advantages and weaknesses of each paradigm in terms of different generation targets and provide theoretical insight and empirical analysis. Based on the review, we suggest promising research directions for the future. Our contributions are threefold: (1) We present a detailed, complete taxonomy for the generative KGC methods; (2) We provide a theoretical and empirical analysis of the generative KGC methods; (3) We propose several research directions that can be developed in the future.</abstract>
      <url hash="8863aea4">2022.emnlp-main.1</url>
      <bibkey>ye-etal-2022-generative</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>CDC</fixed-case>onv: A Benchmark for Contradiction Detection in <fixed-case>C</fixed-case>hinese Conversations</title>
      <author><first>Chujie</first><last>Zheng</last></author>
      <author><first>Jinfeng</first><last>Zhou</last></author>
      <author><first>Yinhe</first><last>Zheng</last></author>
      <author><first>Libiao</first><last>Peng</last></author>
      <author><first>Zhen</first><last>Guo</last></author>
      <author><first>Wenquan</first><last>Wu</last></author>
      <author><first>Zheng-Yu</first><last>Niu</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>18-29</pages>
      <abstract>Dialogue contradiction is a critical issue in open-domain dialogue systems. The contextualization nature of conversations makes dialogue contradiction detection rather challenging. In this work, we propose a benchmark for Contradiction Detection in Chinese Conversations, namely CDConv. It contains 12K multi-turn conversations annotated with three typical contradiction categories: Intra-sentence Contradiction, Role Confusion, and History Contradiction. To efficiently construct the CDConv conversations, we devise a series of methods for automatic conversation generation, which simulate common user behaviors that trigger chatbots to make contradictions. We conduct careful manual quality screening of the constructed conversations and show that state-of-the-art Chinese chatbots can be easily goaded into making contradictions. Experiments on CDConv show that properly modeling contextual information is critical for dialogue contradiction detection, but there are still unresolved challenges that require future research.</abstract>
      <url hash="931916e9">2022.emnlp-main.2</url>
      <bibkey>zheng-etal-2022-cdconv</bibkey>
    </paper>
    <paper id="3">
      <title>Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space</title>
      <author><first>Mor</first><last>Geva</last></author>
      <author><first>Avi</first><last>Caciularu</last></author>
      <author><first>Kevin</first><last>Wang</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <pages>30-45</pages>
      <abstract>Transformer-based language models (LMs) are at the core of modern NLP, but their internal prediction construction process is opaque and largely not understood. In this work, we make a substantial step towards unveiling this underlying prediction process, by reverse-engineering the operation of the feed-forward network (FFN) layers, one of the building blocks of transformer models. We view the token representation as a changing distribution over the vocabulary, and the output from each FFN layer as an additive update to that distribution. Then, we analyze the FFN updates in the vocabulary space, showing that each update can be decomposed to sub-updates corresponding to single FFN parameter vectors, each promoting concepts that are often human-interpretable. We then leverage these findings for controlling LM predictions, where we reduce the toxicity of GPT2 by almost 50%, and for improving computation efficiency with a simple early exit rule, saving 20% of computation on average.</abstract>
      <url hash="2153fe65">2022.emnlp-main.3</url>
      <bibkey>geva-etal-2022-transformer</bibkey>
    </paper>
    <paper id="4">
      <title>Learning to Generate Question by Asking Question: A Primal-Dual Approach with Uncommon Word Generation</title>
      <author><first>Qifan</first><last>Wang</last></author>
      <author><first>Li</first><last>Yang</last></author>
      <author><first>Xiaojun</first><last>Quan</last></author>
      <author><first>Fuli</first><last>Feng</last></author>
      <author><first>Dongfang</first><last>Liu</last></author>
      <author><first>Zenglin</first><last>Xu</last></author>
      <author><first>Sinong</first><last>Wang</last></author>
      <author><first>Hao</first><last>Ma</last></author>
      <pages>46-61</pages>
      <abstract>Automatic question generation (AQG) is the task of generating a question from a given passage and an answer. Most existing AQG methods aim at encoding the passage and the answer to generate the question. However, limited work has focused on modeling the correlation between the target answer and the generated question. Moreover, unseen or rare word generation has not been studied in previous works. In this paper, we propose a novel approach which incorporates question generation with its dual problem, question answering, into a unified primal-dual framework. Specifically, the question generation component consists of an encoder that jointly encodes the answer with the passage, and a decoder that produces the question. The question answering component then re-asks the generated question on the passage to ensure that the target answer is obtained. We further introduce a knowledge distillation module to improve the model generalization ability. We conduct an extensive set of experiments on SQuAD and HotpotQA benchmarks. Experimental results demonstrate the superior performance of the proposed approach over several state-of-the-art methods.</abstract>
      <url hash="032d4257">2022.emnlp-main.4</url>
      <bibkey>wang-etal-2022-learning-generate</bibkey>
    </paper>
    <paper id="5">
      <title>Graph-based Model Generation for Few-Shot Relation Extraction</title>
      <author><first>Wanli</first><last>Li</last></author>
      <author><first>Tieyun</first><last>Qian</last></author>
      <pages>62-71</pages>
      <abstract>Few-shot relation extraction (FSRE) has been a challenging problem since it only has a handful of training instances. Existing models follow a ‘one-for-all’ scheme where one general large model performs all individual N-way-K-shot tasks in FSRE, which prevents the model from achieving the optimal point on each task. In view of this, we propose a model generation framework that consists of one general model for all tasks and many tiny task-specific models for each individual task. The general model generates and passes the universal knowledge to the tiny models which will be further fine-tuned when performing specific tasks. In this way, we decouple the complexity of the entire task space from that of all individual tasks while absorbing the universal knowledge.Extensive experimental results on two public datasets demonstrate that our framework reaches a new state-of-the-art performance for FRSE tasks. Our code is available at: https://github.com/NLPWM-WHU/GM_GEN.</abstract>
      <url hash="a311a859">2022.emnlp-main.5</url>
      <bibkey>li-qian-2022-graph</bibkey>
    </paper>
    <paper id="6">
      <title>Backdoor Attacks in Federated Learning by Rare Embeddings and Gradient Ensembling</title>
      <author><first>Ki Yoon</first><last>Yoo</last></author>
      <author><first>Nojun</first><last>Kwak</last></author>
      <pages>72-88</pages>
      <abstract>Recent advances in federated learning have demonstrated its promising capability to learn on decentralized datasets. However, a considerable amount of work has raised concerns due to the potential risks of adversaries participating in the framework to poison the global model for an adversarial purpose. This paper investigates the feasibility of model poisoning for backdoor attacks through rare word embeddings of NLP models. In text classification, less than 1% of adversary clients suffices to manipulate the model output without any drop in the performance of clean sentences. For a less complex dataset, a mere 0.1% of adversary clients is enough to poison the global model effectively. We also propose a technique specialized in the federated learning scheme called gradient ensemble, which enhances the backdoor performance in all experimental settings.</abstract>
      <url hash="cc80dda2">2022.emnlp-main.6</url>
      <bibkey>yoo-kwak-2022-backdoor</bibkey>
    </paper>
    <paper id="7">
      <title>Generating Natural Language Proofs with Verifier-Guided Search</title>
      <author><first>Kaiyu</first><last>Yang</last></author>
      <author><first>Jia</first><last>Deng</last></author>
      <author><first>Danqi</first><last>Chen</last></author>
      <pages>89-105</pages>
      <abstract>Reasoning over natural language is a challenging problem in NLP. In this work, we focus on proof generation: Given a hypothesis and a set of supporting facts, the model generates a proof tree indicating how to derive the hypothesis from supporting facts. Compared to generating the entire proof in one shot, stepwise generation can better exploit the compositionality and generalize to longer proofs but has achieved limited success on real-world data. Existing stepwise methods struggle to generate proof steps that are both logically valid and relevant to the hypothesis. Instead, they tend to hallucinate invalid steps given the hypothesis. In this paper, we present a novel stepwise method, NLProofS (Natural Language Proof Search), which learns to generate relevant steps conditioning on the hypothesis. At the core of our approach, we train an independent verifier to check the validity of the proof steps to prevent hallucination. Instead of generating steps greedily, we search for proofs maximizing a global proof score judged by the verifier. NLProofS achieves state-of-the-art performance on EntailmentBank and RuleTaker. Specifically, it improves the correctness of predicted proofs from 27.7% to 33.3% in the distractor setting of EntailmentBank, demonstrating the effectiveness of NLProofS in generating challenging human-authored proofs.</abstract>
      <url hash="f487d00b">2022.emnlp-main.7</url>
      <bibkey>yang-etal-2022-generating</bibkey>
    </paper>
    <paper id="8">
      <title>Toward Unifying Text Segmentation and Long Document Summarization</title>
      <author><first>Sangwoo</first><last>Cho</last></author>
      <author><first>Kaiqiang</first><last>Song</last></author>
      <author><first>Xiaoyang</first><last>Wang</last></author>
      <author id="fei-liu-em"><first>Fei</first><last>Liu</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <pages>106-118</pages>
      <abstract>Text segmentation is important for signaling a document’s structure. Without segmenting a long document into topically coherent sections, it is difficult for readers to comprehend the text, let alone find important information. The problem is only exacerbated by a lack of segmentation in transcripts of audio/video recordings. In this paper, we explore the role that section segmentation plays in extractive summarization of written and spoken documents. Our approach learns robust sentence representations by performing summarization and segmentation simultaneously, which is further enhanced by an optimization-based regularizer to promote selection of diverse summary sentences. We conduct experiments on multiple datasets ranging from scientific articles to spoken transcripts to evaluate the model’s performance. Our findings suggest that the model can not only achieve state-of-the-art performance on publicly available benchmarks, but demonstrate better cross-genre transferability when equipped with text segmentation. We perform a series of analyses to quantify the impact of section segmentation on summarizing written and spoken documents of substantial length and complexity.</abstract>
      <url hash="85e1a194">2022.emnlp-main.8</url>
      <bibkey>cho-etal-2022-toward</bibkey>
    </paper>
    <paper id="9">
      <title>The Geometry of Multilingual Language Model Representations</title>
      <author><first>Tyler</first><last>Chang</last></author>
      <author><first>Zhuowen</first><last>Tu</last></author>
      <author><first>Benjamin</first><last>Bergen</last></author>
      <pages>119-136</pages>
      <abstract>We assess how multilingual language models maintain a shared multilingual representation space while still encoding language-sensitive information in each language. Using XLM-R as a case study, we show that languages occupy similar linear subspaces after mean-centering, evaluated based on causal effects on language modeling performance and direct comparisons between subspaces for 88 languages. The subspace means differ along language-sensitive axes that are relatively stable throughout middle layers, and these axes encode information such as token vocabularies. Shifting representations by language means is sufficient to induce token predictions in different languages. However, we also identify stable language-neutral axes that encode information such as token positions and part-of-speech. We visualize representations projected onto language-sensitive and language-neutral axes, identifying language family and part-of-speech clusters, along with spirals, toruses, and curves representing token position information. These results demonstrate that multilingual language models encode information along orthogonal language-sensitive and language-neutral axes, allowing the models to extract a variety of features for downstream tasks and cross-lingual transfer learning.</abstract>
      <url hash="7559bcdd">2022.emnlp-main.9</url>
      <bibkey>chang-etal-2022-geometry</bibkey>
    </paper>
    <paper id="10">
      <title>Improving Complex Knowledge Base Question Answering via Question-to-Action and Question-to-Question Alignment</title>
      <author><first>Yechun</first><last>Tang</last></author>
      <author><first>Xiaoxia</first><last>Cheng</last></author>
      <author><first>Weiming</first><last>Lu</last></author>
      <pages>137-147</pages>
      <abstract>Complex knowledge base question answering can be achieved by converting questions into sequences of predefined actions. However, there is a significant semantic and structural gap between natural language and action sequences, which makes this conversion difficult. In this paper, we introduce an alignment-enhanced complex question answering framework, called ALCQA, which mitigates this gap through question-to-action alignment and question-to-question alignment. We train a question rewriting model to align the question and each action, and utilize a pretrained language model to implicitly align the question and KG artifacts. Moreover, considering that similar questions correspond to similar action sequences, we retrieve top-k similar question-answer pairs at the inference stage through question-to-question alignment and propose a novel reward-guided action sequence selection strategy to select from candidate action sequences. We conduct experiments on CQA and WQSP datasets, and the results show that our approach outperforms state-of-the-art methods and obtains a 9.88% improvements in the F1 metric on CQA dataset. Our source code is available at <url>https://github.com/TTTTTTTTy/ALCQA</url>.</abstract>
      <url hash="a9fe0812">2022.emnlp-main.10</url>
      <bibkey>tang-etal-2022-improving</bibkey>
    </paper>
    <paper id="11">
      <title><fixed-case>PAIR</fixed-case>: Prompt-Aware marg<fixed-case>I</fixed-case>n Ranking for Counselor Reflection Scoring in Motivational Interviewing</title>
      <author><first>Do June</first><last>Min</last></author>
      <author><first>Verónica</first><last>Pérez-Rosas</last></author>
      <author><first>Kenneth</first><last>Resnicow</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <pages>148-158</pages>
      <abstract>Counselor reflection is a core verbal skill used by mental health counselors to express understanding and affirmation of the client’s experience and concerns. In this paper, we propose a system for the analysis of counselor reflections. Specifically, our system takes as input one dialog turn containing a client prompt and a counselor response, and outputs a score indicating the level of reflection in the counselor response. We compile a dataset consisting of different levels of reflective listening skills, and propose the Prompt-Aware margIn Ranking (PAIR) framework that contrasts positive and negative prompt and response pairs using specially designed multi-gap and prompt-aware margin ranking losses. Through empirical evaluations and deployment of our system in a real-life educational environment, we show that our analysis model outperforms several baselines on different metrics, and can be used to provide useful feedback to counseling trainees.</abstract>
      <url hash="1eb8ddb3">2022.emnlp-main.11</url>
      <bibkey>min-etal-2022-pair</bibkey>
    </paper>
    <paper id="12">
      <title>Co-guiding Net: Achieving Mutual Guidances between Multiple Intent Detection and Slot Filling via Heterogeneous Semantics-Label Graphs</title>
      <author><first>Bowen</first><last>Xing</last></author>
      <author><first>Ivor</first><last>Tsang</last></author>
      <pages>159-169</pages>
      <abstract>Recent graph-based models for joint multiple intent detection and slot filling have obtained promising results through modeling the guidance from the prediction of intents to the decoding of slot filling.However, existing methods (1) only model the <i>unidirectional guidance</i> from intent to slot; (2) adopt <i>homogeneous graphs</i> to model the interactions between the slot semantics nodes and intent label nodes, which limit the performance.In this paper, we propose a novel model termed Co-guiding Net, which implements a two-stage framework achieving the <i>mutual guidances</i> between the two tasks.In the first stage, the initial estimated labels of both tasks are produced, and then they are leveraged in the second stage to model the mutual guidances.Specifically, we propose two <i>heterogeneous graph attention networks</i> working on the proposed two <i>heterogeneous semantics-label graphs</i>, which effectively represent the relations among the semantics nodes and label nodes.Experiment results show that our model outperforms existing models by a large margin, obtaining a relative improvement of 19.3% over the previous best model on MixATIS dataset in overall accuracy.</abstract>
      <url hash="bda1df3e">2022.emnlp-main.12</url>
      <bibkey>xing-tsang-2022-co</bibkey>
    </paper>
    <paper id="13">
      <title>The Importance of Being Parameters: An Intra-Distillation Method for Serious Gains</title>
      <author><first>Haoran</first><last>Xu</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <author><first>Kenton</first><last>Murray</last></author>
      <pages>170-183</pages>
      <abstract>Recent model pruning methods have demonstrated the ability to remove redundant parameters without sacrificing model performance. Common methods remove redundant parameters according to the parameter sensitivity, a gradient-based measure reflecting the contribution of the parameters. In this paper, however, we argue that redundant parameters can be trained to make beneficial contributions. We first highlight the large sensitivity (contribution) gap among high-sensitivity and low-sensitivity parameters and show that the model generalization performance can be significantly improved after balancing the contribution of all parameters. Our goal is to balance the sensitivity of all parameters and encourage all of them to contribute equally. We propose a general task-agnostic method, namely intra-distillation, appended to the regular training loss to balance parameter sensitivity. Moreover, we also design a novel adaptive learning method to control the strength of intra-distillation loss for faster convergence. Our experiments show the strong effectiveness of our methods on machine translation, natural language understanding, and zero-shot cross-lingual transfer across up to 48 languages, e.g., a gain of 3.54 BLEU on average across 8 language pairs from the IWSLT’14 dataset.</abstract>
      <url hash="64f3ca05">2022.emnlp-main.13</url>
      <bibkey>xu-etal-2022-importance</bibkey>
    </paper>
    <paper id="14">
      <title>Interpreting Language Models with Contrastive Explanations</title>
      <author><first>Kayo</first><last>Yin</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>184-198</pages>
      <abstract>Model interpretability methods are often used to explain NLP model decisions on tasks such as text classification, where the output space is relatively small. However, when applied to language generation, where the output space often consists of tens of thousands of tokens, these methods are unable to provide informative explanations. Language models must consider various features to predict a token, such as its part of speech, number, tense, or semantics.Existing explanation methods conflate evidence for all these features into a single explanation, which is less interpretable for human understanding.To disentangle the different decisions in language modeling, we focus on explaining language models contrastively: we look for salient input tokens that explain why the model predicted one token instead of another. We demonstrate that contrastive explanations are quantifiably better than non-contrastive explanations in verifying major grammatical phenomena, and that they significantly improve contrastive model simulatability for human observers. We also identify groups of contrastive decisions where the model uses similar evidence, and we are able to characterize what input tokens models use during various language generation decisions.</abstract>
      <url hash="31eab42c">2022.emnlp-main.14</url>
      <bibkey>yin-neubig-2022-interpreting</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>R</fixed-case>ank<fixed-case>G</fixed-case>en: Improving Text Generation with Large Ranking Models</title>
      <author><first>Kalpesh</first><last>Krishna</last></author>
      <author><first>Yapei</first><last>Chang</last></author>
      <author><first>John</first><last>Wieting</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <pages>199-232</pages>
      <abstract>Given an input sequence (or prefix), modern language models often assign high probabilities to output sequences that are repetitive, incoherent, or irrelevant to the prefix; as such, model-generated text also contains such artifacts. To address these issues we present RankGen, a 1.2B parameter encoder model for English that scores model generations given a prefix. RankGen can be flexibly incorporated as a scoring function in beam search and used to decode from any pretrained language model. We train RankGen using large-scale contrastive learning to map a prefix close to the ground-truth sequence that follows it and far away from two types of negatives: (1) random sequences from the same document as the prefix, and (2) sequences generated from a large language model conditioned on the prefix. Experiments across four different language models (345M-11B parameters) and two domains show that RankGen significantly outperforms decoding algorithms like nucleus, top-k, and typical sampling on both automatic metrics (85.0 vs 77.3 MAUVE) as well as human evaluations with English writers (74.5% human preference over nucleus sampling). Analysis reveals that RankGen outputs are more relevant to the prefix and improve continuity and coherence compared to baselines. We release our model checkpoints, code, and human preference data with explanations to facilitate future research.</abstract>
      <url hash="8afeaefb">2022.emnlp-main.15</url>
      <bibkey>krishna-etal-2022-rankgen</bibkey>
    </paper>
    <paper id="16">
      <title>Learning a Grammar Inducer from Massive Uncurated Instructional Videos</title>
      <author><first>Songyang</first><last>Zhang</last></author>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Lifeng</first><last>Jin</last></author>
      <author><first>Haitao</first><last>Mi</last></author>
      <author><first>Kun</first><last>Xu</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <author><first>Jiebo</first><last>Luo</last></author>
      <pages>233-247</pages>
      <abstract>Video-aided grammar induction aims to leverage video information for finding more accurate syntactic grammars for accompanying text. While previous work focuses on building systems for inducing grammars on text that are well-aligned with video content, we investigate the scenario, in which text and video are only in loose correspondence. Such data can be found in abundance online, and the weak correspondence is similar to the indeterminacy problem studied in language acquisition. Furthermore, we build a new model that can better learn video-span correlation without manually designed features adopted by previous work. Experiments show that our model trained only on large-scale YouTube data with no text-video alignment reports strong and robust performances across three unseen datasets, despite domain shift and noisy label issues. Furthermore our model yields higher F1 scores than the previous state-of-the-art systems trained on in-domain data.</abstract>
      <url hash="05d06d51">2022.emnlp-main.16</url>
      <bibkey>zhang-etal-2022-learning-grammar</bibkey>
    </paper>
    <paper id="17">
      <title>Normalized Contrastive Learning for Text-Video Retrieval</title>
      <author><first>Yookoon</first><last>Park</last></author>
      <author><first>Mahmoud</first><last>Azab</last></author>
      <author><first>Seungwhan</first><last>Moon</last></author>
      <author><first>Bo</first><last>Xiong</last></author>
      <author><first>Florian</first><last>Metze</last></author>
      <author><first>Gourab</first><last>Kundu</last></author>
      <author><first>Kirmani</first><last>Ahmed</last></author>
      <pages>248-260</pages>
      <abstract>Cross-modal contrastive learning has led the recent advances in multimodal retrieval with its simplicity and effectiveness. In this work, however, we reveal that cross-modal contrastive learning suffers from incorrect normalization of the sum retrieval probabilities of each text or video instance. Specifically, we show that many test instances are either over- or under-represented during retrieval, significantly hurting the retrieval performance. To address this problem, we propose Normalized Contrastive Learning (NCL) which utilizes the Sinkhorn-Knopp algorithm to compute the instance-wise biases that properly normalize the sum retrieval probabilities of each instance so that every text and video instance is fairly represented during cross-modal retrieval. Empirical study shows that NCL brings consistent and significant gains in text-video retrieval on different model architectures, with new state-of-the-art multimodal retrieval metrics on the ActivityNet, MSVD, and MSR-VTT datasets without any architecture engineering.</abstract>
      <url hash="7be278a9">2022.emnlp-main.17</url>
      <bibkey>park-etal-2022-normalized</bibkey>
    </paper>
    <paper id="18">
      <title>Estimating Soft Labels for Out-of-Domain Intent Detection</title>
      <author><first>Hao</first><last>Lang</last></author>
      <author><first>Yinhe</first><last>Zheng</last></author>
      <author><first>Jian</first><last>Sun</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <author><first>Yongbin</first><last>Li</last></author>
      <pages>261-276</pages>
      <abstract>Out-of-Domain (OOD) intent detection is important for practical dialog systems. To alleviate the issue of lacking OOD training samples, some works propose synthesizing pseudo OOD samples and directly assigning one-hot OOD labels to these pseudo samples. However, these one-hot labels introduce noises to the training process because some “hard” pseudo OOD samples may coincide with In-Domain (IND) intents. In this paper, we propose an adaptive soft pseudo labeling (ASoul) method that can estimate soft labels for pseudo OOD samples when training OOD detectors. Semantic connections between pseudo OOD samples and IND intents are captured using an embedding graph. A co-training framework is further introduced to produce resulting soft labels following the smoothness assumption, i.e., close samples are likely to have similar labels. Extensive experiments on three benchmark datasets show that ASoul consistently improves the OOD detection performance and outperforms various competitive baselines.</abstract>
      <url hash="ca281749">2022.emnlp-main.18</url>
      <bibkey>lang-etal-2022-estimating</bibkey>
    </paper>
    <paper id="19">
      <title>Multi-<fixed-case>VQG</fixed-case>: Generating Engaging Questions for Multiple Images</title>
      <author><first>Min-Hsuan</first><last>Yeh</last></author>
      <author><first>Vincent</first><last>Chen</last></author>
      <author><first>Ting-Hao</first><last>Huang</last></author>
      <author><first>Lun-Wei</first><last>Ku</last></author>
      <pages>277-290</pages>
      <abstract>Generating engaging content has drawn much recent attention in the NLP community. Asking questions is a natural way to respond to photos and promote awareness. However, most answers to questions in traditional question-answering (QA) datasets are factoids, which reduce individuals’ willingness to answer. Furthermore, traditional visual question generation (VQG) confines the source data for question generation to single images, resulting in a limited ability to comprehend time-series information of the underlying event. In this paper, we propose generating engaging questions from multiple images. We present MVQG, a new dataset, and establish a series of baselines, including both end-to-end and dual-stage architectures. Results show that building stories behind the image sequence enables models togenerate engaging questions, which confirms our assumption that people typically construct a picture of the event in their minds before asking questions. These results open up an exciting challenge for visual-and-language models to implicitly construct a story behind a series of photos to allow for creativity and experience sharing and hence draw attention to downstream applications.</abstract>
      <url hash="f2211cd5">2022.emnlp-main.19</url>
      <bibkey>yeh-etal-2022-multi</bibkey>
    </paper>
    <paper id="20">
      <title>Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question Answering Evaluation</title>
      <author><first>Jannis</first><last>Bulian</last></author>
      <author><first>Christian</first><last>Buck</last></author>
      <author><first>Wojciech</first><last>Gajewski</last></author>
      <author><first>Benjamin</first><last>Börschinger</last></author>
      <author><first>Tal</first><last>Schuster</last></author>
      <pages>291-305</pages>
      <abstract>The predictions of question answering (QA) systems are typically evaluated against manually annotated finite sets of one or more answers. This leads to a coverage limitation that results in underestimating the true performance of systems, and is typically addressed by extending over exact match (EM) with predefined rules or with the token-level F1 measure.In this paper, we present the first systematic conceptual and data-driven analysis to examine the shortcomings of token-level equivalence measures.To this end, we define the asymmetric notion of answer equivalence (AE), accepting answers that are equivalent to or improve over the reference, and publish over 23k human judgements for candidates produced by multiple QA systems on SQuAD.Through a careful analysis of this data, we reveal and quantify several concrete limitations of the F1 measure, such as a false impression of graduality, or missing dependence on the question.Since collecting AE annotations for each evaluated model is expensive, we learn a BERT matching (BEM) measure to approximate this task. Being a simpler task than QA, we find BEM to provide significantly better AE approximations than F1, and to more accurately reflect the performance of systems.Finally, we demonstrate the practical utility of AE and BEM on the concrete application of minimal accurate prediction sets, reducing the number of required answers by up to X2.6.</abstract>
      <url hash="56c83281">2022.emnlp-main.20</url>
      <bibkey>bulian-etal-2022-tomayto</bibkey>
    </paper>
    <paper id="21">
      <title>Non-Parametric Domain Adaptation for End-to-End Speech Translation</title>
      <author><first>Yichao</first><last>Du</last></author>
      <author><first>Weizhi</first><last>Wang</last></author>
      <author><first>Zhirui</first><last>Zhang</last></author>
      <author><first>Boxing</first><last>Chen</last></author>
      <author><first>Tong</first><last>Xu</last></author>
      <author><first>Jun</first><last>Xie</last></author>
      <author><first>Enhong</first><last>Chen</last></author>
      <pages>306-320</pages>
      <abstract>The end-to-end speech translation (E2E-ST) has received increasing attention due to the potential of its less error propagation, lower latency and fewer parameters. However, the effectiveness of neural-based approaches to this task is severely limited by the available training corpus, especially for domain adaptation where in-domain triplet data is scarce or nonexistent. In this paper, we propose a novel non-parametric method that leverages in-domain text translation corpus to achieve domain adaptation for E2E-ST systems. To this end, we first incorporate an additional encoder into the pre-trained E2E-ST model to realize text translation modeling, based on which the decoder’s output representations for text and speech translation tasks are unified by reducing the correspondent representation mismatch in available triplet training data. During domain adaptation, a k-nearest-neighbor (kNN) classifier is introduced to produce the final translation distribution using the external datastore built by the domain-specific text translation corpus, while the universal output representation is adopted to perform a similarity search. Experiments on the Europarl-ST benchmark demonstrate that when in-domain text translation data is involved only, our proposed approach significantly improves baseline by 12.82 BLEU on average in all translation directions, even outperforming the strong in-domain fine-tuning strategy.</abstract>
      <url hash="047c7727">2022.emnlp-main.21</url>
      <bibkey>du-etal-2022-non</bibkey>
    </paper>
    <paper id="22">
      <title>Prompting for Multimodal Hateful Meme Classification</title>
      <author><first>Rui</first><last>Cao</last></author>
      <author><first>Roy Ka-Wei</first><last>Lee</last></author>
      <author><first>Wen-Haw</first><last>Chong</last></author>
      <author><first>Jing</first><last>Jiang</last></author>
      <pages>321-332</pages>
      <abstract>Hateful meme classification is a challenging multimodal task that requires complex reasoning and contextual background knowledge. Ideally, we could leverage an explicit external knowledge base to supplement contextual and cultural information in hateful memes. However, there is no known explicit external knowledge base that could provide such hate speech contextual information. To address this gap, we propose PromptHate, a simple yet effective prompt-based model that prompts pre-trained language models (PLMs) for hateful meme classification. Specifically, we construct simple prompts and provide a few in-context examples to exploit the implicit knowledge in the pre-trained RoBERTa language model for hateful meme classification. We conduct extensive experiments on two publicly available hateful and offensive meme datasets. Our experiment results show that PromptHate is able to achieve a high AUC of 90.96, outperforming state-of-the-art baselines on the hateful meme classification task. We also perform fine-grain analyses and case studies on various prompt settings and demonstrate the effectiveness of the prompts on hateful meme classification.</abstract>
      <url hash="f59cfd29">2022.emnlp-main.22</url>
      <bibkey>cao-etal-2022-prompting</bibkey>
    </paper>
    <paper id="23">
      <title>Certified Error Control of Candidate Set Pruning for Two-Stage Relevance Ranking</title>
      <author><first>Minghan</first><last>Li</last></author>
      <author><first>Xinyu</first><last>Zhang</last></author>
      <author><first>Ji</first><last>Xin</last></author>
      <author><first>Hongyang</first><last>Zhang</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>333-345</pages>
      <abstract>In information retrieval (IR), candidate set pruning has been commonly used to speed up two-stage relevance ranking. However, such an approach lacks accurate error control and often trades accuracy against computational efficiency in an empirical fashion, missing theoretical guarantees. In this paper, we propose the concept of certified error control of candidate set pruning for relevance ranking, which means that the test error after pruning is guaranteed to be controlled under a user-specified threshold with high probability. Both in-domain and out-of-domain experiments show that our method successfully prunes the first-stage retrieved candidate sets to improve the second-stage reranking speed while satisfying the pre-specified accuracy constraints in both settings. For example, on MS MARCO Passage v1, our method reduces the average candidate set size from 1000 to 27, increasing reranking speed by about 37 times, while keeping MRR@10 greater than a pre-specified value of 0.38 with about 90% empirical coverage. In contrast, empirical baselines fail to meet such requirements. Code and data are available at: https://github.com/alexlimh/CEC-Ranking.</abstract>
      <url hash="063fbc32">2022.emnlp-main.23</url>
      <bibkey>li-etal-2022-certified</bibkey>
    </paper>
    <paper id="24">
      <title>Linearizing Transformer with Key-Value Memory</title>
      <author><first>Yizhe</first><last>Zhang</last></author>
      <author><first>Deng</first><last>Cai</last></author>
      <pages>346-359</pages>
      <abstract>Efficient transformer variants with linear time complexity have been developed to mitigate the quadratic computational overhead of the vanilla transformer. Among them are low-rank projection methods such as Linformer and kernel-based Transformers. Despite their unique merits, they usually suffer from a performance drop comparing with the vanilla transformer on many sequence generation tasks, and often fail to obtain computation gain when the generation is short. We propose Memsizer, an approach towards closing the performance gap while improving the efficiency even with short generation. It projects the source sequences into lower dimension representations like Linformer, while enjoying efficient recurrent-style incremental computation similar to kernel-based transformers. This yields linear computation time and constant memory complexity at inference time. Memsizer also employs a lightweight multi-head mechanism which renders the computation as light as a single-head model. We demonstrate that Memsizer provides an improved balance between efficiency and accuracy over the vanilla transformer and other efficient transformer variants in three typical sequence generation tasks, including machine translation, abstractive text summarization, and language modeling.</abstract>
      <url hash="6384357b">2022.emnlp-main.24</url>
      <bibkey>zhang-cai-2022-linearizing</bibkey>
    </paper>
    <paper id="25">
      <title>Robustness of Fusion-based Multimodal Classifiers to Cross-Modal Content Dilutions</title>
      <author><first>Gaurav</first><last>Verma</last></author>
      <author><first>Vishwa</first><last>Vinay</last></author>
      <author><first>Ryan</first><last>Rossi</last></author>
      <author><first>Srijan</first><last>Kumar</last></author>
      <pages>360-374</pages>
      <abstract>As multimodal learning finds applications in a wide variety of high-stakes societal tasks, investigating their robustness becomes important. Existing work has focused on understanding the robustness of vision-and-language models to imperceptible variations on benchmark tasks. In this work, we investigate the robustness of multimodal classifiers to cross-modal dilutions – a plausible variation. We develop a model that, given a multimodal (image + text) input, generates additional dilution text that (a) maintains relevance and topical coherence with the image and existing text, and (b) when added to the original text, leads to misclassification of the multimodal input. Via experiments on Crisis Humanitarianism and Sentiment Detection tasks, we find that the performance of task-specific fusion-based multimodal classifiers drops by 23.3% and 22.5%, respectively, in the presence of dilutions generated by our model. Metric-based comparisons with several baselines and human evaluations indicate that our dilutions show higher relevance and topical coherence, while simultaneously being more effective at demonstrating the brittleness of the multimodal classifiers. Our work aims to highlight and encourage further research on the robustness of deep multimodal models to realistic variations, especially in human-facing societal applications.</abstract>
      <url hash="3a897400">2022.emnlp-main.25</url>
      <bibkey>verma-etal-2022-robustness</bibkey>
    </paper>
    <paper id="26">
      <title>Translation between Molecules and Natural Language</title>
      <author><first>Carl</first><last>Edwards</last></author>
      <author><first>Tuan</first><last>Lai</last></author>
      <author><first>Kevin</first><last>Ros</last></author>
      <author><first>Garrett</first><last>Honke</last></author>
      <author><first>Kyunghyun</first><last>Cho</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <pages>375-413</pages>
      <abstract>We present MolT5 - a self-supervised learning framework for pretraining models on a vast amount of unlabeled natural language text and molecule strings. MolT5 allows for new, useful, and challenging analogs of traditional vision-language tasks, such as molecule captioning and text-based de novo molecule generation (altogether: translation between molecules and language), which we explore for the first time. Since MolT5 pretrains models on single-modal data, it helps overcome the chemistry domain shortcoming of data scarcity. Furthermore, we consider several metrics, including a new cross-modal embedding-based metric, to evaluate the tasks of molecule captioning and text-based molecule generation. Our results show that MolT5-based models are able to generate outputs, both molecules and captions, which in many cases are high quality.</abstract>
      <url hash="983aabc4">2022.emnlp-main.26</url>
      <bibkey>edwards-etal-2022-translation</bibkey>
    </paper>
    <paper id="27">
      <title>What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment</title>
      <author><first>Matthew</first><last>Finlayson</last></author>
      <author><first>Kyle</first><last>Richardson</last></author>
      <author><first>Ashish</first><last>Sabharwal</last></author>
      <author><first>Peter</first><last>Clark</last></author>
      <pages>414-426</pages>
      <abstract>The instruction learning paradigm—where a model learns to perform new tasks from task descriptions alone—has become popular in research on general-purpose models. The capabilities of large transformer models as instruction learners, however, remain poorly understood. We use a controlled synthetic environment to characterize such capabilities. Specifically, we use the task of deciding whether a given string matches a regular expression (viewed as an instruction) to identify properties of tasks, instructions, and instances that make instruction learning challenging. For instance, we find that our model, a fine-tuned T5-based text2text transformer, struggles with large regular languages, suggesting that less precise instructions are challenging for models. Instruction executions that require tracking longer contexts of prior steps are also difficult. We use our findings to systematically construct a challenging instruction learning dataset, which we call Hard RegSet. Fine-tuning on Hard RegSet, our large transformer learns to correctly interpret (with at least 90% accuracy) only 65.6% of test instructions, and 11%-24% of the instructions in out-of-distribution generalization settings. We thus propose Hard RegSet as a challenging instruction learning dataset, and a controlled environment for studying instruction learning.</abstract>
      <url hash="e1704ed3">2022.emnlp-main.27</url>
      <bibkey>finlayson-etal-2022-makes</bibkey>
    </paper>
    <paper id="28">
      <title>Sentence-Incremental Neural Coreference Resolution</title>
      <author><first>Matt</first><last>Grenander</last></author>
      <author><first>Shay B.</first><last>Cohen</last></author>
      <author><first>Mark</first><last>Steedman</last></author>
      <pages>427-443</pages>
      <abstract>We propose a sentence-incremental neural coreference resolution system which incrementally builds clusters after marking mention boundaries in a shift-reduce method. The system is aimed at bridging two recent approaches at coreference resolution: (1) state-of-the-art non-incremental models that incur quadratic complexity in document length with high computational cost, and (2) memory network-based models which operate incrementally but do not generalize beyond pronouns. For comparison, we simulate an incremental setting by constraining non-incremental systems to form partial coreference chains before observing new sentences. In this setting, our system outperforms comparable state-of-the-art methods by 2 F1 on OntoNotes and 6.8 F1 on the CODI-CRAC 2021 corpus. In a conventional coreference setup, our system achieves 76.3 F1 on OntoNotes and 45.5 F1 on CODI-CRAC 2021, which is comparable to state-of-the-art baselines. We also analyze variations of our system and show that the degree of incrementality in the encoder has a surprisingly large effect on the resulting performance.</abstract>
      <url hash="c9fcaa79">2022.emnlp-main.28</url>
      <bibkey>grenander-etal-2022-sentence</bibkey>
    </paper>
    <paper id="29">
      <title><fixed-case>SN</fixed-case>a<fixed-case>C</fixed-case>: Coherence Error Detection for Narrative Summarization</title>
      <author><first>Tanya</first><last>Goyal</last></author>
      <author><first>Junyi Jessy</first><last>Li</last></author>
      <author><first>Greg</first><last>Durrett</last></author>
      <pages>444-463</pages>
      <abstract>Progress in summarizing long texts is inhibited by the lack of appropriate evaluation frameworks. A long summary that appropriately covers the facets of that text must also present a coherent narrative, but current automatic and human evaluation methods fail to identify gaps in coherence. In this work, we introduce SNaC, a narrative coherence evaluation framework for fine-grained annotations of long summaries. We develop a taxonomy of coherence errors in generated narrative summaries and collect span-level annotations for 6.6k sentences across 150 book and movie summaries. Our work provides the first characterization of coherence errors generated by state-of-the-art summarization models and a protocol for eliciting coherence judgments from crowdworkers. Furthermore, we show that the collected annotations allow us to benchmark past work in coherence modeling and train a strong classifier for automatically localizing coherence errors in generated summaries. Finally, our SNaC framework can support future work in long document summarization and coherence evaluation, including improved summarization modeling and post-hoc summary correction.</abstract>
      <url hash="8a29315b">2022.emnlp-main.29</url>
      <bibkey>goyal-etal-2022-snac</bibkey>
    </paper>
    <paper id="30">
      <title><fixed-case>H</fixed-case>ydra<fixed-case>S</fixed-case>um: Disentangling Style Features in Text Summarization with Multi-Decoder Models</title>
      <author><first>Tanya</first><last>Goyal</last></author>
      <author><first>Nazneen</first><last>Rajani</last></author>
      <author><first>Wenhao</first><last>Liu</last></author>
      <author><first>Wojciech</first><last>Kryscinski</last></author>
      <pages>464-479</pages>
      <abstract>Summarization systems make numerous “decisions” about summary properties during inference, e.g. degree of copying, specificity and length of outputs, etc. However, these are implicitly encoded within model parameters and specific styles cannot be enforced. To address this, we introduce HydraSum, a new summarization architecture that extends the single decoder framework of current models to a mixture-of-experts version with multiple decoders. We show that HydraSum’s multiple decoders automatically learn contrasting summary styles when trained under the standard training objective without any extra supervision. Through experiments on three summarization datasets (CNN, Newsroom and XSum), we show that HydraSum provides a simple mechanism to obtain stylistically-diverse summaries by sampling from either individual decoders or their mixtures, outperforming baseline models. Finally, we demonstrate that a small modification to the gating strategy during training can enforce an even stricter style partitioning, e.g. high- vs low-abstractiveness or high- vs low-specificity, allowing users to sample from a larger area in the generation space and vary summary styles along multiple dimensions.</abstract>
      <url hash="54218a10">2022.emnlp-main.30</url>
      <bibkey>goyal-etal-2022-hydrasum</bibkey>
    </paper>
    <paper id="31">
      <title>A Good Neighbor, A Found Treasure: Mining Treasured Neighbors for Knowledge Graph Entity Typing</title>
      <author><first>Zhuoran</first><last>Jin</last></author>
      <author><first>Pengfei</first><last>Cao</last></author>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>480-490</pages>
      <abstract>The task of knowledge graph entity typing (KGET) aims to infer the missing types for entities in knowledge graphs. Some pioneering work has proved that neighbor information is very important for the task. However, existing methods only leverage the one-hop neighbor information of the central entity, ignoring the multi-hop neighbor information that can provide valuable clues for inference. Besides, we also observe that there are co-occurrence relations between types, which is very helpful to alleviate false-negative problem. In this paper, we propose a novel method called Mining Treasured Neighbors (MiNer) to make use of these two characteristics. Firstly, we devise a Neighbor Information Aggregation module to aggregate the neighbor information. Then, we propose an Entity Type Inference module to mitigate the adverse impact of the irrelevant neighbor information. Finally, a Type Co-occurrence Regularization module is designed to prevent the model from overfitting the false negative examples caused by missing types. Experimental results on two widely used datasets indicate that our approach significantly outperforms previous state-of-the-art methods.</abstract>
      <url hash="f625c9a0">2022.emnlp-main.31</url>
      <bibkey>jin-etal-2022-good-neighbor</bibkey>
    </paper>
    <paper id="32">
      <title>Guiding Neural Entity Alignment with Compatibility</title>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Harrisen</first><last>Scells</last></author>
      <author><first>Wen</first><last>Hua</last></author>
      <author><first>Guido</first><last>Zuccon</last></author>
      <author><first>Genghong</first><last>Zhao</last></author>
      <author><first>Xia</first><last>Zhang</last></author>
      <pages>491-504</pages>
      <abstract>Entity Alignment (EA) aims to find equivalent entities between two Knowledge Graphs (KGs). While numerous neural EA models have been devised, they are mainly learned using labelled data only. In this work, we argue that different entities within one KG should have compatible counterparts in the other KG due to the potential dependencies among the entities. Making compatible predictions thus should be one of the goals of training an EA model along with fitting the labelled data: this aspect however is neglected in current methods. To power neural EA models with compatibility, we devise a training framework by addressing three problems: (1) how to measure the compatibility of an EA model; (2) how to inject the property of being compatible into an EA model; (3) how to optimise parameters of the compatibility model. Extensive experiments on widely-used datasets demonstrate the advantages of integrating compatibility within EA models. In fact, state-of-the-art neural EA models trained within our framework using just 5% of the labelled data can achieve comparable effectiveness with supervised training using 20% of the labelled data.</abstract>
      <url hash="b34972c5">2022.emnlp-main.32</url>
      <bibkey>liu-etal-2022-guiding</bibkey>
    </paper>
    <paper id="33">
      <title><fixed-case>I</fixed-case>nstruct<fixed-case>D</fixed-case>ial: Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning</title>
      <author><first>Prakhar</first><last>Gupta</last></author>
      <author><first>Cathy</first><last>Jiao</last></author>
      <author><first>Yi-Ting</first><last>Yeh</last></author>
      <author><first>Shikib</first><last>Mehri</last></author>
      <author><first>Maxine</first><last>Eskenazi</last></author>
      <author><first>Jeffrey</first><last>Bigham</last></author>
      <pages>505-525</pages>
      <abstract>Instruction tuning is an emergent paradigm in NLP wherein natural language instructions are leveraged with language models to induce zero-shot performance on unseen tasks. Dialogue is an especially interesting area in which to explore instruction tuning because dialogue systems perform multiple kinds of tasks related to language (e.g., natural language understanding and generation, domain-specific interaction), yet instruction tuning has not been systematically explored for dialogue-related tasks. We introduce InstructDial, an instruction tuning framework for dialogue, which consists of a repository of 48 diverse dialogue tasks in a unified text-to-text format created from 59 openly available dialogue datasets. We explore cross-task generalization ability on models tuned on InstructDial across diverse dialogue tasks. Our analysis reveals that InstructDial enables good zero-shot performance on unseen datasets and tasks such as dialogue evaluation and intent detection, and even better performance in a few-shot setting. To ensure that models adhere to instructions, we introduce novel meta-tasks. We establish benchmark zero-shot and few-shot performance of models trained using the proposed framework on multiple dialogue tasks.</abstract>
      <url hash="d13850b9">2022.emnlp-main.33</url>
      <bibkey>gupta-etal-2022-instructdial</bibkey>
    </paper>
    <paper id="34">
      <title>Unsupervised Boundary-Aware Language Model Pretraining for <fixed-case>C</fixed-case>hinese Sequence Labeling</title>
      <author><first>Peijie</first><last>Jiang</last></author>
      <author><first>Dingkun</first><last>Long</last></author>
      <author><first>Yanzhao</first><last>Zhang</last></author>
      <author><first>Pengjun</first><last>Xie</last></author>
      <author><first>Meishan</first><last>Zhang</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>526-537</pages>
      <abstract>Boundary information is critical for various Chinese language processing tasks, such as word segmentation, part-of-speech tagging, and named entity recognition. Previous studies usually resorted to the use of a high-quality external lexicon, where lexicon items can offer explicit boundary information. However, to ensure the quality of the lexicon, great human effort is always necessary, which has been generally ignored. In this work, we suggest unsupervised statistical boundary information instead, and propose an architecture to encode the information directly into pre-trained language models, resulting in Boundary-Aware BERT (BABERT). We apply BABERT for feature induction of Chinese sequence labeling tasks. Experimental results on ten benchmarks of Chinese sequence labeling demonstrate that BABERT can provide consistent improvements on all datasets. In addition, our method can complement previous supervised lexicon exploration, where further improvements can be achieved when integrated with external lexicon information.</abstract>
      <url hash="1e0cbb7e">2022.emnlp-main.34</url>
      <bibkey>jiang-etal-2022-unsupervised</bibkey>
    </paper>
    <paper id="35">
      <title><fixed-case>R</fixed-case>etro<fixed-case>MAE</fixed-case>: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder</title>
      <author><first>Shitao</first><last>Xiao</last></author>
      <author><first>Zheng</first><last>Liu</last></author>
      <author><first>Yingxia</first><last>Shao</last></author>
      <author><first>Zhao</first><last>Cao</last></author>
      <pages>538-548</pages>
      <abstract>Despite pre-training’s progress in many important NLP tasks, it remains to explore effective pre-training strategies for dense retrieval. In this paper, we propose RetroMAE, a new retrieval oriented pre-training paradigm based on Masked Auto-Encoder (MAE). RetroMAE is highlighted by three critical designs. 1) A novel MAE workflow, where the input sentence is polluted for encoder and decoder with different masks. The sentence embedding is generated from the encoder’s masked input; then, the original sentence is recovered based on the sentence embedding and the decoder’s masked input via masked language modeling. 2) Asymmetric model structure, with a full-scale BERT like transformer as encoder, and a one-layer transformer as decoder. 3) Asymmetric masking ratios, with a moderate ratio for encoder: 15 30%, and an aggressive ratio for decoder: 50 70%. Our framework is simple to realize and empirically competitive: the pre-trained models dramatically improve the SOTA performances on a wide range of dense retrieval benchmarks, like BEIR and MS MARCO. The source code and pre-trained models are made publicly available at https://github.com/staoxiao/RetroMAE so as to inspire more interesting research.</abstract>
      <url hash="37d1ab54">2022.emnlp-main.35</url>
      <bibkey>xiao-etal-2022-retromae</bibkey>
    </paper>
    <paper id="36">
      <title>Aligning Recommendation and Conversation via Dual Imitation</title>
      <author><first>Jinfeng</first><last>Zhou</last></author>
      <author><first>Bo</first><last>Wang</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <author><first>Dongming</first><last>Zhao</last></author>
      <author><first>Kun</first><last>Huang</last></author>
      <author><first>Ruifang</first><last>He</last></author>
      <author><first>Yuexian</first><last>Hou</last></author>
      <pages>549-561</pages>
      <abstract>Human conversations of recommendation naturally involve the shift of interests which can align the recommendation actions and conversation process to make accurate recommendations with rich explanations. However, existing conversational recommendation systems (CRS) ignore the advantage of user interest shift in connecting recommendation and conversation, which leads to an ineffective loose coupling structure of CRS. To address this issue, by modeling the recommendation actions as recommendation paths in a knowledge graph (KG), we propose DICR (<b>D</b>ual <b>I</b>mitation for <b>C</b>onversational <b>R</b>ecommendation), which designs a dual imitation to explicitly align the recommendation paths and user interest shift paths in a recommendation module and a conversation module, respectively. By exchanging alignment signals, DICR achieves bidirectional promotion between recommendation and conversation modules and generates high-quality responses with accurate recommendations and coherent explanations. Experiments demonstrate that DICR outperforms the state-of-the-art models on recommendation and conversation performance with automatic, human, and novel explainability metrics.</abstract>
      <url hash="49db45f6">2022.emnlp-main.36</url>
      <bibkey>zhou-etal-2022-aligning</bibkey>
    </paper>
    <paper id="37">
      <title><fixed-case>QR</fixed-case>el<fixed-case>S</fixed-case>core: Better Evaluating Generated Questions with Deeper Understanding of Context-aware Relevance</title>
      <author><first>Xiaoqiang</first><last>Wang</last></author>
      <author><first>Bang</first><last>Liu</last></author>
      <author><first>Siliang</first><last>Tang</last></author>
      <author><first>Lingfei</first><last>Wu</last></author>
      <pages>562-581</pages>
      <abstract>Existing metrics for assessing question generation not only require costly human reference but also fail to take into account the input context of generation, rendering the lack of deep understanding of the relevance between the generated questions and input contexts. As a result, they may wrongly penalize a legitimate and reasonable candidate question when it (1) involves complicated reasoning with the context or (2) can be grounded by multiple evidences in the context.In this paper, we propose QRelScore, a context-aware Relevance evaluation metric for Question Generation.Based on off-the-shelf language models such as BERT and GPT2, QRelScore employs both word-level hierarchical matching and sentence-level prompt-based generation to cope with the complicated reasoning and diverse generation from multiple evidences, respectively.Compared with existing metrics, our experiments demonstrate that QRelScore is able to achieve a higher correlation with human judgments while being much more robust to adversarial samples.</abstract>
      <url hash="42da9e1e">2022.emnlp-main.37</url>
      <bibkey>wang-etal-2022-qrelscore</bibkey>
    </paper>
    <paper id="38">
      <title>Abstract Visual Reasoning with Tangram Shapes</title>
      <author><first>Anya</first><last>Ji</last></author>
      <author><first>Noriyuki</first><last>Kojima</last></author>
      <author><first>Noah</first><last>Rush</last></author>
      <author><first>Alane</first><last>Suhr</last></author>
      <author><first>Wai Keen</first><last>Vong</last></author>
      <author><first>Robert</first><last>Hawkins</last></author>
      <author><first>Yoav</first><last>Artzi</last></author>
      <pages>582-601</pages>
      <abstract>We introduce KiloGram, a resource for studying abstract visual reasoning in humans and machines. Drawing on the history of tangram puzzles as stimuli in cognitive science, we build a richly annotated dataset that, with &gt;1k distinct stimuli, is orders of magnitude larger and more diverse than prior resources. It is both visually and linguistically richer, moving beyond whole shape descriptions to include segmentation maps and part labels. We use this resource to evaluate the abstract visual reasoning capacities of recent multi-modal models. We observe that pre-trained weights demonstrate limited abstract reasoning, which dramatically improves with fine-tuning. We also observe that explicitly describing parts aids abstract reasoning for both humans and models, especially when jointly encoding the linguistic and visual inputs.</abstract>
      <url hash="69bd853d">2022.emnlp-main.38</url>
      <bibkey>ji-etal-2022-abstract</bibkey>
    </paper>
    <paper id="39">
      <title><fixed-case>U</fixed-case>nified<fixed-case>SKG</fixed-case>: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models</title>
      <author><first>Tianbao</first><last>Xie</last></author>
      <author><first>Chen Henry</first><last>Wu</last></author>
      <author><first>Peng</first><last>Shi</last></author>
      <author><first>Ruiqi</first><last>Zhong</last></author>
      <author><first>Torsten</first><last>Scholak</last></author>
      <author><first>Michihiro</first><last>Yasunaga</last></author>
      <author><first>Chien-Sheng</first><last>Wu</last></author>
      <author><first>Ming</first><last>Zhong</last></author>
      <author><first>Pengcheng</first><last>Yin</last></author>
      <author><first>Sida I.</first><last>Wang</last></author>
      <author><first>Victor</first><last>Zhong</last></author>
      <author><first>Bailin</first><last>Wang</last></author>
      <author><first>Chengzu</first><last>Li</last></author>
      <author><first>Connor</first><last>Boyle</last></author>
      <author><first>Ansong</first><last>Ni</last></author>
      <author><first>Ziyu</first><last>Yao</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <author><first>Lingpeng</first><last>Kong</last></author>
      <author><first>Rui</first><last>Zhang</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <author><first>Tao</first><last>Yu</last></author>
      <pages>602-631</pages>
      <abstract>Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests, such as semantic parsing over databases and question answering over knowledge bases. Since the inputs and outputs of SKG tasks are heterogeneous, they have been studied separately by different communities, which limits systematic and compatible research on SKG. In this paper, we overcome this limitation by proposing the UnifiedSKG framework, which unifies 21 SKG tasks into a text-to-text format, aiming to promote systematic SKG research, instead of being exclusive to a single task, domain, or dataset. We use UnifiedSKG to benchmark T5 with different sizes and show that T5, with simple modifications when necessary, achieves state-of-the-art performance on almost all of the 21 tasks. We further demonstrate that multi-task prefix-tuning improves the performance on most tasks, largely improving the overall performance. UnifiedSKG also facilitates the investigation of zero-shot and few-shot learning, and we show that T0, GPT-3, and Codex struggle in zero-shot and few-shot learning for SKG. We also use UnifiedSKG to conduct a series of controlled experiments on structured knowledge encoding variants across SKG tasks. UnifiedSKG is easily extensible to more tasks, and it is open-sourced at https://github.com/hkunlp/unifiedskg.</abstract>
      <url hash="79e7a5f2">2022.emnlp-main.39</url>
      <bibkey>xie-etal-2022-unifiedskg</bibkey>
    </paper>
    <paper id="40">
      <title>Balanced Adversarial Training: Balancing Tradeoffs between Fickleness and Obstinacy in <fixed-case>NLP</fixed-case> Models</title>
      <author><first>Hannah</first><last>Chen</last></author>
      <author><first>Yangfeng</first><last>Ji</last></author>
      <author><first>David</first><last>Evans</last></author>
      <pages>632-647</pages>
      <abstract>Traditional (fickle) adversarial examples involve finding a small perturbation that does not change an input’s true label but confuses the classifier into outputting a different prediction. Conversely, obstinate adversarial examples occur when an adversary finds a small perturbation that preserves the classifier’s prediction but changes the true label of an input.Adversarial training and certified robust training have shown some effectiveness in improving the robustness of machine learnt models to fickle adversarial examples. We show that standard adversarial training methods focused on reducing vulnerability to fickle adversarial examples may make a model more vulnerable to obstinate adversarial examples, with experiments for both natural language inference and paraphrase identification tasks. To counter this phenomenon, we introduce Balanced Adversarial Training, which incorporates contrastive learning to increase robustness against both fickle and obstinate adversarial examples.</abstract>
      <url hash="161f50e6">2022.emnlp-main.40</url>
      <bibkey>chen-etal-2022-balanced</bibkey>
    </paper>
    <paper id="41">
      <title>When Can Transformers Ground and Compose: Insights from Compositional Generalization Benchmarks</title>
      <author><first>Ankur</first><last>Sikarwar</last></author>
      <author><first>Arkil</first><last>Patel</last></author>
      <author><first>Navin</first><last>Goyal</last></author>
      <pages>648-669</pages>
      <abstract>Humans can reason compositionally whilst grounding language utterances to the real world. Recent benchmarks like ReaSCAN (Wu et al., 2021) use navigation tasks grounded in a grid world to assess whether neural models exhibit similar capabilities. In this work, we present a simple transformer-based model that outperforms specialized architectures on ReaSCAN and a modified version (Qiu et al., 2021) of gSCAN (Ruis et al., 2020). On analyzing the task, we find that identifying the target location in the grid world is the main challenge for the models. Furthermore, we show that a particular split in ReaSCAN, which tests depth generalization, is unfair. On an amended version of this split, we show that transformers can generalize to deeper input structures. Finally, we design a simpler grounded compositional generalization task, RefEx, to investigate how transformers reason compositionally. We show that a single self-attention layer with a single head generalizes to novel combinations of object attributes. Moreover, we derive a precise mathematical construction of the transformer’s computations from the learned network. Overall, we provide valuable insights about the grounded compositional generalization task and the behaviour of transformers on it, which would be useful for researchers working in this area.</abstract>
      <url hash="47225d59">2022.emnlp-main.41</url>
      <bibkey>sikarwar-etal-2022-transformers</bibkey>
    </paper>
    <paper id="42">
      <title>Generative Language Models for Paragraph-Level Question Generation</title>
      <author><first>Asahi</first><last>Ushio</last></author>
      <author><first>Fernando</first><last>Alva-Manchego</last></author>
      <author><first>Jose</first><last>Camacho-Collados</last></author>
      <pages>670-688</pages>
      <abstract>Powerful generative models have led to recent progress in question generation (QG). However, it is difficult to measure advances in QG research since there are no standardized resources that allow a uniform comparison among approaches. In this paper, we introduce QG-Bench, a multilingual and multidomain benchmark for QG that unifies existing question answering datasets by converting them to a standard QG setting. It includes general-purpose datasets such as SQuAD for English, datasets from ten domains and two styles, as well as datasets in eight different languages. Using QG-Bench as a reference, we perform an extensive analysis of the capabilities of language models for the task. First, we propose robust QG baselines based on fine-tuning generative language models. Then, we complement automatic evaluation based on standard metrics with an extensive manual evaluation, which in turn sheds light on the difficulty of evaluating QG models. Finally, we analyse both the domain adaptability of these models as well as the effectiveness of multilingual models in languages other than English.QG-Bench is released along with the fine-tuned models presented in the paper (https://github.com/asahi417/lm-question-generation), which are also available as a demo (https://autoqg.net/).</abstract>
      <url hash="12104166">2022.emnlp-main.42</url>
      <bibkey>ushio-etal-2022-generative</bibkey>
    </paper>
    <paper id="43">
      <title>A Unified Encoder-Decoder Framework with Entity Memory</title>
      <author><first>Zhihan</first><last>Zhang</last></author>
      <author><first>Wenhao</first><last>Yu</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Meng</first><last>Jiang</last></author>
      <pages>689-705</pages>
      <abstract>Entities, as important carriers of real-world knowledge, play a key role in many NLP tasks.We focus on incorporating entity knowledge into an encoder-decoder framework for informative text generation. Existing approaches tried to index, retrieve, and read external documents as evidence, but they suffered from a large computational overhead. In this work, we propose an encoder-decoder framework with an entity memory, namely EDMem. The entity knowledge is stored in the memory as latent representations, and the memory is pre-trained on Wikipedia along with encoder-decoder parameters. To precisely generate entity names, we design three decoding methods to constrain entity generation by linking entities in the memory. EDMem is a unified framework that can be used on various entity-intensive question answering and generation tasks. Extensive experimental results show that EDMem outperforms both memory-based auto-encoder models and non-memory encoder-decoder models.</abstract>
      <url hash="950d3c41">2022.emnlp-main.43</url>
      <bibkey>zhang-etal-2022-unified</bibkey>
    </paper>
    <paper id="44">
      <title>Segmenting Numerical Substitution Ciphers</title>
      <author><first>Nada</first><last>Aldarrab</last></author>
      <author><first>Jonathan</first><last>May</last></author>
      <pages>706-714</pages>
      <abstract>Deciphering historical substitution ciphers is a challenging problem. Example problems that have been previously studied include detecting cipher type, detecting plaintext language, and acquiring the substitution key for segmented ciphers. However, attacking unsegmented ciphers is still a challenging task. Segmentation (i.e. finding substitution units) is essential for cracking those ciphers. In this work, we propose the first automatic methods to segment those ciphers using Byte Pair Encoding (BPE) and unigram language models. Our methods achieve an average segmentation error of 2% on 100 randomly-generated monoalphabetic ciphers and 27% on 3 real historical homophonic ciphers. We also propose a method for solving non-deterministic ciphers with existing keys using a lattice and a pretrained language model. Our method leads to the full solution of the IA cipher; a real historical cipher that has not been fully solved until this work.</abstract>
      <url hash="c2cac4ed">2022.emnlp-main.44</url>
      <bibkey>aldarrab-may-2022-segmenting</bibkey>
    </paper>
    <paper id="45">
      <title>Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset</title>
      <author><first>Ashish V.</first><last>Thapliyal</last></author>
      <author><first>Jordi</first><last>Pont Tuset</last></author>
      <author><first>Xi</first><last>Chen</last></author>
      <author><first>Radu</first><last>Soricut</last></author>
      <pages>715-729</pages>
      <abstract>Research in massively multilingual image captioning has been severely hampered by a lack of high-quality evaluation datasets. In this paper we present the Crossmodal-3600 dataset (XM3600 in short), a geographically diverse set of 3600 images annotated with human-generated reference captions in 36 languages. The images were selected from across the world, covering regions where the 36 languages are spoken, and annotated with captions that achieve consistency in terms of style across all languages, while avoiding annotation artifacts due to direct translation. We apply this benchmark to model selection for massively multilingual image captioning models, and show superior correlation results with human evaluations when using XM3600 as golden references for automatic metrics.</abstract>
      <url hash="18af2a2a">2022.emnlp-main.45</url>
      <bibkey>thapliyal-etal-2022-crossmodal</bibkey>
    </paper>
    <paper id="46">
      <title><fixed-case>R</fixed-case>e<fixed-case>S</fixed-case>el: N-ary Relation Extraction from Scientific Text and Tables by Learning to Retrieve and Select</title>
      <author><first>Yuchen</first><last>Zhuang</last></author>
      <author><first>Yinghao</first><last>Li</last></author>
      <author><first>Junyang</first><last>Zhang</last></author>
      <author><first>Yue</first><last>Yu</last></author>
      <author><first>Yingjun</first><last>Mou</last></author>
      <author><first>Xiang</first><last>Chen</last></author>
      <author><first>Le</first><last>Song</last></author>
      <author><first>Chao</first><last>Zhang</last></author>
      <pages>730-744</pages>
      <abstract>We study the problem of extracting N-ary relation tuples from scientific articles. This task is challenging because the target knowledge tuples can reside in multiple parts and modalities of the document. Our proposed method ReSel decomposes this task into a two-stage procedure that first retrieves the most relevant paragraph/table and then selects the target entity from the retrieved component. For the high-level retrieval stage, ReSel designs a simple and effective feature set, which captures multi-level lexical and semantic similarities between the query and components. For the low-level selection stage, ReSel designs a cross-modal entity correlation graph along with a multi-view architecture, which models both semantic and document-structural relations between entities. Our experiments on three scientific information extraction datasets show that ReSel outperforms state-of-the-art baselines significantly.</abstract>
      <url hash="c3fe5e4c">2022.emnlp-main.46</url>
      <bibkey>zhuang-etal-2022-resel</bibkey>
    </paper>
    <paper id="47">
      <title><fixed-case>G</fixed-case>amma<fixed-case>E</fixed-case>: Gamma Embeddings for Logical Queries on Knowledge Graphs</title>
      <author><first>Dong</first><last>Yang</last></author>
      <author><first>Peijun</first><last>Qing</last></author>
      <author><first>Yang</first><last>Li</last></author>
      <author><first>Haonan</first><last>Lu</last></author>
      <author><first>Xiaodong</first><last>Lin</last></author>
      <pages>745-760</pages>
      <abstract>Embedding knowledge graphs (KGs) for multi-hop logical reasoning is a challenging problem due to massive and complicated structures in many KGs. Recently, many promising works projected entities and queries into a geometric space to efficiently find answers. However, it remains challenging to model the negation and union operator. The negation operator has no strict boundaries, which generates overlapped embeddings and leads to obtaining ambiguous answers. An additional limitation is that the union operator is non-closure, which undermines the model to handle a series of union operators. To address these problems, we propose a novel probabilistic embedding model, namely Gamma Embeddings (GammaE), for encoding entities and queries to answer different types of FOL queries on KGs. We utilize the linear property and strong boundary support of the Gamma distribution to capture more features of entities and queries, which dramatically reduces model uncertainty. Furthermore, GammaE implements the Gamma mixture method to design the closed union operator. The performance of GammaE is validated on three large logical query datasets. Experimental results show that GammaE significantly outperforms state-of-the-art models on public benchmarks.</abstract>
      <url hash="b460d1e8">2022.emnlp-main.47</url>
      <bibkey>yang-etal-2022-gammae</bibkey>
    </paper>
    <paper id="48">
      <title>Reasoning Like Program Executors</title>
      <author><first>Xinyu</first><last>Pi</last></author>
      <author><first>Qian</first><last>Liu</last></author>
      <author><first>Bei</first><last>Chen</last></author>
      <author><first>Morteza</first><last>Ziyadi</last></author>
      <author><first>Zeqi</first><last>Lin</last></author>
      <author><first>Qiang</first><last>Fu</last></author>
      <author><first>Yan</first><last>Gao</last></author>
      <author><first>Jian-Guang</first><last>Lou</last></author>
      <author><first>Weizhu</first><last>Chen</last></author>
      <pages>761-779</pages>
      <abstract>Reasoning over natural language is a long-standing goal for the research community. However, studies have shown that existing language models are inadequate in reasoning. To address the issue, we present POET, a novel reasoning pre-training paradigm. Through pre-training language models with programs and their execution results, POET empowers language models to harvest the reasoning knowledge possessed by program executors via a data-driven approach. POET is conceptually simple and can be instantiated by different kinds of program executors. In this paper, we showcase two simple instances POET-Math and POET-Logic, in addition to a complex instance, POET-SQL. Experimental results on six benchmarks demonstrate that POET can significantly boost model performance in natural language reasoning, such as numerical reasoning, logical reasoning, and multi-hop reasoning. POET opens a new gate on reasoning-enhancement pre-training, and we hope our analysis would shed light on the future research of reasoning like program executors.</abstract>
      <url hash="527d232a">2022.emnlp-main.48</url>
      <bibkey>pi-etal-2022-reasoning</bibkey>
    </paper>
    <paper id="49">
      <title><fixed-case>SEM</fixed-case>-F1: an Automatic Way for Semantic Evaluation of Multi-Narrative Overlap Summaries at Scale</title>
      <author><first>Naman</first><last>Bansal</last></author>
      <author><first>Mousumi</first><last>Akter</last></author>
      <author><first>Shubhra Kanti</first><last>Karmaker Santu</last></author>
      <pages>780-792</pages>
      <abstract>Recent work has introduced an important yet relatively under-explored NLP task called Semantic Overlap Summarization (SOS) that entails generating a summary from multiple alternative narratives which conveys the common information provided by those narratives. Previous work also published a benchmark dataset for this task by collecting 2,925 alternative narrative pairs from the web and manually annotating 411 different reference summaries by engaging human annotators. In this paper, we exclusively focus on the automated evaluation of the SOS task using the benchmark dataset. More specifically, we first use the popular ROUGE metric from text-summarization literature and conduct a systematic study to evaluate the SOS task. Our experiments discover that ROUGE is not suitable for this novel task and therefore, we propose a new sentence-level precision-recall style automated evaluation metric, called SEM-F1 (Semantic F1). It is inspired by the benefits of the sentence-wise annotation technique using overlap labels reported by the previous work. Our experiments show that the proposed SEM-F1 metric yields a higher correlation with human judgment and higher inter-rater agreement compared to the ROUGE metric.</abstract>
      <url hash="61af40f4">2022.emnlp-main.49</url>
      <bibkey>bansal-etal-2022-sem</bibkey>
    </paper>
    <paper id="50">
      <title>Inducer-tuning: Connecting Prefix-tuning and Adapter-tuning</title>
      <author><first>Yifan</first><last>Chen</last></author>
      <author><first>Devamanyu</first><last>Hazarika</last></author>
      <author><first>Mahdi</first><last>Namazifar</last></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last></author>
      <author><first>Di</first><last>Jin</last></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last></author>
      <pages>793-808</pages>
      <abstract>Prefix-tuning, or more generally continuous prompt tuning, has become an essential paradigm of parameter-efficient transfer learning. Using a large pre-trained language model (PLM), prefix-tuning can obtain strong performance by training only a small portion of parameters. In this paper, we propose to understand and further develop prefix-tuning through the kernel lens. Specifically, we make an analogy between <i>prefixes</i> and <i>inducing variables</i> in kernel methods and hypothesize that <i>prefixes</i> serving as <i>inducing variables</i> would improve their overall mechanism. From the kernel estimator perspective, we suggest a new variant of prefix-tuning—<i>inducer-tuning</i>, which shares the exact mechanism as prefix-tuning while leveraging the residual form found in adapter-tuning. This mitigates the initialization issue in prefix-tuning. Through comprehensive empirical experiments on natural language understanding and generation tasks, we demonstrate that inducer-tuning can close the performance gap between prefix-tuning and fine-tuning.</abstract>
      <url hash="816c1cd4">2022.emnlp-main.50</url>
      <bibkey>chen-etal-2022-inducer</bibkey>
    </paper>
    <paper id="51">
      <title><fixed-case>D</fixed-case>oc<fixed-case>I</fixed-case>nfer: Document-level Natural Language Inference using Optimal Evidence Selection</title>
      <author><first>Puneet</first><last>Mathur</last></author>
      <author><first>Gautam</first><last>Kunapuli</last></author>
      <author><first>Riyaz</first><last>Bhat</last></author>
      <author><first>Manish</first><last>Shrivastava</last></author>
      <author><first>Dinesh</first><last>Manocha</last></author>
      <author><first>Maneesh</first><last>Singh</last></author>
      <pages>809-824</pages>
      <abstract>We present DocInfer - a novel, end-to-end Document-level Natural Language Inference model that builds a hierarchical document graph enriched through inter-sentence relations (topical, entity-based, concept-based), performs paragraph pruning using the novel SubGraph Pooling layer, followed by optimal evidence selection based on REINFORCE algorithm to identify the most important context sentences for a given hypothesis. Our evidence selection mechanism allows it to transcend the input length limitation of modern BERT-like Transformer models while presenting the entire evidence together for inferential reasoning. We show this is an important property needed to reason on large documents where the evidence may be fragmented and located arbitrarily far from each other. Extensive experiments on popular corpora - DocNLI, ContractNLI, and ConTRoL datasets, and our new proposed dataset called CaseHoldNLI on the task of legal judicial reasoning, demonstrate significant performance gains of 8-12% over SOTA methods. Our ablation studies validate the impact of our model. Performance improvement of 3-6% on annotation-scarce downstream tasks of fact verification, multiple-choice QA, and contract clause retrieval demonstrates the usefulness of DocInfer beyond primary NLI tasks.</abstract>
      <url hash="08ab4a7b">2022.emnlp-main.51</url>
      <bibkey>mathur-etal-2022-docinfer</bibkey>
    </paper>
    <paper id="52">
      <title><fixed-case>L</fixed-case>ight<fixed-case>EA</fixed-case>: A Scalable, Robust, and Interpretable Entity Alignment Framework via Three-view Label Propagation</title>
      <author><first>Xin</first><last>Mao</last></author>
      <author><first>Wenting</first><last>Wang</last></author>
      <author><first>Yuanbin</first><last>Wu</last></author>
      <author><first>Man</first><last>Lan</last></author>
      <pages>825-838</pages>
      <abstract>Entity Alignment (EA) aims to find equivalent entity pairs between KGs, which is the core step to bridging and integrating multi-source KGs. In this paper, we argue that existing complex EA methods inevitably inherit the inborn defects from their neural network lineage: poor interpretability and weak scalability. Inspired by recent studies, we reinvent the classical Label Propagation algorithm to effectively run on KGs and propose a neural-free EA framework — LightEA, consisting of three efficient components: (i) Random Orthogonal Label Generation, (ii) Three-view Label Propagation, and (iii) Sparse Sinkhorn Operation.According to the extensive experiments on public datasets, LightEA has impressive scalability, robustness, and interpretability. With a mere tenth of time consumption, LightEA achieves comparable results to state-of-the-art methods across all datasets and even surpasses them on many. Besides, due to the computational process of LightEA being entirely linear, we could trace the propagation process at each step and clearly explain how the entities are aligned.</abstract>
      <url hash="619e9b01">2022.emnlp-main.52</url>
      <bibkey>mao-etal-2022-lightea</bibkey>
    </paper>
    <paper id="53">
      <title>Metric-guided Distillation: Distilling Knowledge from the Metric to Ranker and Retriever for Generative Commonsense Reasoning</title>
      <author><first>Xingwei</first><last>He</last></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <author><first>A-Long</first><last>Jin</last></author>
      <author><first>Weizhen</first><last>Qi</last></author>
      <author><first>Hang</first><last>Zhang</last></author>
      <author><first>Jian</first><last>Jiao</last></author>
      <author><first>Bartuer</first><last>Zhou</last></author>
      <author><first>Biao</first><last>Cheng</last></author>
      <author><first>Sm</first><last>Yiu</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <pages>839-852</pages>
      <abstract>Commonsense generation aims to generate a realistic sentence describing a daily scene under the given concepts, which is very challenging, since it requires models to have relational reasoning and compositional generalization capabilities. Previous work focuses on retrieving prototype sentences for the provided concepts to assist generation. They first use a sparse retriever to retrieve candidate sentences, then re-rank the candidates with a ranker. However, the candidates returned by their ranker may not be the most relevant sentences, since the ranker treats all candidates equally without considering their relevance to the reference sentences of the given concepts. Another problem is that re-ranking is very expensive, but only using retrievers will seriously degrade the performance of their generation models. To solve these problems, we propose the metric distillation rule to distill knowledge from the metric (e.g., BLEU) to the ranker. We further transfer the critical knowledge summarized by the distilled ranker to the retriever. In this way, the relevance scores of candidate sentences predicted by the ranker and retriever will be more consistent with their quality measured by the metric. Experimental results on the CommonGen benchmark verify the effectiveness of our proposed method: (1) Our generation model with the distilled ranker achieves a new state-of-the-art result. (2) Our generation model with the distilled retriever even surpasses the previous SOTA.</abstract>
      <url hash="f4a54e60">2022.emnlp-main.53</url>
      <bibkey>he-etal-2022-metric</bibkey>
    </paper>
    <paper id="54">
      <title>Efficient Document Retrieval by End-to-End Refining and Quantizing <fixed-case>BERT</fixed-case> Embedding with Contrastive Product Quantization</title>
      <author><first>Zexuan</first><last>Qiu</last></author>
      <author><first>Qinliang</first><last>Su</last></author>
      <author><first>Jianxing</first><last>Yu</last></author>
      <author><first>Shijing</first><last>Si</last></author>
      <pages>853-863</pages>
      <abstract>Efficient document retrieval heavily relies on the technique of semantic hashing, which learns a binary code for every document and employs Hamming distance to evaluate document distances. However, existing semantic hashing methods are mostly established on outdated TFIDF features, which obviously do not contain lots of important semantic information about documents. Furthermore, the Hamming distance can only be equal to one of several integer values, significantly limiting its representational ability for document distances. To address these issues, in this paper, we propose to leverage BERT embeddings to perform efficient retrieval based on the product quantization technique, which will assign for every document a real-valued codeword from the codebook, instead of a binary code as in semantic hashing. Specifically, we first transform the original BERT embeddings via a learnable mapping and feed the transformed embedding into a probabilistic product quantization module to output the assigned codeword. The refining and quantizing modules can be optimized in an end-to-end manner by minimizing the probabilistic contrastive loss. A mutual information maximization based method is further proposed to improve the representativeness of codewords, so that documents can be quantized more accurately. Extensive experiments conducted on three benchmarks demonstrate that our proposed method significantly outperforms current state-of-the-art baselines.</abstract>
      <url hash="34e99b4e">2022.emnlp-main.54</url>
      <bibkey>qiu-etal-2022-efficient</bibkey>
    </paper>
    <paper id="55">
      <title>Curriculum Knowledge Distillation for Emoji-supervised Cross-lingual Sentiment Analysis</title>
      <author><first>Jianyang</first><last>Zhang</last></author>
      <author><first>Tao</first><last>Liang</last></author>
      <author><first>Mingyang</first><last>Wan</last></author>
      <author><first>Guowu</first><last>Yang</last></author>
      <author><first>Fengmao</first><last>Lv</last></author>
      <pages>864-875</pages>
      <abstract>Existing sentiment analysis models have achieved great advances with the help of sufficient sentiment annotations. Unfortunately, many languages do not have sufficient sentiment corpus. To this end, recent studies have proposed cross-lingual sentiment analysis to transfer sentiment analysis models from resource-rich languages to low-resource languages. However, these studies either rely on external cross-lingual supervision (e.g., parallel corpora and translation model), or are limited by the cross-lingual gaps. In this work, based on the intuitive assumption that the relationships between emojis and sentiments are consistent across different languages, we investigate transferring sentiment knowledge across languages with the help of emojis. To this end, we propose a novel cross-lingual sentiment analysis approach dubbed Curriculum Knowledge Distiller (CKD). The core idea of CKD is to use emojis to bridge the source and target languages. Note that, compared with texts, emojis are more transferable, but cannot reveal the precise sentiment. Thus, we distill multiple Intermediate Sentiment Classifiers (ISC) on source language corpus with emojis to get ISCs with different attention weights of texts. To transfer them into the target language, we distill ISCs into the Target Language Sentiment Classifier (TSC) following the curriculum learning mechanism. In this way, TSC can learn delicate sentiment knowledge, meanwhile, avoid being affected by cross-lingual gaps. Experimental results on five cross-lingual benchmarks clearly verify the effectiveness of our approach.</abstract>
      <url hash="a34b8832">2022.emnlp-main.55</url>
      <bibkey>zhang-etal-2022-curriculum</bibkey>
    </paper>
    <paper id="56">
      <title>Correctable-<fixed-case>DST</fixed-case>: Mitigating Historical Context Mismatch between Training and Inference for Improved Dialogue State Tracking</title>
      <author><first>Hongyan</first><last>Xie</last></author>
      <author><first>Haoxiang</first><last>Su</last></author>
      <author><first>Shuangyong</first><last>Song</last></author>
      <author><first>Hao</first><last>Huang</last></author>
      <author><first>Bo</first><last>Zou</last></author>
      <author><first>Kun</first><last>Deng</last></author>
      <author><first>Jianghua</first><last>Lin</last></author>
      <author><first>Zhihui</first><last>Zhang</last></author>
      <author><first>Xiaodong</first><last>He</last></author>
      <pages>876-889</pages>
      <abstract>Recently proposed dialogue state tracking (DST) approaches predict the dialogue state of a target turn sequentially based on the previous dialogue state. During the training time, the ground-truth previous dialogue state is utilized as the historical context. However, only the previously predicted dialogue state can be used in inference. This discrepancy might lead to error propagation, i.e., mistakes made by the model in the current turn are likely to be carried over to the following turns.To solve this problem, we propose Correctable Dialogue State Tracking (Correctable-DST). Specifically, it consists of three stages: (1) a Predictive State Simulator is exploited to generate a previously “predicted” dialogue state based on the ground-truth previous dialogue state during training; (2) a Slot Detector is proposed to determine the slots with an incorrect value in the previously “predicted” state and the slots whose values are to be updated in the current turn; (3) a State Generator takes the name of the above-selected slots as a prompt to generate the current state.Empirical results show that our approach achieves 67.51%, 68.24%, 70.30%, 71.38%, and 81.27% joint goal accuracy on MultiWOZ 2.0-2.4 datasets, respectively, and achieves a new state-of-the-art performance with significant improvements.</abstract>
      <url hash="02413be4">2022.emnlp-main.56</url>
      <bibkey>xie-etal-2022-correctable</bibkey>
    </paper>
    <paper id="57">
      <title><fixed-case>D</fixed-case>rop<fixed-case>M</fixed-case>ix: A Textual Data Augmentation Combining Dropout with Mixup</title>
      <author><first>Fanshuang</first><last>Kong</last></author>
      <author><first>Richong</first><last>Zhang</last></author>
      <author><first>Xiaohui</first><last>Guo</last></author>
      <author><first>Samuel</first><last>Mensah</last></author>
      <author><first>Yongyi</first><last>Mao</last></author>
      <pages>890-899</pages>
      <abstract>Overfitting is a notorious problem when there is insufficient data to train deep neural networks in machine learning tasks. Data augmentation regularization methods such as Dropout, Mixup, and their enhanced variants are effective and prevalent, and achieve promising performance to overcome overfitting. However, in text learning, most of the existing regularization approaches merely adopt ideas from computer vision without considering the importance of dimensionality in natural language processing. In this paper, we argue that the property is essential to overcome overfitting in text learning. Accordingly, we present a saliency map informed textual data augmentation and regularization framework, which combines Dropout and Mixup, namely DropMix, to mitigate the overfitting problem in text learning. In addition, we design a procedure that drops and patches fine grained shapes of the saliency map under the DropMix framework to enhance regularization. Empirical studies confirm the effectiveness of the proposed approach on 12 text classification tasks.</abstract>
      <url hash="ca139e52">2022.emnlp-main.57</url>
      <bibkey>kong-etal-2022-dropmix</bibkey>
    </paper>
    <paper id="58">
      <title>Cross-document Event Coreference Search: Task, Dataset and Modeling</title>
      <author><first>Alon</first><last>Eirew</last></author>
      <author><first>Avi</first><last>Caciularu</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <pages>900-913</pages>
      <abstract>The task of Cross-document Coreference Resolution has been traditionally formulated as requiring to identify all coreference links across a given set of documents. We propose an appealing, and often more applicable, complementary set up for the task – Cross-document Coreference Search, focusing in this paper on event coreference. Concretely, given a mention in context of an event of interest, considered as a query, the task is to find all coreferring mentions for the query event in a large document collection. To support research on this task, we create a corresponding dataset, which is derived from Wikipedia while leveraging annotations in the available Wikipedia Event Coreferecene dataset (WEC-Eng). Observing that the coreference search setup is largely analogous to the setting of Open Domain Question Answering, we adapt the prominent Deep Passage Retrieval (DPR) model to our setting, as an appealing baseline. Finally, we present a novel model that integrates a powerful coreference scoring scheme into the DPR architecture, yielding improved performance.</abstract>
      <url hash="99970b4c">2022.emnlp-main.58</url>
      <bibkey>eirew-etal-2022-cross</bibkey>
    </paper>
    <paper id="59">
      <title><fixed-case>VIRT</fixed-case>: Improving Representation-based Text Matching via Virtual Interaction</title>
      <author><first>Dan</first><last>Li</last></author>
      <author><first>Yang</first><last>Yang</last></author>
      <author><first>Hongyin</first><last>Tang</last></author>
      <author><first>Jiahao</first><last>Liu</last></author>
      <author><first>Qifan</first><last>Wang</last></author>
      <author><first>Jingang</first><last>Wang</last></author>
      <author><first>Tong</first><last>Xu</last></author>
      <author><first>Wei</first><last>Wu</last></author>
      <author><first>Enhong</first><last>Chen</last></author>
      <pages>914-925</pages>
      <abstract>Text matching is a fundamental research problem in natural language understanding. Interaction-based approaches treat the text pair as a single sequence and encode it through cross encoders, while representation-based models encode the text pair independently with siamese or dual encoders. Interaction-based models require dense computations and thus are impractical in real-world applications. Representation-based models have become the mainstream paradigm for efficient text matching. However, these models suffer from severe performance degradation due to the lack of interactions between the pair of texts. To remedy this, we propose a Virtual InteRacTion mechanism (VIRT) for improving representation-based text matching while maintaining its efficiency. In particular, we introduce an interactive knowledge distillation module that is only applied during training. It enables deep interaction between texts by effectively transferring knowledge from the interaction-based model. A light interaction strategy is designed to fully leverage the learned interactive knowledge. Experimental results on six text matching benchmarks demonstrate the superior performance of our method over several state-of-the-art representation-based models. We further show that VIRT can be integrated into existing methods as plugins to lift their performances.</abstract>
      <url hash="bd86ab18">2022.emnlp-main.59</url>
      <bibkey>li-etal-2022-virt</bibkey>
    </paper>
    <paper id="60">
      <title><fixed-case>MAVEN</fixed-case>-<fixed-case>ERE</fixed-case>: A Unified Large-scale Dataset for Event Coreference, Temporal, Causal, and Subevent Relation Extraction</title>
      <author><first>Xiaozhi</first><last>Wang</last></author>
      <author><first>Yulin</first><last>Chen</last></author>
      <author><first>Ning</first><last>Ding</last></author>
      <author><first>Hao</first><last>Peng</last></author>
      <author><first>Zimu</first><last>Wang</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Xu</first><last>Han</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>926-941</pages>
      <abstract>The diverse relationships among real-world events, including coreference, temporal, causal, and subevent relations, are fundamental to understanding natural languages. However, two drawbacks of existing datasets limit event relation extraction (ERE) tasks: (1) Small scale. Due to the annotation complexity, the data scale of existing datasets is limited, which cannot well train and evaluate data-hungry models. (2) Absence of unified annotation. Different types of event relations naturally interact with each other, but existing datasets only cover limited relation types at once, which prevents models from taking full advantage of relation interactions. To address these issues, we construct a unified large-scale human-annotated ERE dataset MAVEN-ERE with improved annotation schemes. It contains 103,193 event coreference chains, 1,216,217 temporal relations, 57,992 causal relations, and 15,841 subevent relations, which is larger than existing datasets of all the ERE tasks by at least an order of magnitude. Experiments show that ERE on MAVEN-ERE is quite challenging, and considering relation interactions with joint learning can improve performances. The dataset and source codes can be obtained from https://github.com/THU-KEG/MAVEN-ERE.</abstract>
      <url hash="16864774">2022.emnlp-main.60</url>
      <bibkey>wang-etal-2022-maven</bibkey>
    </paper>
    <paper id="61">
      <title>Entity Extraction in Low Resource Domains with Selective Pre-training of Large Language Models</title>
      <author><first>Aniruddha</first><last>Mahapatra</last></author>
      <author><first>Sharmila Reddy</first><last>Nangi</last></author>
      <author><first>Aparna</first><last>Garimella</last></author>
      <author><first>Anandhavelu</first><last>N</last></author>
      <pages>942-951</pages>
      <abstract>Transformer-based language models trained on large natural language corpora have been very useful in downstream entity extraction tasks. However, they often result in poor performances when applied to domains that are different from those they are pretrained on. Continued pretraining using unlabeled data from target domains can help improve the performances of these language models on the downstream tasks. However, using all of the available unlabeled data for pretraining can be time-intensive; also, it can be detrimental to the performance of the downstream tasks, if the unlabeled data is not aligned with the data distribution for the target tasks. Previous works employed external supervision in the form of ontologies for selecting appropriate data samples for pretraining, but external supervision can be quite hard to obtain in low-resource domains. In this paper, we introduce effective ways to select data from unlabeled corpora of target domains for language model pretraining to improve the performances in target entity extraction tasks. Our data selection strategies do not require any external supervision. We conduct extensive experiments for the task of named entity recognition (NER) on seven different domains and show that language models pretrained on target domain unlabeled data obtained using our data selection strategies achieve better performances compared to those using data selection strategies in previous works that use external supervision. We also show that these pretrained language models using our data selection strategies outperform those pretrained on all of the available unlabeled target domain data.</abstract>
      <url hash="301e25ef">2022.emnlp-main.61</url>
      <bibkey>mahapatra-etal-2022-entity</bibkey>
    </paper>
    <paper id="62">
      <title>How Large Language Models are Transforming Machine-Paraphrase Plagiarism</title>
      <author><first>Jan Philip</first><last>Wahle</last></author>
      <author><first>Terry</first><last>Ruas</last></author>
      <author><first>Frederic</first><last>Kirstein</last></author>
      <author><first>Bela</first><last>Gipp</last></author>
      <pages>952-963</pages>
      <abstract>The recent success of large language models for text generation poses a severe threat to academic integrity, as plagiarists can generate realistic paraphrases indistinguishable from original work.However, the role of large autoregressive models in generating machine-paraphrased plagiarism and their detection is still incipient in the literature.This work explores T5 and GPT3 for machine-paraphrase generation on scientific articles from arXiv, student theses, and Wikipedia.We evaluate the detection performance of six automated solutions and one commercial plagiarism detection software and perform a human study with 105 participants regarding their detection performance and the quality of generated examples.Our results suggest that large language models can rewrite text humans have difficulty identifying as machine-paraphrased (53% mean acc.).Human experts rate the quality of paraphrases generated by GPT-3 as high as original texts (clarity 4.0/5, fluency 4.2/5, coherence 3.8/5).The best-performing detection model (GPT-3) achieves 66% F1-score in detecting paraphrases.We make our code, data, and findings publicly available to facilitate the development of detection solutions.</abstract>
      <url hash="c033400b">2022.emnlp-main.62</url>
      <bibkey>wahle-etal-2022-large</bibkey>
    </paper>
    <paper id="63">
      <title><fixed-case>M</fixed-case>2<fixed-case>D</fixed-case>2: A Massively Multi-Domain Language Modeling Dataset</title>
      <author><first>Machel</first><last>Reid</last></author>
      <author><first>Victor</first><last>Zhong</last></author>
      <author><first>Suchin</first><last>Gururangan</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <pages>964-975</pages>
      <abstract>We present M2D2, a fine-grained, massively multi-domain corpus for studying domain adaptation in language models (LMs). M2D2 consists of 8.5B tokens and spans 145 domains extracted from Wikipedia and Semantic Scholar. Using ontologies derived from Wikipedia and ArXiv categories, we organize the domains in each data source into 22 groups. This two-level hierarchy enables the study of relationships between domains and their effects on in- and out-of-domain performance after adaptation. We also present a number of insights into the nature of effective domain adaptation in LMs, as examples of the new types of studies M2D2 enables. To improve in-domain performance, we show the benefits of adapting the LM along a domain hierarchy; adapting to smaller amounts of fine-grained domain-specific data can lead to larger in-domain performance gains than larger amounts of weakly relevant data. We further demonstrate a trade-off between in-domain specialization and out-of-domain generalization within and across ontologies, as well as a strong correlation between out-of-domain performance and lexical overlap between domains.</abstract>
      <url hash="195099c5">2022.emnlp-main.63</url>
      <bibkey>reid-etal-2022-m2d2</bibkey>
    </paper>
    <paper id="64">
      <title>“Will You Find These Shortcuts?” A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification</title>
      <author><first>Jasmijn</first><last>Bastings</last></author>
      <author><first>Sebastian</first><last>Ebert</last></author>
      <author><first>Polina</first><last>Zablotskaia</last></author>
      <author><first>Anders</first><last>Sandholm</last></author>
      <author><first>Katja</first><last>Filippova</last></author>
      <pages>976-991</pages>
      <abstract>Feature attribution a.k.a. input salience methods which assign an importance score to a feature are abundant but may produce surprisingly different results for the same model on the same input. While differences are expected if disparate definitions of importance are assumed, most methods claim to provide faithful attributions and point at the features most relevant for a model’s prediction. Existing work on faithfulness evaluation is not conclusive and does not provide a clear answer as to how different methods are to be compared.Focusing on text classification and the model debugging scenario, our main contribution is a protocol for faithfulness evaluation that makes use of partially synthetic data to obtain ground truth for feature importance ranking. Following the protocol, we do an in-depth analysis of four standard salience method classes on a range of datasets and lexical shortcuts for BERT and LSTM models. We demonstrate that some of the most popular method configurations provide poor results even for simple shortcuts while a method judged to be too simplistic works remarkably well for BERT.</abstract>
      <url hash="7092b931">2022.emnlp-main.64</url>
      <bibkey>bastings-etal-2022-will</bibkey>
    </paper>
    <paper id="65">
      <title>Information-Transport-based Policy for Simultaneous Translation</title>
      <author><first>Shaolei</first><last>Zhang</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <pages>992-1013</pages>
      <abstract>Simultaneous translation (ST) outputs translation while receiving the source inputs, and hence requires a policy to determine whether to translate a target token or wait for the next source token. The major challenge of ST is that each target token can only be translated based on the current received source tokens, where the received source information will directly affect the translation quality. So naturally, how much source information is received for the translation of the current target token is supposed to be the pivotal evidence for the ST policy to decide between translating and waiting. In this paper, we treat the translation as information transport from source to target and accordingly propose an Information-Transport-based Simultaneous Translation (ITST). ITST quantifies the transported information weight from each source token to the current target token, and then decides whether to translate the target token according to its accumulated received information. Experiments on both text-to-text ST and speech-to-text ST (a.k.a., streaming speech translation) tasks show that ITST outperforms strong baselines and achieves state-of-the-art performance.</abstract>
      <url hash="50a579c0">2022.emnlp-main.65</url>
      <bibkey>zhang-feng-2022-information</bibkey>
    </paper>
    <paper id="66">
      <title>Learning to Adapt to Low-Resource Paraphrase Generation</title>
      <author><first>Zhigen</first><last>Li</last></author>
      <author><first>Yanmeng</first><last>Wang</last></author>
      <author><first>Rizhao</first><last>Fan</last></author>
      <author><first>Ye</first><last>Wang</last></author>
      <author><first>Jianfeng</first><last>Li</last></author>
      <author><first>Shaojun</first><last>Wang</last></author>
      <pages>1014-1022</pages>
      <abstract>Paraphrase generation is a longstanding NLP task and achieves great success with the aid of large corpora. However, transferring a paraphrasing model to another domain encounters the problem of domain shifting especially when the data is sparse. At the same time, widely using large pre-trained language models (PLMs) faces the overfitting problem when training on scarce labeled data. To mitigate these two issues, we propose, LAPA, an effective adapter for PLMs optimized by meta-learning. LAPA has three-stage training on three types of related resources to solve this problem: 1. pre-training PLMs on unsupervised corpora, 2. inserting an adapter layer and meta-training on source domain labeled data, and 3. fine-tuning adapters on a small amount of target domain labeled data. This method enables paraphrase generation models to learn basic language knowledge first, then learn the paraphrasing task itself later, and finally adapt to the target task. Our experimental results demonstrate that LAPA achieves state-of-the-art in supervised, unsupervised, and low-resource settings on three benchmark datasets. With only 2% of trainable parameters and 1% labeled data of the target task, our approach can achieve a competitive performance with previous work.</abstract>
      <url hash="98dd7452">2022.emnlp-main.66</url>
      <bibkey>li-etal-2022-learning-adapt</bibkey>
    </paper>
    <paper id="67">
      <title>A Distributional Lens for Multi-Aspect Controllable Text Generation</title>
      <author><first>Yuxuan</first><last>Gu</last></author>
      <author><first>Xiaocheng</first><last>Feng</last></author>
      <author><first>Sicheng</first><last>Ma</last></author>
      <author><first>Lingyuan</first><last>Zhang</last></author>
      <author><first>Heng</first><last>Gong</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <pages>1023-1043</pages>
      <abstract>Multi-aspect controllable text generation is a more challenging and practical task than single-aspect control. Existing methods achieve complex multi-aspect control by fusing multiple controllers learned from single-aspect, but suffer from attribute degeneration caused by the mutual interference of these controllers. To address this, we provide observations on attribute fusion from a distributional perspective and propose to directly search for the intersection areas of multiple attribute distributions as their combination for generation. Our method first estimates the attribute space with an autoencoder structure. Afterward, we iteratively approach the intersections by jointly minimizing distances to points representing different attributes. Finally, we map them to attribute-relevant sentences with a prefix-tuning-based decoder. Experiments on the three-aspect control task, including sentiment, topic, and detoxification aspects, reveal that our method outperforms several strong baselines on attribute relevance and text quality and achieves the SOTA. Further analysis also supplies some explanatory support for the effectiveness of our approach.</abstract>
      <url hash="e356fcb7">2022.emnlp-main.67</url>
      <bibkey>gu-etal-2022-distributional</bibkey>
    </paper>
    <paper id="68">
      <title><fixed-case>ELMER</fixed-case>: A Non-Autoregressive Pre-trained Language Model for Efficient and Effective Text Generation</title>
      <author><first>Junyi</first><last>Li</last></author>
      <author><first>Tianyi</first><last>Tang</last></author>
      <author><first>Wayne Xin</first><last>Zhao</last></author>
      <author><first>Jian-Yun</first><last>Nie</last></author>
      <author><first>Ji-Rong</first><last>Wen</last></author>
      <pages>1044-1058</pages>
      <abstract>We study the text generation task under the approach of pre-trained language models (PLMs). Typically, an auto-regressive (AR) method is adopted for generating texts in a token-by-token manner. Despite many advantages of AR generation, it usually suffers from inefficient inference. Therefore, non-autoregressive (NAR) models are proposed to generate all target tokens simultaneously. However, NAR models usually generate texts of lower quality due to the absence of token dependency in the output text. In this paper, we propose ELMER: an efficient and effective PLM for NAR text generation to explicitly model the token dependency during NAR generation. By leveraging the early exit technique, ELMER enables the token generations at different layers, according to their prediction confidence (a more confident token will exit at a lower layer). Besides, we propose a novel pre-training objective, Layer Permutation Language Modeling, to pre-train ELMER by permuting the exit layer for each token in sequences. Experiments on three text generation tasks show that ELMER significantly outperforms NAR models and further narrows the performance gap with AR PLMs (ELMER (29.92) vs BART (30.61) ROUGE-L in XSUM) while achieving over 10 times inference speedup.</abstract>
      <url hash="fcd35de5">2022.emnlp-main.68</url>
      <bibkey>li-etal-2022-elmer</bibkey>
    </paper>
    <paper id="69">
      <title>Multilingual Relation Classification via Efficient and Effective Prompting</title>
      <author><first>Yuxuan</first><last>Chen</last></author>
      <author><first>David</first><last>Harbecke</last></author>
      <author><first>Leonhard</first><last>Hennig</last></author>
      <pages>1059-1075</pages>
      <abstract>Prompting pre-trained language models has achieved impressive performance on various NLP tasks, especially in low data regimes. Despite the success of prompting in monolingual settings, applying prompt-based methods in multilingual scenarios has been limited to a narrow set of tasks, due to the high cost of handcrafting multilingual prompts. In this paper, we present the first work on prompt-based multilingual relation classification (RC), by introducing an efficient and effective method that constructs prompts from relation triples and involves only minimal translation for the class labels. We evaluate its performance in fully supervised, few-shot and zero-shot scenarios, and analyze its effectiveness across 14 languages, prompt variants, and English-task training in cross-lingual settings. We find that in both fully supervised and few-shot scenarios, our prompt method beats competitive baselines: fine-tuning XLM-R_EM and null prompts. It also outperforms the random baseline by a large margin in zero-shot experiments. Our method requires little in-language knowledge and can be used as a strong baseline for similar multilingual classification tasks.</abstract>
      <url hash="a7e372b6">2022.emnlp-main.69</url>
      <bibkey>chen-etal-2022-multilingual</bibkey>
    </paper>
    <paper id="70">
      <title>Topic-Regularized Authorship Representation Learning</title>
      <author><first>Jitkapat</first><last>Sawatphol</last></author>
      <author><first>Nonthakit</first><last>Chaiwong</last></author>
      <author><first>Can</first><last>Udomcharoenchaikit</last></author>
      <author><first>Sarana</first><last>Nutanong</last></author>
      <pages>1076-1082</pages>
      <abstract>Authorship attribution is a task that aims to identify the author of a given piece of writing. We aim to develop a generalized solution that can handle a large number of texts from authors and topics unavailable in training data. Previous studies have proposed strategies to address only either unseen authors or unseen topics. Authorship representation learning has been shown to work in open-set environments with a large number of unseen authors but has not been explicitly designed for cross-topic environments at the same time. To handle a large number of unseen authors and topics, we propose Authorship Representation Regularization (ARR), a distillation framework that creates authorship representation with reduced reliance on topic-specific information. To assess the performance of our framework, we also propose a cross-topic-open-set evaluation method. Our proposed method has improved performances in the cross-topic-open set setup over baselines in 4 out of 6 cases.</abstract>
      <url hash="b0eb68c2">2022.emnlp-main.70</url>
      <bibkey>sawatphol-etal-2022-topic</bibkey>
    </paper>
    <paper id="71">
      <title>Fine-grained Contrastive Learning for Relation Extraction</title>
      <author><first>William</first><last>Hogan</last></author>
      <author><first>Jiacheng</first><last>Li</last></author>
      <author><first>Jingbo</first><last>Shang</last></author>
      <pages>1083-1095</pages>
      <abstract>Recent relation extraction (RE) works have shown encouraging improvements by conducting contrastive learning on silver labels generated by distant supervision before fine-tuning on gold labels. Existing methods typically assume all these silver labels are accurate and treat them equally; however, distant supervision is inevitably noisy–some silver labels are more reliable than others. In this paper, we propose fine-grained contrastive learning (FineCL) for RE, which leverages fine-grained information about which silver labels are and are not noisy to improve the quality of learned relationship representations for RE. We first assess the quality of silver labels via a simple and automatic approach we call “learning order denoising,” where we train a language model to learn these relations and record the order of learned training instances. We show that learning order largely corresponds to label accuracy–early-learned silver labels have, on average, more accurate labels than later-learned silver labels. Then, during pre-training, we increase the weights of accurate labels within a novel contrastive learning objective. Experiments on several RE benchmarks show that FineCL makes consistent and significant performance gains over state-of-the-art methods.</abstract>
      <url hash="7d3b0619">2022.emnlp-main.71</url>
      <bibkey>hogan-etal-2022-fine</bibkey>
    </paper>
    <paper id="72">
      <title>Curriculum Prompt Learning with Self-Training for Abstractive Dialogue Summarization</title>
      <author><first>Changqun</first><last>Li</last></author>
      <author><first>Linlin</first><last>Wang</last></author>
      <author><first>Xin</first><last>Lin</last></author>
      <author><first>Gerard</first><last>de Melo</last></author>
      <author><first>Liang</first><last>He</last></author>
      <pages>1096-1106</pages>
      <abstract>Succinctly summarizing dialogue is a task of growing interest, but inherent challenges, such as insufficient training data and low information density impede our ability to train abstractive models. In this work, we propose a novel curriculum-based prompt learning method with self-training to address these problems. Specifically, prompts are learned using a curriculum learning strategy that gradually increases the degree of prompt perturbation, thereby improving the dialogue understanding and modeling capabilities of our model. Unlabeled dialogue is incorporated by means of self-training so as to reduce the dependency on labeled data. We further investigate topic-aware prompts to better plan for the generation of summaries. Experiments confirm that our model substantially outperforms strong baselines and achieves new state-of-the-art results on the AMI and ICSI datasets. Human evaluations also show the superiority of our model with regard to the summary generation quality.</abstract>
      <url hash="ad2916d1">2022.emnlp-main.72</url>
      <bibkey>li-etal-2022-curriculum</bibkey>
    </paper>
    <paper id="73">
      <title>Zero-Shot Text Classification with Self-Training</title>
      <author><first>Ariel</first><last>Gera</last></author>
      <author><first>Alon</first><last>Halfon</last></author>
      <author><first>Eyal</first><last>Shnarch</last></author>
      <author><first>Yotam</first><last>Perlitz</last></author>
      <author><first>Liat</first><last>Ein-Dor</last></author>
      <author><first>Noam</first><last>Slonim</last></author>
      <pages>1107-1119</pages>
      <abstract>Recent advances in large pretrained language models have increased attention to zero-shot text classification. In particular, models finetuned on natural language inference datasets have been widely adopted as zero-shot classifiers due to their promising results and off-the-shelf availability. However, the fact that such models are unfamiliar with the target task can lead to instability and performance issues. We propose a plug-and-play method to bridge this gap using a simple self-training approach, requiring only the class names along with an unlabeled dataset, and without the need for domain expertise or trial and error. We show that fine-tuning the zero-shot classifier on its most confident predictions leads to significant performance gains across a wide range of text classification tasks, presumably since self-training adapts the zero-shot model to the task at hand.</abstract>
      <url hash="1e2861c2">2022.emnlp-main.73</url>
      <bibkey>gera-etal-2022-zero</bibkey>
    </paper>
    <paper id="74">
      <title>Deconfounding Legal Judgment Prediction for <fixed-case>E</fixed-case>uropean Court of Human Rights Cases Towards Better Alignment with Experts</title>
      <author><first>T.y.s.s</first><last>Santosh</last></author>
      <author><first>Shanshan</first><last>Xu</last></author>
      <author><first>Oana</first><last>Ichim</last></author>
      <author><first>Matthias</first><last>Grabmair</last></author>
      <pages>1120-1138</pages>
      <abstract>This work demonstrates that Legal Judgement Prediction systems without expert-informed adjustments can be vulnerable to shallow, distracting surface signals that arise from corpus construction, case distribution, and confounding factors. To mitigate this, we use domain expertise to strategically identify statistically predictive but legally irrelevant information. We adopt adversarial training to prevent the system from relying on it. We evaluate our deconfounded models by employing interpretability techniques and comparing to expert annotations. Quantitative experiments and qualitative analysis show that our deconfounded model consistently aligns better with expert rationales than baselines trained for prediction only. We further contribute a set of reference expert annotations to the validation and testing partitions of an existing benchmark dataset of European Court of Human Rights cases.</abstract>
      <url hash="59ae1a4b">2022.emnlp-main.74</url>
      <bibkey>santosh-etal-2022-deconfounding</bibkey>
    </paper>
    <paper id="75">
      <title><fixed-case>SQ</fixed-case>u<fixed-case>ALITY</fixed-case>: Building a Long-Document Summarization Dataset the Hard Way</title>
      <author><first>Alex</first><last>Wang</last></author>
      <author><first>Richard Yuanzhe</first><last>Pang</last></author>
      <author><first>Angelica</first><last>Chen</last></author>
      <author><first>Jason</first><last>Phang</last></author>
      <author><first>Samuel R.</first><last>Bowman</last></author>
      <pages>1139-1156</pages>
      <abstract>Summarization datasets are often assembled either by scraping naturally occurring public-domain summaries—which are nearly always in difficult-to-work-with technical domains—or by using approximate heuristics to extract them from everyday text—which frequently yields unfaithful summaries. In this work, we turn to a slower but more straightforward approach to developing summarization benchmark data: We hire highly-qualified contractors to read stories and write original summaries from scratch. To amortize reading time, we collect five summaries per document, with the first giving an overview and the subsequent four addressing specific questions. We use this protocol to collect SQuALITY, a dataset of question-focused summaries built on the same public-domain short stories as the multiple-choice dataset QuALITY (Pang et al., 2021). Experiments with state-of-the-art summarization systems show that our dataset is challenging and that existing automatic evaluation metrics are weak indicators of quality.</abstract>
      <url hash="2acc6bbb">2022.emnlp-main.75</url>
      <bibkey>wang-etal-2022-squality</bibkey>
    </paper>
    <paper id="76">
      <title><fixed-case>M</fixed-case>eta<fixed-case>ASSIST</fixed-case>: Robust Dialogue State Tracking with Meta Learning</title>
      <author><first>Fanghua</first><last>Ye</last></author>
      <author><first>Xi</first><last>Wang</last></author>
      <author><first>Jie</first><last>Huang</last></author>
      <author><first>Shenghui</first><last>Li</last></author>
      <author><first>Samuel</first><last>Stern</last></author>
      <author><first>Emine</first><last>Yilmaz</last></author>
      <pages>1157-1169</pages>
      <abstract>Existing dialogue datasets contain lots of noise in their state annotations. Such noise can hurt model training and ultimately lead to poor generalization performance. A general framework named ASSIST has recently been proposed to train robust dialogue state tracking (DST) models. It introduces an auxiliary model to generate pseudo labels for the noisy training set. These pseudo labels are combined with vanilla labels by a common fixed weighting parameter to train the primary DST model. Notwithstanding the improvements of ASSIST on DST, tuning the weighting parameter is challenging. Moreover, a single parameter shared by all slots and all instances may be suboptimal. To overcome these limitations, we propose a meta learning-based framework MetaASSIST to adaptively learn the weighting parameter. Specifically, we propose three schemes with varying degrees of flexibility, ranging from slot-wise to both slot-wise and instance-wise, to convert the weighting parameter into learnable functions. These functions are trained in a meta-learning manner by taking the validation set as meta data. Experimental results demonstrate that all three schemes can achieve competitive performance. Most impressively, we achieve a state-of-the-art joint goal accuracy of 80.10% on MultiWOZ 2.4.</abstract>
      <url hash="7fc01eee">2022.emnlp-main.76</url>
      <bibkey>ye-etal-2022-metaassist</bibkey>
    </paper>
    <paper id="77">
      <title>Multilingual Machine Translation with Hyper-Adapters</title>
      <author><first>Christos</first><last>Baziotis</last></author>
      <author><first>Mikel</first><last>Artetxe</last></author>
      <author><first>James</first><last>Cross</last></author>
      <author><first>Shruti</first><last>Bhosale</last></author>
      <pages>1170-1185</pages>
      <abstract>Multilingual machine translation suffers from negative interference across languages. A common solution is to relax parameter sharing with language-specific modules like adapters. However, adapters of related languages are unable to transfer information, and their total number of parameters becomes prohibitively expensive as the number of languages grows. In this work, we overcome these drawbacks using hyper-adapters – hyper-networks that generate adapters from language and layer embeddings. While past work had poor results when scaling hyper-networks, we propose a rescaling fix that significantly improves convergence and enables training larger hyper-networks. We find that hyper-adapters are more parameter efficient than regular adapters, reaching the same performance with up to 12 times less parameters. When using the same number of parameters and FLOPS, our approach consistently outperforms regular adapters. Also, hyper-adapters converge faster than alternative approaches and scale better than regular dense networks. Our analysis shows that hyper-adapters learn to encode language relatedness, enabling positive transfer across languages.</abstract>
      <url hash="b8fb0a85">2022.emnlp-main.77</url>
      <bibkey>baziotis-etal-2022-multilingual</bibkey>
    </paper>
    <paper id="78">
      <title><fixed-case>Z</fixed-case>-<fixed-case>L</fixed-case>a<fixed-case>VI</fixed-case>: Zero-Shot Language Solver Fueled by Visual Imagination</title>
      <author><first>Yue</first><last>Yang</last></author>
      <author><first>Wenlin</first><last>Yao</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Xiaoyang</first><last>Wang</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <author><first>Jianshu</first><last>Chen</last></author>
      <pages>1186-1203</pages>
      <abstract>Large-scale pretrained language models have made significant advances in solving downstream language understanding tasks. However, they generally suffer from reporting bias, the phenomenon describing the lack of explicit commonsense knowledge in written text, e.g., ”an orange is orange”. To overcome this limitation, we develop a novel approach, Z-LaVI, to endow language models with visual imagination capabilities. Specifically, we leverage two complementary types of ”imaginations”: (i) recalling existing images through retrieval and (ii) synthesizing nonexistent images via text-to-image generation. Jointly exploiting the language inputs and the imagination, a pretrained vision-language model (e.g., CLIP) eventually composes a zero-shot solution to the original language tasks. Notably, fueling language models with imagination can effectively leverage visual knowledge to solve plain language tasks. In consequence, Z-LaVI consistently improves the zero-shot performance of existing language models across a diverse set of language tasks.</abstract>
      <url hash="945ad826">2022.emnlp-main.78</url>
      <bibkey>yang-etal-2022-z</bibkey>
    </paper>
    <paper id="79">
      <title>Using Commonsense Knowledge to Answer Why-Questions</title>
      <author><first>Yash Kumar</first><last>Lal</last></author>
      <author><first>Niket</first><last>Tandon</last></author>
      <author><first>Tanvi</first><last>Aggarwal</last></author>
      <author><first>Horace</first><last>Liu</last></author>
      <author><first>Nathanael</first><last>Chambers</last></author>
      <author><first>Raymond</first><last>Mooney</last></author>
      <author><first>Niranjan</first><last>Balasubramanian</last></author>
      <pages>1204-1219</pages>
      <abstract>Answering questions in narratives about why events happened often requires commonsense knowledge external to the text. What aspects of this knowledge are available in large language models? What aspects can be made accessible via external commonsense resources? We study these questions in the context of answering questions in the TellMeWhy dataset using COMET as a source of relevant commonsense relations. We analyze the effects of model size (T5 and GPT3) along with methods of injecting knowledge (COMET) into these models. Results show that the largest models, as expected, yield substantial improvements over base models. Injecting external knowledge helps models of various sizes, but the amount of improvement decreases with larger model size. We also find that the format in which knowledge is provided is critical, and that smaller models benefit more from larger amounts of knowledge. Finally, we develop an ontology of knowledge types and analyze the relative coverage of the models across these categories.</abstract>
      <url hash="3371f256">2022.emnlp-main.79</url>
      <bibkey>lal-etal-2022-using</bibkey>
    </paper>
    <paper id="80">
      <title>Affective Idiosyncratic Responses to Music</title>
      <author><first>Sky</first><last>CH-Wang</last></author>
      <author><first>Evan</first><last>Li</last></author>
      <author><first>Oliver</first><last>Li</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <pages>1220-1250</pages>
      <abstract>Affective responses to music are highly personal. Despite consensus that idiosyncratic factors play a key role in regulating how listeners emotionally respond to music, precisely measuring the marginal effects of these variables has proved challenging. To address this gap, we develop computational methods to measure affective responses to music from over 403M listener comments on a Chinese social music platform. Building on studies from music psychology in systematic and quasi-causal analyses, we test for musical, lyrical, contextual, demographic, and mental health effects that drive listener affective responses. Finally, motivated by the social phenomenon known as 网抑云 (wǎng-yì-yún), we identify influencing factors of platform user self-disclosures, the social support they receive, and notable differences in discloser user activity.</abstract>
      <url hash="94f499d4">2022.emnlp-main.80</url>
      <bibkey>ch-wang-etal-2022-affective</bibkey>
    </paper>
    <paper id="81">
      <title>Successive Prompting for Decomposing Complex Questions</title>
      <author><first>Dheeru</first><last>Dua</last></author>
      <author><first>Shivanshu</first><last>Gupta</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <pages>1251-1265</pages>
      <abstract>Answering complex questions that require making latent decisions is a challenging task, especially when limited supervision is available. Recent works leverage the capabilities of large language models (LMs) to perform complex question answering in a few-shot setting by demonstrating how to output intermediate rationalizations while solving the complex question in a single pass. We introduce “Successive Prompting” where, we iteratively break down a complex task into a simple task, solve it, and then repeat the process until we get the final solution. Successive prompting decouples the supervision for decomposing complex questions from the supervision for answering simple questions, allowing us to (1) have multiple opportunities to query in-context examples at each reasoning step (2) learn question decomposition separately from question answering, including using synthetic data, and (3) use bespoke (fine-tuned) components for reasoning steps where a large LM does not perform well. The intermediate supervision is typically manually written, which can be expensive to collect. We introduce a way to generate synthetic dataset which can be used to bootstrap model’s ability to decompose and answer intermediate questions. Our best model (with successive prompting) achieves an improvement in F1 of ~5% when compared with a state-of-the-art model with synthetic augmentations and few-shot version of the DROP dataset.</abstract>
      <url hash="3f038dcc">2022.emnlp-main.81</url>
      <bibkey>dua-etal-2022-successive</bibkey>
    </paper>
    <paper id="82">
      <title>Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations</title>
      <author><first>Jaehun</first><last>Jung</last></author>
      <author><first>Lianhui</first><last>Qin</last></author>
      <author><first>Sean</first><last>Welleck</last></author>
      <author><first>Faeze</first><last>Brahman</last></author>
      <author><first>Chandra</first><last>Bhagavatula</last></author>
      <author><first>Ronan</first><last>Le Bras</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>1266-1279</pages>
      <abstract>Pre-trained language models (LMs) struggle with consistent reasoning; recently, prompting LMs to generate explanations that self-guide the inference has emerged as a promising direction to amend this. However, these approaches are fundamentally bounded by the correctness of explanations, which themselves are often noisy and inconsistent. In this work, we develop Maieutic Prompting, which aims to infer a correct answer to a question even from the unreliable generations of LM. Maieutic Prompting induces a tree of explanations abductively (e.g. X is true, because ...) and recursively, then frames the inference as a satisfiability problem over these explanations and their logical relations. We test Maieutic Prompting for true/false QA on three challenging benchmarks that require complex commonsense reasoning. Maieutic Prompting achieves up to 20% better accuracy than state-of-the-art prompting methods, and as a fully unsupervised approach, performs competitively with supervised models. We also show that Maieutic Prompting improves robustness in inference while providing interpretable rationales.</abstract>
      <url hash="5b081da5">2022.emnlp-main.82</url>
      <bibkey>jung-etal-2022-maieutic</bibkey>
    </paper>
    <paper id="83">
      <title><fixed-case>DANLI</fixed-case>: Deliberative Agent for Following Natural Language Instructions</title>
      <author><first>Yichi</first><last>Zhang</last></author>
      <author><first>Jianing</first><last>Yang</last></author>
      <author><first>Jiayi</first><last>Pan</last></author>
      <author><first>Shane</first><last>Storks</last></author>
      <author><first>Nikhil</first><last>Devraj</last></author>
      <author><first>Ziqiao</first><last>Ma</last></author>
      <author><first>Keunwoo</first><last>Yu</last></author>
      <author><first>Yuwei</first><last>Bao</last></author>
      <author><first>Joyce</first><last>Chai</last></author>
      <pages>1280-1298</pages>
      <abstract>Recent years have seen an increasing amount of work on embodied AI agents that can perform tasks by following human language instructions. However, most of these agents are reactive, meaning that they simply learn and imitate behaviors encountered in the training data. These reactive agents are insufficient for long-horizon complex tasks. To address this limitation, we propose a neuro-symbolic deliberative agent that, while following language instructions, proactively applies reasoning and planning based on its neural and symbolic representations acquired from past experience (e.g., natural language and egocentric vision). We show that our deliberative agent achieves greater than 70% improvement over reactive baselines on the challenging TEACh benchmark. Moreover, the underlying reasoning and planning processes, together with our modular framework, offer impressive transparency and explainability to the behaviors of the agent. This enables an in-depth understanding of the agent’s capabilities, which shed light on challenges and opportunities for future embodied agents for instruction following. The code is available at https://github.com/sled-group/DANLI.</abstract>
      <url hash="30fa2167">2022.emnlp-main.83</url>
      <bibkey>zhang-etal-2022-danli</bibkey>
    </paper>
    <paper id="84">
      <title>Tracing Semantic Variation in Slang</title>
      <author><first>Zhewei</first><last>Sun</last></author>
      <author><first>Yang</first><last>Xu</last></author>
      <pages>1299-1313</pages>
      <abstract>The meaning of a slang term can vary in different communities. However, slang semantic variation is not well understood and under-explored in the natural language processing of slang. One existing view argues that slang semantic variation is driven by culture-dependent communicative needs. An alternative view focuses on slang’s social functions suggesting that the desire to foster semantic distinction may have led to the historical emergence of community-specific slang senses. We explore these theories using computational models and test them against historical slang dictionary entries, with a focus on characterizing regularity in the geographical variation of slang usages attested in the US and the UK over the past two centuries. We show that our models are able to predict the regional identity of emerging slang word meanings from historical slang records. We offer empirical evidence that both communicative need and semantic distinction play a role in the variation of slang meaning yet their relative importance fluctuates over the course of history. Our work offers an opportunity for incorporating historical cultural elements into the natural language processing of slang.</abstract>
      <url hash="4680194c">2022.emnlp-main.84</url>
      <bibkey>sun-xu-2022-tracing</bibkey>
    </paper>
    <paper id="85">
      <title>Fine-grained Category Discovery under Coarse-grained supervision with Hierarchical Weighted Self-contrastive Learning</title>
      <author><first>Wenbin</first><last>An</last></author>
      <author><first>Feng</first><last>Tian</last></author>
      <author><first>Ping</first><last>Chen</last></author>
      <author><first>Siliang</first><last>Tang</last></author>
      <author><first>Qinghua</first><last>Zheng</last></author>
      <author><first>QianYing</first><last>Wang</last></author>
      <pages>1314-1323</pages>
      <abstract>Novel category discovery aims at adapting models trained on known categories to novel categories. Previous works only focus on the scenario where known and novel categories are of the same granularity.In this paper, we investigate a new practical scenario called Fine-grained Category Discovery under Coarse-grained supervision (FCDC). FCDC aims at discovering fine-grained categories with only coarse-grained labeled data, which can adapt models to categories of different granularity from known ones and reduce significant labeling cost. It is also a challenging task since supervised training on coarse-grained categories tends to focus on inter-class distance (distance between coarse-grained classes) but ignore intra-class distance (distance between fine-grained sub-classes) which is essential for separating fine-grained categories.Considering most current methods cannot transfer knowledge from coarse-grained level to fine-grained level, we propose a hierarchical weighted self-contrastive network by building a novel weighted self-contrastive module and combining it with supervised learning in a hierarchical manner.Extensive experiments on public datasets show both effectiveness and efficiency of our model over compared methods.</abstract>
      <url hash="669f64f5">2022.emnlp-main.85</url>
      <bibkey>an-etal-2022-fine</bibkey>
    </paper>
    <paper id="86">
      <title><fixed-case>PLM</fixed-case>-based World Models for Text-based Games</title>
      <author><first>Minsoo</first><last>Kim</last></author>
      <author><first>Yeonjoon</first><last>Jung</last></author>
      <author><first>Dohyeon</first><last>Lee</last></author>
      <author><first>Seung-won</first><last>Hwang</last></author>
      <pages>1324-1341</pages>
      <abstract>World models have improved the ability of reinforcement learning agents to operate in a sample efficient manner, by being trained to predict plausible changes in the underlying environment. As the core tasks of world models are future prediction and commonsense understanding, our claim is that pre-trained language models (PLMs) already provide a strong base upon which to build world models. Worldformer is a recently proposed world model for text-based game environments, based only partially on PLM and transformers. Our distinction is to fully leverage PLMs as actionable world models in text-based game environments, by reformulating generation as constrained decoding which decomposes actions into verb templates and objects. We show that our model improves future valid action prediction and graph change prediction. Additionally, we show that our model better reflects commonsense than standard PLM.</abstract>
      <url hash="044e66e9">2022.emnlp-main.86</url>
      <bibkey>kim-etal-2022-plm</bibkey>
    </paper>
    <paper id="87">
      <title>Prompt-Based Meta-Learning For Few-shot Text Classification</title>
      <author><first>Haoxing</first><last>Zhang</last></author>
      <author><first>Xiaofeng</first><last>Zhang</last></author>
      <author><first>Haibo</first><last>Huang</last></author>
      <author><first>Lei</first><last>Yu</last></author>
      <pages>1342-1357</pages>
      <abstract>Few-shot Text Classification predicts the semantic label of a given text with a handful of supporting instances. Current meta-learning methods have achieved satisfying results in various few-shot situations. Still, they often require a large amount of data to construct many few-shot tasks for meta-training, which is not practical in real-world few-shot scenarios. Prompt-tuning has recently proved to be another effective few-shot learner by bridging the gap between pre-train and downstream tasks. In this work, we closely combine the two promising few-shot learning methodologies in structure and propose a Prompt-Based Meta-Learning (PBML) model to overcome the above meta-learning problem by adding the prompting mechanism. PBML assigns label word learning to base-learners and template learning to meta-learner, respectively. Experimental results show state-of-the-art performance on four text classification datasets under few-shot settings, with higher accuracy and good robustness. We demonstrate through low-resource experiments that our method alleviates the shortcoming that meta-learning requires too much data for meta-training. In the end, we use the visualization to interpret and verify that the meta-learning framework can help the prompting method converge better. We release our code to reproduce our experiments.</abstract>
      <url hash="21f5bcbc">2022.emnlp-main.87</url>
      <bibkey>zhang-etal-2022-prompt-based</bibkey>
    </paper>
    <paper id="88">
      <title>How well can Text-to-Image Generative Models understand Ethical Natural Language Interventions?</title>
      <author><first>Hritik</first><last>Bansal</last></author>
      <author><first>Da</first><last>Yin</last></author>
      <author><first>Masoud</first><last>Monajatipoor</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>1358-1370</pages>
      <abstract>Text-to-image generative models have achieved unprecedented success in generating high-quality images based on natural language descriptions. However, it is shown that these models tend to favor specific social groups when prompted with neutral text descriptions (e.g., ‘a photo of a lawyer’). Following Zhao et al. (2021), we study the effect on the diversity of the generated images when adding <i>ethical intervention</i> that supports equitable judgment (e.g., ‘if all individuals can be a lawyer irrespective of their gender’) in the input prompts. To this end, we introduce an Ethical NaTural Language Interventions in Text-to-Image GENeration (ENTIGEN) benchmark dataset to evaluate the change in image generations conditional on ethical interventions across three social axes – gender, skin color, and culture. Through CLIP-based and human evaluation on minDALL.E, DALL.E-mini and Stable Diffusion, we find that the model generations cover diverse social groups while preserving the image quality. In some cases, the generations would be anti-stereotypical (e.g., models tend to create images with individuals that are perceived as man when fed with prompts about makeup) in the presence of ethical intervention. Preliminary studies indicate that a large change in the model predictions is triggered by certain phrases such as ‘irrespective of gender’ in the context of gender bias in the ethical interventions. We release code and annotated data at https://github.com/Hritikbansal/entigen_emnlp.</abstract>
      <url hash="7cac9621">2022.emnlp-main.88</url>
      <bibkey>bansal-etal-2022-well</bibkey>
    </paper>
    <paper id="89">
      <title>Geographic Citation Gaps in <fixed-case>NLP</fixed-case> Research</title>
      <author><first>Mukund</first><last>Rungta</last></author>
      <author><first>Janvijay</first><last>Singh</last></author>
      <author><first>Saif M.</first><last>Mohammad</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>1371-1383</pages>
      <abstract>In a fair world, people have equitable opportunities to education, to conduct scientific research, to publish, and to get credit for their work, regardless of where they live. However, it is common knowledge among researchers that a vast number of papers accepted at top NLP venues come from a handful of western countries and (lately) China; whereas, very few papers from Africa and South America get published. Similar disparities are also believed to exist for paper citation counts. In the spirit of “what we do not measure, we cannot improve”, this work asks a series of questions on the relationship between geographical location and publication success (acceptance in top NLP venues and citation impact). We first created a dataset of 70,000 papers from the ACL Anthology, extracted their meta-information, andgenerated their citation network. We then show that not only are there substantial geographical disparities in paper acceptance and citation but also that these disparities persist even when controlling for a number of variables such as venue of publication and sub-field of NLP. Further, despite some steps taken by the NLP community to improve geographical diversity, we show that the disparity in publication metrics across locations is still on an increasing trend since the early 2000s. We release our code and dataset here: https://github.com/iamjanvijay/acl-cite-net</abstract>
      <url hash="0f5b45aa">2022.emnlp-main.89</url>
      <bibkey>rungta-etal-2022-geographic</bibkey>
    </paper>
    <paper id="90">
      <title>Language Models of Code are Few-Shot Commonsense Learners</title>
      <author><first>Aman</first><last>Madaan</last></author>
      <author><first>Shuyan</first><last>Zhou</last></author>
      <author><first>Uri</first><last>Alon</last></author>
      <author><first>Yiming</first><last>Yang</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>1384-1403</pages>
      <abstract>We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event or a reasoning-graph.To employ large language models (LMs) for this task, existing approaches ‘serialize’ the output graph as a flat list of nodes and edges.Although feasible, these serialized graphs strongly deviate from the natural language corpora that LMs were pre-trained on, hindering LMs from generating them correctly. In this paper, we show that when we instead frame structured commonsense reasoning tasks as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than LMs of natural language, even when the downstream task does not involve source code at all.We demonstrate our approach across three diverse structured commonsense reasoning tasks. In all these natural language tasks, we show that using our approach, a code generation LM (codex) outperforms natural-LMs that are fine-tuned on the target task (T5) and other strong LMs such as GPT-3 in the few-shot setting.</abstract>
      <url hash="980aac14">2022.emnlp-main.90</url>
      <bibkey>madaan-etal-2022-language</bibkey>
    </paper>
    <paper id="91">
      <title>Numerical Optimizations for Weighted Low-rank Estimation on Language Models</title>
      <author><first>Ting</first><last>Hua</last></author>
      <author><first>Yen-Chang</first><last>Hsu</last></author>
      <author><first>Felicity</first><last>Wang</last></author>
      <author><first>Qian</first><last>Lou</last></author>
      <author><first>Yilin</first><last>Shen</last></author>
      <author><first>Hongxia</first><last>Jin</last></author>
      <pages>1404-1416</pages>
      <abstract>Singular value decomposition (SVD) is one of the most popular compression methods that approximate a target matrix with smaller matrices. However, standard SVD treats the parameters within the matrix with equal importance, which is a simple but unrealistic assumption. The parameters of a trained neural network model may affect the task performance unevenly, which suggests non-equal importance among the parameters. Compared to SVD, the decomposition method aware of parameter importance is the more practical choice in real cases. Unlike standard SVD, weighed value decomposition is a non-convex optimization problem that lacks a closed-form solution. We systematically investigated multiple optimization strategies to tackle the problem and examined our method by compressing Transformer-based language models.Further, we designed a metric to predict when the SVD may introduce a significant performance drop, for which our method can be a rescue strategy.The extensive evaluations demonstrate that our method can perform better than current SOTA methods in compressing Transformer-based language models.</abstract>
      <url hash="1da18558">2022.emnlp-main.91</url>
      <bibkey>hua-etal-2022-numerical</bibkey>
    </paper>
    <paper id="92">
      <title>Generative Multi-hop Retrieval</title>
      <author><first>Hyunji</first><last>Lee</last></author>
      <author><first>Sohee</first><last>Yang</last></author>
      <author><first>Hanseok</first><last>Oh</last></author>
      <author><first>Minjoon</first><last>Seo</last></author>
      <pages>1417-1436</pages>
      <abstract>A common practice for text retrieval is to use an encoder to map the documents and the query to a common vector space and perform a nearest neighbor search (NNS); multi-hop retrieval also often adopts the same paradigm, usually with a modification of iteratively reformulating the query vector so that it can retrieve different documents at each hop. However, such a bi-encoder approach has limitations in multi-hop settings; (1) the reformulated query gets longer as the number of hops increases, which further tightens the embedding bottleneck of the query vector, and (2) it is prone to error propagation. In this paper, we focus on alleviating these limitations in multi-hop settings by formulating the problem in a fully generative way. We propose an encoder-decoder model that performs multi-hop retrieval by simply generating the entire text sequences of the retrieval targets, which means the query and the documents interact in the language model’s parametric space rather than L2 or inner product space as in the bi-encoder approach. Our approach, Generative Multi-hop Retrieval (GMR), consistently achieves comparable or higher performance than bi-encoder models in five datasets while demonstrating superior GPU memory and storage footprint.</abstract>
      <url hash="9052fc07">2022.emnlp-main.92</url>
      <bibkey>lee-etal-2022-generative</bibkey>
    </paper>
    <paper id="93">
      <title>Visual Spatial Description: Controlled Spatial-Oriented Image-to-Text Generation</title>
      <author><first>Yu</first><last>Zhao</last></author>
      <author><first>Jianguo</first><last>Wei</last></author>
      <author><first>ZhiChao</first><last>Lin</last></author>
      <author><first>Yueheng</first><last>Sun</last></author>
      <author><first>Meishan</first><last>Zhang</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>1437-1449</pages>
      <abstract>Image-to-text tasks such as open-ended image captioning and controllable image description have received extensive attention for decades. Here we advance this line of work further, presenting Visual Spatial Description (VSD), a new perspective for image-to-text toward spatial semantics. Given an image and two objects inside it, VSD aims to produce one description focusing on the spatial perspective between the two objects. Accordingly, we annotate a dataset manually to facilitate the investigation of the newly-introduced task, and then build several benchmark encoder-decoder models by using VL-BART and VL-T5 as backbones. In addition, we investigate visual spatial relationship classification (VSRC) information into our model by pipeline and end-to-end architectures. Finally, we conduct experiments on our benchmark dataset to evaluate all our models. Results show that our models are awe-inspiring, offering accurate and human-like spatial-oriented text descriptions. Besides, VSRC has great potential for VSD, and the joint end-to-end architecture is the better choice for their integration. We will make the dataset and codes publicly available for research purposes.</abstract>
      <url hash="1e230fc8">2022.emnlp-main.93</url>
      <bibkey>zhao-etal-2022-visual</bibkey>
    </paper>
    <paper id="94">
      <title><fixed-case>M</fixed-case>3: A Multi-View Fusion and Multi-Decoding Network for Multi-Document Reading Comprehension</title>
      <author><first>Liang</first><last>Wen</last></author>
      <author><first>Houfeng</first><last>Wang</last></author>
      <author><first>Yingwei</first><last>Luo</last></author>
      <author><first>Xiaolin</first><last>Wang</last></author>
      <pages>1450-1461</pages>
      <abstract>Multi-document reading comprehension task requires collecting evidences from different documents for answering questions. Previous research works either use the extractive modeling method to naively integrate the scores from different documents on the encoder side or use the generative modeling method to collect the clues from different documents on the decoder side individually. However, any single modeling method cannot make full of the advantages of both. In this work, we propose a novel method that tries to employ a multi-view fusion and multi-decoding mechanism to achieve it. For one thing, our approach leverages question-centered fusion mechanism and cross-attention mechanism to gather fine-grained fusion of evidence clues from different documents in the encoder and decoder concurrently. For another, our method simultaneously employs both the extractive decoding approach and the generative decoding method to effectively guide the training process. Compared with existing methods, our method can perform both extractive decoding and generative decoding independently and optionally. Our experiments on two mainstream multi-document reading comprehension datasets (Natural Questions and TriviaQA) demonstrate that our method can provide consistent improvements over previous state-of-the-art methods.</abstract>
      <url hash="217afd09">2022.emnlp-main.94</url>
      <bibkey>wen-etal-2022-m3</bibkey>
    </paper>
    <paper id="95">
      <title><fixed-case>COCO</fixed-case>-<fixed-case>DR</fixed-case>: Combating the Distribution Shift in Zero-Shot Dense Retrieval with Contrastive and Distributionally Robust Learning</title>
      <author><first>Yue</first><last>Yu</last></author>
      <author><first>Chenyan</first><last>Xiong</last></author>
      <author><first>Si</first><last>Sun</last></author>
      <author><first>Chao</first><last>Zhang</last></author>
      <author><first>Arnold</first><last>Overwijk</last></author>
      <pages>1462-1479</pages>
      <abstract>We present a new zero-shot dense retrieval (ZeroDR) method, COCO-DR, to improve the generalization ability of dense retrieval by combating the distribution shifts between source training tasks and target scenarios. To mitigate the impact of document differences, COCO-DR continues pretraining the language model on the target corpora to adapt the model to target distributions via COtinuous COtrastive learning. To prepare for unseen target queries, COCO-DR leverages implicit Distributionally Robust Optimization (iDRO) to reweight samples from different source query clusters for improving model robustness over rare queries during fine-tuning. COCO-DR achieves superior average performance on BEIR, the zero-shot retrieval benchmark. At BERT_Base scale, COCO-DR Base outperforms other ZeroDR models with 60x larger size. At BERT_Large scale, COCO-DR Large outperforms the giant GPT-3 embedding model which has 500x more parameters. Our analysis shows the correlation between COCO-DR’s effectiveness in combating distribution shifts and improving zero-shot accuracy. Our code and model can be found at <url>https://github.com/OpenMatch/COCO-DR</url>.</abstract>
      <url hash="1067772a">2022.emnlp-main.95</url>
      <bibkey>yu-etal-2022-coco</bibkey>
    </paper>
    <paper id="96">
      <title>Language Model Pre-Training with Sparse Latent Typing</title>
      <author><first>Liliang</first><last>Ren</last></author>
      <author><first>Zixuan</first><last>Zhang</last></author>
      <author><first>Han</first><last>Wang</last></author>
      <author><first>Clare</first><last>Voss</last></author>
      <author><first>ChengXiang</first><last>Zhai</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <pages>1480-1494</pages>
      <abstract>Modern large-scale Pre-trained Language Models (PLMs) have achieved tremendous success on a wide range of downstream tasks. However, most of the LM pre-training objectives only focus on text reconstruction, but have not sought to learn latent-level interpretable representations of sentences. In this paper, we manage to push the language models to obtain a deeper understanding of sentences by proposing a new pre-training objective, Sparse Latent Typing, which enables the model to sparsely extract sentence-level keywords with diverse latent types. Experimental results show that our model is able to learn interpretable latent type categories in a self-supervised manner without using any external knowledge. Besides, the language model pre-trained with such an objective also significantly improves Information Extraction related downstream tasks in both supervised and few-shot settings. Our code is publicly available at https://github.com/renll/SparseLT.</abstract>
      <url hash="b5c1731b">2022.emnlp-main.96</url>
      <bibkey>ren-etal-2022-language</bibkey>
    </paper>
    <paper id="97">
      <title>On the Transformation of Latent Space in Fine-Tuned <fixed-case>NLP</fixed-case> Models</title>
      <author><first>Nadir</first><last>Durrani</last></author>
      <author><first>Hassan</first><last>Sajjad</last></author>
      <author><first>Fahim</first><last>Dalvi</last></author>
      <author><first>Firoj</first><last>Alam</last></author>
      <pages>1495-1516</pages>
      <abstract>We study the evolution of latent space in fine-tuned NLP models. Different from the commonly used probing-framework, we opt for an unsupervised method to analyze representations. More specifically, we discover latent concepts in the representational space using hierarchical clustering. We then use an alignment function to gauge the similarity between the latent space of a pre-trained model and its fine-tuned version. We use traditional linguistic concepts to facilitate our understanding and also study how the model space transforms towards task-specific information. We perform a thorough analysis, comparing pre-trained and fine-tuned models across three models and three downstream tasks. The notable findings of our work are: i) the latent space of the higher layers evolve towards task-specific concepts, ii) whereas the lower layers retain generic concepts acquired in the pre-trained model, iii) we discovered that some concepts in the higher layers acquire polarity towards the output class, and iv) that these concepts can be used for generating adversarial triggers.</abstract>
      <url hash="54b7e5d0">2022.emnlp-main.97</url>
      <bibkey>durrani-etal-2022-transformation</bibkey>
    </paper>
    <paper id="98">
      <title>Watch the Neighbors: A Unified K-Nearest Neighbor Contrastive Learning Framework for <fixed-case>OOD</fixed-case> Intent Discovery</title>
      <author><first>Yutao</first><last>Mou</last></author>
      <author><first>Keqing</first><last>He</last></author>
      <author><first>Pei</first><last>Wang</last></author>
      <author><first>Yanan</first><last>Wu</last></author>
      <author><first>Jingang</first><last>Wang</last></author>
      <author><first>Wei</first><last>Wu</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <pages>1517-1529</pages>
      <abstract>Discovering out-of-domain (OOD) intent is important for developing new skills in task-oriented dialogue systems. The key challenges lie in how to transfer prior in-domain (IND) knowledge to OOD clustering, as well as jointly learn OOD representations and cluster assignments. Previous methods suffer from in-domain overfitting problem, and there is a natural gap between representation learning and clustering objectives. In this paper, we propose a unified K-nearest neighbor contrastive learning framework to discover OOD intents. Specifically, for IND pre-training stage, we propose a KCL objective to learn inter-class discriminative features, while maintaining intra-class diversity, which alleviates the in-domain overfitting problem. For OOD clustering stage, we propose a KCC method to form compact clusters by mining true hard negative samples, which bridges the gap between clustering and representation learning. Extensive experiments on three benchmark datasets show that our method achieves substantial improvements over the state-of-the-art methods.</abstract>
      <url hash="e5570504">2022.emnlp-main.98</url>
      <bibkey>mou-etal-2022-watch</bibkey>
    </paper>
    <paper id="99">
      <title>Extracted <fixed-case>BERT</fixed-case> Model Leaks More Information than You Think!</title>
      <author><first>Xuanli</first><last>He</last></author>
      <author><first>Lingjuan</first><last>Lyu</last></author>
      <author><first>Chen</first><last>Chen</last></author>
      <author><first>Qiongkai</first><last>Xu</last></author>
      <pages>1530-1537</pages>
      <abstract>The collection and availability of big data, combined with advances in pre-trained models (e.g. BERT), have revolutionized the predictive performance of natural language processing tasks. This allows corporations to provide machine learning as a service (MLaaS) by encapsulating fine-tuned BERT-based models as APIs. Due to significant commercial interest, there has been a surge of attempts to steal remote services via model extraction. Although previous works have made progress in defending against model extraction attacks, there has been little discussion on their performance in preventing privacy leakage. This work bridges this gap by launching an attribute inference attack against the extracted BERT model. Our extensive experiments reveal that model extraction can cause severe privacy leakage even when victim models are facilitated with state-of-the-art defensive strategies.</abstract>
      <url hash="d6954126">2022.emnlp-main.99</url>
      <bibkey>he-etal-2022-extracted</bibkey>
    </paper>
    <paper id="100">
      <title>Do Vision-and-Language Transformers Learn Grounded Predicate-Noun Dependencies?</title>
      <author><first>Mitja</first><last>Nikolaus</last></author>
      <author><first>Emmanuelle</first><last>Salin</last></author>
      <author><first>Stephane</first><last>Ayache</last></author>
      <author><first>Abdellah</first><last>Fourtassi</last></author>
      <author><first>Benoit</first><last>Favre</last></author>
      <pages>1538-1555</pages>
      <abstract>Recent advances in vision-and-language modeling have seen the development of Transformer architectures that achieve remarkable performance on multimodal reasoning tasks.Yet, the exact capabilities of these black-box models are still poorly understood. While much of previous work has focused on studying their ability to learn meaning at the word-level, their ability to track syntactic dependencies between words has received less attention.We take a first step in closing this gap by creating a new multimodal task targeted at evaluating understanding of predicate-noun dependencies in a controlled setup.We evaluate a range of state-of-the-art models and find that their performance on the task varies considerably, with some models performing relatively well and others at chance level. In an effort to explain this variability, our analyses indicate that the quality (and not only sheer quantity) of pretraining data is essential. Additionally, the best performing models leverage fine-grained multimodal pretraining objectives in addition to the standard image-text matching objectives.This study highlights that targeted and controlled evaluations are a crucial step for a precise and rigorous test of the multimodal knowledge of vision-and-language models.</abstract>
      <url hash="6dbb3cc4">2022.emnlp-main.100</url>
      <bibkey>nikolaus-etal-2022-vision</bibkey>
    </paper>
    <paper id="101">
      <title>A Multilingual Perspective Towards the Evaluation of Attribution Methods in Natural Language Inference</title>
      <author><first>Kerem</first><last>Zaman</last></author>
      <author><first>Yonatan</first><last>Belinkov</last></author>
      <pages>1556-1576</pages>
      <abstract>Most evaluations of attribution methods focus on the English language. In this work, we present a multilingual approach for evaluating attribution methods for the Natural Language Inference (NLI) task in terms of faithfulness and plausibility.First, we introduce a novel cross-lingual strategy to measure faithfulness based on word alignments, which eliminates the drawbacks of erasure-based evaluations.We then perform a comprehensive evaluation of attribution methods, considering different output mechanisms and aggregation methods.Finally, we augment the XNLI dataset with highlight-based explanations, providing a multilingual NLI dataset with highlights, to support future exNLP studies. Our results show that attribution methods performing best for plausibility and faithfulness are different.</abstract>
      <url hash="c41e384b">2022.emnlp-main.101</url>
      <bibkey>zaman-belinkov-2022-multilingual</bibkey>
    </paper>
    <paper id="102">
      <title>Graph-Based Multilingual Label Propagation for Low-Resource Part-of-Speech Tagging</title>
      <author><first>Ayyoob</first><last>ImaniGooghari</last></author>
      <author><first>Silvia</first><last>Severini</last></author>
      <author><first>Masoud</first><last>Jalili Sabet</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>1577-1589</pages>
      <abstract>Part-of-Speech (POS) tagging is an important component of the NLP pipeline, but many low-resource languages lack labeled data for training. An established method for training a POS tagger in such a scenario is to create a labeled training set by transferring from high-resource languages. In this paper, we propose a novel method for transferring labels from multiple high-resource source to low-resource target languages. We formalize POS tag projection as graph-based label propagation. Given translations of a sentence in multiple languages, we create a graph with words as nodes and alignment links as edges by aligning words for all language pairs. We then propagate node labels from source to target using a Graph Neural Network augmented with transformer layers. We show that our propagation creates training sets that allow us to train POS taggers for a diverse set of languages. When combined with enhanced contextualized embeddings, our method achieves a new state-of-the-art for unsupervised POS tagging of low-resource languages.</abstract>
      <url hash="6600a809">2022.emnlp-main.102</url>
      <bibkey>imanigooghari-etal-2022-graph</bibkey>
    </paper>
    <paper id="103">
      <title><fixed-case>S</fixed-case>ubevent<fixed-case>W</fixed-case>riter: Iterative Sub-event Sequence Generation with Coherence Controller</title>
      <author><first>Zhaowei</first><last>Wang</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Tianqing</first><last>Fang</last></author>
      <author><first>Yangqiu</first><last>Song</last></author>
      <author><first>Ginny</first><last>Wong</last></author>
      <author><first>Simon</first><last>See</last></author>
      <pages>1590-1604</pages>
      <abstract>In this paper, we propose a new task of sub-event generation for an unseen process to evaluate the understanding of the coherence of sub-event actions and objects. To solve the problem, we design SubeventWriter, a sub-event sequence generation framework with a coherence controller. Given an unseen process, the framework can iteratively construct the sub-event sequence by generating one sub-event at each iteration. We also design a very effective coherence controller to decode more coherent sub-events. As our extensive experiments and analysis indicate, SubeventWriter can generate more reliable and meaningful sub-event sequences for unseen processes.</abstract>
      <url hash="c2d99a7b">2022.emnlp-main.103</url>
      <bibkey>wang-etal-2022-subeventwriter</bibkey>
    </paper>
    <paper id="104">
      <title>Infinite <fixed-case>SCAN</fixed-case>: An Infinite Model of Diachronic Semantic Change</title>
      <author><first>Seiichi</first><last>Inoue</last></author>
      <author><first>Mamoru</first><last>Komachi</last></author>
      <author><first>Toshinobu</first><last>Ogiso</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <author><first>Daichi</first><last>Mochihashi</last></author>
      <pages>1605-1616</pages>
      <abstract>In this study, we propose a Bayesian model that can jointly estimate the number of senses of words and their changes through time.The model combines a dynamic topic model on Gaussian Markov random fields with a logistic stick-breaking process that realizes Dirichlet process. In the experiments, we evaluated the proposed model in terms of interpretability, accuracy in estimating the number of senses, and tracking their changes using both artificial data and real data.We quantitatively verified that the model behaves as expected through evaluation using artificial data.Using the CCOHA corpus, we showed that our model outperforms the baseline model and investigated the semantic changes of several well-known target words.</abstract>
      <url hash="8bf138e7">2022.emnlp-main.104</url>
      <bibkey>inoue-etal-2022-infinite</bibkey>
    </paper>
    <paper id="105">
      <title>Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization</title>
      <author><first>Yuxian</first><last>Gu</last></author>
      <author><first>Pei</first><last>Ke</last></author>
      <author><first>Xiaoyan</first><last>Zhu</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>1617-1634</pages>
      <abstract>Training language models to learn from human instructions for zero-shot cross-task generalization has attracted much attention in NLP communities. Recently, instruction tuning (IT), which fine-tunes a pre-trained language model on a massive collection of tasks described via human-craft instructions, has been shown effective in instruction learning for unseen tasks. However, IT relies on a large amount of human-annotated samples, which restricts its generalization. Unlike labeled data, unlabeled data are often massive and cheap to obtain. In this work, we study how IT can be improved with unlabeled data. We first empirically explore the IT performance trends versus the number of labeled data, instructions, and training tasks. We find it critical to enlarge the number of training instructions, and the instructions can be underutilized due to the scarcity of labeled data. Then, we propose Unlabeled Data Augmented Instruction Tuning (UDIT) to take better advantage of the instructions during IT by constructing pseudo-labeled data from unlabeled plain texts. We conduct extensive experiments to show UDIT’s effectiveness in various scenarios of tasks and datasets. We also comprehensively analyze the key factors of UDIT to investigate how to better improve IT with unlabeled data. The code is publicly available at https://github.com/thu-coai/UDIT.</abstract>
      <url hash="e46de851">2022.emnlp-main.105</url>
      <bibkey>gu-etal-2022-learning</bibkey>
    </paper>
    <paper id="106">
      <title>Counterfactual Data Augmentation via Perspective Transition for Open-Domain Dialogues</title>
      <author><first>Jiao</first><last>Ou</last></author>
      <author><first>Jinchao</first><last>Zhang</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>1635-1648</pages>
      <abstract>The construction of open-domain dialogue systems requires high-quality dialogue datasets. The dialogue data admits a wide variety of responses for a given dialogue history, especially responses with different semantics. However, collecting high-quality such a dataset in most scenarios is labor-intensive and time-consuming. In this paper, we propose a data augmentation method to automatically augment high-quality responses with different semantics by counterfactual inference. Specifically, given an observed dialogue, our counterfactual generation model first infers semantically different responses by replacing the observed reply perspective with substituted ones. Furthermore, our data selection method filters out detrimental augmented responses. Experimental results show that our data augmentation method can augment high-quality responses with different semantics for a given dialogue history, and can outperform competitive baselines on multiple downstream tasks.</abstract>
      <url hash="290ee6db">2022.emnlp-main.106</url>
      <bibkey>ou-etal-2022-counterfactual</bibkey>
    </paper>
    <paper id="107">
      <title><fixed-case>SQUIRE</fixed-case>: A Sequence-to-sequence Framework for Multi-hop Knowledge Graph Reasoning</title>
      <author><first>Yushi</first><last>Bai</last></author>
      <author><first>Xin</first><last>Lv</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Yincen</first><last>Qu</last></author>
      <author><first>Zelin</first><last>Dai</last></author>
      <author><first>Feiyu</first><last>Xiong</last></author>
      <pages>1649-1662</pages>
      <abstract>Multi-hop knowledge graph (KG) reasoning has been widely studied in recent years to provide interpretable predictions on missing links with evidential paths. Most previous works use reinforcement learning (RL) based methods that learn to navigate the path towards the target entity. However, these methods suffer from slow and poor convergence, and they may fail to infer a certain path when there is a missing edge along the path. Here we present SQUIRE, the first Sequence-to-sequence based multi-hop reasoning framework, which utilizes an encoder-decoder Transformer structure to translate the query to a path. Our framework brings about two benefits: (1) It can learn and predict in an end-to-end fashion, which gives better and faster convergence; (2) Our transformer model does not rely on existing edges to generate the path, and has the flexibility to complete missing edges along the path, especially in sparse KGs. Experiments on standard and sparse KGs show that our approach yields significant improvement over prior methods, while converging 4x-7x faster.</abstract>
      <url hash="cd95e47f">2022.emnlp-main.107</url>
      <bibkey>bai-etal-2022-squire</bibkey>
    </paper>
    <paper id="108">
      <title><fixed-case>S</fixed-case>peech<fixed-case>UT</fixed-case>: Bridging Speech and Text with Hidden-Unit for Encoder-Decoder Based Speech-Text Pre-training</title>
      <author><first>Ziqiang</first><last>Zhang</last></author>
      <author><first>Long</first><last>Zhou</last></author>
      <author><first>Junyi</first><last>Ao</last></author>
      <author><first>Shujie</first><last>Liu</last></author>
      <author><first>Lirong</first><last>Dai</last></author>
      <author><first>Jinyu</first><last>Li</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <pages>1663-1676</pages>
      <abstract>The rapid development of single-modal pre-training has prompted researchers to pay more attention to cross-modal pre-training methods. In this paper, we propose a unified-modal speech-unit-text pre-training model, SpeechUT, to connect the representations of a speech encoder and a text decoder with a shared unit encoder. Leveraging hidden-unit as an interface to align speech and text, we can decompose the speech-to-text model into a speech-to-unit model and a unit-to-text model, which can be jointly pre-trained with unpaired speech and text data respectively. Our proposed SpeechUT is fine-tuned and evaluated on automatic speech recognition (ASR) and speech translation (ST) tasks. Experimental results show that SpeechUT gets substantial improvements over strong baselines, and achieves state-of-the-art performance on both the LibriSpeech ASR and MuST-C ST tasks. To better understand the proposed SpeechUT, detailed analyses are conducted. The code and pre-trained models are available at https://aka.ms/SpeechUT.</abstract>
      <url hash="1b3f536c">2022.emnlp-main.108</url>
      <bibkey>zhang-etal-2022-speechut</bibkey>
    </paper>
    <paper id="109">
      <title>Learning Label Modular Prompts for Text Classification in the Wild</title>
      <author><first>Hailin</first><last>Chen</last></author>
      <author><first>Amrita</first><last>Saha</last></author>
      <author><first>Shafiq</first><last>Joty</last></author>
      <author><first>Steven C.H.</first><last>Hoi</last></author>
      <pages>1677-1690</pages>
      <abstract>Machine learning models usually assume i.i.d data during training and testing, but data and tasks in real world often change over time. To emulate the transient nature of real world, we propose a challenging but practical task: text classification in-the-wild, which introduces different non-stationary training/testing stages. Decomposing a complex task into modular components can enable robust generalisation under such non-stationary environment. However, current modular approaches in NLP do not take advantage of recent advances in parameter efficient tuning of pretrained language models. To close this gap, we propose ModularPrompt, a label-modular prompt tuning framework for text classification tasks. In ModularPrompt, the input prompt consists of a sequence of soft label prompts, each encoding modular knowledge related to the corresponding class label. In two of most formidable settings, ModularPrompt outperforms relevant baselines by a large margin demonstrating strong generalisation ability. We also conduct comprehensive analysis to validate whether the learned prompts satisfy properties of a modular representation.</abstract>
      <url hash="91ebb5b6">2022.emnlp-main.109</url>
      <bibkey>chen-etal-2022-learning-label</bibkey>
    </paper>
    <paper id="110">
      <title>Unbiased and Efficient Sampling of Dependency Trees</title>
      <author><first>Miloš</first><last>Stanojević</last></author>
      <pages>1691-1706</pages>
      <abstract>Most computational models of dependency syntax consist of distributions over spanning trees. However, the majority of dependency treebanks require that every valid dependency tree has a single edge coming out of the ROOT node, a constraint that is not part of the definition of spanning trees. For this reason all standard inference algorithms for spanning trees are sub-optimal for inference over dependency trees.Zmigrod et al (2021) proposed algorithms for sampling with and without replacement from the dependency tree distribution that incorporate the single-root constraint. In this paper we show that their fastest algorithm for sampling with replacement, Wilson-RC, is in fact producing biased samples and we provide two alternatives that are unbiased. Additionally, we propose two algorithms (one incremental, one parallel) that reduce the asymptotic runtime of algorithm for sampling k trees without replacement to O(kn^3). These algorithms are both asymptotically and practically more efficient.</abstract>
      <url hash="05b63479">2022.emnlp-main.110</url>
      <bibkey>stanojevic-2022-unbiased</bibkey>
    </paper>
    <paper id="111">
      <title>Continual Learning of Neural Machine Translation within Low Forgetting Risk Regions</title>
      <author><first>Shuhao</first><last>Gu</last></author>
      <author><first>Bojie</first><last>Hu</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <pages>1707-1718</pages>
      <abstract>This paper considers continual learning of large-scale pretrained neural machine translation model without accessing the previous training data or introducing model separation. We argue that the widely used regularization-based methods, which perform multi-objective learning with an auxiliary loss, suffer from the misestimate problem and cannot always achieve a good balance between the previous and new tasks. To solve the problem, we propose a two-stage training method based on the local features of the real loss. We first search low forgetting risk regions, where the model can retain the performance on the previous task as the parameters are updated, to avoid the catastrophic forgetting problem. Then we can continually train the model within this region only with the new training data to fit the new task. Specifically, we propose two methods to search the low forgetting risk regions, which are based on the curvature of loss and the impacts of the parameters on the model output, respectively. We conduct experiments on domain adaptation and more challenging language adaptation tasks, and the experimental results show that our method can achieve significant improvements compared with several strong baselines.</abstract>
      <url hash="3ececc7b">2022.emnlp-main.111</url>
      <bibkey>gu-etal-2022-continual</bibkey>
    </paper>
    <paper id="112">
      <title><fixed-case>COST</fixed-case>-<fixed-case>EFF</fixed-case>: Collaborative Optimization of Spatial and Temporal Efficiency with Slenderized Multi-exit Language Models</title>
      <author><first>Bowen</first><last>Shen</last></author>
      <author><first>Zheng</first><last>Lin</last></author>
      <author><first>Yuanxin</first><last>Liu</last></author>
      <author><first>Zhengxiao</first><last>Liu</last></author>
      <author><first>Lei</first><last>Wang</last></author>
      <author><first>Weiping</first><last>Wang</last></author>
      <pages>1719-1730</pages>
      <abstract>Transformer-based pre-trained language models (PLMs) mostly suffer from excessive overhead despite their advanced capacity. For resource-constrained devices, there is an urgent need for a spatially and temporally efficient model which retains the major capacity of PLMs. However, existing statically compressed models are unaware of the diverse complexities between input instances, potentially resulting in redundancy and inadequacy for simple and complex inputs. Also, miniature models with early exiting encounter challenges in the trade-off between making predictions and serving the deeper layers. Motivated by such considerations, we propose a collaborative optimization for PLMs that integrates static model compression and dynamic inference acceleration. Specifically, the PLM is slenderized in width while the depth remains intact, complementing layer-wise early exiting to speed up inference dynamically. To address the trade-off of early exiting, we propose a joint training approach that calibrates slenderization and preserves contributive structures to each exit instead of only the final layer. Experiments are conducted on GLUE benchmark and the results verify the Pareto optimality of our approach at high compression and acceleration rate with 1/8 parameters and 1/19 FLOPs of BERT.</abstract>
      <url hash="f712e079">2022.emnlp-main.112</url>
      <bibkey>shen-etal-2022-cost</bibkey>
    </paper>
    <paper id="113">
      <title>Rescue Implicit and Long-tail Cases: Nearest Neighbor Relation Extraction</title>
      <author><first>Zhen</first><last>Wan</last></author>
      <author><first>Qianying</first><last>Liu</last></author>
      <author><first>Zhuoyuan</first><last>Mao</last></author>
      <author><first>Fei</first><last>Cheng</last></author>
      <author><first>Sadao</first><last>Kurohashi</last></author>
      <author><first>Jiwei</first><last>Li</last></author>
      <pages>1731-1738</pages>
      <abstract>Relation extraction (RE) has achieved remarkable progress with the help of pre-trained language models. However, existing RE models are usually incapable of handling two situations: implicit expressions and long-tail relation types, caused by language complexity and data sparsity. In this paper, we introduce a simple enhancement of RE using <tex-math>k</tex-math> nearest neighbors (<tex-math>k</tex-math>NN-RE). <tex-math>k</tex-math>NN-RE allows the model to consult training relations at test time through a nearest-neighbor search and provides a simple yet effective means to tackle the two issues above. Additionally, we observe that <tex-math>k</tex-math>NN-RE serves as an effective way to leverage distant supervision (DS) data for RE. Experimental results show that the proposed <tex-math>k</tex-math>NN-RE achieves state-of-the-art performances on a variety of supervised RE datasets, i.e., ACE05, SciERC, and Wiki80, along with outperforming the best model to date on the i2b2 and Wiki80 datasets in the setting of allowing using DS. Our code and models are available at: https://github.com/YukinoWan/kNN-RE.</abstract>
      <url hash="10d97eaa">2022.emnlp-main.113</url>
      <bibkey>wan-etal-2022-rescue</bibkey>
    </paper>
    <paper id="114">
      <title><fixed-case>S</fixed-case>tory<fixed-case>ER</fixed-case>: Automatic Story Evaluation via Ranking, Rating and Reasoning</title>
      <author><first>Hong</first><last>Chen</last></author>
      <author><first>Duc</first><last>Vo</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <author><first>Hideki</first><last>Nakayama</last></author>
      <pages>1739-1753</pages>
      <abstract>Existing automatic story evaluation methods place a premium on story lexical level coherence, deviating from human preference.We go beyond this limitation by considering a novel Story Evaluation method that mimics human preference when judging a story, namely StoryER, which consists of three sub-tasks: Ranking, Rating and Reasoning.Given either a machine-generated or a human-written story, StoryER requires the machine to output 1) a preference score that corresponds to human preference, 2) specific ratings and their corresponding confidences and 3) comments for various aspects (e.g., opening, character-shaping).To support these tasks, we introduce a well-annotated dataset comprising (i) 100k ranked story pairs; and (ii) a set of 46k ratings and comments on various aspects of the story.We finetune Longformer-Encoder-Decoder (LED) on the collected dataset, with the encoder responsible for preference score and aspect prediction and the decoder for comment generation.Our comprehensive experiments result a competitive benchmark for each task, showing the high correlation to human preference.In addition, we have witnessed the joint learning of the preference scores, the aspect ratings, and the comments brings gain each single task.Our dataset and benchmarks are publicly available to advance the research of story evaluation tasks.</abstract>
      <url hash="1022c08a">2022.emnlp-main.114</url>
      <bibkey>chen-etal-2022-storyer</bibkey>
    </paper>
    <paper id="115">
      <title>Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference</title>
      <author><first>Eric</first><last>Mitchell</last></author>
      <author><first>Joseph</first><last>Noh</last></author>
      <author><first>Siyan</first><last>Li</last></author>
      <author><first>Will</first><last>Armstrong</last></author>
      <author><first>Ananth</first><last>Agarwal</last></author>
      <author><first>Patrick</first><last>Liu</last></author>
      <author><first>Chelsea</first><last>Finn</last></author>
      <author><first>Christopher</first><last>Manning</last></author>
      <pages>1754-1768</pages>
      <abstract>While large pre-trained language models are powerful, their predictions often lack logical consistency across test inputs. For example, a state-of-the-art Macaw question-answering (QA) model answers &lt;i&gt;Yes&lt;/i&gt; to &lt;i&gt;Is a sparrow a bird?&lt;/i&gt; and &lt;i&gt;Does a bird have feet?&lt;/i&gt; but answers &lt;i&gt;No&lt;/i&gt; to &lt;i&gt;Does a sparrow have feet?&lt;/i&gt;. To address this failure mode, we propose a framework, Consistency Correction through Relation Detection, or &lt;b&gt;ConCoRD&lt;/b&gt;, for boosting the consistency and accuracy of pre-trained NLP models using pre-trained natural language inference (NLI) models without fine-tuning or re-training. Given a batch of test inputs, ConCoRD samples several candidate outputs for each input and instantiates a factor graph that accounts for both the model’s belief about the likelihood of each answer choice in isolation and the NLI model’s beliefs about pair-wise answer choice compatibility. We show that a weighted MaxSAT solver can efficiently compute high-quality answer choices under this factor graph, improving over the raw model’s predictions. Our experiments demonstrate that ConCoRD consistently boosts accuracy and consistency of off-the-shelf closed-book QA and VQA models using off-the-shelf NLI models, notably increasing accuracy of LXMERT on ConVQA by 5% absolute. See the project website (https://ericmitchell.ai/emnlp-2022-concord/) for code and data.</abstract>
      <url hash="7ce83d21">2022.emnlp-main.115</url>
      <bibkey>mitchell-etal-2022-enhancing</bibkey>
    </paper>
    <paper id="116">
      <title>Robustness of Demonstration-based Learning Under Limited Data Scenario</title>
      <author><first>Hongxin</first><last>Zhang</last></author>
      <author><first>Yanzhe</first><last>Zhang</last></author>
      <author><first>Ruiyi</first><last>Zhang</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>1769-1782</pages>
      <abstract>Demonstration-based learning has shown great potential in stimulating pretrained language models’ ability under limited data scenario. Simply augmenting the input with some demonstrations can significantly improve performance on few-shot NER. However, why such demonstrations are beneficial for the learning process remains unclear since there is no explicit alignment between the demonstrations and the predictions. In this paper, we design pathological demonstrations by gradually removing intuitively useful information from the standard ones to take a deep dive of the robustness of demonstration-based sequence labeling and show that (1) demonstrations composed of random tokens still make the model a better few-shot learner; (2) the length of random demonstrations and the relevance of random tokens are the main factors affecting the performance; (3) demonstrations increase the confidence of model predictions on captured superficial patterns. We have publicly released our code at https://github.com/SALT-NLP/RobustDemo.</abstract>
      <url hash="e08c1169">2022.emnlp-main.116</url>
      <bibkey>zhang-etal-2022-robustness</bibkey>
    </paper>
    <paper id="117">
      <title>Modeling Information Change in Science Communication with Semantically Matched Paraphrases</title>
      <author><first>Dustin</first><last>Wright</last></author>
      <author><first>Jiaxin</first><last>Pei</last></author>
      <author><first>David</first><last>Jurgens</last></author>
      <author><first>Isabelle</first><last>Augenstein</last></author>
      <pages>1783-1807</pages>
      <abstract>Whether the media faithfully communicate scientific information has long been a core issue to the science community. Automatically identifying paraphrased scientific findings could enable large-scale tracking and analysis of information changes in the science communication process, but this requires systems to understand the similarity between scientific information across multiple domains. To this end, we present the SCIENTIFIC PARAPHRASE AND INFORMATION CHANGE DATASET (SPICED), the first paraphrase dataset of scientific findings annotated for degree of information change. SPICED contains 6,000 scientific finding pairs extracted from news stories, social media discussions, and full texts of original papers. We demonstrate that SPICED poses a challenging task and that models trained on SPICED improve downstream performance on evidence retrieval for fact checking of real-world scientific claims. Finally, we show that models trained on SPICED can reveal large-scale trends in the degrees to which people and organizations faithfully communicate new scientific findings. Data, code, and pre-trained models are available at http://www.copenlu.com/publication/2022_emnlp_wright/.</abstract>
      <url hash="1c84fa22">2022.emnlp-main.117</url>
      <bibkey>wright-etal-2022-modeling</bibkey>
    </paper>
    <paper id="118">
      <title>Word Order Matters When You Increase Masking</title>
      <author><first>Karim</first><last>Lasri</last></author>
      <author><first>Alessandro</first><last>Lenci</last></author>
      <author><first>Thierry</first><last>Poibeau</last></author>
      <pages>1808-1815</pages>
      <abstract>Word order, an essential property of natural languages, is injected in Transformer-based neural language models using position encoding. However, recent experiments have shown that explicit position encoding is not always useful, since some models without such feature managed to achieve state-of-the art performance on some tasks. To understand better this phenomenon, we examine the effect of removing position encodings on the pre-training objective itself (i.e., masked language modelling), to test whether models can reconstruct position information from co-occurrences alone. We do so by controlling the amount of masked tokens in the input sentence, as a proxy to affect the importance of position information for the task. We find that the necessity of position information increases with the amount of masking, and that masked language models without position encodings are not able to reconstruct this information on the task. These findings point towards a direct relationship between the amount of masking and the ability of Transformers to capture order-sensitive aspects of language using position encoding.</abstract>
      <url hash="76456fd8">2022.emnlp-main.118</url>
      <bibkey>lasri-etal-2022-word</bibkey>
    </paper>
    <paper id="119">
      <title>An Empirical Analysis of Memorization in Fine-tuned Autoregressive Language Models</title>
      <author><first>Fatemehsadat</first><last>Mireshghallah</last></author>
      <author><first>Archit</first><last>Uniyal</last></author>
      <author><first>Tianhao</first><last>Wang</last></author>
      <author><first>David</first><last>Evans</last></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last></author>
      <pages>1816-1826</pages>
      <abstract>Large language models are shown to present privacy risks through memorization of training data, andseveral recent works have studied such risks for the pre-training phase. Little attention, however, has been given to the fine-tuning phase and it is not well understood how different fine-tuning methods (such as fine-tuning the full model, the model head, and adapter) compare in terms of memorization risk. This presents increasing concern as the “pre-train and fine-tune” paradigm proliferates. In this paper, we empirically study memorization of fine-tuning methods using membership inference and extraction attacks, and show that their susceptibility to attacks is very different. We observe that fine-tuning the head of the model has the highest susceptibility to attacks, whereas fine-tuning smaller adapters appears to be less vulnerable to known extraction attacks.</abstract>
      <url hash="32d3aeff">2022.emnlp-main.119</url>
      <bibkey>mireshghallah-etal-2022-empirical</bibkey>
    </paper>
    <paper id="120">
      <title>Style Transfer as Data Augmentation: A Case Study on Named Entity Recognition</title>
      <author><first>Shuguang</first><last>Chen</last></author>
      <author><first>Leonardo</first><last>Neves</last></author>
      <author><first>Thamar</first><last>Solorio</last></author>
      <pages>1827-1841</pages>
      <abstract>In this work, we take the named entity recognition task in the English language as a case study and explore style transfer as a data augmentation method to increase the size and diversity of training data in low-resource scenarios. We propose a new method to effectively transform the text from a high-resource domain to a low-resource domain by changing its style-related attributes to generate synthetic data for training. Moreover, we design a constrained decoding algorithm along with a set of key ingredients for data selection to guarantee the generation of valid and coherent data. Experiments and analysis on five different domain pairs under different data regimes demonstrate that our approach can significantly improve results compared to current state-of-the-art data augmentation methods. Our approach is a practical solution to data scarcity, and we expect it to be applicable to other NLP tasks.</abstract>
      <url hash="cc7fb419">2022.emnlp-main.120</url>
      <bibkey>chen-etal-2022-style</bibkey>
    </paper>
    <paper id="121">
      <title>Linguistic Corpus Annotation for Automatic Text Simplification Evaluation</title>
      <author><first>Rémi</first><last>Cardon</last></author>
      <author><first>Adrien</first><last>Bibal</last></author>
      <author><first>Rodrigo</first><last>Wilkens</last></author>
      <author><first>David</first><last>Alfter</last></author>
      <author><first>Magali</first><last>Norré</last></author>
      <author><first>Adeline</first><last>Müller</last></author>
      <author><first>Watrin</first><last>Patrick</last></author>
      <author><first>Thomas</first><last>François</last></author>
      <pages>1842-1866</pages>
      <abstract>Evaluating automatic text simplification (ATS) systems is a difficult task that is either performed by automatic metrics or user-based evaluations. However, from a linguistic point-of-view, it is not always clear on what bases these evaluations operate. In this paper, we propose annotations of the ASSET corpus that can be used to shed more light on ATS evaluation. In addition to contributing with this resource, we show how it can be used to analyze SARI’s behavior and to re-evaluate existing ATS systems. We present our insights as a step to improve ATS evaluation protocols in the future.</abstract>
      <url hash="f38e0720">2022.emnlp-main.121</url>
      <bibkey>cardon-etal-2022-linguistic</bibkey>
    </paper>
    <paper id="122">
      <title>Semantic Framework based Query Generation for Temporal Question Answering over Knowledge Graphs</title>
      <author><first>Wentao</first><last>Ding</last></author>
      <author><first>Hao</first><last>Chen</last></author>
      <author><first>Huayu</first><last>Li</last></author>
      <author><first>Yuzhong</first><last>Qu</last></author>
      <pages>1867-1877</pages>
      <abstract>Answering factual questions with temporal intent over knowledge graphs (temporal KGQA) attracts rising attention in recent years.In the generation of temporal queries, existing KGQA methods ignore the fact that some intrinsic connections between events can make them temporally related, which may limit their capability.We systematically analyze the possible interpretation of temporal constraints and conclude the interpretation structures as the Semantic Framework of Temporal Constraints, SF-TCons.Based on the semantic framework, we propose a temporal question answering method, SF-TQA, which generates query graphs by exploring the relevant facts of mentioned entities, where the exploring process is restricted by SF-TCons. Our evaluations show that SF-TQA significantly outperforms existing methods on two benchmarks over different knowledge graphs.</abstract>
      <url hash="ad103abe">2022.emnlp-main.122</url>
      <bibkey>ding-etal-2022-semantic</bibkey>
    </paper>
    <paper id="123">
      <title>There Is No Standard Answer: Knowledge-Grounded Dialogue Generation with Adversarial Activated Multi-Reference Learning</title>
      <author><first>Xueliang</first><last>Zhao</last></author>
      <author><first>Tingchen</first><last>Fu</last></author>
      <author><first>Chongyang</first><last>Tao</last></author>
      <author><first>Rui</first><last>Yan</last></author>
      <pages>1878-1891</pages>
      <abstract>Knowledge-grounded dialogue (KGC) shows excellent potential to deliver an engaging and informative response. However, existing approaches emphasize selecting one golden knowledge given a particular dialogue context, overlooking the one-to-many phenomenon in dialogue. As a result, existing paradigm limits the diversity of knowledge selection and generation. To this end, we establish a multi-reference KGC dataset and propose a series of metrics to systematically assess the one-to-many efficacy of existing KGC models. Furthermore, to extend the hypothesis space of knowledge selection to enhance the mapping relationship between multiple knowledge and multiple responses, we devise a span-based variational model and optimize the model in a wake-sleep style with an ameliorated evidence lower bound objective to learn the one-to-many generalization. Both automatic and human evaluations demonstrate the efficacy of our approach.</abstract>
      <url hash="e87f740c">2022.emnlp-main.123</url>
      <bibkey>zhao-etal-2022-standard</bibkey>
    </paper>
    <paper id="124">
      <title>Stop Measuring Calibration When Humans Disagree</title>
      <author><first>Joris</first><last>Baan</last></author>
      <author><first>Wilker</first><last>Aziz</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <author><first>Raquel</first><last>Fernandez</last></author>
      <pages>1892-1915</pages>
      <abstract>Calibration is a popular framework to evaluate whether a classifier knows when it does not know - i.e., its predictive probabilities are a good indication of how likely a prediction is to be correct. Correctness is commonly estimated against the human majority class. Recently, calibration to human majority has been measured on tasks where humans inherently disagree about which class applies. We show that measuring calibration to human majority given inherent disagreements is theoretically problematic, demonstrate this empirically on the ChaosNLI dataset, and derive several instance-level measures of calibration that capture key statistical properties of human judgements - including class frequency, ranking and entropy.</abstract>
      <url hash="d0d9eec3">2022.emnlp-main.124</url>
      <bibkey>baan-etal-2022-stop</bibkey>
    </paper>
    <paper id="125">
      <title>Improving compositional generalization for multi-step quantitative reasoning in question answering</title>
      <author><first>Armineh</first><last>Nourbakhsh</last></author>
      <author><first>Cathy</first><last>Jiao</last></author>
      <author><first>Sameena</first><last>Shah</last></author>
      <author><first>Carolyn</first><last>Rosé</last></author>
      <pages>1916-1932</pages>
      <abstract>Quantitative reasoning is an important aspect of question answering, especially when numeric and verbal cues interact to indicate sophisticated, multi-step programs. In this paper, we demonstrate how modeling the compositional nature of quantitative text can enhance the performance and robustness of QA models, allowing them to capture arithmetic logic that is expressed verbally. Borrowing from the literature on semantic parsing, we propose a method that encourages the QA models to adjust their attention patterns and capture input/output alignments that are meaningful to the reasoning task. We show how this strategy improves program accuracy and renders the models more robust against overfitting as the number of reasoning steps grows. Our approach is designed as a standalone module which can be prepended to many existing models and trained in an end-to-end fashion without the need for additional supervisory signal. As part of this exercise, we also create a unified dataset building on four previously released numerical QA datasets over tabular data.</abstract>
      <url hash="5757b708">2022.emnlp-main.125</url>
      <bibkey>nourbakhsh-etal-2022-improving</bibkey>
    </paper>
    <paper id="126">
      <title>A Comprehensive Comparison of Neural Networks as Cognitive Models of Inflection</title>
      <author><first>Adam</first><last>Wiemerslage</last></author>
      <author><first>Shiran</first><last>Dudy</last></author>
      <author><first>Katharina</first><last>Kann</last></author>
      <pages>1933-1945</pages>
      <abstract>Neural networks have long been at the center of a debate around the cognitive mechanism by which humans process inflectional morphology. This debate has gravitated into NLP by way of the question: Are neural networks a feasible account for human behavior in morphological inflection?We address that question by measuring the correlation between human judgments and neural network probabilities for unknown word inflections. We test a larger range of architectures than previously studied on two important tasks for the cognitive processing debate: English past tense, and German number inflection. We find evidence that the Transformer may be a better account of human behavior than LSTMs on these datasets, and that LSTM features known to increase inflection accuracy do not always result in more human-like behavior.</abstract>
      <url hash="7ee60502">2022.emnlp-main.126</url>
      <bibkey>wiemerslage-etal-2022-comprehensive</bibkey>
    </paper>
    <paper id="127">
      <title>Can Visual Context Improve Automatic Speech Recognition for an Embodied Agent?</title>
      <author><first>Pradip</first><last>Pramanick</last></author>
      <author><first>Chayan</first><last>Sarkar</last></author>
      <pages>1946-1957</pages>
      <abstract>The usage of automatic speech recognition (ASR) systems are becoming omnipresent ranging from personal assistant to chatbots, home, and industrial automation systems, etc. Modern robots are also equipped with ASR capabilities for interacting with humans as speech is the most natural interaction modality. However, ASR in robots faces additional challenges as compared to a personal assistant. Being an embodied agent, a robot must recognize the physical entities around it and therefore reliably recognize the speech containing the description of such entities. However, current ASR systems are often unable to do so due to limitations in ASR training, such as generic datasets and open-vocabulary modeling. Also, adverse conditions during inference, such as noise, accented, and far-field speech makes the transcription inaccurate. In this work, we present a method to incorporate a robot’s visual information into an ASR system and improve the recognition of a spoken utterance containing a visible entity. Specifically, we propose a new decoder biasing technique to incorporate the visual context while ensuring the ASR output does not degrade for incorrect context. We achieve a 59% relative reduction in WER from an unmodified ASR system.</abstract>
      <url hash="e75531bd">2022.emnlp-main.127</url>
      <bibkey>pramanick-sarkar-2022-visual</bibkey>
    </paper>
    <paper id="128">
      <title><fixed-case>A</fixed-case>fro<fixed-case>LID</fixed-case>: A Neural Language Identification Tool for <fixed-case>A</fixed-case>frican Languages</title>
      <author><first>Ife</first><last>Adebara</last></author>
      <author><first>AbdelRahim</first><last>Elmadany</last></author>
      <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
      <author><first>Alcides</first><last>Inciarte</last></author>
      <pages>1958-1981</pages>
      <abstract>Language identification (LID) is a crucial precursor for NLP, especially for mining web data. Problematically, most of the world’s 7000+ languages today are not covered by LID technologies. We address this pressing issue for Africa by introducing AfroLID, a neural LID toolkit for 517 African languages and varieties. AfroLID exploits a multi-domain web dataset manually curated from across 14 language families utilizing five orthographic systems. When evaluated on our blind Test set, AfroLID achieves 95.89 F_1-score. We also compare AfroLID to five existing LID tools that each cover a small number of African languages, finding it to outperform them on most languages. We further show the utility of AfroLID in the wild by testing it on the acutely under-served Twitter domain. Finally, we offer a number of controlled case studies and perform a linguistically-motivated error analysis that allow us to both showcase AfroLID’s powerful capabilities and limitations</abstract>
      <url hash="2f2ac05a">2022.emnlp-main.128</url>
      <bibkey>adebara-etal-2022-afrolid</bibkey>
    </paper>
    <paper id="129">
      <title><fixed-case>E</fixed-case>v<fixed-case>E</fixed-case>nt<fixed-case>S</fixed-case> <fixed-case>R</fixed-case>ea<fixed-case>LM</fixed-case>: Event Reasoning of Entity States via Language Models</title>
      <author><first>Evangelia</first><last>Spiliopoulou</last></author>
      <author><first>Artidoro</first><last>Pagnoni</last></author>
      <author><first>Yonatan</first><last>Bisk</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <pages>1982-1997</pages>
      <abstract>This paper investigates models of event implications. Specifically, how well models predict entity state-changes, by targeting their understanding of physical attributes. Nominally, Large Language models (LLM) have been exposed to procedural knowledge about how objects interact, yet our benchmarking shows they fail to reason about the world. Conversely, we also demonstrate that existing approaches often misrepresent the surprising abilities of LLMs via improper task encodings and that proper model prompting can dramatically improve performance of reported baseline results across multiple tasks. In particular, our results indicate that our prompting technique is especially useful for unseen attributes (out-of-domain) or when only limited data is available.</abstract>
      <url hash="c475f34b">2022.emnlp-main.129</url>
      <bibkey>spiliopoulou-etal-2022-events</bibkey>
    </paper>
    <paper id="130">
      <title>Large language models are few-shot clinical information extractors</title>
      <author><first>Monica</first><last>Agrawal</last></author>
      <author><first>Stefan</first><last>Hegselmann</last></author>
      <author><first>Hunter</first><last>Lang</last></author>
      <author><first>Yoon</first><last>Kim</last></author>
      <author><first>David</first><last>Sontag</last></author>
      <pages>1998-2022</pages>
      <abstract>A long-running goal of the clinical NLP community is the extraction of important variables trapped in clinical notes. However, roadblocks have included dataset shift from the general domain and a lack of public clinical corpora and annotations. In this work, we show that large language models, such as InstructGPT (Ouyang et al., 2022), perform well at zero- and few-shot information extraction from clinical text despite not being trained specifically for the clinical domain. Whereas text classification and generation performance have already been studied extensively in such models, here we additionally demonstrate how to leverage them to tackle a diverse set of NLP tasks which require more structured outputs, including span identification, token-level sequence classification, and relation extraction. Further, due to the dearth of available data to evaluate these systems, we introduce new datasets for benchmarking few-shot clinical information extraction based on a manual re-annotation of the CASI dataset (Moon et al., 2014) for new tasks. On the clinical extraction tasks we studied, the GPT-3 systems significantly outperform existing zero- and few-shot baselines.</abstract>
      <url hash="287866fb">2022.emnlp-main.130</url>
      <bibkey>agrawal-etal-2022-large</bibkey>
    </paper>
    <paper id="131">
      <title>Towards a Unified Multi-Dimensional Evaluator for Text Generation</title>
      <author><first>Ming</first><last>Zhong</last></author>
      <author id="yang-liu-microsoft"><first>Yang</first><last>Liu</last></author>
      <author><first>Da</first><last>Yin</last></author>
      <author><first>Yuning</first><last>Mao</last></author>
      <author><first>Yizhu</first><last>Jiao</last></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>2023-2038</pages>
      <abstract>Multi-dimensional evaluation is the dominant paradigm for human evaluation in Natural Language Generation (NLG), i.e., evaluating the generated text from multiple explainable dimensions, such as coherence and fluency. However, automatic evaluation in NLG is still dominated by similarity-based metrics, and we lack a reliable framework for a more comprehensive evaluation of advanced models. In this paper, we propose a unified multi-dimensional evaluator UniEval for NLG. We re-frame NLG evaluation as a Boolean Question Answering (QA) task, and by guiding the model with different questions, we can use one evaluator to evaluate from multiple dimensions. Furthermore, thanks to the unified Boolean QA format, we are able to introduce an intermediate learning phase that enables UniEval to incorporate external knowledge from multiple related tasks and gain further improvement. Experiments on three typical NLG tasks show that UniEval correlates substantially better with human judgments than existing metrics. Specifically, compared to the top-performing unified evaluators, UniEval achieves a 23% higher correlation on text summarization, and over 43% on dialogue response generation. Also, UniEval demonstrates a strong zero-shot learning ability for unseen evaluation dimensions and tasks. Source code, data, and all pre-trained evaluators are available at https://github.com/maszhongming/UniEval.</abstract>
      <url hash="5edddb57">2022.emnlp-main.131</url>
      <bibkey>zhong-etal-2022-towards</bibkey>
    </paper>
    <paper id="132">
      <title><fixed-case>G</fixed-case>eo<fixed-case>MLAMA</fixed-case>: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models</title>
      <author><first>Da</first><last>Yin</last></author>
      <author><first>Hritik</first><last>Bansal</last></author>
      <author><first>Masoud</first><last>Monajatipoor</last></author>
      <author><first>Liunian Harold</first><last>Li</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>2039-2055</pages>
      <abstract>Recent work has shown that Pre-trained Language Models (PLMs) store the relational knowledge learned from data and utilize it for performing downstream tasks. However, commonsense knowledge across different regions may vary. For instance, the color of bridal dress is white in American weddings whereas it is red in Chinese weddings. In this paper, we introduce a benchmark dataset, Geo-diverse Commonsense Multilingual Language Models Analysis (GeoMLAMA), for probing the diversity of the relational knowledge in multilingual PLMs. GeoMLAMA contains 3125 prompts in English, Chinese, Hindi, Persian, and Swahili, with a wide coverage of concepts shared by people from American, Chinese, Indian, Iranian and Kenyan cultures. We benchmark 11 standard multilingual PLMs on GeoMLAMA. Interestingly, we find that 1) larger multilingual PLMs variants do not necessarily store geo-diverse concepts better than its smaller variant; 2) multilingual PLMs are not intrinsically biased towards knowledge from the Western countries (the United States); 3) the native language of a country may not be the best language to probe its knowledge and 4) a language may better probe knowledge about a non-native country than its native country.</abstract>
      <url hash="3e15ef42">2022.emnlp-main.132</url>
      <bibkey>yin-etal-2022-geomlama</bibkey>
    </paper>
    <paper id="133">
      <title>The (Undesired) Attenuation of Human Biases by Multilinguality</title>
      <author><first>Cristina</first><last>España-Bonet</last></author>
      <author><first>Alberto</first><last>Barrón-Cedeño</last></author>
      <pages>2056-2077</pages>
      <abstract>Some human preferences are universal. The odor of vanilla is perceived as pleasant all around the world. We expect neural models trained on human texts to exhibit these kind of preferences, i.e. biases, but we show that this is not always the case. We explore 16 static and contextual embedding models in 9 languages and, when possible, compare them under similar training conditions. We introduce and release CA-WEAT, multilingual cultural aware tests to quantify biases, and compare them to previous English-centric tests. Our experiments confirm that monolingual static embeddings do exhibit human biases, but values differ across languages, being far from universal. Biases are less evident in contextual models, to the point that the original human association might be reversed. Multilinguality proves to be another variable that attenuates and even reverses the effect of the bias, specially in contextual multilingual models. In order to explain this variance among models and languages, we examine the effect of asymmetries in the training corpus, departures from isomorphism in multilingual embedding spaces and discrepancies in the testing measures between languages.</abstract>
      <url hash="f733b5c7">2022.emnlp-main.133</url>
      <bibkey>espana-bonet-barron-cedeno-2022-undesired</bibkey>
    </paper>
    <paper id="134">
      <title>Entailer: Answering Questions with Faithful and Truthful Chains of Reasoning</title>
      <author><first>Oyvind</first><last>Tafjord</last></author>
      <author><first>Bhavana</first><last>Dalvi Mishra</last></author>
      <author><first>Peter</first><last>Clark</last></author>
      <pages>2078-2093</pages>
      <abstract>Our goal is a question-answering (QA) system that can show how its answers are implied by its own internal beliefs via a systematic chain of reasoning. Such a capability would allow better understanding of why a model produced the answer it did. Our approach is to recursively combine a trained backward-chainingmodel, capable of generating a set of premises entailing an answer hypothesis, with a verifier that checks that the model itself believes those premises (and the entailment itself) through self-querying. To our knowledge, this is the first system to generate multistep chains that are both faithful (the answer follows from the reasoning) and truthful (the chain reflects the system’s own internal beliefs). In evaluation using two different datasets, users judge that a majority (70%+) of generated chains clearly show how an answer follows from a set of facts - substantially better than a high-performance baseline - while preserving answer accuracy. By materializing model beliefs that systematically support an answer, new opportunities arise for understanding the model’s system of belief, and diagnosing and correcting its misunderstandings when an answer is wrong.</abstract>
      <url hash="618944cb">2022.emnlp-main.134</url>
      <bibkey>tafjord-etal-2022-entailer</bibkey>
    </paper>
    <paper id="135">
      <title>Near-Negative Distinction: Giving a Second Life to Human Evaluation Datasets</title>
      <author><first>Philippe</first><last>Laban</last></author>
      <author><first>Chien-Sheng</first><last>Wu</last></author>
      <author><first>Wenhao</first><last>Liu</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <pages>2094-2108</pages>
      <abstract>Precisely assessing the progress in natural language generation (NLG) tasks is challenging, and human evaluation to establish a preference in a model’s output over another is often necessary.However, human evaluation is usually costly, difficult to reproduce, and non-reusable.In this paper, we propose a new and simple automatic evaluation method for NLG called Near-Negative Distinction (NND) that repurposes prior human annotations into NND tests.In an NND test, an NLG model must place a higher likelihood on a high-quality output candidate than on a near-negative candidate with a known error.Model performance is established by the number of NND tests a model passes, as well as the distribution over task-specific errors the model fails on.Through experiments on three NLG tasks (question generation, question answering, and summarization), we show that NND achieves a higher correlation with human judgments than standard NLG evaluation metrics. We then illustrate NND evaluation in four practical scenarios, for example performing fine-grain model analysis, or studying model training dynamics. Our findings suggest that NND can give a second life to human annotations and provide low-cost NLG evaluation.</abstract>
      <url hash="b41098da">2022.emnlp-main.135</url>
      <bibkey>laban-etal-2022-near</bibkey>
    </paper>
    <paper id="136">
      <title><fixed-case>T</fixed-case>o<fixed-case>K</fixed-case>en: Task Decomposition and Knowledge Infusion for Few-Shot Hate Speech Detection</title>
      <author><first>Badr</first><last>AlKhamissi</last></author>
      <author><first>Faisal</first><last>Ladhak</last></author>
      <author><first>Srinivasan</first><last>Iyer</last></author>
      <author><first>Veselin</first><last>Stoyanov</last></author>
      <author><first>Zornitsa</first><last>Kozareva</last></author>
      <author><first>Xian</first><last>Li</last></author>
      <author><first>Pascale</first><last>Fung</last></author>
      <author><first>Lambert</first><last>Mathias</last></author>
      <author><first>Asli</first><last>Celikyilmaz</last></author>
      <author><first>Mona</first><last>Diab</last></author>
      <pages>2109-2120</pages>
      <abstract>Hate speech detection is complex; it relies on commonsense reasoning, knowledge of stereotypes, and an understanding of social nuance that differs from one culture to the next. It is also difficult to collect a large-scale hate speech annotated dataset. In this work, we frame this problem as a few-shot learning task, and show significant gains with decomposing the task into its “constituent” parts. In addition, we see that infusing knowledge from reasoning datasets (e.g. ATOMIC2020) improves the performance even further. Moreover, we observe that the trained models generalize to out-of-distribution datasets, showing the superiority of task decomposition and knowledge infusion compared to previously used methods. Concretely, our method outperforms the baseline by 17.83% absolute gain in the 16-shot case.</abstract>
      <url hash="570c583e">2022.emnlp-main.136</url>
      <bibkey>alkhamissi-etal-2022-token</bibkey>
    </paper>
    <paper id="137">
      <title>Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations</title>
      <author><first>Swarnadeep</first><last>Saha</last></author>
      <author><first>Peter</first><last>Hase</last></author>
      <author><first>Nazneen</first><last>Rajani</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>2121-2131</pages>
      <abstract>Recent work on explainable NLP has shown that few-shot prompting can enable large pre-trained language models (LLMs) to generate grammatical and factual natural language explanations for data labels. In this work, we study the connection between explainability and sample hardness by investigating the following research question – “Are LLMs and humans equally good at explaining data labels for both easy and hard samples?” We answer this question by first collecting human-written explanations in the form of generalizable commonsense rules on the task of Winograd Schema Challenge (Winogrande dataset). We compare these explanations with those generated by GPT-3 while varying the hardness of the test samples as well as the in-context samples. We observe that (1) GPT-3 explanations are as grammatical as human explanations regardless of the hardness of the test samples, (2) for easy examples, GPT-3 generates highly supportive explanations but human explanations are more generalizable, and (3) for hard examples, human explanations are significantly better than GPT-3 explanations both in terms of label-supportiveness and generalizability judgements. We also find that hardness of the in-context examples impacts the quality of GPT-3 explanations. Finally, we show that the supportiveness and generalizability aspects of human explanations are also impacted by sample hardness, although by a much smaller margin than models.</abstract>
      <url hash="ab814b59">2022.emnlp-main.137</url>
      <bibkey>saha-etal-2022-hard</bibkey>
    </paper>
    <paper id="138">
      <title>Stanceosaurus: Classifying Stance Towards Multicultural Misinformation</title>
      <author><first>Jonathan</first><last>Zheng</last></author>
      <author><first>Ashutosh</first><last>Baheti</last></author>
      <author><first>Tarek</first><last>Naous</last></author>
      <author><first>Wei</first><last>Xu</last></author>
      <author><first>Alan</first><last>Ritter</last></author>
      <pages>2132-2151</pages>
      <abstract>We present Stanceosaurus, a new corpus of 28,033 tweets in English, Hindi and Arabic annotated with stance towards 250 misinformation claims. As far as we are aware, it is the largest corpus annotated with stance towards misinformation claims. The claims in Stanceosaurus originate from 15 fact-checking sources that cover diverse geographical regions and cultures. Unlike existing stance datasets, we introduce a more fine-grained 5-class labeling strategy with additional subcategories to distinguish implicit stance. Pre-trained transformer-based stance classifiers that are fine-tuned on our corpus show good generalization on unseen claims and regional claims from countries outside the training data. Cross-lingual experiments demonstrate Stanceosaurus’ capability of training multilingual models, achieving 53.1 F1 on Hindi and 50.4 F1 on Arabic without any target-language fine-tuning. Finally, we show how a domain adaptation method can be used to improve performance on Stanceosaurus using additional RumourEval-2019 data. We will make Stanceosaurus publicly available to the research community upon publication and hope it will encourage further work on misinformation identification across languages and cultures.</abstract>
      <url hash="db2dd29a">2022.emnlp-main.138</url>
      <bibkey>zheng-etal-2022-stanceosaurus</bibkey>
    </paper>
    <paper id="139">
      <title>Gendered Mental Health Stigma in Masked Language Models</title>
      <author><first>Inna</first><last>Lin</last></author>
      <author><first>Lucille</first><last>Njoo</last></author>
      <author><first>Anjalie</first><last>Field</last></author>
      <author><first>Ashish</first><last>Sharma</last></author>
      <author><first>Katharina</first><last>Reinecke</last></author>
      <author><first>Tim</first><last>Althoff</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <pages>2152-2170</pages>
      <abstract>Mental health stigma prevents many individuals from receiving the appropriate care, and social psychology studies have shown that mental health tends to be overlooked in men. In this work, we investigate gendered mental health stigma in masked language models. In doing so, we operationalize mental health stigma by developing a framework grounded in psychology research: we use clinical psychology literature to curate prompts, then evaluate the models’ propensity to generate gendered words. We find that masked language models capture societal stigma about gender in mental health: models are consistently more likely to predict female subjects than male in sentences about having a mental health condition (32% vs. 19%), and this disparity is exacerbated for sentences that indicate treatment-seeking behavior. Furthermore, we find that different models capture dimensions of stigma differently for men and women, associating stereotypes like anger, blame, and pity more with women with mental health conditions than with men. In showing the complex nuances of models’ gendered mental health stigma, we demonstrate that context and overlapping dimensions of identity are important considerations when assessing computational models’ social biases.</abstract>
      <url hash="0438531d">2022.emnlp-main.139</url>
      <bibkey>lin-etal-2022-gendered</bibkey>
    </paper>
    <paper id="140">
      <title>Efficient Nearest Neighbor Search for Cross-Encoder Models using Matrix Factorization</title>
      <author><first>Nishant</first><last>Yadav</last></author>
      <author><first>Nicholas</first><last>Monath</last></author>
      <author><first>Rico</first><last>Angell</last></author>
      <author><first>Manzil</first><last>Zaheer</last></author>
      <author><first>Andrew</first><last>McCallum</last></author>
      <pages>2171-2194</pages>
      <abstract>Efficient k-nearest neighbor search is a fundamental task, foundational for many problems in NLP. When the similarity is measured by dot-product between dual-encoder vectors or L2-distance, there already exist many scalable and efficient search methods. But not so when similarity is measured by more accurate and expensive black-box neural similarity models, such as cross-encoders, which jointly encode the query and candidate neighbor. The cross-encoders’ high computational cost typically limits their use to reranking candidates retrieved by a cheaper model, such as dual encoder or TF-IDF. However, the accuracy of such a two-stage approach is upper-bounded by the recall of the initial candidate set, and potentially requires additional training to align the auxiliary retrieval model with the cross-encoder model. In this paper, we present an approach that avoids the use of a dual-encoder for retrieval, relying solely on the cross-encoder. Retrieval is made efficient with CUR decomposition, a matrix decomposition approach that approximates all pairwise cross-encoder distances from a small subset of rows and columns of the distance matrix. Indexing items using our approach is computationally cheaper than training an auxiliary dual-encoder model through distillation. Empirically, for k &gt; 10, our approach provides test-time recall-vs-computational cost trade-offs superior to the current widely-used methods that re-rank items retrieved using a dual-encoder or TF-IDF.</abstract>
      <url hash="fe1d8a5a">2022.emnlp-main.140</url>
      <bibkey>yadav-etal-2022-efficient</bibkey>
    </paper>
    <paper id="141">
      <title>Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual Style Transfer with Small Language Models</title>
      <author><first>Mirac</first><last>Suzgun</last></author>
      <author><first>Luke</first><last>Melas-Kyriazi</last></author>
      <author><first>Dan</first><last>Jurafsky</last></author>
      <pages>2195-2222</pages>
      <abstract>We propose a method for arbitrary textual style transfer (TST)—the task of transforming a text into any given style—utilizing general-purpose pre-trained language models. Our method, Prompt-and-Rerank, is based on a mathematical formulation of the TST task, decomposing it into three constituent components: textual similarity, target style strength, and fluency. Our method uses zero-shot or few-shot prompting to obtain a set of candidate generations in the target style, and then re-ranks them according to the three components. Our method enables small pre-trained language models to perform on par with state-of-the-art large-scale models while using two orders of magnitude less compute and memory. We also investigate the effect of model size and prompt design (e.g., prompt paraphrasing and delimiter-pair choice) on style transfer quality across seven diverse textual style transfer datasets, finding, among other things, that delimiter-pair choice has a large impact on performance, and that models have biases on the direction of style transfer.</abstract>
      <url hash="70e8c47c">2022.emnlp-main.141</url>
      <bibkey>suzgun-etal-2022-prompt</bibkey>
    </paper>
    <paper id="142">
      <title>Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts</title>
      <author><first>Ben</first><last>Zhou</last></author>
      <author><first>Kyle</first><last>Richardson</last></author>
      <author><first>Xiaodong</first><last>Yu</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>2223-2235</pages>
      <abstract>Explicit decomposition modeling, which involves breaking down complex tasks into more straightforward and often more interpretable sub-tasks, has long been a central theme in developing robust and interpretable NLU systems. However, despite the many datasets and resources built as part of this effort, the majority have small-scale annotations and limited scope, which is insufficient to solve general decomposition tasks. In this paper, we look at large-scale intermediate pre-training of decomposition-based transformers using distant supervision from comparable texts, particularly large-scale parallel news. We show that with such intermediate pre-training, developing robust decomposition-based models for a diverse range of tasks becomes more feasible. For example, on semantic parsing, our model, DecompT5, improves 20% to 30% on two datasets, Overnight and TORQUE, over the baseline language model. We further use DecompT5 to build a novel decomposition-based QA system named DecompEntail, improving over state-of-the-art models, including GPT-3, on both HotpotQA and StrategyQA by 8% and 4%, respectively.</abstract>
      <url hash="3e92edac">2022.emnlp-main.142</url>
      <bibkey>zhou-etal-2022-learning-decompose</bibkey>
    </paper>
    <paper id="143">
      <title>Why is Winoground Hard? Investigating Failures in Visuolinguistic Compositionality</title>
      <author><first>Anuj</first><last>Diwan</last></author>
      <author><first>Layne</first><last>Berry</last></author>
      <author><first>Eunsol</first><last>Choi</last></author>
      <author><first>David</first><last>Harwath</last></author>
      <author><first>Kyle</first><last>Mahowald</last></author>
      <pages>2236-2250</pages>
      <abstract>Recent visuolinguistic pre-trained models show promising progress on various end tasks such as image retrieval and video captioning. Yet, they fail miserably on the recently proposed Winoground dataset, which challenges models to match paired images and English captions, with items constructed to overlap lexically but differ in meaning (e.g., “there is a mug in some grass” vs. “there is some grass in a mug”). By annotating the dataset using new fine-grained tags, we show that solving the Winoground task requires not just compositional language understanding, but a host of other abilities like commonsense reasoning or locating small, out-of-focus objects in low-resolution images. In this paper, we identify the dataset’s main challenges through a suite of experiments on related tasks (probing task, image retrieval task), data augmentation, and manual inspection of the dataset. Our analysis suggests that a main challenge in visuolinguistic models may lie in fusing visual and textual representations, rather than in compositional language understanding. We release our annotation and code at https://github.com/ajd12342/why-winoground-hard.</abstract>
      <url hash="6e175287">2022.emnlp-main.143</url>
      <bibkey>diwan-etal-2022-winoground</bibkey>
    </paper>
    <paper id="144">
      <title>Gradient-based Constrained Sampling from Language Models</title>
      <author><first>Sachin</first><last>Kumar</last></author>
      <author><first>Biswajit</first><last>Paria</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <pages>2251-2277</pages>
      <abstract>Large pretrained language models are successful at generating fluent text but are notoriously hard to controllably sample from. In this work, we study constrained sampling from such language models, i.e., generating text that satisfies user-defined constraints, while maintaining fluency and model’s performance in a downstream task. We propose MuCoLa—a sampling procedure that combines the log-likelihood of the language model with arbitrary (differentiable) constraints in a single energy function, and then generates samples in a non-autoregressive manner. Specifically, it initializes the entire output sequence with noise and follows a Markov chain defined by Langevin Dynamics using the gradients of this energy. We evaluate MuCoLa on text generation with soft and hard constraints as well as their combinations, obtaining significant improvements over competitive baselines for toxicity avoidance, sentiment control, and keyword-guided generation.</abstract>
      <url hash="16de7364">2022.emnlp-main.144</url>
      <bibkey>kumar-etal-2022-gradient</bibkey>
    </paper>
    <paper id="145">
      <title><fixed-case>T</fixed-case>a<fixed-case>C</fixed-case>ube: Pre-computing Data Cubes for Answering Numerical-Reasoning Questions over Tabular Data</title>
      <author><first>Fan</first><last>Zhou</last></author>
      <author><first>Mengkang</first><last>Hu</last></author>
      <author><first>Haoyu</first><last>Dong</last></author>
      <author><first>Zhoujun</first><last>Cheng</last></author>
      <author><first>Fan</first><last>Cheng</last></author>
      <author><first>Shi</first><last>Han</last></author>
      <author><first>Dongmei</first><last>Zhang</last></author>
      <pages>2278-2291</pages>
      <abstract>Existing auto-regressive pre-trained language models (PLMs) like T5 and BART, have been well applied to table question answering by UNIFIEDSKG and TAPEX, respectively, and demonstrated state-of-the-art results on multiple benchmarks. However, auto-regressive PLMs are challenged by recent emerging numerical reasoning datasets, such as TAT-QA, due to the error-prone implicit calculation. In this paper, we present TaCube, to pre-compute aggregation/arithmetic results for the table in advance, so that they are handy and readily available for PLMs to answer numerical reasoning questions. TaCube systematically and comprehensively covers a collection of computational operations over table segments. By simply concatenating TaCube to the input sequence of PLMs, it shows significant experimental effectiveness. TaCube promotes the F1 score from 49.6% to 66.2% on TAT-QA and achieves new state-of-the-art results on WikiTQ (59.6% denotation accuracy). TaCube’s improvements on numerical reasoning cases are even more notable: on TAT-QA, TaCube promotes the exact match accuracy of BART-large by 39.6% on sum, 52.5% on average, 36.6% on substraction, and 22.2% on division. We believe that TaCube is a general and portable pre-computation solution that can be potentially integrated to various numerical reasoning frameworks</abstract>
      <url hash="f34e5335">2022.emnlp-main.145</url>
      <bibkey>zhou-etal-2022-tacube</bibkey>
    </paper>
    <paper id="146">
      <title>Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence</title>
      <author><first>Hung-Ting</first><last>Chen</last></author>
      <author><first>Michael</first><last>Zhang</last></author>
      <author><first>Eunsol</first><last>Choi</last></author>
      <pages>2292-2307</pages>
      <abstract>Question answering models can use rich knowledge sources — up to one hundred retrieved passages and parametric knowledge in the large-scale language model (LM). Prior work assumes information in such knowledge sources is consistent with each other, paying little attention to how models blend information stored in their LM parameters with that from retrieved evidence documents. In this paper, we simulate knowledge conflicts (i.e., where parametric knowledge suggests one answer and different passages suggest different answers) and examine model behaviors. We find retrieval performance heavily impacts which sources models rely on, and current models mostly rely on non-parametric knowledgein their best-performing settings. We discover a troubling trend that contradictions among knowledge sources affect model confidence only marginally. To address this issue, we present a new calibration study, where models are discouraged from presenting any single answer when presented with multiple conflicting answer candidates in retrieved evidences.</abstract>
      <url hash="31317448">2022.emnlp-main.146</url>
      <bibkey>chen-etal-2022-rich</bibkey>
    </paper>
    <paper id="147">
      <title><fixed-case>QA</fixed-case> Domain Adaptation using Hidden Space Augmentation and Self-Supervised Contrastive Adaptation</title>
      <author><first>Zhenrui</first><last>Yue</last></author>
      <author><first>Huimin</first><last>Zeng</last></author>
      <author><first>Bernhard</first><last>Kratzwald</last></author>
      <author><first>Stefan</first><last>Feuerriegel</last></author>
      <author><first>Dong</first><last>Wang</last></author>
      <pages>2308-2321</pages>
      <abstract>Question answering (QA) has recently shown impressive results for answering questions from customized domains. Yet, a common challenge is to adapt QA models to an unseen target domain. In this paper, we propose a novel self-supervised framework called QADA for QA domain adaptation. QADA introduces a novel data augmentation pipeline used to augment training QA samples. Different from existing methods, we enrich the samples via hidden space augmentation. For questions, we introduce multi-hop synonyms and sample augmented token embeddings with Dirichlet distributions. For contexts, we develop an augmentation method which learns to drop context spans via a custom attentive sampling strategy. Additionally, contrastive learning is integrated in the proposed self-supervised adaptation framework QADA. Unlike existing approaches, we generate pseudo labels and propose to train the model via a novel attention-based contrastive adaptation method. The attention weights are used to build informative features for discrepancy estimation that helps the QA model separate answers and generalize across source and target domains. To the best of our knowledge, our work is the first to leverage hidden space augmentation and attention-based contrastive adaptation for self-supervised domain adaptation in QA. Our evaluation shows that QADA achieves considerable improvements on multiple target datasets over state-of-the-art baselines in QA domain adaptation.</abstract>
      <url hash="a2e1c73d">2022.emnlp-main.147</url>
      <bibkey>yue-etal-2022-qa</bibkey>
    </paper>
    <paper id="148">
      <title>When <fixed-case>FLUE</fixed-case> Meets <fixed-case>FLANG</fixed-case>: Benchmarks and Large Pretrained Language Model for Financial Domain</title>
      <author><first>Raj</first><last>Shah</last></author>
      <author><first>Kunal</first><last>Chawla</last></author>
      <author><first>Dheeraj</first><last>Eidnani</last></author>
      <author><first>Agam</first><last>Shah</last></author>
      <author><first>Wendi</first><last>Du</last></author>
      <author><first>Sudheer</first><last>Chava</last></author>
      <author><first>Natraj</first><last>Raman</last></author>
      <author><first>Charese</first><last>Smiley</last></author>
      <author><first>Jiaao</first><last>Chen</last></author>
      <author><first>Diyi</first><last>Yang</last></author>
      <pages>2322-2335</pages>
      <abstract>Pre-trained language models have shown impressive performance on a variety of tasks and domains. Previous research on financial language models usually employs a generic training scheme to train standard model architectures, without completely leveraging the richness of the financial data. We propose a novel domain specific Financial LANGuage model (FLANG) which uses financial keywords and phrases for better masking, together with span boundary objective and in-filing objective. Additionally, the evaluation benchmarks in the field have been limited. To this end, we contribute the Financial Language Understanding Evaluation (FLUE), an open-source comprehensive suite of benchmarks for the financial domain. These include new benchmarks across 5 NLP tasks in financial domain as well as common benchmarks used in the previous research. Experiments on these benchmarks suggest that our model outperforms those in prior literature on a variety of NLP tasks. Our models, code and benchmark data will be made publicly available on Github and Huggingface.</abstract>
      <url hash="fd6e1520">2022.emnlp-main.148</url>
      <bibkey>shah-etal-2022-flue</bibkey>
    </paper>
    <paper id="149">
      <title>Retrieval as Attention: End-to-end Learning of Retrieval and Reading within a Single Transformer</title>
      <author><first>Zhengbao</first><last>Jiang</last></author>
      <author><first>Luyu</first><last>Gao</last></author>
      <author><first>Zhiruo</first><last>Wang</last></author>
      <author><first>Jun</first><last>Araki</last></author>
      <author><first>Haibo</first><last>Ding</last></author>
      <author><first>Jamie</first><last>Callan</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>2336-2349</pages>
      <abstract>Systems for knowledge-intensive tasks such as open-domain question answering (QA) usually consist of two stages: efficient retrieval of relevant documents from a large corpus and detailed reading of the selected documents. This is usually done through two separate models, a retriever that encodes the query and finds nearest neighbors, and a reader based on Transformers. These two components are usually modeled separately, which necessitates a cumbersome implementation and is awkward to optimize in an end-to-end fashion. In this paper, we revisit this design and eschew the separate architecture and training in favor of a single Transformer that performs retrieval as attention (RAA), and end-to-end training solely based on supervision from the end QA task. We demonstrate for the first time that an end-to-end trained single Transformer can achieve both competitive retrieval and QA performance on in-domain datasets, matching or even slightly outperforming state-of-the-art dense retrievers and readers. Moreover, end-to-end adaptation of our model significantly boosts its performance on out-of-domain datasets in both supervised and unsupervised settings, making our model a simple and adaptable end-to-end solution for knowledge-intensive tasks.</abstract>
      <url hash="e5061585">2022.emnlp-main.149</url>
      <bibkey>jiang-etal-2022-retrieval</bibkey>
    </paper>
    <paper id="150">
      <title>Reproducibility in Computational Linguistics: Is Source Code Enough?</title>
      <author><first>Mohammad</first><last>Arvan</last></author>
      <author><first>Luís</first><last>Pina</last></author>
      <author><first>Natalie</first><last>Parde</last></author>
      <pages>2350-2361</pages>
      <abstract>The availability of source code has been put forward as one of the most critical factors for improving the reproducibility of scientific research. This work studies trends in source code availability at major computational linguistics conferences, namely, ACL, EMNLP, LREC, NAACL, and COLING. We observe positive trends, especially in conferences that actively promote reproducibility. We follow this by conducting a reproducibility study of eight papers published in EMNLP 2021, finding that source code releases leave much to be desired. Moving forward, we suggest all conferences require self-contained artifacts and provide a venue to evaluate such artifacts at the time of publication. Authors can include small-scale experiments and explicit scripts to generate each result to improve the reproducibility of their work.</abstract>
      <url hash="42b38564">2022.emnlp-main.150</url>
      <bibkey>arvan-etal-2022-reproducibility</bibkey>
    </paper>
    <paper id="151">
      <title>Generating Information-Seeking Conversations from Unlabeled Documents</title>
      <author><first>Gangwoo</first><last>Kim</last></author>
      <author><first>Sungdong</first><last>Kim</last></author>
      <author><first>Kang Min</first><last>Yoo</last></author>
      <author><first>Jaewoo</first><last>Kang</last></author>
      <pages>2362-2378</pages>
      <abstract>Synthesizing datasets for conversational question answering (CQA) from unlabeled documents remains challenging due to its interactive nature.Moreover, while modeling information needs is an essential key, only few studies have discussed it.In this paper, we introduce a novel framework, **SimSeek**, (**Sim**ulating information-**Seek**ing conversation from unlabeled documents), and compare its two variants.In our baseline, **SimSeek-sym**, a questioner generates follow-up questions upon the predetermined answer by an answerer.On the contrary, **SimSeek-asym** first generates the question and then finds its corresponding answer under the conversational context.Our experiments show that they can synthesize effective training resources for CQA and conversational search tasks.As a result, conversations from **SimSeek-asym** not only make more improvements in our experiments but also are favorably reviewed in a human evaluation.We finally release a large-scale resource of synthetic conversations, **Wiki-SimSeek**, containing 2 million CQA pairs built upon Wikipedia documents.With the dataset, our CQA model achieves the state-of-the-art performance on a recent CQA benchmark, QuAC.The code and dataset are available at https://github.com/naver-ai/simseek</abstract>
      <url hash="50cd673c">2022.emnlp-main.151</url>
      <bibkey>kim-etal-2022-generating</bibkey>
    </paper>
    <paper id="152">
      <title>Distill The Image to Nowhere: Inversion Knowledge Distillation for Multimodal Machine Translation</title>
      <author><first>Ru</first><last>Peng</last></author>
      <author><first>Yawen</first><last>Zeng</last></author>
      <author><first>Jake</first><last>Zhao</last></author>
      <pages>2379-2390</pages>
      <abstract>Past works on multimodal machine translation (MMT) elevate bilingual setup by incorporating additional aligned vision information.However, an image-must requirement of the multimodal dataset largely hinders MMT’s development — namely that it demands an aligned form of [image, source text, target text].This limitation is generally troublesome during the inference phase especially when the aligned image is not provided as in the normal NMT setup.Thus, in this work, we introduce IKD-MMT, a novel MMT framework to support the image-free inference phase via an inversion knowledge distillation scheme.In particular, a multimodal feature generator is executed with a knowledge distillation module, which directly generates the multimodal feature from (only) source texts as the input.While there have been a few prior works entertaining the possibility to support image-free inference for machine translation, their performances have yet to rival the image-must translation.In our experiments, we identify our method as the first image-free approach to comprehensively rival or even surpass (almost) all image-must frameworks, and achieved the state-of-the-art result on the often-used Multi30k benchmark. Our code and data are availableat: https://github.com/pengr/IKD-mmt/tree/master..</abstract>
      <url hash="45f535ae">2022.emnlp-main.152</url>
      <bibkey>peng-etal-2022-distill</bibkey>
    </paper>
    <paper id="153">
      <title>A Multifaceted Framework to Evaluate Evasion, Content Preservation, and Misattribution in Authorship Obfuscation Techniques</title>
      <author><first>Malik</first><last>Altakrori</last></author>
      <author><first>Thomas</first><last>Scialom</last></author>
      <author><first>Benjamin C. M.</first><last>Fung</last></author>
      <author><first>Jackie Chi Kit</first><last>Cheung</last></author>
      <pages>2391-2406</pages>
      <abstract>Authorship obfuscation techniques have commonly been evaluated based on their ability to hide the author’s identity (evasion) while preserving the content of the original text. However, to avoid overstating the systems’ effectiveness, evasion detection must be evaluated using competitive identification techniques in settings that mimic real-life scenarios, and the outcomes of the content-preservation evaluation have to be interpretable by potential users of these obfuscation tools. Motivated by recent work on cross-topic authorship identification and content preservation in summarization, we re-evaluate different authorship obfuscation techniques on detection evasion and content preservation. Furthermore, we propose a new information-theoretic measure to characterize the misattribution harm that can be caused by detection evasion. Our results reveal key weaknesses in state-of-the-art obfuscation techniques and a surprisingly competitive effectiveness from a back-translation baseline in all evaluation aspects.</abstract>
      <url hash="b95623b8">2022.emnlp-main.153</url>
      <bibkey>altakrori-etal-2022-multifaceted</bibkey>
    </paper>
    <paper id="154">
      <title><fixed-case>S</fixed-case>afe<fixed-case>T</fixed-case>ext: A Benchmark for Exploring Physical Safety in Language Models</title>
      <author><first>Sharon</first><last>Levy</last></author>
      <author><first>Emily</first><last>Allaway</last></author>
      <author><first>Melanie</first><last>Subbiah</last></author>
      <author><first>Lydia</first><last>Chilton</last></author>
      <author><first>Desmond</first><last>Patton</last></author>
      <author><first>Kathleen</first><last>McKeown</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>2407-2421</pages>
      <abstract>Understanding what constitutes safe text is an important issue in natural language processing and can often prevent the deployment of models deemed harmful and unsafe. One such type of safety that has been scarcely studied is commonsense physical safety, i.e. text that is not explicitly violent and requires additional commonsense knowledge to comprehend that it leads to physical harm. We create the first benchmark dataset, SafeText, comprising real-life scenarios with paired safe and physically unsafe pieces of advice. We utilize SafeText to empirically study commonsense physical safety across various models designed for text generation and commonsense reasoning tasks. We find that state-of-the-art large language models are susceptible to the generation of unsafe text and have difficulty rejecting unsafe advice. As a result, we argue for further studies of safety and the assessment of commonsense physical safety in models before release.</abstract>
      <url hash="0458a52d">2022.emnlp-main.154</url>
      <bibkey>levy-etal-2022-safetext</bibkey>
    </paper>
    <paper id="155">
      <title>Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations</title>
      <author><first>Kang Min</first><last>Yoo</last></author>
      <author><first>Junyeob</first><last>Kim</last></author>
      <author><first>Hyuhng Joon</first><last>Kim</last></author>
      <author><first>Hyunsoo</first><last>Cho</last></author>
      <author><first>Hwiyeol</first><last>Jo</last></author>
      <author><first>Sang-Woo</first><last>Lee</last></author>
      <author><first>Sang-goo</first><last>Lee</last></author>
      <author><first>Taeuk</first><last>Kim</last></author>
      <pages>2422-2437</pages>
      <abstract>Despite recent explosion of interests in in-context learning, the underlying mechanism and the precise impact of the quality of demonstrations remain elusive.Intuitively, ground-truth labels should have as much impact in in-context learning (ICL) as supervised learning, but recent work reported that the input-label correspondence is significantly less important than previously thought.Intrigued by this counter-intuitive observation, we re-examine the importance of ground-truth labels in in-context learning.With the introduction of two novel metrics, namely Label-Correctness Sensitivity and Ground-truth Label Effect Ratio (GLER), we were able to conduct quantifiable analysis on the impact of ground-truth label demonstrations.Through extensive analyses, we find that the correct input-label mappings can have varying impacts on the downstream in-context learning performances, depending on the experimental configuration.Through additional studies, we identify key components, such as the verbosity of prompt templates and the language model size, as the controlling factor to achieve more noise-resilient ICL.</abstract>
      <url hash="093be70a">2022.emnlp-main.155</url>
      <bibkey>yoo-etal-2022-ground</bibkey>
    </paper>
    <paper id="156">
      <title>D4: a <fixed-case>C</fixed-case>hinese Dialogue Dataset for Depression-Diagnosis-Oriented Chat</title>
      <author><first>Binwei</first><last>Yao</last></author>
      <author><first>Chao</first><last>Shi</last></author>
      <author><first>Likai</first><last>Zou</last></author>
      <author><first>Lingfeng</first><last>Dai</last></author>
      <author><first>Mengyue</first><last>Wu</last></author>
      <author><first>Lu</first><last>Chen</last></author>
      <author><first>Zhen</first><last>Wang</last></author>
      <author><first>Kai</first><last>Yu</last></author>
      <pages>2438-2459</pages>
      <abstract>In a depression-diagnosis-directed clinical session, doctors initiate a conversation with ample emotional support that guides the patients to expose their symptoms based on clinical diagnosis criteria. Such a dialogue system is distinguished from existing single-purpose human-machine dialog systems, as it combines task-oriented and chit-chats with uniqueness in dialogue topics and procedures. However, due to the social stigma associated with mental illness, the dialogue data related to depression consultation and diagnosis are rarely disclosed. Based on clinical depression diagnostic criteria ICD-11 and DSM-5, we designed a 3-phase procedure to construct D<tex-math>^4</tex-math>: a Chinese Dialogue Dataset for Depression-Diagnosis-Oriented Chat, which simulates the dialogue between doctors and patients during the diagnosis of depression, including diagnosis results and symptom summary given by professional psychiatrists for each conversation. Upon the newly-constructed dataset, four tasks mirroring the depression diagnosis process are established: response generation, topic prediction, dialog summary, and severity classification of depressive episode and suicide risk. Multi-scale evaluation results demonstrate that a more empathy-driven and diagnostic-accurate consultation dialogue system trained on our dataset can be achieved compared to rule-based bots.</abstract>
      <url hash="28789eb5">2022.emnlp-main.156</url>
      <bibkey>yao-etal-2022-d4</bibkey>
    </paper>
    <paper id="157">
      <title>Exploiting domain-slot related keywords description for Few-Shot Cross-Domain Dialogue State Tracking</title>
      <author><first>Gao</first><last>Qixiang</last></author>
      <author><first>Guanting</first><last>Dong</last></author>
      <author><first>Yutao</first><last>Mou</last></author>
      <author><first>Liwen</first><last>Wang</last></author>
      <author><first>Chen</first><last>Zeng</last></author>
      <author><first>Daichi</first><last>Guo</last></author>
      <author><first>Mingyang</first><last>Sun</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <pages>2460-2465</pages>
      <abstract>Collecting dialogue data with domain-slot-value labels for dialogue state tracking (DST) could be a costly process. In this paper, we propose a novel framework based on domain-slot related description to tackle the challenge of few-shot cross-domain DST. Specifically, we design an extraction module to extract domain-slot related verbs and nouns in the dialogue. Then, we integrates them into the description, which aims to prompt the model to identify the slot information. Furthermore, we introduce a random sampling strategy to improve the domain generalization ability of the model. We utilize a pre-trained model to encode contexts and description and generates answers with an auto-regressive manner. Experimental results show that our approaches substantially outperform the existing few-shot DST methods on MultiWOZ and gain strong improvements on the slot accuracy comparing to existing slot description methods.</abstract>
      <url hash="55c543e9">2022.emnlp-main.157</url>
      <bibkey>qixiang-etal-2022-exploiting</bibkey>
    </paper>
    <paper id="158">
      <title><fixed-case>C</fixed-case>o<fixed-case>C</fixed-case>oa: An Encoder-Decoder Model for Controllable Code-switched Generation</title>
      <author><first>Sneha</first><last>Mondal</last></author>
      <author><first>Ritika</first><last>.</last></author>
      <author><first>Shreya</first><last>Pathak</last></author>
      <author><first>Preethi</first><last>Jyothi</last></author>
      <author><first>Aravindan</first><last>Raghuveer</last></author>
      <pages>2466-2479</pages>
      <abstract>Code-switching has seen growing interest in recent years as an important multilingual NLP phenomenon. Generating code-switched text for data augmentation has been sufficiently well-explored. However, there is no prior work on generating code-switched text with fine-grained control on the degree of code-switching and the lexical choices used to convey formality. We present CoCoa, an encoder-decoder translation model that converts monolingual Hindi text to Hindi-English code-switched text with both encoder-side and decoder-side interventions to achieve fine-grained controllable generation. CoCoa can be invoked at test-time to synthesize code-switched text that is simultaneously faithful to syntactic and lexical attributes relevant to code-switching. CoCoa outputs were subjected to rigorous subjective and objective evaluations. Human evaluations establish that our outputs are of superior quality while being faithful to desired attributes. We show significantly improved BLEU scores when compared with human-generated code-switched references. Compared to competitive baselines, we show 10% reduction in perplexity on a language modeling task and also demonstrate clear improvements on a downstream code-switched sentiment analysis task.</abstract>
      <url hash="43c62bcc">2022.emnlp-main.158</url>
      <bibkey>mondal-etal-2022-cocoa</bibkey>
    </paper>
    <paper id="159">
      <title>Towards Climate Awareness in <fixed-case>NLP</fixed-case> Research</title>
      <author><first>Daniel</first><last>Hershcovich</last></author>
      <author><first>Nicolas</first><last>Webersinke</last></author>
      <author><first>Mathias</first><last>Kraus</last></author>
      <author><first>Julia</first><last>Bingler</last></author>
      <author><first>Markus</first><last>Leippold</last></author>
      <pages>2480-2494</pages>
      <abstract>The climate impact of AI, and NLP research in particular, has become a serious issue given the enormous amount of energy that is increasingly being used for training and running computational models. Consequently, increasing focus is placed on efficient NLP. However, this important initiative lacks simple guidelines that would allow for systematic climate reporting of NLP research. We argue that this deficiency is one of the reasons why very few publications in NLP report key figures that would allow a more thorough examination of environmental impact, and present a quantitative survey to demonstrate this. As a remedy, we propose a climate performance model card with the primary purpose of being practically usable with only limited information about experiments and the underlying computer hardware. We describe why this step is essential to increase awareness about the environmental impact of NLP research and, thereby, paving the way for more thorough discussions.</abstract>
      <url hash="a36a6cc2">2022.emnlp-main.159</url>
      <bibkey>hershcovich-etal-2022-towards</bibkey>
    </paper>
    <paper id="160">
      <title>Navigating Connected Memories with a Task-oriented Dialog System</title>
      <author><first>Satwik</first><last>Kottur</last></author>
      <author><first>Seungwhan</first><last>Moon</last></author>
      <author><first>Alborz</first><last>Geramifard</last></author>
      <author><first>Babak</first><last>Damavandi</last></author>
      <pages>2495-2507</pages>
      <abstract>Recent years have seen an increasing trend in the volume of personal media captured by users, thanks to the advent of smartphones and smart glasses, resulting in large media collections. Despite conversation being an intuitive human-computer interface, current efforts focus mostly on single-shot natural language based media retrieval to aid users query their media and re-live their memories. This severely limits the search functionality as users can neither ask follow-up queries nor obtain information without first formulating a single-turn query.In this work, we propose dialogs for connected memories as a powerful tool to empower users to search their media collection through a multi-turn, interactive conversation. Towards this, we collect a new task-oriented dialog dataset COMET, which contains 11.5k user↔assistant dialogs (totalling 103k utterances), grounded in simulated personal memory graphs. We employ a resource-efficient, two-phase data collection pipeline that uses: (1) a novel multimodal dialog simulator that generates synthetic dialog flows grounded in memory graphs, and, (2) manual paraphrasing to obtain natural language utterances. We analyze COMET, formulate four main tasks to benchmark meaningful progress, and adopt state-of-the-art language models as strong baselines, in order to highlight the multimodal challenges captured by our dataset.</abstract>
      <url hash="e5552bcb">2022.emnlp-main.160</url>
      <bibkey>kottur-etal-2022-navigating</bibkey>
    </paper>
    <paper id="161">
      <title>Language Model Decomposition: Quantifying the Dependency and Correlation of Language Models</title>
      <author><first>Hao</first><last>Zhang</last></author>
      <pages>2508-2517</pages>
      <abstract>Pre-trained language models (LMs), such as BERT (Devlin et al., 2018) and its variants, have led to significant improvements on various NLP tasks in past years. However, a theoretical framework for studying their relationships is still missing. In this paper, we fill this gap by investigating the linear dependency between pre-trained LMs. The linear dependency of LMs is defined analogously to the linear dependency of vectors. We propose Language Model Decomposition (LMD) to represent a LM using a linear combination of other LMs as basis, and derive the closed-form solution. A goodness-of-fit metric for LMD similar to the coefficient of determination is defined and used to measure the linear dependency of a set of LMs. In experiments, we find that BERT and eleven (11) BERT-like LMs are 91% linearly dependent. This observation suggests that current state-of-the-art (SOTA) LMs are highly “correlated”. To further advance SOTA we need more diverse and novel LMs that are less dependent on existing LMs.</abstract>
      <url hash="8fcf1197">2022.emnlp-main.161</url>
      <bibkey>zhang-2022-language</bibkey>
    </paper>
    <paper id="162">
      <title><fixed-case>S</fixed-case>yn<fixed-case>GEC</fixed-case>: Syntax-Enhanced Grammatical Error Correction with a Tailored <fixed-case>GEC</fixed-case>-Oriented Parser</title>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Bo</first><last>Zhang</last></author>
      <author><first>Zhenghua</first><last>Li</last></author>
      <author><first>Zuyi</first><last>Bao</last></author>
      <author><first>Chen</first><last>Li</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>2518-2531</pages>
      <abstract>This work proposes a syntax-enhanced grammatical error correction (GEC) approach named SynGEC that effectively incorporates dependency syntactic information into the encoder part of GEC models. The key challenge for this idea is that off-the-shelf parsers are unreliable when processing ungrammatical sentences. To confront this challenge, we propose to build a tailored GEC-oriented parser (GOPar) using parallel GEC training data as a pivot. First, we design an extended syntax representation scheme that allows us to represent both grammatical errors and syntax in a unified tree structure. Then, we obtain parse trees of the source incorrect sentences by projecting trees of the target correct sentences. Finally, we train GOPar with such projected trees. For GEC, we employ the graph convolution network to encode source-side syntactic information produced by GOPar, and fuse them with the outputs of the Transformer encoder. Experiments on mainstream English and Chinese GEC datasets show that our proposed SynGEC approach consistently and substantially outperforms strong baselines and achieves competitive performance. Our code and data are all publicly available at https://github.com/HillZhang1999/SynGEC.</abstract>
      <url hash="eb163174">2022.emnlp-main.162</url>
      <bibkey>zhang-etal-2022-syngec</bibkey>
    </paper>
    <paper id="163">
      <title>Varifocal Question Generation for Fact-checking</title>
      <author><first>Nedjma</first><last>Ousidhoum</last></author>
      <author><first>Zhangdie</first><last>Yuan</last></author>
      <author><first>Andreas</first><last>Vlachos</last></author>
      <pages>2532-2544</pages>
      <abstract>Fact-checking requires retrieving evidence related to a claim under investigation. The task can be formulated as question generation based on a claim, followed by question answering.However, recent question generation approaches assume that the answer is known and typically contained in a passage given as input,whereas such passages are what is being sought when verifying a claim.In this paper, we present <i>Varifocal</i>, a method that generates questions based on different focal points within a given claim, i.e. different spans of the claim and its metadata, such as its source and date.Our method outperforms previous work on a fact-checking question generation dataset on a wide range of automatic evaluation metrics.These results are corroborated by our manual evaluation, which indicates that our method generates more relevant and informative questions.We further demonstrate the potential of focal points in generating sets of clarification questions for product descriptions.</abstract>
      <url hash="d7ddb38e">2022.emnlp-main.163</url>
      <bibkey>ousidhoum-etal-2022-varifocal</bibkey>
    </paper>
    <paper id="164">
      <title>Bilingual Lexicon Induction for Low-Resource Languages using Graph Matching via Optimal Transport</title>
      <author><first>Kelly</first><last>Marchisio</last></author>
      <author><first>Ali</first><last>Saad-Eldin</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <author><first>Carey</first><last>Priebe</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>2545-2561</pages>
      <abstract>Bilingual lexicons form a critical component of various natural language processing applications, including unsupervised and semisupervised machine translation and crosslingual information retrieval. In this work, we improve bilingual lexicon induction performance across 40 language pairs with a graph-matching method based on optimal transport. The method is especially strong with low amounts of supervision.</abstract>
      <url hash="b88537e9">2022.emnlp-main.164</url>
      <bibkey>marchisio-etal-2022-bilingual</bibkey>
    </paper>
    <paper id="165">
      <title>Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection</title>
      <author><first>Suchin</first><last>Gururangan</last></author>
      <author><first>Dallas</first><last>Card</last></author>
      <author><first>Sarah</first><last>Dreier</last></author>
      <author><first>Emily</first><last>Gade</last></author>
      <author><first>Leroy</first><last>Wang</last></author>
      <author><first>Zeyu</first><last>Wang</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>2562-2580</pages>
      <abstract>Language models increasingly rely on massive web crawls for diverse text data. However, these sources are rife with undesirable content. As such, resources like Wikipedia, books, and news often serve as anchors for automatically selecting web text most suitable for language modeling, a process typically referred to as quality filtering. Using a new dataset of U.S. high school newspaper articles—written by students from across the country—we investigate whose language is preferred by the quality filter used for GPT-3. We find that newspapers from larger schools, located in wealthier, educated, and urban zones (ZIP codes) are more likely to be classified as high quality. We also show that this quality measurement is unaligned with other sensible metrics, such as factuality or literary acclaim. We argue that privileging any corpus as high quality entails a language ideology, and more care is needed to construct training corpora for language models, with better transparency and justification for the inclusion or exclusion of various texts.</abstract>
      <url hash="a791f147">2022.emnlp-main.165</url>
      <bibkey>gururangan-etal-2022-whose</bibkey>
    </paper>
    <paper id="166">
      <title><fixed-case>C</fixed-case>on<fixed-case>R</fixed-case>eader: Exploring Implicit Relations in Contracts for Contract Clause Extraction</title>
      <author><first>Weiwen</first><last>Xu</last></author>
      <author><first>Yang</first><last>Deng</last></author>
      <author><first>Wenqiang</first><last>Lei</last></author>
      <author><first>Wenlong</first><last>Zhao</last></author>
      <author><first>Tat-Seng</first><last>Chua</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <pages>2581-2594</pages>
      <abstract>We study automatic Contract Clause Extraction (CCE) by modeling implicit relations in legal contracts. Existing CCE methods mostly treat contracts as plain text, creating a substantial barrier to understanding contracts of high complexity. In this work, we first comprehensively analyze the complexity issues of contracts and distill out three implicit relations commonly found in contracts, namely, 1) Long-range Context Relation that captures the correlations of distant clauses; 2) Term-Definition Relation that captures the relation between important terms with their corresponding definitions, and 3) Similar Clause Relation that captures the similarities between clauses of the same type. Then we propose a novel framework ConReader to exploit the above three relations for better contract understanding and improving CCE. Experimental results show that ConReader makes the prediction more interpretable and achieves new state-of-the-art on two CCE tasks in both conventional and zero-shot settings.</abstract>
      <url hash="8993f4e5">2022.emnlp-main.166</url>
      <bibkey>xu-etal-2022-conreader</bibkey>
    </paper>
    <paper id="167">
      <title>Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual <fixed-case>NLU</fixed-case></title>
      <author><first>Fenia</first><last>Christopoulou</last></author>
      <author><first>Gerasimos</first><last>Lampouras</last></author>
      <author><first>Ignacio</first><last>Iacobacci</last></author>
      <pages>2595-2611</pages>
      <abstract>Curriculum Learning (CL) is a technique of training models via ranking examples in a typically increasing difficulty trend with the aim of accelerating convergence and improving generalisability. Current approaches for Natural Language Understanding (NLU) tasks use CL to improve in-distribution data performance often via heuristic-oriented or task-agnostic difficulties. In this work, instead, we employ CL for NLU by taking advantage of training dynamics as difficulty metrics, i.e., statistics that measure the behavior of the model at hand on specific task-data instances during training and propose modifications of existing CL schedulers based on these statistics. Differently from existing works, we focus on evaluating models on in-distribution (ID), out-of-distribution (OOD) as well as zero-shot (ZS) cross-lingual transfer datasets. We show across several NLU tasks that CL with training dynamics can result in better performance mostly on zero-shot cross-lingual transfer and OOD settings with improvements up by 8.5% in certain cases. Overall, experiments indicate that training dynamics can lead to better performing models with smoother training compared to other difficulty metrics while being 20% faster on average. In addition, through analysis we shed light on the correlations of task-specific versus task-agnostic metrics.</abstract>
      <url hash="77166a2c">2022.emnlp-main.167</url>
      <bibkey>christopoulou-etal-2022-training</bibkey>
    </paper>
    <paper id="168">
      <title>Revisiting Parameter-Efficient Tuning: Are We Really There Yet?</title>
      <author><first>Guanzheng</first><last>Chen</last></author>
      <author><first>Fangyu</first><last>Liu</last></author>
      <author><first>Zaiqiao</first><last>Meng</last></author>
      <author><first>Shangsong</first><last>Liang</last></author>
      <pages>2612-2626</pages>
      <abstract>Parameter-Efficient Tuning (PETuning) methods have been deemed by many as the new paradigm for using pretrained language models (PLMs). By tuning just a fraction amount of parameters comparing to full model finetuning, PETuning methods claim to have achieved performance on par with or even better than finetuning. In this work, we take a step back and re-examine these PETuning methods by conducting the first comprehensive investigation into the training and evaluation of them. We found the problematic validation and testing practice in current studies, when accompanied by the instability nature of PETuning methods, has led to unreliable conclusions. When being compared under a truly fair evaluation protocol, PETuning cannot yield consistently competitive performance while finetuning remains to be the best-performing method in medium- and high-resource settings. We delve deeper into the cause of the instability and observed that the number of trainable parameters and training iterations are two main factors: reducing trainable parameters and prolonging training iterations may lead to higher stability in PETuning methods.</abstract>
      <url hash="92b02436">2022.emnlp-main.168</url>
      <bibkey>chen-etal-2022-revisiting</bibkey>
    </paper>
    <paper id="169">
      <title>Transfer Learning from Semantic Role Labeling to Event Argument Extraction with Template-based Slot Querying</title>
      <author><first>Zhisong</first><last>Zhang</last></author>
      <author><first>Emma</first><last>Strubell</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <pages>2627-2647</pages>
      <abstract>In this work, we investigate transfer learning from semantic role labeling (SRL) to event argument extraction (EAE), considering their similar argument structures. We view the extraction task as a role querying problem, unifying various methods into a single framework. There are key discrepancies on role labels and distant arguments between semantic role and event argument annotations. To mitigate these discrepancies, we specify natural language-like queries to tackle the label mismatch problem and devise argument augmentation to recover distant arguments. We show that SRL annotations can serve as a valuable resource for EAE, and a template-based slot querying strategy is especially effective for facilitating the transfer. In extensive evaluations on two English EAE benchmarks, our proposed model obtains impressive zero-shot results by leveraging SRL annotations, reaching nearly 80% of the fullysupervised scores. It further provides benefits in low-resource cases, where few EAE annotations are available. Moreover, we show that our approach generalizes to cross-domain and multilingual scenarios.</abstract>
      <url hash="e73c7809">2022.emnlp-main.169</url>
      <bibkey>zhang-etal-2022-transfer</bibkey>
    </paper>
    <paper id="170">
      <title>Calibrating Zero-shot Cross-lingual (Un-)structured Predictions</title>
      <author><first>Zhengping</first><last>Jiang</last></author>
      <author><first>Anqi</first><last>Liu</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>2648-2674</pages>
      <abstract>We investigate model calibration in the setting of zero-shot cross-lingual transfer with large-scale pre-trained language models. The level of model calibration is an important metric for evaluating the trustworthiness of predictive models. There exists an essential need for model calibration when natural language models are deployed in critical tasks. We study different post-training calibration methods in structured and unstructured prediction tasks. We find that models trained with data from the source language become less calibrated when applied to the target language and that calibration errors increase with intrinsic task difficulty and relative sparsity of training data. Moreover, we observe a potential connection between the level of calibration error and an earlier proposed measure of the distance from English to other languages. Finally, our comparison demonstrates that among other methods Temperature Scaling (TS) generalizes well to distant languages, but TS fails to calibrate more complex confidence estimation in structured predictions compared to more expressive alternatives like Gaussian Process Calibration.</abstract>
      <url hash="df8510f6">2022.emnlp-main.170</url>
      <bibkey>jiang-etal-2022-calibrating</bibkey>
    </paper>
    <paper id="171">
      <title><fixed-case>PRINCE</fixed-case>: Prefix-Masked Decoding for Knowledge Enhanced Sequence-to-Sequence Pre-Training</title>
      <author><first>Song</first><last>Xu</last></author>
      <author><first>Haoran</first><last>Li</last></author>
      <author><first>Peng</first><last>Yuan</last></author>
      <author><first>Youzheng</first><last>Wu</last></author>
      <author><first>Xiaodong</first><last>He</last></author>
      <pages>2675-2681</pages>
      <abstract>Pre-trained Language Models (PLMs) have shown effectiveness in various Natural Language Processing (NLP) tasks. Denoising autoencoder is one of the most successful pre-training frameworks, learning to recompose the original text given a noise-corrupted one. The existing studies mainly focus on injecting noises into the input. This paper introduces a simple yet effective pre-training paradigm, equipped with a knowledge-enhanced decoder that predicts the next entity token with noises in the prefix, explicitly strengthening the representation learning of entities that span over multiple input tokens. Specifically, when predicting the next token within an entity, we feed masks into the prefix in place of some of the previous ground-truth tokens that constitute the entity. Our model achieves new state-of-the-art results on two knowledge-driven data-to-text generation tasks with up to 2% BLEU gains.</abstract>
      <url hash="fff641b3">2022.emnlp-main.171</url>
      <bibkey>xu-etal-2022-prince</bibkey>
    </paper>
    <paper id="172">
      <title>How Far are We from Robust Long Abstractive Summarization?</title>
      <author><first>Huan Yee</first><last>Koh</last></author>
      <author><first>Jiaxin</first><last>Ju</last></author>
      <author><first>He</first><last>Zhang</last></author>
      <author><first>Ming</first><last>Liu</last></author>
      <author><first>Shirui</first><last>Pan</last></author>
      <pages>2682-2698</pages>
      <abstract>Abstractive summarization has made tremendous progress in recent years. In this work, we perform fine-grained human annotations to evaluate long document abstractive summarization systems (i.e., models and metrics) with the aim of implementing them to generate reliable summaries. For long document abstractive models, we show that the constant strive for state-of-the-art ROUGE results can lead us to generate more relevant summaries but not factual ones. For long document evaluation metrics, human evaluation results show that ROUGE remains the best at evaluating the relevancy of a summary. It also reveals important limitations of factuality metrics in detecting different types of factual errors and the reasons behind the effectiveness of BARTScore. We then suggest promising directions in the endeavor of developing factual consistency metrics. Finally, we release our annotated long document dataset with the hope that it can contribute to the development of metrics across a broader range of summarization settings.</abstract>
      <url hash="e6a1ec76">2022.emnlp-main.172</url>
      <bibkey>koh-etal-2022-far</bibkey>
    </paper>
    <paper id="173">
      <title>Measuring Context-Word Biases in Lexical Semantic Datasets</title>
      <author><first>Qianchu</first><last>Liu</last></author>
      <author><first>Diana</first><last>McCarthy</last></author>
      <author><first>Anna</first><last>Korhonen</last></author>
      <pages>2699-2713</pages>
      <abstract>State-of-the-art pretrained contextualized models (PCM) eg. BERT use tasks such as WiC and WSD to evaluate their word-in-context representations. This inherently assumes that performance in these tasks reflect how well a model represents the coupled word and context semantics. We question this assumption by presenting the first quantitative analysis on the context-word interaction being tested in major contextual lexical semantic tasks. To achieve this, we run probing baselines on masked input, and propose measures to calculate and visualize the degree of context or word biases in existing datasets. The analysis was performed on both models and humans. Our findings demonstrate that models are usually not being tested for word-in-context semantics in the same way as humans are in these tasks, which helps us better understand the model-human gap. Specifically, to PCMs, most existing datasets fall into the extreme ends (the retrieval-based tasks exhibit strong target word bias while WiC-style tasks and WSD show strong context bias); In comparison, humans are less biased and achieve much better performance when both word and context are available than with masked input. We recommend our framework for understanding and controlling these biases for model interpretation and future task design.</abstract>
      <url hash="1a43652c">2022.emnlp-main.173</url>
      <bibkey>liu-etal-2022-measuring</bibkey>
    </paper>
    <paper id="174">
      <title>Iteratively Prompt Pre-trained Language Models for Chain of Thought</title>
      <author><first>Boshi</first><last>Wang</last></author>
      <author><first>Xiang</first><last>Deng</last></author>
      <author><first>Huan</first><last>Sun</last></author>
      <pages>2714-2730</pages>
      <abstract>While Pre-trained Language Models (PLMs) internalize a great amount of world knowledge, they have been shown incapable of recalling these knowledge to solve tasks requiring complex &amp; multi-step reasoning. Similar to how humans develop a “chain of thought” for these tasks, how can we equip PLMs with such abilities? In this work, we explore an iterative prompting framework, a new prompting paradigm which progressively elicits relevant knowledge from PLMs for multi-step inference. We identify key limitations of existing prompting methods, namely they are either restricted to queries with a single identifiable relation/predicate, or being agnostic to input contexts, which makes it difficult to capture variabilities across different inference steps. We propose an iterative context-aware prompter, which addresses these limitations by learning to dynamically synthesize prompts conditioned on the current step’s contexts. Experiments on three datasets involving multi-step reasoning show the effectiveness of the iterative scheme and the context-aware prompter design.</abstract>
      <url hash="6b8640d8">2022.emnlp-main.174</url>
      <bibkey>wang-etal-2022-iteratively</bibkey>
    </paper>
    <paper id="175">
      <title>Unobserved Local Structures Make Compositional Generalization Hard</title>
      <author><first>Ben</first><last>Bogin</last></author>
      <author><first>Shivanshu</first><last>Gupta</last></author>
      <author><first>Jonathan</first><last>Berant</last></author>
      <pages>2731-2747</pages>
      <abstract>While recent work has shown that sequence-to-sequence models struggle to generalize to new compositions (termed compositional generalization), little is known on what makes compositional generalization hard on a particular test instance. In this work, we investigate the factors that make generalization to certain test instances challenging. We first substantiate that some examples are more difficult than others by showing that different models consistently fail or succeed on the same test instances. Then, we propose a criterion for the difficulty of an example: a test instance is hard if it contains a local structure that was not observed at training time. We formulate a simple decision rule based on this criterion and empirically show it predicts instance-level generalization well across 5 different semantic parsing datasets, substantially better than alternative decision rules. Last, we show local structures can be leveraged for creating difficult adversarial compositional splits and also to improve compositional generalization under limited training budgets by strategically selecting examples for the training set.</abstract>
      <url hash="4573d54c">2022.emnlp-main.175</url>
      <bibkey>bogin-etal-2022-unobserved</bibkey>
    </paper>
    <paper id="176">
      <title>Mitigating Data Sparsity for Short Text Topic Modeling by Topic-Semantic Contrastive Learning</title>
      <author><first>Xiaobao</first><last>Wu</last></author>
      <author><first>Anh Tuan</first><last>Luu</last></author>
      <author><first>Xinshuai</first><last>Dong</last></author>
      <pages>2748-2760</pages>
      <abstract>To overcome the data sparsity issue in short text topic modeling, existing methods commonly rely on data augmentation or the data characteristic of short texts to introduce more word co-occurrence information. However, most of them do not make full use of the augmented data or the data characteristic: they insufficiently learn the relations among samples in data, leading to dissimilar topic distributions of semantically similar text pairs. To better address data sparsity, in this paper we propose a novel short text topic modeling framework, Topic-Semantic Contrastive Topic Model (TSCTM). To sufficiently model the relations among samples, we employ a new contrastive learning method with efficient positive and negative sampling strategies based on topic semantics. This contrastive learning method refines the representations, enriches the learning signals, and thus mitigates the sparsity issue. Extensive experimental results show that our TSCTM outperforms state-of-the-art baselines regardless of the data augmentation availability, producing high-quality topics and topic distributions.</abstract>
      <url hash="d7f3a4a7">2022.emnlp-main.176</url>
      <bibkey>wu-etal-2022-mitigating</bibkey>
    </paper>
    <paper id="177">
      <title>Back to the Future: Bidirectional Information Decoupling Network for Multi-turn Dialogue Modeling</title>
      <author><first>Yiyang</first><last>Li</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <author><first>Zhuosheng</first><last>Zhang</last></author>
      <pages>2761-2774</pages>
      <abstract>Multi-turn dialogue modeling as a challenging branch of natural language understanding (NLU), aims to build representations for machines to understand human dialogues, which provides a solid foundation for multiple downstream tasks. Recent studies of dialogue modeling commonly employ pre-trained language models (PrLMs) to encode the dialogue history as successive tokens, which is insufficient in capturing the temporal characteristics of dialogues. Therefore, we propose Bidirectional Information Decoupling Network (BiDeN) as a universal dialogue encoder, which explicitly incorporates both the past and future contexts and can be generalized to a wide range of dialogue-related tasks. Experimental results on datasets of different downstream tasks demonstrate the universality and effectiveness of our BiDeN.</abstract>
      <url hash="f8b74823">2022.emnlp-main.177</url>
      <bibkey>li-etal-2022-back</bibkey>
    </paper>
    <paper id="178">
      <title>Calibration Meets Explanation: A Simple and Effective Approach for Model Confidence Estimates</title>
      <author><first>Dongfang</first><last>Li</last></author>
      <author><first>Baotian</first><last>Hu</last></author>
      <author><first>Qingcai</first><last>Chen</last></author>
      <pages>2775-2784</pages>
      <abstract>Calibration strengthens the trustworthiness of black-box models by producing better accurate confidence estimates on given examples. However, little is known about if model explanations can help confidence calibration. Intuitively, humans look at important features attributions and decide whether the model is trustworthy. Similarly, the explanations may tell us when the model might know and when it does not. Inspired by this, we propose a method named CME that leverages model explanations to make the model less confident with non-inductive attributions. The idea is that when the model is not highly confident, it is difficult to identify strong indications of any class, and the tokens accordingly do not have high attribution scores for any class and vice versa. We conduct extensive experiments on six datasets with two popular pre-trained language models in the in-domain and out-of-domain settings. The results show that CME improves calibration performance in all settings. The expected calibration errors are further reduced when combined with temperature scaling. Our findings highlight that model explanations can help calibrate posterior estimates.</abstract>
      <url hash="17cf2864">2022.emnlp-main.178</url>
      <bibkey>li-etal-2022-calibration</bibkey>
    </paper>
    <paper id="179">
      <title>Non-Autoregressive Neural Machine Translation: A Call for Clarity</title>
      <author><first>Robin</first><last>Schmidt</last></author>
      <author><first>Telmo</first><last>Pires</last></author>
      <author><first>Stephan</first><last>Peitz</last></author>
      <author><first>Jonas</first><last>Lööf</last></author>
      <pages>2785-2799</pages>
      <abstract>Non-autoregressive approaches aim to improve the inference speed of translation models by only requiring a single forward pass to generate the output sequence instead of iteratively producing each predicted token. Consequently, their translation quality still tends to be inferior to their autoregressive counterparts due to several issues involving output token interdependence. In this work, we take a step back and revisit several techniques that have been proposed for improving non-autoregressive translation models and compare their combined translation quality and speed implications under third-party testing environments. We provide novel insights for establishing strong baselines using length prediction or CTC-based architecture variants and contribute standardized BLEU, chrF++, and TER scores using sacreBLEU on four translation tasks, which crucially have been missing as inconsistencies in the use of tokenized BLEU lead to deviations of up to 1.7 BLEU points. Our open-sourced code is integrated into fairseq for reproducibility.</abstract>
      <url hash="ffcc1e48">2022.emnlp-main.179</url>
      <bibkey>schmidt-etal-2022-non</bibkey>
    </paper>
    <paper id="180">
      <title><fixed-case>RED</fixed-case>-<fixed-case>ACE</fixed-case>: Robust Error Detection for <fixed-case>ASR</fixed-case> using Confidence Embeddings</title>
      <author><first>Zorik</first><last>Gekhman</last></author>
      <author><first>Dina</first><last>Zverinski</last></author>
      <author><first>Jonathan</first><last>Mallinson</last></author>
      <author><first>Genady</first><last>Beryozkin</last></author>
      <pages>2800-2808</pages>
      <abstract>ASR Error Detection (AED) models aim to post-process the output of Automatic Speech Recognition (ASR) systems, in order to detect transcription errors. Modern approaches usually use text-based input, comprised solely of the ASR transcription hypothesis, disregarding additional signals from the ASR model. Instead, we utilize the ASR system’s word-level confidence scores for improving AED performance. Specifically, we add an ASR Confidence Embedding (ACE) layer to the AED model’s encoder, allowing us to jointly encode the confidence scores and the transcribed text into a contextualized representation. Our experiments show the benefits of ASR confidence scores for AED, their complementary effect over the textual signal, as well as the effectiveness and robustness of ACE for combining these signals. To foster further research, we publish a novel AED dataset consisting of ASR outputs on the LibriSpeech corpus with annotated transcription errors.</abstract>
      <url hash="e3203ec2">2022.emnlp-main.180</url>
      <bibkey>gekhman-etal-2022-red</bibkey>
    </paper>
    <paper id="181">
      <title>Fast-<fixed-case>R</fixed-case>2<fixed-case>D</fixed-case>2: A Pretrained Recursive Neural Network based on Pruned <fixed-case>CKY</fixed-case> for Grammar Induction and Text Representation</title>
      <author><first>Xiang</first><last>Hu</last></author>
      <author><first>Haitao</first><last>Mi</last></author>
      <author><first>Liang</first><last>Li</last></author>
      <author><first>Gerard</first><last>de Melo</last></author>
      <pages>2809-2821</pages>
      <abstract>Chart-based models have shown great potential in unsupervised grammar induction, running recursively and hierarchically, but requiring O(n³) time-complexity. The Recursive Transformer based on Differentiable Trees (R2D2) makes it possible to scale to large language model pretraining even with a complex tree encoder, by introducing a heuristic pruning method.However, its rule-based pruning process suffers from local optima and slow inference. In this paper, we propose a unified R2D2 method that overcomes these issues. We use a top-down unsupervised parser as a model-guided pruning method, which also enables parallel encoding during inference. Our parser casts parsing as a split point scoring task by first scoring all split points for a given sentence and then using the highest-scoring one to recursively split a span into two parts. The reverse order of the splits is considered as the order of pruning in the encoder. We optimize the unsupervised parser by minimizing the Kullback–Leibler distance between tree probabilities from the parser and the R2D2 model.Our experiments show that our Fast-R2D2 significantly improves the grammar induction quality and achieves competitive results in downstream tasks.</abstract>
      <url hash="0a56cf30">2022.emnlp-main.181</url>
      <bibkey>hu-etal-2022-fast</bibkey>
    </paper>
    <paper id="182">
      <title>A Localized Geometric Method to Match Knowledge in Low-dimensional Hyperbolic Space</title>
      <author><first>Bo</first><last>Hui</last></author>
      <author><first>Tian</first><last>Xia</last></author>
      <author><first>Wei-Shinn</first><last>Ku</last></author>
      <pages>2822-2832</pages>
      <abstract>Matching equivalent entities across Knowledge graphs is a pivotal step for knowledge fusion. Previous approaches usually study the problem in Euclidean space. However, recent works have shown that hyperbolic space has a higher capacity than Euclidean space and hyperbolic embedding can represent the hierarchical structure in a knowledge graph. In this paper, we propose a localized geometric method to find equivalent entities in hyperbolic space. Specifically, we use a hyperbolic neural network to encode the lingual information of entities and the structure of both knowledge graphs into a low-dimensional hyperbolic space. To address the asymmetry of structure on different KGs and the localized nature of relations, we learn an instance-specific geometric mapping function based on rotation to match entity pairs. A contrastive loss function is used to train the model. The experiment verifies the power of low-dimensional hyperbolic space for entity matching and shows that our method outperforms the state of the art by a large margin.</abstract>
      <url hash="c4bc080e">2022.emnlp-main.182</url>
      <bibkey>hui-etal-2022-localized</bibkey>
    </paper>
    <paper id="183">
      <title>Memory-assisted prompt editing to improve <fixed-case>GPT</fixed-case>-3 after deployment</title>
      <author><first>Aman</first><last>Madaan</last></author>
      <author><first>Niket</first><last>Tandon</last></author>
      <author><first>Peter</first><last>Clark</last></author>
      <author><first>Yiming</first><last>Yang</last></author>
      <pages>2833-2861</pages>
      <abstract>Large LMs such as GPT-3 are powerful, but can commit mistakes that are obvious to humans. For example, GPT-3 would mistakenly interpret “What word is similar to good?” to mean a homophone, while the user intended a synonym. Our goal is to effectively correct such errors via user interactions with the system but without retraining, which will be prohibitively costly. We pair GPT-3 with a growing memory of recorded cases where the model misunderstood the user’s intents, along with user feedback for clarification. Such a memory allows our system to produce enhanced prompts for any new query based on the user feedback for error correction on similar cases in the past. On four tasks (two lexical tasks, two advanced ethical reasoning tasks), we show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT-3. Our approach is a step towards the low-cost utility enhancement for very large pre-trained LMs.</abstract>
      <url hash="72b01328">2022.emnlp-main.183</url>
      <bibkey>madaan-etal-2022-memory</bibkey>
    </paper>
    <paper id="184">
      <title><fixed-case>LVP</fixed-case>-<fixed-case>M</fixed-case>3: Language-aware Visual Prompt for Multilingual Multimodal Machine Translation</title>
      <author><first>Hongcheng</first><last>Guo</last></author>
      <author><first>Jiaheng</first><last>Liu</last></author>
      <author><first>Haoyang</first><last>Huang</last></author>
      <author><first>Jian</first><last>Yang</last></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <author><first>Dongdong</first><last>Zhang</last></author>
      <author><first>Zheng</first><last>Cui</last></author>
      <pages>2862-2872</pages>
      <abstract>Multimodal Machine Translation (MMT) focuses on enhancing text-only translation with visual features, which has attracted considerable attention from both natural language processing and computer vision communities. Recent advances still struggle to train a separate model for each language pair, which is costly and unaffordable when the number of languages increases in the real world. In other words, the multilingual multimodal machine translation (Multilingual MMT) task has not been investigated, which aims to handle the aforementioned issues by providing a shared semantic space for multiple languages. Besides, the image modality has no language boundaries, which is superior to bridging the semantic gap between languages. To this end,we first propose the Multilingual MMT task by establishing two new Multilingual MMT benchmark datasets covering seven languages.Then, an effective baseline LVP-M3 using visual prompts is proposed to support translations between different languages,which includes three stages (token encoding, language-aware visual prompt generation, and language translation). Extensive experimental results on our constructed benchmark datasets demonstrate the effectiveness of LVP-M3 method for Multilingual MMT.</abstract>
      <url hash="4d45aede">2022.emnlp-main.184</url>
      <bibkey>guo-etal-2022-lvp</bibkey>
    </paper>
    <paper id="185">
      <title><fixed-case>P</fixed-case>rompt<fixed-case>EHR</fixed-case>: Conditional Electronic Healthcare Records Generation with Prompt Learning</title>
      <author><first>Zifeng</first><last>Wang</last></author>
      <author><first>Jimeng</first><last>Sun</last></author>
      <pages>2873-2885</pages>
      <abstract>Accessing longitudinal multimodal Electronic Healthcare Records (EHRs) is challenging due to privacy concerns, which hinders the use of ML for healthcare applications. Synthetic EHRs generation bypasses the need to share sensitive real patient records. However, existing methods generate single-modal EHRs by unconditional generation or by longitudinal inference, which falls short of low flexibility and makes unrealistic EHRs. In this work, we propose to formulate EHRs generation as a text-to-text translation task by language models (LMs), which suffices to highly flexible event imputation during generation. We also design prompt learning to control the generation conditioned by numerical and categorical demographic features. We evaluate synthetic EHRs quality by two perplexity measures accounting for their longitudinal pattern (longitudinal imputation perplexity, lpl) and the connections cross modalities (cross-modality imputation perplexity, mpl). Moreover, we utilize two adversaries: membership and attribute inference attacks for privacy-preserving evaluation. Experiments on MIMIC-III data demonstrate the superiority of our methods on realistic EHRs generation (53.1% decrease of lpl and 45.3% decrease of mpl on average compared to the best baselines) with low privacy risks. Software is available at https://github.com/RyanWangZf/PromptEHR.</abstract>
      <url hash="eef7a47a">2022.emnlp-main.185</url>
      <bibkey>wang-sun-2022-promptehr</bibkey>
    </paper>
    <paper id="186">
      <title><fixed-case>ROSE</fixed-case>: Robust Selective Fine-tuning for Pre-trained Language Models</title>
      <author><first>Lan</first><last>Jiang</last></author>
      <author><first>Hao</first><last>Zhou</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Rui</first><last>Jiang</last></author>
      <pages>2886-2897</pages>
      <abstract>Even though the large-scale language models have achieved excellent performances, they suffer from various adversarial attacks.A large body of defense methods has been proposed. However, they are still limited due to redundant attack search spaces and the inability to defend against various types of attacks.In this work, we present a novel fine-tuning approach called <b>RO</b>bust <b>SE</b>letive fine-tuning (<b>ROSE</b>) to address this issue.ROSE conducts selective updates when adapting pre-trained models to downstream tasks, filtering out invaluable and unrobust updates of parameters.Specifically, we propose two strategies: the first-order and second-order ROSE for selecting target robust parameters.The experimental results show that ROSE achieves significant improvements in adversarial robustness on various downstream NLP tasks, and the ensemble method even surpasses both variants above.Furthermore, ROSE can be easily incorporated into existing fine-tuning methods to improve their adversarial robustness further.The empirical analysis confirms that ROSE eliminates unrobust spurious updates during fine-tuning, leading to solutions corresponding to flatter and wider optima than the conventional method.Code is available at <url>https://github.com/jiangllan/ROSE</url>.</abstract>
      <url hash="5c981948">2022.emnlp-main.186</url>
      <bibkey>jiang-etal-2022-rose</bibkey>
    </paper>
    <paper id="187">
      <title><fixed-case>C</fixed-case>ode<fixed-case>R</fixed-case>etriever: A Large Scale Contrastive Pre-Training Method for Code Search</title>
      <author><first>Xiaonan</first><last>Li</last></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <author><first>Yelong</first><last>Shen</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <author><first>Hang</first><last>Zhang</last></author>
      <author><first>Bolun</first><last>Yao</last></author>
      <author><first>Weizhen</first><last>Qi</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <author><first>Weizhu</first><last>Chen</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <pages>2898-2910</pages>
      <abstract>In this paper, we propose the CodeRetriever model, which learns the function-level code semantic representations through large-scale code-text contrastive pre-training. We adopt two contrastive learning schemes in CodeRetriever: unimodal contrastive learning and bimodal contrastive learning. For unimodal contrastive learning, we design an unsupervised learning approach to build semantic-related code pairs based on the documentation and function name. For bimodal contrastive learning, we leverage the documentation and in-line comments of code to build code-text pairs. Both contrastive objectives can fully leverage large-scale code corpus for pre-training. Extensive experimental results show that CodeRetriever achieves new state-of-the-art with significant improvement over existing code pre-trained models, on eleven domain/language-specific code search tasks with six programming languages in different code granularity (function-level, snippet-level and statement-level).These results demonstrate the effectiveness and robustness of CodeRetriever.The codes and resources are available at <url>https://github.com/microsoft/AR2/tree/main/CodeRetriever</url>.</abstract>
      <url hash="21177450">2022.emnlp-main.187</url>
      <bibkey>li-etal-2022-coderetriever</bibkey>
    </paper>
    <paper id="188">
      <title>Open-Topic False Information Detection on Social Networks with Contrastive Adversarial Learning</title>
      <author><first>Guanghui</first><last>Ma</last></author>
      <author><first>Chunming</first><last>Hu</last></author>
      <author><first>Ling</first><last>Ge</last></author>
      <author><first>Hong</first><last>Zhang</last></author>
      <pages>2911-2923</pages>
      <abstract>Current works about false information detection based on conversation graphs on social networks focus primarily on two research streams from the standpoint of topic distribution: in-topic and cross-topic techniques, which assume that the data topic distribution is identical or cross, respectively. This signifies that all test data topics are seen or unseen by the model.However, these assumptions are too harsh for actual social networks that contain both seen and unseen topics simultaneously, hence restricting their practical application.In light of this, this paper develops a novel open-topic scenario that is better suited to actual social networks. In this open-topic scenario, we empirically find that the existing models suffer from impairment in the detection performance for seen or unseen topic data, resulting in poor overall model performance. To address this issue, we propose a novel Contrastive Adversarial Learning Network, CALN, that employs an unsupervised topic clustering method to capture topic-specific features to enhance the model’s performance for seen topics and an unsupervised adversarial learning method to align data representation distributions to enhance the model’s generalisation to unseen topics.Experiments on two benchmark datasets and a variety of graph neural networks demonstrate the effectiveness of our approach.</abstract>
      <url hash="ca4d8be7">2022.emnlp-main.188</url>
      <bibkey>ma-etal-2022-open-topic</bibkey>
    </paper>
    <paper id="189">
      <title>Mitigating Inconsistencies in Multimodal Sentiment Analysis under Uncertain Missing Modalities</title>
      <author><first>Jiandian</first><last>Zeng</last></author>
      <author><first>Jiantao</first><last>Zhou</last></author>
      <author><first>Tianyi</first><last>Liu</last></author>
      <pages>2924-2934</pages>
      <abstract>For the missing modality problem in Multimodal Sentiment Analysis (MSA), the inconsistency phenomenon occurs when the sentiment changes due to the absence of a modality. The absent modality that determines the overall semantic can be considered as a key missing modality. However, previous works all ignored the inconsistency phenomenon, simply discarding missing modalities or solely generating associated features from available modalities. The neglect of the key missing modality case may lead to incorrect semantic results. To tackle the issue, we propose an Ensemble-based Missing Modality Reconstruction (EMMR) network to detect and recover semantic features of the key missing modality. Specifically, we first learn joint representations with remaining modalities via a backbone encoder-decoder network. Then, based on the recovered features, we check the semantic consistency to determine whether the absent modality is crucial to the overall sentiment polarity. Once the inconsistency problem due to the key missing modality exists, we integrate several encoder-decoder approaches for better decision making. Extensive experiments and analyses are conducted on CMU-MOSI and IEMOCAP datasets, validating the superiority of the proposed method.</abstract>
      <url hash="0640ac70">2022.emnlp-main.189</url>
      <bibkey>zeng-etal-2022-mitigating</bibkey>
    </paper>
    <paper id="190">
      <title><fixed-case>C</fixed-case>onv<fixed-case>T</fixed-case>rans: Transforming Web Search Sessions for Conversational Dense Retrieval</title>
      <author><first>Kelong</first><last>Mao</last></author>
      <author><first>Zhicheng</first><last>Dou</last></author>
      <author><first>Hongjin</first><last>Qian</last></author>
      <author><first>Fengran</first><last>Mo</last></author>
      <author><first>Xiaohua</first><last>Cheng</last></author>
      <author><first>Zhao</first><last>Cao</last></author>
      <pages>2935-2946</pages>
      <abstract>Conversational search provides users with a natural and convenient new search experience. Recently, conversational dense retrieval has shown to be a promising technique for realizing conversational search. However, as conversational search systems have not been widely deployed, it is hard to get large-scale real conversational search sessions and relevance labels to support the training of conversational dense retrieval. To tackle this data scarcity problem, previous methods focus on developing better few-shot learning approaches or generating pseudo relevance labels, but the data they use for training still heavily rely on manual generation.In this paper, we present ConvTrans, a data augmentation method that can automatically transform easily-accessible web search sessions into conversational search sessions to fundamentally alleviate the data scarcity problem for conversational dense retrieval. ConvTrans eliminates the gaps between these two types of sessions in terms of session quality and query form to achieve effective session transformation. Extensive evaluations on two widely used conversational search benchmarks, i.e., CAsT-19 and CAsT-20, demonstrate that the same model trained on the data generated by ConvTrans can achieve comparable retrieval performance as it trained on high-quality but expensive artificial conversational search data.</abstract>
      <url hash="b36db810">2022.emnlp-main.190</url>
      <bibkey>mao-etal-2022-convtrans</bibkey>
    </paper>
    <paper id="191">
      <title><fixed-case>MUSIED</fixed-case>: A Benchmark for Event Detection from Multi-Source Heterogeneous Informal Texts</title>
      <author><first>Xiangyu</first><last>Xi</last></author>
      <author><first>Jianwei</first><last>Lv</last></author>
      <author><first>Shuaipeng</first><last>Liu</last></author>
      <author><first>Wei</first><last>Ye</last></author>
      <author><first>Fan</first><last>Yang</last></author>
      <author><first>Guanglu</first><last>Wan</last></author>
      <pages>2947-2964</pages>
      <abstract>Event detection (ED) identifies and classifies event triggers from unstructured texts, serving as a fundamental task for information extraction. Despite the remarkable progress achieved in the past several years, most research efforts focus on detecting events from formal texts (e.g., news articles, Wikipedia documents, financial announcements). Moreover, the texts in each dataset are either from a single source or multiple yet relatively homogeneous sources. With massive amounts of user-generated text accumulating on the Web and inside enterprises, identifying meaningful events in these informal texts, usually from multiple heterogeneous sources, has become a problem of significant practical value. As a pioneering exploration that expands event detection to the scenarios involving informal and heterogeneous texts, we propose a new large-scale Chinese event detection dataset based on user reviews, text conversations, and phone conversations in a leading e-commerce platform for food service. We carefully investigate the proposed dataset’s textual informality and multi-domain heterogeneity characteristics by inspecting data samples quantitatively and qualitatively. Extensive experiments with state-of-the-art event detection methods verify the unique challenges posed by these characteristics, indicating that multi-domain informal event detection remains an open problem and requires further efforts. Our benchmark and code are released at https://github.com/myeclipse/MUSIED.</abstract>
      <url hash="66e94653">2022.emnlp-main.191</url>
      <bibkey>xi-etal-2022-musied</bibkey>
    </paper>
    <paper id="192">
      <title>Reproducibility Issues for <fixed-case>BERT</fixed-case>-based Evaluation Metrics</title>
      <author><first>Yanran</first><last>Chen</last></author>
      <author><first>Jonas</first><last>Belouadi</last></author>
      <author><first>Steffen</first><last>Eger</last></author>
      <pages>2965-2989</pages>
      <abstract>Reproducibility is of utmost concern in machine learning and natural language processing (NLP). In the field of natural language generation (especially machine translation), the seminal paper of Post (2018) has pointed out problems of reproducibility of the dominant metric, BLEU, at the time of publication. Nowadays, BERT-based evaluation metrics considerably outperform BLEU. In this paper, we ask whether results and claims from four recent BERT-based metrics can be reproduced. We find that reproduction of claims and results often fails because of (i) heavy undocumented preprocessing involved in the metrics, (ii) missing code and (iii) reporting weaker results for the baseline metrics. (iv) In one case, the problem stems from correlating not to human scores but to a wrong column in the csv file, inflating scores by 5 points. Motivated by the impact of preprocessing, we then conduct a second study where we examine its effects more closely (for one of the metrics). We find that preprocessing can have large effects, especially for highly inflectional languages. In this case, the effect of preprocessing may be larger than the effect of the aggregation mechanism (e.g., greedy alignment vs. Word Mover Distance).</abstract>
      <url hash="9efc2358">2022.emnlp-main.192</url>
      <bibkey>chen-etal-2022-reproducibility</bibkey>
    </paper>
    <paper id="193">
      <title>Improving Multi-task Stance Detection with Multi-task Interaction Network</title>
      <author><first>Heyan</first><last>Chai</last></author>
      <author><first>Siyu</first><last>Tang</last></author>
      <author><first>Jinhao</first><last>Cui</last></author>
      <author><first>Ye</first><last>Ding</last></author>
      <author><first>Binxing</first><last>Fang</last></author>
      <author><first>Qing</first><last>Liao</last></author>
      <pages>2990-3000</pages>
      <abstract>Stance detection aims to identify people’s standpoints expressed in the text towards a target, which can provide powerful information for various downstream tasks.Recent studies have proposed multi-task learning models that introduce sentiment information to boost stance detection.However, they neglect to explore capturing the fine-grained task-specific interaction between stance detection and sentiment tasks, thus degrading performance.To address this issue, this paper proposes a novel multi-task interaction network (MTIN) for improving the performance of stance detection and sentiment analysis tasks simultaneously.Specifically, we construct heterogeneous task-related graphs to automatically identify and adapt the roles that a word plays with respect to a specific task. Also, a multi-task interaction module is designed to capture the word-level interaction between tasks, so as to obtain richer task representations.Extensive experiments on two real-world datasets show that our proposed approach outperforms state-of-the-art methods in both stance detection and sentiment analysis tasks.</abstract>
      <url hash="9087b15d">2022.emnlp-main.193</url>
      <bibkey>chai-etal-2022-improving</bibkey>
    </paper>
    <paper id="194">
      <title>Neural-based Mixture Probabilistic Query Embedding for Answering <fixed-case>FOL</fixed-case> queries on Knowledge Graphs</title>
      <author><first>Xiao</first><last>Long</last></author>
      <author><first>Liansheng</first><last>Zhuang</last></author>
      <author><first>Li</first><last>Aodi</last></author>
      <author><first>Shafei</first><last>Wang</last></author>
      <author><first>Houqiang</first><last>Li</last></author>
      <pages>3001-3013</pages>
      <abstract>Query embedding (QE)—which aims to embed entities and first-order logical (FOL) queries in a vector space, has shown great power in answering FOL queries on knowledge graphs (KGs). Existing QE methods divide a complex query into a sequence of mini-queries according to its computation graph and perform logical operations on the answer sets of mini-queries to get answers. However, most of them assume that answer sets satisfy an individual distribution (e.g., Uniform, Beta, or Gaussian), which is often violated in real applications and limit their performance. In this paper, we propose a Neural-based Mixture Probabilistic Query Embedding Model (NMP-QEM) that encodes the answer set of each mini-query as a mixed Gaussian distribution with multiple means and covariance parameters, which can approximate any random distribution arbitrarily well in real KGs. Additionally, to overcome the difficulty in defining the closed solution of negation operation, we introduce neural-based logical operators of projection, intersection and negation for a mixed Gaussian distribution to answer all the FOL queries. Extensive experiments demonstrate that NMP-QEM significantly outperforms existing state-of-the-art methods on benchmark datasets. In NELL995, NMP-QEM achieves a 31% relative improvement over the state-of-the-art.</abstract>
      <url hash="6a4fb48a">2022.emnlp-main.194</url>
      <bibkey>long-etal-2022-neural</bibkey>
    </paper>
    <paper id="195">
      <title>Improving Multi-turn Emotional Support Dialogue Generation with Lookahead Strategy Planning</title>
      <author><first>Yi</first><last>Cheng</last></author>
      <author><first>Wenge</first><last>Liu</last></author>
      <author><first>Wenjie</first><last>Li</last></author>
      <author><first>Jiashuo</first><last>Wang</last></author>
      <author><first>Ruihui</first><last>Zhao</last></author>
      <author><first>Bang</first><last>Liu</last></author>
      <author><first>Xiaodan</first><last>Liang</last></author>
      <author><first>Yefeng</first><last>Zheng</last></author>
      <pages>3014-3026</pages>
      <abstract>Providing Emotional Support (ES) to soothe people in emotional distress is an essential capability in social interactions. Most existing researches on building ES conversation systems only considered single-turn interactions with users, which was over-simplified. In comparison, multi-turn ES conversation systems can provide ES more effectively, but face several new technical challenges, including: (1) how to adopt appropriate support strategies to achieve the long-term dialogue goal of comforting the user’s emotion; (2) how to dynamically model the user’s state. In this paper, we propose a novel system MultiESC to address these issues. For strategy planning, drawing inspiration from the A* search algorithm, we propose lookahead heuristics to estimate the future user feedback after using particular strategies, which helps to select strategies that can lead to the best long-term effects. For user state modeling, MultiESC focuses on capturing users’ subtle emotional expressions and understanding their emotion causes. Extensive experiments show that MultiESC significantly outperforms competitive baselines in both dialogue generation and strategy planning.</abstract>
      <url hash="3e97509c">2022.emnlp-main.195</url>
      <bibkey>cheng-etal-2022-improving</bibkey>
    </paper>
    <paper id="196">
      <title>Conformal Predictor for Improving Zero-Shot Text Classification Efficiency</title>
      <author><first>Prafulla Kumar</first><last>Choubey</last></author>
      <author><first>Yu</first><last>Bai</last></author>
      <author><first>Chien-Sheng</first><last>Wu</last></author>
      <author><first>Wenhao</first><last>Liu</last></author>
      <author><first>Nazneen</first><last>Rajani</last></author>
      <pages>3027-3034</pages>
      <abstract>Pre-trained language models (PLMs) have been shown effective for zero-shot (0shot) text classification. 0shot models based on natural language inference (NLI) and next sentence prediction (NSP) employ cross-encoder architecture and infer by making a forward pass through the model for each label-text pair separately. This increases the computational cost to make inferences linearly in the number of labels. In this work, we improve the efficiency of such cross-encoder-based 0shot models by restricting the number of likely labels using another fast base classifier-based conformal predictor (CP) calibrated on samples labeled by the 0shot model. Since a CP generates prediction sets with coverage guarantees, it reduces the number of target labels without excluding the most probable label based on the 0shot model. We experiment with three intent and two topic classification datasets. With a suitable CP for each dataset, we reduce the average inference time for NLI- and NSP-based models by 25.6% and 22.2% respectively, without dropping performance below the predefined error rate of 1%.</abstract>
      <url hash="60033c4d">2022.emnlp-main.196</url>
      <bibkey>choubey-etal-2022-conformal</bibkey>
    </paper>
    <paper id="197">
      <title>Effective and Efficient Query-aware Snippet Extraction for Web Search</title>
      <author><first>Jingwei</first><last>Yi</last></author>
      <author><first>Fangzhao</first><last>Wu</last></author>
      <author><first>Chuhan</first><last>Wu</last></author>
      <author><first>Xiaolong</first><last>Huang</last></author>
      <author><first>Binxing</first><last>Jiao</last></author>
      <author><first>Guangzhong</first><last>Sun</last></author>
      <author><first>Xing</first><last>Xie</last></author>
      <pages>3035-3046</pages>
      <abstract>Query-aware webpage snippet extraction is widely used in search engines to help users better understand the content of the returned webpages before clicking. The extracted snippet is expected to summarize the webpage in the context of the input query. Existing snippet extraction methods mainly rely on handcrafted features of overlapping words, which cannot capture deep semantic relationships between the query and webpages. Another idea is to extract the sentences which are most relevant to queries as snippets with existing text matching methods. However, these methods ignore the contextual information of webpages, which may be sub-optimal. In this paper, we propose an effective query-aware webpage snippet extraction method named DeepQSE. In DeepQSE, the concatenation of title, query and each candidate sentence serves as an input of query-aware sentence encoder, aiming to capture the fine-grained relevance between the query and sentences. Then, these query-aware sentence representations are modeled jointly through a document-aware relevance encoder to capture contextual information of the webpage. Since the query and each sentence are jointly modeled in DeepQSE, its online inference may be slow. Thus, we further propose an efficient version of DeepQSE, named Efficient-DeepQSE, which can significantly improve the inference speed of DeepQSE without affecting its performance. The core idea of Efficient-DeepQSE is to decompose the query-aware snippet extraction task into two stages, i.e., a coarse-grained candidate sentence selection stage where sentence representations can be cached, and a fine-grained relevance modeling stage. Experiments on two datasets validate the effectiveness and efficiency of our methods.</abstract>
      <url hash="e08548f9">2022.emnlp-main.197</url>
      <bibkey>yi-etal-2022-effective</bibkey>
    </paper>
    <paper id="198">
      <title>You Only Need One Model for Open-domain Question Answering</title>
      <author><first>Haejun</first><last>Lee</last></author>
      <author><first>Akhil</first><last>Kedia</last></author>
      <author><first>Jongwon</first><last>Lee</last></author>
      <author><first>Ashwin</first><last>Paranjape</last></author>
      <author><first>Christopher</first><last>Manning</last></author>
      <author><first>Kyoung-Gu</first><last>Woo</last></author>
      <pages>3047-3060</pages>
      <abstract>Recent approaches to Open-domain Question Answering refer to an external knowledge base using a retriever model, optionally rerank passages with a separate reranker model and generate an answer using another reader model. Despite performing related tasks, the models have separate parameters and are weakly-coupled during training. We propose casting the retriever and the reranker as internal passage-wise attention mechanisms applied sequentially within the transformer architecture and feeding computed representations to the reader, with the hidden representations progressively refined at each stage. This allows us to use a single question answering model trained end-to-end, which is a more efficient use of model capacity and also leads to better gradient flow. We present a pre-training method to effectively train this architecture and evaluate our model on the Natural Questions and TriviaQA open datasets. For a fixed parameter budget, our model outperforms the previous state-of-the-art model by 1.0 and 0.7 exact match scores.</abstract>
      <url hash="3bcf3732">2022.emnlp-main.198</url>
      <bibkey>lee-etal-2022-need</bibkey>
    </paper>
    <paper id="199">
      <title>Generative Entity Typing with Curriculum Learning</title>
      <author><first>Siyu</first><last>Yuan</last></author>
      <author><first>Deqing</first><last>Yang</last></author>
      <author><first>Jiaqing</first><last>Liang</last></author>
      <author><first>Zhixu</first><last>Li</last></author>
      <author><first>Jinxi</first><last>Liu</last></author>
      <author><first>Jingyue</first><last>Huang</last></author>
      <author><first>Yanghua</first><last>Xiao</last></author>
      <pages>3061-3073</pages>
      <abstract>Entity typing aims to assign types to the entity mentions in given texts. The traditional classification-based entity typing paradigm has two unignorable drawbacks: 1) it fails to assign an entity to the types beyond the predefined type set, and 2) it can hardly handle few-shot and zero-shot situations where many long-tail types only have few or even no training instances. To overcome these drawbacks, we propose a novel generative entity typing (GET) paradigm: given a text with an entity mention, the multiple types for the role that the entity plays in the text are generated with a pre-trained language model (PLM). However, PLMs tend to generate coarse-grained types after fine-tuning upon the entity typing dataset. In addition, only the heterogeneous training data consisting of a small portion of human-annotated data and a large portion of auto-generated but low-quality data are provided for model training. To tackle these problems, we employ curriculum learning (CL) to train our GET model on heterogeneous data, where the curriculum could be self-adjusted with the self-paced learning according to its comprehension of the type granularity and data heterogeneity. Our extensive experiments upon the datasets of different languages and downstream tasks justify the superiority of our GET model over the state-of-the-art entity typing models. The code has been released on https://github.com/siyuyuan/GET.</abstract>
      <url hash="d2efe2ed">2022.emnlp-main.199</url>
      <bibkey>yuan-etal-2022-generative-entity</bibkey>
    </paper>
    <paper id="200">
      <title><fixed-case>S</fixed-case>et<fixed-case>GNER</fixed-case>: General Named Entity Recognition as Entity Set Generation</title>
      <author><first>Yuxin</first><last>He</last></author>
      <author><first>Buzhou</first><last>Tang</last></author>
      <pages>3074-3085</pages>
      <abstract>Recently, joint recognition of flat, nested and discontinuous entities has received increasing attention. Motivated by the observation that the target output of NER is essentially a set of sequences, we propose a novel entity set generation framework for general NER scenes in this paper. Different from sequence-to-sequence NER methods, our method does not force the entities to be generated in a predefined order and can get rid of the problem of error propagation and inefficient decoding. Distinguished from the set-prediction NER framework, our method treats each entity as a sequence and is capable of recognizing discontinuous mentions. Given an input sentence, the model first encodes the sentence in word-level and detects potential entity mentions based on the encoder’s output, then reconstructs entity mentions from the detected entity heads in parallel. To let the encoder of our model capture better right-to-left semantic structure, we also propose an auxiliary Inverse Generation Training task. Extensive experiments show that our model (w/o. Inverse Generation Training) outperforms state-of-the-art generative NER models by a large margin on two discontinuous NER datasets, two nested NER datasets and one flat NER dataset. Besides, the auxiliary Inverse Generation Training task is found to further improve the model’s performance on the five datasets.</abstract>
      <url hash="03115e01">2022.emnlp-main.200</url>
      <bibkey>he-tang-2022-setgner</bibkey>
    </paper>
    <paper id="201">
      <title>Opinion Summarization by Weak-Supervision from Mix-structured Data</title>
      <author><first>Yizhu</first><last>Liu</last></author>
      <author><first>Qi</first><last>Jia</last></author>
      <author><first>Kenny</first><last>Zhu</last></author>
      <pages>3086-3096</pages>
      <abstract>Opinion summarization of multiple reviews suffers from the lack of reference summaries for training.Most previous approaches construct multiple reviews and their summary based on textual similarities between reviews,resulting in information mismatch between the review input and the summary. In this paper, we convert each review into a mixof structured and unstructured data, which we call opinion-aspect pairs (OAs) and implicit sentences (ISs).We propose a new method to synthesize training pairs of such mix-structured data as input and the textual summary as output,and design a summarization model with OA encoder and IS encoder.Experiments show that our approach outperforms previous methods on Yelp, Amazon and RottenTomatos datasets.</abstract>
      <url hash="8f711800">2022.emnlp-main.201</url>
      <bibkey>liu-etal-2022-opinion</bibkey>
    </paper>
    <paper id="202">
      <title>Multi-level Distillation of Semantic Knowledge for Pre-training Multilingual Language Model</title>
      <author><first>Mingqi</first><last>Li</last></author>
      <author><first>Fei</first><last>Ding</last></author>
      <author><first>Dan</first><last>Zhang</last></author>
      <author><first>Long</first><last>Cheng</last></author>
      <author><first>Hongxin</first><last>Hu</last></author>
      <author><first>Feng</first><last>Luo</last></author>
      <pages>3097-3106</pages>
      <abstract>Pre-trained multilingual language models play an important role in cross-lingual natural language understanding tasks. However, existing methods did not focus on learning the semantic structure of representation, and thus could not optimize their performance. In this paper, we propose Multi-level Multilingual Knowledge Distillation (MMKD), a novel method for improving multilingual language models. Specifically, we employ a teacher-student framework to adopt rich semantic representation knowledge in English BERT. We propose token-, word-, sentence-, and structure-level alignment objectives to encourage multiple levels of consistency between source-target pairs and correlation similarity between teacher and student models. We conduct experiments on cross-lingual evaluation benchmarks including XNLI, PAWS-X, and XQuAD. Experimental results show that MMKD outperforms other baseline models of similar size on XNLI and XQuAD and obtains comparable performance on PAWS-X. Especially, MMKD obtains significant performance gains on low-resource languages.</abstract>
      <url hash="31f45a62">2022.emnlp-main.202</url>
      <bibkey>li-etal-2022-multi-level</bibkey>
    </paper>
    <paper id="203">
      <title>Empowering Dual-Encoder with Query Generator for Cross-Lingual Dense Retrieval</title>
      <author><first>Houxing</first><last>Ren</last></author>
      <author><first>Linjun</first><last>Shou</last></author>
      <author><first>Ning</first><last>Wu</last></author>
      <author><first>Ming</first><last>Gong</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <pages>3107-3121</pages>
      <abstract>In monolingual dense retrieval, lots of works focus on how to distill knowledge from cross-encoder re-ranker to dual-encoder retriever and these methods achieve better performance due to the effectiveness of cross-encoder re-ranker. However, we find that the performance of the cross-encoder re-ranker is heavily influenced by the number of training samples and the quality of negative samples, which is hard to obtain in the cross-lingual setting. In this paper, we propose to use a query generator as the teacher in the cross-lingual setting, which is less dependent on enough training samples and high-quality negative samples. In addition to traditional knowledge distillation, we further propose a novel enhancement method, which uses the query generator to help the dual-encoder align queries from different languages, but does not need any additional parallel sentences. The experimental results show that our method outperforms the state-of-the-art methods on two benchmark datasets.</abstract>
      <url hash="222c1c77">2022.emnlp-main.203</url>
      <bibkey>ren-etal-2022-empowering</bibkey>
    </paper>
    <paper id="204">
      <title><fixed-case>R</fixed-case>2<fixed-case>F</fixed-case>: A General Retrieval, Reading and Fusion Framework for Document-level Natural Language Inference</title>
      <author><first>Hao</first><last>Wang</last></author>
      <author><first>Yixin</first><last>Cao</last></author>
      <author><first>Yangguang</first><last>Li</last></author>
      <author><first>Zhen</first><last>Huang</last></author>
      <author><first>Kun</first><last>Wang</last></author>
      <author><first>Jing</first><last>Shao</last></author>
      <pages>3122-3134</pages>
      <abstract>Document-level natural language inference (DOCNLI) is a new challenging task in natural language processing, aiming at judging the entailment relationship between a pair of hypothesis and premise documents. Current datasets and baselines largely follow sentence-level settings, but fail to address the issues raised by longer documents. In this paper, we establish a general solution, named Retrieval, Reading and Fusion (R2F) framework, and a new setting, by analyzing the main challenges of DOCNLI: interpretability, long-range dependency, and cross-sentence inference. The basic idea of the framework is to simplify document-level task into a set of sentence-level tasks, and improve both performance and interpretability with the power of evidence. For each hypothesis sentence, the framework retrieves evidence sentences from the premise, and reads to estimate its credibility. Then the sentence-level results are fused to judge the relationship between the documents. For the setting, we contribute complementary evidence and entailment label annotation on hypothesis sentences, for interpretability study. Our experimental results show that R2F framework can obtain state-of-the-art performance and is robust for diverse evidence retrieval methods. Moreover, it can give more interpretable prediction results. Our model and code are released at https://github.com/phoenixsecularbird/R2F.</abstract>
      <url hash="0f1fe6b6">2022.emnlp-main.204</url>
      <bibkey>wang-etal-2022-r2f</bibkey>
    </paper>
    <paper id="205">
      <title>Revisiting Pre-trained Language Models and their Evaluation for <fixed-case>A</fixed-case>rabic Natural Language Processing</title>
      <author><first>Abbas</first><last>Ghaddar</last></author>
      <author><first>Yimeng</first><last>Wu</last></author>
      <author><first>Sunyam</first><last>Bagga</last></author>
      <author><first>Ahmad</first><last>Rashid</last></author>
      <author><first>Khalil</first><last>Bibi</last></author>
      <author><first>Mehdi</first><last>Rezagholizadeh</last></author>
      <author><first>Chao</first><last>Xing</last></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Xinyu</first><last>Duan</last></author>
      <author><first>Zhefeng</first><last>Wang</last></author>
      <author><first>Baoxing</first><last>Huai</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <author><first>Phillippe</first><last>Langlais</last></author>
      <pages>3135-3151</pages>
      <abstract>There is a growing body of work in recent years to develop pre-trained language models (PLMs) for the Arabic language. This work addresses two major problems in existing Arabic PLMs that limit the progress of the Arabic NLU and NLG fields. First, existing Arabic PLMs are not well-explored and their pre-training can be improved significantly using a more methodical approach. Second, there is a lack of systematic and reproducible evaluation of these models in the literature. We revisit both the pre-training and evaluation of Arabic PLMs. In terms of pre-training, we explore the impact of the quality of the pretraining data, the size of the model, and the incorporation of character-level information on Arabic PLM. As a result, we release three new Arabic BERT-style models ( JABER, Char-JABER, and SABER), and two T5-style models (AT5S and AT5B). In terms of evaluation, we conduct a comprehensive empirical study to systematically evaluate the performance of existing state-of-the-art models on ALUE, a leaderboard-powered benchmark for Arabic NLU tasks, and on a subset of the Arabic generative tasks. We show that our models significantly outperform existing Arabic PLMs and achieve a new state-of-the-art performance on discriminative and generative Arabic NLU and NLG tasks. Our models and source code to reproduce results will be made available upon acceptance.</abstract>
      <url hash="f8b168d0">2022.emnlp-main.205</url>
      <bibkey>ghaddar-etal-2022-revisiting</bibkey>
    </paper>
    <paper id="206">
      <title><fixed-case>KECP</fixed-case>: Knowledge Enhanced Contrastive Prompting for Few-shot Extractive Question Answering</title>
      <author><first>Jianing</first><last>Wang</last></author>
      <author><first>Chengyu</first><last>Wang</last></author>
      <author><first>Minghui</first><last>Qiu</last></author>
      <author><first>Qiuhui</first><last>Shi</last></author>
      <author><first>Hongbin</first><last>Wang</last></author>
      <author><first>Jun</first><last>Huang</last></author>
      <author><first>Ming</first><last>Gao</last></author>
      <pages>3152-3163</pages>
      <abstract>Extractive Question Answering (EQA) is one of the most essential tasks in Machine Reading Comprehension (MRC), which can be solved by fine-tuning the span selecting heads of Pre-trained Language Models (PLMs). However, most existing approaches for MRC may perform poorly in the few-shot learning scenario. To solve this issue, we propose a novel framework named Knowledge Enhanced Contrastive Prompt-tuning (KECP). Instead of adding pointer heads to PLMs, we introduce a seminal paradigm for EQA that transforms the task into a non-autoregressive Masked Language Modeling (MLM) generation problem. Simultaneously, rich semantics from the external knowledge base (KB) and the passage context support enhancing the query’s representations. In addition, to boost the performance of PLMs, we jointly train the model by the MLM and contrastive learning objectives. Experiments on multiple benchmarks demonstrate that our method consistently outperforms state-of-the-art approaches in few-shot settings by a large margin.</abstract>
      <url hash="782a9a78">2022.emnlp-main.206</url>
      <bibkey>wang-etal-2022-kecp</bibkey>
    </paper>
    <paper id="207">
      <title>Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding</title>
      <author><first>Jianing</first><last>Wang</last></author>
      <author><first>Wenkang</first><last>Huang</last></author>
      <author><first>Minghui</first><last>Qiu</last></author>
      <author><first>Qiuhui</first><last>Shi</last></author>
      <author><first>Hongbin</first><last>Wang</last></author>
      <author><first>Xiang</first><last>Li</last></author>
      <author><first>Ming</first><last>Gao</last></author>
      <pages>3164-3177</pages>
      <abstract>Knowledge-enhanced Pre-trained Language Model (PLM) has recently received significant attention, which aims to incorporate factual knowledge into PLMs. However, most existing methods modify the internal structures of fixed types of PLMs by stacking complicated modules, and introduce redundant and irrelevant factual knowledge from knowledge bases (KBs). In this paper, to address these problems, we introduce a seminal knowledge prompting paradigm and further propose a knowledge-prompting-based PLM framework KP-PLM. This framework can be flexibly combined with existing mainstream PLMs. Specifically, we first construct a knowledge sub-graph from KBs for each context. Then we design multiple continuous prompts rules and transform the knowledge sub-graph into natural language prompts. To further leverage the factual knowledge from these prompts, we propose two novel knowledge-aware self-supervised tasks including prompt relevance inspection and masked prompt modeling. Extensive experiments on multiple natural language understanding (NLU) tasks show the superiority of KP-PLM over other state-of-the-art methods in both full-resource and low-resource settings. Our source codes will be released upon the acceptance of the paper.</abstract>
      <url hash="136270ea">2022.emnlp-main.207</url>
      <bibkey>wang-etal-2022-knowledge</bibkey>
    </paper>
    <paper id="208">
      <title>On the Evaluation Metrics for Paraphrase Generation</title>
      <author><first>Lingfeng</first><last>Shen</last></author>
      <author><first>Lemao</first><last>Liu</last></author>
      <author><first>Haiyun</first><last>Jiang</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <pages>3178-3190</pages>
      <abstract>In this paper we revisit automatic metrics for paraphrase evaluation and obtain two findings that disobey conventional wisdom: (1) Reference-free metrics achieve better performance than their reference-based counterparts. (2) Most commonly used metrics do not align well with human annotation.Underlying reasons behind the above findings are explored through additional experiments and in-depth analyses.Based on the experiments and analyses, we propose ParaScore, a new evaluation metric for paraphrase generation. It possesses the merits of reference-based and reference-free metrics and explicitly models lexical divergence. Based on our analysis and improvements, our proposed reference-based outperforms than reference-free metrics.Experimental results demonstrate that ParaScore significantly outperforms existing metrics.</abstract>
      <url hash="ce954f19">2022.emnlp-main.208</url>
      <bibkey>shen-etal-2022-evaluation</bibkey>
    </paper>
    <paper id="209">
      <title>Curriculum Learning Meets Weakly Supervised Multimodal Correlation Learning</title>
      <author><first>Sijie</first><last>Mai</last></author>
      <author><first>Ya</first><last>Sun</last></author>
      <author><first>Haifeng</first><last>Hu</last></author>
      <pages>3191-3203</pages>
      <abstract>In the field of multimodal sentiment analysis (MSA), a few studies have leveraged the inherent modality correlation information stored in samples for self-supervised learning. However, they feed the training pairs in a random order without consideration of difficulty. Without human annotation, the generated training pairs of self-supervised learning often contain noise. If noisy or hard pairs are used for training at the easy stage, the model might be stuck in bad local optimum. In this paper, we inject curriculum learning into weakly supervised multimodal correlation learning. The weakly supervised correlation learning leverages the label information to generate scores for negative pairs to learn a more discriminative embedding space, where negative pairs are defined as two unimodal embeddings from different samples. To assist the correlation learning, we feed the training pairs to the model according to difficulty by the proposed curriculum learning, which consists of elaborately designed scoring and feeding functions. The scoring function computes the difficulty of pairs using pre-trained and current correlation predictors, where the pairs with large losses are defined as hard pairs. Notably, the hardest pairs are discarded in our algorithm, which are assumed as noisy pairs. Moreover, the feeding function takes the difference of correlation losses as feedback to determine the feeding actions (‘stay’, ‘step back’, or ‘step forward’). The proposed method reaches state-of-the-art performance on MSA.</abstract>
      <url hash="53f5eb41">2022.emnlp-main.209</url>
      <bibkey>mai-etal-2022-curriculum</bibkey>
    </paper>
    <paper id="210">
      <title>Rethinking Positional Encoding in Tree Transformer for Code Representation</title>
      <author><first>Han</first><last>Peng</last></author>
      <author><first>Ge</first><last>Li</last></author>
      <author><first>Yunfei</first><last>Zhao</last></author>
      <author><first>Zhi</first><last>Jin</last></author>
      <pages>3204-3214</pages>
      <abstract>Transformers are now widely used in code representation, and several recent works further develop tree Transformers to capture the syntactic structure in source code. Specifically, novel tree positional encodings have been proposed to incorporate inductive bias into Transformer.In this work, we propose a novel tree Transformer encoding node positions based on our new description method for tree structures.Technically, local and global soft bias shown in previous works is both introduced as positional encodings of our Transformer model.Our model finally outperforms strong baselines on code summarization and completion tasks across two languages, demonstrating our model’s effectiveness.Besides, extensive experiments and ablation study shows that combining both local and global paradigms is still helpful in improving model performance. We release our code at <url>https://github.com/AwdHanPeng/TreeTransformer</url>.</abstract>
      <url hash="57e64b6c">2022.emnlp-main.210</url>
      <bibkey>peng-etal-2022-rethinking</bibkey>
    </paper>
    <paper id="211">
      <title><fixed-case>RASAT</fixed-case>: Integrating Relational Structures into Pretrained <fixed-case>S</fixed-case>eq2<fixed-case>S</fixed-case>eq Model for Text-to-<fixed-case>SQL</fixed-case></title>
      <author><first>Jiexing</first><last>Qi</last></author>
      <author><first>Jingyao</first><last>Tang</last></author>
      <author><first>Ziwei</first><last>He</last></author>
      <author><first>Xiangpeng</first><last>Wan</last></author>
      <author><first>Yu</first><last>Cheng</last></author>
      <author><first>Chenghu</first><last>Zhou</last></author>
      <author><first>Xinbing</first><last>Wang</last></author>
      <author><first>Quanshi</first><last>Zhang</last></author>
      <author><first>Zhouhan</first><last>Lin</last></author>
      <pages>3215-3229</pages>
      <abstract>Relational structures such as schema linking and schema encoding have been validated as a key component to qualitatively translating natural language into SQL queries. However, introducing these structural relations comes with prices: they often result in a specialized model structure, which largely prohibits using large pretrained models in text-to-SQL. To address this problem, we propose RASAT: a Transformer seq2seq architecture augmented with relation-aware self-attention that could leverage a variety of relational structures while inheriting the pretrained parameters from the T5 model effectively. Our model can incorporate almost all types of existing relations in the literature, and in addition, we propose introducing co-reference relations for the multi-turn scenario. Experimental results on three widely used text-to-SQL datasets, covering both single-turn and multi-turn scenarios, have shown that RASAT could achieve competitive results in all three benchmarks, achieving state-of-the-art execution accuracy (75.5% EX on Spider, 52.6% IEX on SParC, and 37.4% IEX on CoSQL).</abstract>
      <url hash="c5411bcb">2022.emnlp-main.211</url>
      <bibkey>qi-etal-2022-rasat</bibkey>
    </paper>
    <paper id="212">
      <title><fixed-case>COM</fixed-case>-<fixed-case>MRC</fixed-case>: A <fixed-case>CO</fixed-case>ntext-Masked Machine Reading Comprehension Framework for Aspect Sentiment Triplet Extraction</title>
      <author><first>Zepeng</first><last>Zhai</last></author>
      <author><first>Hao</first><last>Chen</last></author>
      <author><first>Fangxiang</first><last>Feng</last></author>
      <author><first>Ruifan</first><last>Li</last></author>
      <author><first>Xiaojie</first><last>Wang</last></author>
      <pages>3230-3241</pages>
      <abstract>Aspect Sentiment Triplet Extraction (ASTE) aims to extract sentiment triplets from sentences, which was recently formalized as an effective machine reading comprehension (MRC) based framework. However, when facing multiple aspect terms, the MRC-based methods could fail due to the interference from other aspect terms. In this paper, we propose a novel <i>COntext-Masked MRC</i> (COM-MRC) framework for ASTE. Our COM-MRC framework comprises three closely-related components: a context augmentation strategy, a discriminative model, and an inference method. Specifically, a context augmentation strategy is designed by enumerating all masked contexts for each aspect term. The discriminative model comprises four modules, i.e., aspect and opinion extraction modules, sentiment classification and aspect detection modules. In addition, a two-stage inference method first extracts all aspects and then identifies their opinions and sentiment through iteratively masking the aspects. Extensive experimental results on benchmark datasets show the effectiveness of our proposed COM-MRC framework, which outperforms state-of-the-art methods consistently.</abstract>
      <url hash="a718d169">2022.emnlp-main.212</url>
      <bibkey>zhai-etal-2022-com</bibkey>
    </paper>
    <paper id="213">
      <title><fixed-case>CEM</fixed-case>: Machine-Human Chatting Handoff via Causal-Enhance Module</title>
      <author><first>Shanshan</first><last>Zhong</last></author>
      <author><first>Jinghui</first><last>Qin</last></author>
      <author><first>Zhongzhan</first><last>Huang</last></author>
      <author><first>Daifeng</first><last>Li</last></author>
      <pages>3242-3253</pages>
      <abstract>Aiming to ensure chatbot quality by predicting chatbot failure and enabling human-agent collaboration, Machine-Human Chatting Handoff (MHCH) has attracted lots of attention from both industry and academia in recent years. However, most existing methods mainly focus on the dialogue context or assist with global satisfaction prediction based on multi-task learning, which ignore the grounded relationships among the causal variables, like the user state and labor cost. These variables are significantly associated with handoff decisions, resulting in prediction bias and cost increasement. Therefore, we propose Causal-Enhance Module (CEM) by establishing the causal graph of MHCH based on these two variables, which is a simple yet effective module and can be easy to plug into the existing MHCH methods. For the impact of users, we use the user state to correct the prediction bias according to the causal relationship of multi-task. For the labor cost, we train an auxiliary cost simulator to calculate unbiased labor cost through counterfactual learning so that a model becomes cost-aware.Extensive experiments conducted on four real-world benchmarks demonstrate the effectiveness of CEM in generally improving the performance of existing MHCH methods without any elaborated model crafting.</abstract>
      <url hash="fea6c486">2022.emnlp-main.213</url>
      <bibkey>zhong-etal-2022-cem</bibkey>
    </paper>
    <paper id="214">
      <title>Nearest Neighbor Zero-Shot Inference</title>
      <author><first>Weijia</first><last>Shi</last></author>
      <author><first>Julian</first><last>Michael</last></author>
      <author><first>Suchin</first><last>Gururangan</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <pages>3254-3265</pages>
      <abstract>Retrieval-augmented language models (LMs) use non-parametric memory to substantially outperform their non-retrieval counterparts on perplexity-based evaluations, but it is an open question whether they achieve similar gains in few- and zero-shot end-task accuracy. We extensively study one such model, the k-nearest neighbor LM (kNN-LM), showing that the gains marginally transfer. The main challenge is to achieve coverage of the verbalizer tokens that define the different end-task class labels. To address this challenge, we also introduce kNN-Prompt, a simple and effective kNN-LM with automatically expanded fuzzy verbalizers (e.g. to expand “terrible” to also include “silly” and other task-specific synonyms for sentiment classification). Across nine diverse end-tasks, using kNN-Prompt with GPT-2 large yields significant performance boosts over strong zeroshot baselines (13.4% absolute improvement over the base LM on average). We also show that other advantages of non-parametric augmentation hold for end tasks; kNN-Prompt is effective for domain adaptation with no further training, and gains increase with the size of the retrieval model.</abstract>
      <url hash="ebe5e0f1">2022.emnlp-main.214</url>
      <bibkey>shi-etal-2022-nearest</bibkey>
    </paper>
    <paper id="215">
      <title>Robots-Dont-Cry: Understanding Falsely Anthropomorphic Utterances in Dialog Systems</title>
      <author><first>David</first><last>Gros</last></author>
      <author><first>Yu</first><last>Li</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <pages>3266-3284</pages>
      <abstract>Dialog systems are often designed or trained to output human-like responses. However, some responses may be impossible for a machine to truthfully say (e.g. “that movie made me cry”). Highly anthropomorphic responses might make users uncomfortable or implicitly deceive them into thinking they are interacting with a human. We collect human ratings on the feasibility of approximately 900 two-turn dialogs sampled from 9 diverse data sources. Ratings are for two hypothetical machine embodiments: a futuristic humanoid robot and a digital assistant. We find that for some data-sources commonly used to train dialog systems, 20-30% of utterances are not viewed as possible for a machine. Rating is marginally affected by machine embodiment. We explore qualitative and quantitative reasons for these ratings. Finally, we build classifiers and explore how modeling configuration might affect output permissibly, and discuss implications for building less falsely anthropomorphic dialog systems.</abstract>
      <url hash="9bb1959b">2022.emnlp-main.215</url>
      <bibkey>gros-etal-2022-robots</bibkey>
    </paper>
    <paper id="216">
      <title>A Joint Learning Framework for Restaurant Survival Prediction and Explanation</title>
      <author><first>Xin</first><last>Li</last></author>
      <author><first>Xiaojie</first><last>Zhang</last></author>
      <author><first>Peng</first><last>JiaHao</last></author>
      <author><first>Rui</first><last>Mao</last></author>
      <author><first>Mingyang</first><last>Zhou</last></author>
      <author><first>Xing</first><last>Xie</last></author>
      <author><first>Hao</first><last>Liao</last></author>
      <pages>3285-3297</pages>
      <abstract>The bloom of the Internet and the recent breakthroughs in deep learning techniques open a new door to AI for E-commence, with a trend of evolving from using a few financial factors such as liquidity and profitability to using more advanced AI techniques to process complex and multi-modal data. In this paper, we tackle the practical problem of restaurant survival prediction. We argue that traditional methods ignore two essential respects, which are very helpful for the task: 1) modeling customer reviews and 2) jointly considering status prediction and result explanation. Thus, we propose a novel joint learning framework for explainable restaurant survival prediction based on the multi-modal data of user-restaurant interactions and users’ textual reviews. Moreover, we design a graph neural network to capture the high-order interactions and design a co-attention mechanism to capture the most informative and meaningful signal from noisy textual reviews. Our results on two datasets show a significant and consistent improvement over the SOTA techniques (average 6.8% improvement in prediction and 45.3% improvement in explanation).</abstract>
      <url hash="e1aff508">2022.emnlp-main.216</url>
      <bibkey>li-etal-2022-joint</bibkey>
    </paper>
    <paper id="217">
      <title>Making Pretrained Language Models Good Long-tailed Learners</title>
      <author><first>Chen</first><last>Zhang</last></author>
      <author><first>Lei</first><last>Ren</last></author>
      <author><first>Jingang</first><last>Wang</last></author>
      <author><first>Wei</first><last>Wu</last></author>
      <author><first>Dawei</first><last>Song</last></author>
      <pages>3298-3312</pages>
      <abstract>Prompt-tuning has shown appealing performance in few-shot classification by virtue of its capability in effectively exploiting pre-trained knowledge. This motivates us to check the hypothesis that prompt-tuning is also a promising choice for long-tailed classification, since the tail classes are intuitively few-shot ones. To achieve this aim, we conduct empirical studies to examine the hypothesis. The results demonstrate that prompt-tuning makes pretrained language models at least good long-tailed learners. For intuitions on why prompt-tuning can achieve good performance in long-tailed classification, we carry out in-depth analyses by progressively bridging the gap between prompt-tuning and commonly used finetuning. The summary is that the classifier structure and parameterization form the key to making good long-tailed learners, in comparison with the less important input structure. Finally, we verify the applicability of our finding to few-shot classification.</abstract>
      <url hash="3a60fb5b">2022.emnlp-main.217</url>
      <bibkey>zhang-etal-2022-making</bibkey>
    </paper>
    <paper id="218">
      <title><fixed-case>U</fixed-case>ni<fixed-case>G</fixed-case>eo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression</title>
      <author><first>Jiaqi</first><last>Chen</last></author>
      <author><first>Tong</first><last>Li</last></author>
      <author><first>Jinghui</first><last>Qin</last></author>
      <author><first>Pan</first><last>Lu</last></author>
      <author><first>Liang</first><last>Lin</last></author>
      <author><first>Chongyu</first><last>Chen</last></author>
      <author><first>Xiaodan</first><last>Liang</last></author>
      <pages>3313-3323</pages>
      <abstract>Geometry problem solving is a well-recognized testbed for evaluating the high-level multi-modal reasoning capability of deep models. In most existing works, two main geometry problems: calculation and proving, are usually treated as two specific tasks, hindering a deep model to unify its reasoning capability on multiple math tasks. However, in essence, these two tasks have similar problem representations and overlapped math knowledge which can improve the understanding and reasoning ability of a deep model on both two tasks. Therefore, we construct a large-scale Unified Geometry problem benchmark, UniGeo, which contains 4,998 calculation problems and 9,543 proving problems. Each proving problem is annotated with a multi-step proof with reasons and mathematical expressions. The proof can be easily reformulated as a proving sequence that shares the same formats with the annotated program sequence for calculation problems. Naturally, we also present a unified multi-task Geometric Transformer framework, Geoformer, to tackle calculation and proving problems simultaneously in the form of sequence generation, which finally shows the reasoning ability can be improved on both two tasks by unifying formulation. Furthermore, we propose a Mathematical Expression Pretraining (MEP) method that aims to predict the mathematical expressions in the problem solution, thus improving the Geoformer model. Experiments on the UniGeo demonstrate that our proposed Geoformer obtains state-of-the-art performance by outperforming task-specific model NGS with over 5.6% and 3.2% accuracies on calculation and proving problems, respectively.</abstract>
      <url hash="0dae0bf7">2022.emnlp-main.218</url>
      <bibkey>chen-etal-2022-unigeo</bibkey>
    </paper>
    <paper id="219">
      <title>Face-Sensitive Image-to-Emotional-Text Cross-modal Translation for Multimodal Aspect-based Sentiment Analysis</title>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Yanyan</first><last>Zhao</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <pages>3324-3335</pages>
      <abstract>Aspect-level multimodal sentiment analysis, which aims to identify the sentiment of the target aspect from multimodal data, recently has attracted extensive attention in the community of multimedia and natural language processing. Despite the recent success in textual aspect-based sentiment analysis, existing models mainly focused on utilizing the object-level semantic information in the image but ignore explicitly using the visual emotional cues, especially the facial emotions. How to distill visual emotional cues and align them with the textual content remains a key challenge to solve the problem. In this work, we introduce a face-sensitive image-to-emotional-text translation (FITE) method, which focuses on capturing visual sentiment cues through facial expressions and selectively matching and fusing with the target aspect in textual modality. To the best of our knowledge, we are the first that explicitly utilize the emotional information from images in the multimodal aspect-based sentiment analysis task. Experiment results show that our method achieves state-of-the-art results on the Twitter-2015 and Twitter-2017 datasets. The improvement demonstrates the superiority of our model in capturing aspect-level sentiment in multimodal data with facial expressions.</abstract>
      <url hash="9f81e8e7">2022.emnlp-main.219</url>
      <bibkey>yang-etal-2022-face</bibkey>
    </paper>
    <paper id="220">
      <title><fixed-case>F</fixed-case>ine<fixed-case>D</fixed-case>-Eval: Fine-grained Automatic Dialogue-Level Evaluation</title>
      <author><first>Chen</first><last>Zhang</last></author>
      <author><first>Luis Fernando</first><last>D’Haro</last></author>
      <author><first>Qiquan</first><last>Zhang</last></author>
      <author><first>Thomas</first><last>Friedrichs</last></author>
      <author><first>Haizhou</first><last>Li</last></author>
      <pages>3336-3355</pages>
      <abstract>Recent model-based reference-free metrics for open-domain dialogue evaluation exhibit promising correlations with human judgment. However, they either perform turn-level evaluation or look at a single dialogue quality dimension. One would expect a good evaluation metric to assess multiple quality dimensions at the dialogue level. To this end, we are motivated to propose a multi-dimensional dialogue-level metric, which consists of three sub-metrics with each targeting a specific dimension. The sub-metrics are trained with novel self-supervised objectives and exhibit strong correlations with human judgment for their respective dimensions. Moreover, we explore two approaches to combine the sub-metrics: metric ensemble and multitask learning. Both approaches yield a holistic metric that significantly outperforms individual sub-metrics. Compared to the existing state-of-the-art metric, the combined metrics achieve around 16% relative improvement on average across three high-quality dialogue-level evaluation benchmarks.</abstract>
      <url hash="28427051">2022.emnlp-main.220</url>
      <bibkey>zhang-etal-2022-fined</bibkey>
    </paper>
    <paper id="221">
      <title>Sentence Representation Learning with Generative Objective rather than Contrastive Objective</title>
      <author><first>Bohong</first><last>Wu</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <pages>3356-3368</pages>
      <abstract>Though offering amazing contextualized token-level representations, current pre-trained language models take less attention on accurately acquiring sentence-level representation during their self-supervised pre-training. However, contrastive objectives which dominate the current sentence representation learning bring little linguistic interpretability and no performance guarantee on downstream semantic tasks. We instead propose a novel generative self-supervised learning objective based on phrase reconstruction. To overcome the drawbacks of previous generative methods, we carefully model intra-sentence structure by breaking down one sentence into pieces of important phrases. Empirical studies show that our generative learning achieves powerful enough performance improvement and outperforms the current state-of-the-art contrastive methods not only on the STS benchmarks, but also on downstream semantic retrieval and reranking tasks. Our code is available at https://github.com/chengzhipanpan/PaSeR.</abstract>
      <url hash="2eb902d2">2022.emnlp-main.221</url>
      <bibkey>wu-zhao-2022-sentence</bibkey>
    </paper>
    <paper id="222">
      <title><fixed-case>RLP</fixed-case>rompt: Optimizing Discrete Text Prompts with Reinforcement Learning</title>
      <author><first>Mingkai</first><last>Deng</last></author>
      <author><first>Jianyu</first><last>Wang</last></author>
      <author><first>Cheng-Ping</first><last>Hsieh</last></author>
      <author><first>Yihan</first><last>Wang</last></author>
      <author><first>Han</first><last>Guo</last></author>
      <author><first>Tianmin</first><last>Shu</last></author>
      <author><first>Meng</first><last>Song</last></author>
      <author><first>Eric</first><last>Xing</last></author>
      <author><first>Zhiting</first><last>Hu</last></author>
      <pages>3369-3391</pages>
      <abstract>Prompting has shown impressive success in enabling large pre-trained language models (LMs) to perform diverse NLP tasks, especially with only few downstream data. Automatically finding the optimal prompt for each task, however, is challenging. Most existing work resorts to tuning *soft* prompts (e.g., embeddings) which fall short of interpretability, reusability across LMs, and applicability when gradients are not accessible. *Discrete* prompts, on the other hand, are difficult to optimize, and are often created by “enumeration (e.g., paraphrasing)-then-selection” heuristics that do not explore the prompt space systematically. This paper proposes RLPrompt, an efficient discrete prompt optimization approach with reinforcement learning (RL). RLPrompt formulates a parameter-efficient policy network that generates the optimized discrete prompt after training with reward. To harness the complex and stochastic reward signals from the large LM environment, we incorporate effective reward stabilization that substantially enhances training efficiency. RLPrompt is flexibly applicable to different types of LMs, such as masked (e.g., BERT) and left-to-right models (e.g., GPTs), for both classification and generation tasks. Experiments on few-shot classification and unsupervised text style transfer show superior performance over a wide range of existing fine-tuning or prompting methods. Interestingly, the resulting optimized prompts are often ungrammatical gibberish text; and surprisingly, those gibberish prompts are transferrable between different LMs to retain significant performance, indicating that LM prompting may not follow human language patterns.</abstract>
      <url hash="662d2d5e">2022.emnlp-main.222</url>
      <bibkey>deng-etal-2022-rlprompt</bibkey>
    </paper>
    <paper id="223">
      <title><fixed-case>D</fixed-case>is<fixed-case>C</fixed-case>up: Discriminator Cooperative Unlikelihood Prompt-tuning for Controllable Text Generation</title>
      <author><first>Hanqing</first><last>Zhang</last></author>
      <author><first>Dawei</first><last>Song</last></author>
      <pages>3392-3406</pages>
      <abstract>Prompt learning with immensely large Casual Language Models (CLMs) has been shown promising for attribute-controllable text generation (CTG). However, vanilla prompt tuning tends to imitate training corpus characteristics beyond the control attributes, resulting in a poor generalization ability. Moreover, it is less able to capture the relationship between different attributes, further limiting the control performance. In this paper, we propose a new CTG approach, namely DisCup, which incorporates the attribute knowledge of discriminator to optimize the control-prompts, steering a frozen CLM to produce attribute-specific texts. Specifically, the frozen CLM model, capable of producing multitudinous texts, is first used to generate the next-token candidates based on the context, so as to ensure the diversity of tokens to be predicted. Then, we leverage an attribute-discriminator to select desired/undesired tokens from those candidates, providing the inter-attribute knowledge. Finally, we bridge the above two traits by an unlikelihood objective for prompt-tuning. Extensive experimental results show that DisCup can achieve a new state-of-the-art control performance while maintaining an efficient and high-quality text generation, only relying on around 10 virtual tokens.</abstract>
      <url hash="6376801d">2022.emnlp-main.223</url>
      <bibkey>zhang-song-2022-discup</bibkey>
    </paper>
    <paper id="224">
      <title><fixed-case>CPL</fixed-case>: Counterfactual Prompt Learning for Vision and Language Models</title>
      <author><first>Xuehai</first><last>He</last></author>
      <author><first>Diji</first><last>Yang</last></author>
      <author><first>Weixi</first><last>Feng</last></author>
      <author><first>Tsu-Jui</first><last>Fu</last></author>
      <author><first>Arjun</first><last>Akula</last></author>
      <author><first>Varun</first><last>Jampani</last></author>
      <author><first>Pradyumna</first><last>Narayana</last></author>
      <author><first>Sugato</first><last>Basu</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <author><first>Xin</first><last>Wang</last></author>
      <pages>3407-3418</pages>
      <abstract>Prompt tuning is a new few-shot transfer learning technique that only tunes the learnable prompt for pre-trained vision and language models such as CLIP. However, existing prompt tuning methods tend to learn spurious or entangled representations, which leads to poor generalization to unseen concepts.Towards non-spurious and efficient prompt learning from limited examples, this paper presents a novel Counterfactual Prompt Learning (CPL) method for vision and language models, which simultaneously employs counterfactual generation and contrastive learning in a joint optimization framework.Particularly, CPL constructs counterfactual by identifying minimal non-spurious feature change between semantically-similar positive and negative samples that causes concept change, and learns more generalizable prompt representation from both factual and counterfactual examples via contrastive learning. Extensive experiments demonstrate that CPL can obtain superior few-shot performance on different vision and language tasks than previous prompt tuning methods on CLIP. On image classification, we achieve 3.55% average relative improvement on unseen classes across seven datasets; on image-text retrieval and visual question answering, we gain up to 4.09% and 25.08% relative improvements across three few-shot scenarios on unseen test sets respectively.</abstract>
      <url hash="6bcb8ed5">2022.emnlp-main.224</url>
      <bibkey>he-etal-2022-cpl</bibkey>
    </paper>
    <paper id="225">
      <title>Red Teaming Language Models with Language Models</title>
      <author><first>Ethan</first><last>Perez</last></author>
      <author><first>Saffron</first><last>Huang</last></author>
      <author><first>Francis</first><last>Song</last></author>
      <author><first>Trevor</first><last>Cai</last></author>
      <author><first>Roman</first><last>Ring</last></author>
      <author><first>John</first><last>Aslanides</last></author>
      <author><first>Amelia</first><last>Glaese</last></author>
      <author><first>Nat</first><last>McAleese</last></author>
      <author><first>Geoffrey</first><last>Irving</last></author>
      <pages>3419-3448</pages>
      <abstract>Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases (“red teaming”) using another LM. We evaluate the target LM’s replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot’s own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.</abstract>
      <url hash="f5e90916">2022.emnlp-main.225</url>
      <bibkey>perez-etal-2022-red</bibkey>
    </paper>
    <paper id="226">
      <title><fixed-case>C</fixed-case>ap<fixed-case>O</fixed-case>n<fixed-case>I</fixed-case>mage: Context-driven Dense-Captioning on Image</title>
      <author><first>Yiqi</first><last>Gao</last></author>
      <author><first>Xinglin</first><last>Hou</last></author>
      <author><first>Yuanmeng</first><last>Zhang</last></author>
      <author><first>Tiezheng</first><last>Ge</last></author>
      <author><first>Yuning</first><last>Jiang</last></author>
      <author><first>Peng</first><last>Wang</last></author>
      <pages>3449-3465</pages>
      <abstract>Existing image captioning systems are dedicated to generating narrative captions for images, which are spatially detached from theimage in presentation. However, texts can also be used as decorations on the image to highlight the key points and increase theattractiveness of images. In this work, we introduce a new taskcalled captioning on image (CapOnImage), which aims to generatedense captions at different locations of the image based on contextual information. To fully exploit the surrounding visual context togenerate the most suitable caption for each location, we propose amulti-modal pre-training model with multi-level pre-training tasksthat progressively learn the correspondence between texts and image locations from easy to difficult. Since the model may generateredundant captions for nearby locations, we further enhance thelocation embedding with neighbor locations as context. For thisnew task, we also introduce a large-scale benchmark called CapOnImage2M, which contains 2.1 million product images, each with anaverage of 4.8 spatially localized captions. Compared with other image captioning model variants, our model achieves the best resultsin both captioning accuracy and diversity aspects.</abstract>
      <url hash="131a513d">2022.emnlp-main.226</url>
      <bibkey>gao-etal-2022-caponimage</bibkey>
    </paper>
    <paper id="227">
      <title><fixed-case>S</fixed-case>pan<fixed-case>P</fixed-case>roto: A Two-stage Span-based Prototypical Network for Few-shot Named Entity Recognition</title>
      <author><first>Jianing</first><last>Wang</last></author>
      <author><first>Chengyu</first><last>Wang</last></author>
      <author><first>Chuanqi</first><last>Tan</last></author>
      <author><first>Minghui</first><last>Qiu</last></author>
      <author><first>Songfang</first><last>Huang</last></author>
      <author><first>Jun</first><last>Huang</last></author>
      <author><first>Ming</first><last>Gao</last></author>
      <pages>3466-3476</pages>
      <abstract>Few-shot Named Entity Recognition (NER) aims to identify named entities with very little annotated data. Previous methods solve this problem based on token-wise classification, which ignores the information of entity boundaries, and inevitably the performance is affected by the massive non-entity tokens. To this end, we propose a seminal span-based prototypical network (SpanProto) that tackles few-shot NER via a two-stage approach, including span extraction and mention classification. In the span extraction stage, we transform the sequential tags into a global boundary matrix, enabling the model to focus on the explicit boundary information. For mention classification, we leverage prototypical learning to capture the semantic representations for each labeled span and make the model better adapt to novel-class entities. To further improve the model performance, we split out the false positives generated by the span extractor but not labeled in the current episode set, and then present a margin-based loss to separate them from each prototype region. Experiments over multiple benchmarks demonstrate that our model outperforms strong baselines by a large margin.</abstract>
      <url hash="4c90dff2">2022.emnlp-main.227</url>
      <bibkey>wang-etal-2022-spanproto</bibkey>
    </paper>
    <paper id="228">
      <title>Discovering Differences in the Representation of People using Contextualized Semantic Axes</title>
      <author><first>Li</first><last>Lucy</last></author>
      <author><first>Divya</first><last>Tadimeti</last></author>
      <author><first>David</first><last>Bamman</last></author>
      <pages>3477-3494</pages>
      <abstract>A common paradigm for identifying semantic differences across social and temporal contexts is the use of static word embeddings and their distances. In particular, past work has compared embeddings against “semantic axes” that represent two opposing concepts. We extend this paradigm to BERT embeddings, and construct contextualized axes that mitigate the pitfall where antonyms have neighboring representations. We validate and demonstrate these axes on two people-centric datasets: occupations from Wikipedia, and multi-platform discussions in extremist, men’s communities over fourteen years. In both studies, contextualized semantic axes can characterize differences among instances of the same word type. In the latter study, we show that references to women and the contexts around them have become more detestable over time.</abstract>
      <url hash="fc95dc5f">2022.emnlp-main.228</url>
      <bibkey>lucy-etal-2022-discovering</bibkey>
    </paper>
    <paper id="229">
      <title>Generating Literal and Implied Subquestions to Fact-check Complex Claims</title>
      <author><first>Jifan</first><last>Chen</last></author>
      <author><first>Aniruddh</first><last>Sriram</last></author>
      <author><first>Eunsol</first><last>Choi</last></author>
      <author><first>Greg</first><last>Durrett</last></author>
      <pages>3495-3516</pages>
      <abstract>Verifying political claims is a challenging task, as politicians can use various tactics to subtly misrepresent the facts for their agenda. Existing automatic fact-checking systems fall short here, and their predictions like “half-true” are not very useful in isolation, since it is unclear which parts of a claim are true and which are not. In this work, we focus on decomposing a complex claim into a comprehensive set of yes-no subquestions whose answers influence the veracity of the claim. We present CLAIMDECOMP, a dataset of decompositions for over 1000 claims. Given a claim and its verification paragraph written by fact-checkers, our trained annotators write subquestions covering both explicit propositions of the original claim and its implicit facets, such as asking about additional political context that changes our view of the claim’s veracity. We study whether state-of-the-art models can generate such subquestions, showing that these models generate reasonable questions to ask, but predicting the comprehensive set of subquestions from the original claim without evidence remains challenging. We further show that these subquestions can help identify relevant evidence to fact-check the full claim and derive the veracity through their answers, suggesting that they can be useful pieces of a fact-checking pipeline.</abstract>
      <url hash="282f386b">2022.emnlp-main.229</url>
      <bibkey>chen-etal-2022-generating</bibkey>
    </paper>
    <paper id="230">
      <title>Machine Translation Robustness to Natural Asemantic Variation</title>
      <author><first>Jacob</first><last>Bremerman</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <author><first>Jonathan</first><last>May</last></author>
      <pages>3517-3532</pages>
      <abstract>Current Machine Translation (MT) models still struggle with more challenging input, such as noisy data and tail-end words and phrases. Several works have addressed this robustness issue by identifying specific categories of noise and variation then tuning models to perform better on them. An important yet under-studied category involves minor variations in nuance (non-typos) that preserve meaning w.r.t. the target language. We introduce and formalize this category as Natural Asemantic Variation (NAV) and investigate it in the context of MT robustness. We find that existing MT models fail when presented with NAV data, but we demonstrate strategies to improve performance on NAV by fine-tuning them with human-generated variations. We also show that NAV robustness can be transferred across languages and find that synthetic perturbations can achieve some but not all of the benefits of organic NAV data.</abstract>
      <url hash="cc29c366">2022.emnlp-main.230</url>
      <bibkey>bremerman-etal-2022-machine</bibkey>
    </paper>
    <paper id="231">
      <title>Natural Language to Code Translation with Execution</title>
      <author><first>Freda</first><last>Shi</last></author>
      <author><first>Daniel</first><last>Fried</last></author>
      <author><first>Marjan</first><last>Ghazvininejad</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <author><first>Sida I.</first><last>Wang</last></author>
      <pages>3533-3546</pages>
      <abstract>Generative models of code, pretrained on large corpora of programs, have shown great success in translating natural language to code (Chen et al., 2021; Austin et al., 2021; Li et al., 2022, inter alia). While these models do not explicitly incorporate program semantics (i.e., execution results) during training, they are able to generate correct solutions for many problems. However, choosing a single correct program from a generated set for each problem remains challenging. In this work, we introduce execution result–based minimum Bayes risk decoding (MBR-EXEC) for program selection and show that it improves the few-shot performance of pretrained code models on natural-language-to-code tasks. We select output programs from a generated candidate set by marginalizing over program implementations that share the same semantics. Because exact equivalence is intractable, we execute each program on a small number of test inputs to approximate semantic equivalence. Across datasets, execution or simulated execution significantly outperforms the methods that do not involve program semantics. We find that MBR-EXEC consistently improves over all execution-unaware selection methods, suggesting it as an effective approach for natural language to code translation.</abstract>
      <url hash="9f6f76e2">2022.emnlp-main.231</url>
      <bibkey>shi-etal-2022-natural</bibkey>
    </paper>
    <paper id="232">
      <title>Life is a Circus and We are the Clowns: Automatically Finding Analogies between Situations and Processes</title>
      <author><first>Oren</first><last>Sultan</last></author>
      <author><first>Dafna</first><last>Shahaf</last></author>
      <pages>3547-3562</pages>
      <abstract>Analogy-making gives rise to reasoning, abstraction, flexible categorization and counterfactual inference – abilities lacking in even the best AI systems today. Much research has suggested that analogies are key to non-brittle systems that can adapt to new domains. Despite their importance, analogies received little attention in the NLP community, with most research focusing on simple word analogies. Work that tackled more complex analogies relied heavily on manually constructed, hard-to-scale input representations.In this work, we explore a more realistic, challenging setup: our input is a pair of natural language procedural texts, describing a situation or a process (e.g., how the heart works/how a pump works). Our goal is to automatically extract entities and their relations from the text and find a mapping between the different domains based on relational similarity (e.g., blood is mapped to water). We develop an interpretable, scalable algorithm and demonstrate that it identifies the correct mappings 87% of the time for procedural texts and 94% for stories from cognitive-psychology literature. We show it can extract analogies from a large dataset of procedural texts, achieving 79% precision (analogy prevalence in data: 3%). Lastly, we demonstrate that our algorithm is robust to paraphrasing the input texts</abstract>
      <url hash="6722e508">2022.emnlp-main.232</url>
      <bibkey>sultan-shahaf-2022-life</bibkey>
    </paper>
    <paper id="233">
      <title>Language Contamination Helps Explains the Cross-lingual Capabilities of <fixed-case>E</fixed-case>nglish Pretrained Models</title>
      <author><first>Terra</first><last>Blevins</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <pages>3563-3574</pages>
      <abstract>English pretrained language models, which make up the backbone of many modern NLP systems, require huge amounts of unlabeled training data. These models are generally presented as being trained only on English text but have been found to transfer surprisingly well to other languages. We investigate this phenomenon and find that common English pretraining corpora actually contain significant amounts of non-English text: even when less than 1% of data is not English (well within the error rate of strong language classifiers), this leads to hundreds of millions of foreign language tokens in large-scale datasets. We then demonstrate that even these small percentages of non-English data facilitate cross-lingual transfer for models trained on them, with target language performance strongly correlated to the amount of in-language data seen during pretraining. In light of these findings, we argue that no model is truly monolingual when pretrained at scale, which should be considered when evaluating cross-lingual transfer.</abstract>
      <url hash="a31aec98">2022.emnlp-main.233</url>
      <bibkey>blevins-zettlemoyer-2022-language</bibkey>
    </paper>
    <paper id="234">
      <title>Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models</title>
      <author><first>Terra</first><last>Blevins</last></author>
      <author><first>Hila</first><last>Gonen</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <pages>3575-3590</pages>
      <abstract>The emergent cross-lingual transfer seen in multilingual pretrained models has sparked significant interest in studying their behavior. However, because these analyses have focused on fully trained multilingual models, little is known about the dynamics of the multilingual pretraining process. We investigate when these models acquire their in-language and cross-lingual abilities by probing checkpoints taken from throughout XLM-R pretraining, using a suite of linguistic tasks. Our analysis shows that the model achieves high in-language performance early on, with lower-level linguistic skills acquired before more complex ones. In contrast, the point in pretraining when the model learns to transfer cross-lingually differs across language pairs. Interestingly, we also observe that, across many languages and tasks, the final model layer exhibits significant performance degradation over time, while linguistic knowledge propagates to lower layers of the network. Taken together, these insights highlight the complexity of multilingual pretraining and the resulting varied behavior for different languages over time.</abstract>
      <url hash="c7f1fc93">2022.emnlp-main.234</url>
      <bibkey>blevins-etal-2022-analyzing</bibkey>
    </paper>
    <paper id="235">
      <title>Neural Machine Translation with Contrastive Translation Memories</title>
      <author><first>Xin</first><last>Cheng</last></author>
      <author><first>Shen</first><last>Gao</last></author>
      <author><first>Lemao</first><last>Liu</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <author><first>Rui</first><last>Yan</last></author>
      <pages>3591-3601</pages>
      <abstract>Retrieval-augmented Neural Machine Translation models have been successful in many translation scenarios. Different from previous works that make use of mutually similar but redundant translation memories (TMs), we propose a new retrieval-augmented NMT to model contrastively retrieved translation memories that are holistically similar to the source sentence while individually contrastive to each other providing maximal information gain in three phases. First, in TM retrieval phase, we adopt contrastive retrieval algorithm to avoid redundancy and uninformativeness of similar translation pieces. Second, in memory encoding stage, given a set of TMs we propose a novel Hierarchical Group Attention module to gather both local context of each TM and global context of the whole TM set. Finally, in training phase, a Multi-TM contrastive learning objective is introduced to learn salient feature of each TM with respect to target sentence. Experimental results show that our framework obtains substantial improvements over strong baselines in the benchmark dataset.</abstract>
      <url hash="a9363bdd">2022.emnlp-main.235</url>
      <bibkey>cheng-etal-2022-neural</bibkey>
    </paper>
    <paper id="236">
      <title>Distilling Causal Effect from Miscellaneous Other-Class for Continual Named Entity Recognition</title>
      <author><first>Junhao</first><last>Zheng</last></author>
      <author><first>Zhanxian</first><last>Liang</last></author>
      <author><first>Haibin</first><last>Chen</last></author>
      <author><first>Qianli</first><last>Ma</last></author>
      <pages>3602-3615</pages>
      <abstract>Continual Learning for Named Entity Recognition (CL-NER) aims to learn a growing number of entity types over time from a stream of data. However, simply learning Other-Class in the same way as new entity types amplifies the catastrophic forgetting and leads to a substantial performance drop. The main cause behind this is that Other-Class samples usually contain old entity types, and the old knowledge in these Other-Class samples is not preserved properly. Thanks to the causal inference, we identify that the forgetting is caused by the missing causal effect from the old data.To this end, we propose a unified causal framework to retrieve the causality from both new entity types and Other-Class.Furthermore, we apply curriculum learning to mitigate the impact of label noise and introduce a self-adaptive weight for balancing the causal effects between new entity types and Other-Class. Experimental results on three benchmark datasets show that our method outperforms the state-of-the-art method by a large margin. Moreover, our method can be combined with the existing state-of-the-art methods to improve the performance in CL-NER.</abstract>
      <url hash="9a68e66c">2022.emnlp-main.236</url>
      <bibkey>zheng-etal-2022-distilling</bibkey>
    </paper>
    <paper id="237">
      <title>Exploring the Secrets Behind the Learning Difficulty of Meaning Representations for Semantic Parsing</title>
      <author><first>Zhenwen</first><last>Li</last></author>
      <author><first>Jiaqi</first><last>Guo</last></author>
      <author><first>Qian</first><last>Liu</last></author>
      <author><first>Jian-Guang</first><last>Lou</last></author>
      <author><first>Tao</first><last>Xie</last></author>
      <pages>3616-3625</pages>
      <abstract>Previous research has shown that the design of Meaning Representation (MR) greatly influences the final model performance of a neural semantic parser. Therefore, designing a good MR is a long-term goal for semantic parsing. However, it is still an art as there is no quantitative indicator that can tell us which MR among a set of candidates may have the best final model performance. In practice, in order toselect an MR design, researchers often have to go through the whole training-testing process for all design candidates, and the process often costs a lot. In this paper, we propose a data-aware metric called ISS (denoting incremental structural stability) of MRs, and demonstrate that ISS is highly correlated with the final performance. The finding shows that ISS can be used as an indicator for MR design to avoid the costly training-testing process.</abstract>
      <url hash="c0c3c0f7">2022.emnlp-main.237</url>
      <bibkey>li-etal-2022-exploring-secrets</bibkey>
    </paper>
    <paper id="238">
      <title>That’s the Wrong Lung! Evaluating and Improving the Interpretability of Unsupervised Multimodal Encoders for Medical Data</title>
      <author><first>Jered</first><last>McInerney</last></author>
      <author><first>Geoffrey</first><last>Young</last></author>
      <author><first>Jan-Willem</first><last>van de Meent</last></author>
      <author><first>Byron</first><last>Wallace</last></author>
      <pages>3626-3648</pages>
      <abstract>Pretraining multimodal models on Electronic Health Records (EHRs) provides a means of learning representations that can transfer to downstream tasks with minimal supervision. Recent multimodal models induce soft local alignments between image regions and sentences. This is of particular interest in the medical domain, where alignments might highlight regions in an image relevant to specific phenomena described in free-text. While past work has suggested that attention “heatmaps” can be interpreted in this manner, there has been little evaluation of such alignments. We compare alignments from a state-of-the-art multimodal (image and text) model for EHR with human annotations that link image regions to sentences. Our main finding is that the text has an often weak or unintuitive influence on attention; alignments do not consistently reflect basic anatomical information. Moreover, synthetic modifications — such as substituting “left” for “right” — do not substantially influence highlights. Simple techniques such as allowing the model to opt out of attending to the image and few-shot finetuning show promise in terms of their ability to improve alignments with very little or no supervision. We make our code and checkpoints open-source.</abstract>
      <url hash="a54aa702">2022.emnlp-main.238</url>
      <bibkey>mcinerney-etal-2022-thats</bibkey>
    </paper>
    <paper id="239">
      <title>Unsupervised Tokenization Learning</title>
      <author><first>Anton</first><last>Kolonin</last></author>
      <author><first>Vignav</first><last>Ramesh</last></author>
      <pages>3649-3664</pages>
      <abstract>In the presented study, we discover that the so-called “transition freedom” metric appears superior for unsupervised tokenization purposes in comparison to statistical metrics such as mutual information and conditional probability, providing F-measure scores in range from 0.71 to 1.0 across explored multilingual corpora. We find that different languages require different offshoots of that metric (such as derivative, variance, and “peak values”) for successful tokenization. Larger training corpora do not necessarily result in better tokenization quality, while compressing the models by eliminating statistically weak evidence tends to improve performance. The proposed unsupervised tokenization technique provides quality better than or comparable to lexicon-based ones, depending on the language.</abstract>
      <url hash="63cde39a">2022.emnlp-main.239</url>
      <bibkey>kolonin-ramesh-2022-unsupervised</bibkey>
    </paper>
    <paper id="240">
      <title>A Template-based Method for Constrained Neural Machine Translation</title>
      <author><first>Shuo</first><last>Wang</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Zhixing</first><last>Tan</last></author>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last></author>
      <pages>3665-3679</pages>
      <abstract>Machine translation systems are expected to cope with various types of constraints in many practical scenarios. While neural machine translation (NMT) has achieved strong performance in unconstrained cases, it is non-trivial to impose pre-specified constraints into the translation process of NMT models. Although many approaches have been proposed to address this issue, most existing methods can not satisfy the following three desiderata at the same time: (1) high translation quality, (2) high match accuracy, and (3) low latency. In this work, we propose a template-based method that can yield results with high translation quality and match accuracy and the inference speed of our method is comparable with unconstrained NMT models. Our basic idea is to rearrange the generation of constrained and unconstrained tokens through a template. Our method does not require any changes in the model architecture and the decoding algorithm. Experimental results show that the proposed template-based approach can outperform several representative baselines in both lexically and structurally constrained translation tasks.</abstract>
      <url hash="ab4c275c">2022.emnlp-main.240</url>
      <bibkey>wang-etal-2022-template</bibkey>
    </paper>
    <paper id="241">
      <title><fixed-case>PATS</fixed-case>: Sensitivity-aware Noisy Learning for Pretrained Language Models</title>
      <author><first>Yupeng</first><last>Zhang</last></author>
      <author><first>Hongzhi</first><last>Zhang</last></author>
      <author><first>Sirui</first><last>Wang</last></author>
      <author><first>Wei</first><last>Wu</last></author>
      <author><first>Zhoujun</first><last>Li</last></author>
      <pages>3680-3687</pages>
      <abstract>A wide range of NLP tasks benefit from the fine-tuning of pretrained language models (PLMs). However, a number of redundant parameters which contribute less to the downstream task are observed in a directly fine-tuned model. We consider the gap between pretraining and downstream tasks hinders the training of these redundant parameters, and results in a suboptimal performance of the overall model. In this paper, we present PATS (Perturbation According To Sensitivity), a noisy training mechanism which considers each parameter’s importance in the downstream task to help fine-tune PLMs. The main idea of PATS is to add bigger noise to parameters with lower sensitivity and vice versa, in order to activate more parameters’ contributions to downstream tasks without affecting the sensitive ones much. Extensive experiments conducted on different tasks of the GLUE benchmark show PATS can consistently empower the fine-tuning of different sizes of PLMs, and the parameters in the well-performing models always have more concentrated distributions of sensitivities, which experimentally proves the effectiveness of our method.</abstract>
      <url hash="8273204f">2022.emnlp-main.241</url>
      <bibkey>zhang-etal-2022-pats</bibkey>
    </paper>
    <paper id="242">
      <title>Towards Reinterpreting Neural Topic Models via Composite Activations</title>
      <author><first>Jia Peng</first><last>Lim</last></author>
      <author><first>Hady</first><last>Lauw</last></author>
      <pages>3688-3703</pages>
      <abstract>Most Neural Topic Models (NTM) use a variational auto-encoder framework producing K topics limited to the size of the encoder’s output. These topics are interpreted through the selection of the top activated words via the weights or reconstructed vector of the decoder that are directly connected to each neuron. In this paper, we present a model-free two-stage process to reinterpret NTM and derive further insights on the state of the trained model. Firstly, building on the original information from a trained NTM, we generate a pool of potential candidate “composite topics” by exploiting possible co-occurrences within the original set of topics, which decouples the strict interpretation of topics from the original NTM. This is followed by a combinatorial formulation to select a final set of composite topics, which we evaluate for coherence and diversity on a large external corpus. Lastly, we employ a user study to derive further insights on the reinterpretation process.</abstract>
      <url hash="b40f0bf3">2022.emnlp-main.242</url>
      <bibkey>lim-lauw-2022-towards</bibkey>
    </paper>
    <paper id="243">
      <title>Few-shot Query-Focused Summarization with Prefix-Merging</title>
      <author><first>Ruifeng</first><last>Yuan</last></author>
      <author><first>Zili</first><last>Wang</last></author>
      <author><first>Ziqiang</first><last>Cao</last></author>
      <author><first>Wenjie</first><last>Li</last></author>
      <pages>3704-3714</pages>
      <abstract>Query-focused summarization has been considered as an important extension for text summarization. It aims to generate a concise highlight for a given query. Different from text summarization, query-focused summarization has long been plagued by the problem of lacking high-quality large-scale datasets. In this paper, we investigate the idea that whether we can integrate and transfer the knowledge of text summarization and question answering to assist the few-shot learning in query-focused summarization. Here, we propose prefix-merging, a prefix-based pretraining strategy for few-shot learning in query-focused summarization. Drawn inspiration from prefix-tuning, we are allowed to integrate the task knowledge from text summarization and question answering into a properly designed prefix and apply the merged prefix to query-focused summarization. With only a small amount of trainable parameters, prefix-merging outperforms fine-tuning on query-focused summarization. We further discuss the influence of different prefix designs and propose a visualized explanation for how prefix-merging works.</abstract>
      <url hash="b254e092">2022.emnlp-main.243</url>
      <bibkey>yuan-etal-2022-shot</bibkey>
    </paper>
    <paper id="244">
      <title>Cross-Align: Modeling Deep Cross-lingual Interactions for Word Alignment</title>
      <author><first>Siyu</first><last>Lai</last></author>
      <author><first>Zhen</first><last>Yang</last></author>
      <author><first>Fandong</first><last>Meng</last></author>
      <author><first>Yufeng</first><last>Chen</last></author>
      <author><first>Jinan</first><last>Xu</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>3715-3725</pages>
      <abstract>Word alignment which aims to extract lexicon translation equivalents between source and target sentences, serves as a fundamental tool for natural language processing. Recent studies in this area have yielded substantial improvements by generating alignments from contextualized embeddings of the pre-trained multilingual language models. However, we find that the existing approaches capture few interactions between the input sentence pairs, which degrades the word alignment quality severely, especially for the ambiguous words in the monolingual context. To remedy this problem, we propose Cross-Align to model deep interactions between the input sentence pairs, in which the source and target sentences are encoded separately with the shared self-attention modules in the shallow layers, while cross-lingual interactions are explicitly constructed by the cross-attention modules in the upper layers. Besides, to train our model effectively, we propose a two-stage training framework, where the model is trained with a simple Translation Language Modeling (TLM) objective in the first stage and then finetuned with a self-supervised alignment objective in the second stage. Experiments show that the proposed Cross-Align achieves the state-of-the-art (SOTA) performance on four out of five language pairs.</abstract>
      <url hash="7a861505">2022.emnlp-main.244</url>
      <bibkey>lai-etal-2022-cross</bibkey>
    </paper>
    <paper id="245">
      <title><fixed-case>BERTS</fixed-case>core is Unfair: On Social Bias in Language Model-Based Metrics for Text Generation</title>
      <author><first>Tianxiang</first><last>Sun</last></author>
      <author><first>Junliang</first><last>He</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <pages>3726-3739</pages>
      <abstract>Automatic evaluation metrics are crucial to the development of generative systems. In recent years, pre-trained language model (PLM) based metrics, such as BERTScore, have been commonly adopted in various generation tasks. However, it has been demonstrated that PLMs encode a range of stereotypical societal biases, leading to a concern about the fairness of PLMs as metrics. To that end, this work presents the first systematic study on the social bias in PLM-based metrics. We demonstrate that popular PLM-based metrics exhibit significantly higher social bias than traditional metrics on 6 sensitive attributes, namely race, gender, religion, physical appearance, age, and socioeconomic status. In-depth analysis suggests that choosing paradigms (matching, regression, or generation) of the metric has a greater impact on fairness than choosing PLMs. In addition, we develop debiasing adapters that are injected into PLM layers, mitigating bias in PLM-based metrics while retaining high performance for evaluating text generation.</abstract>
      <url hash="ae2788aa">2022.emnlp-main.245</url>
      <bibkey>sun-etal-2022-bertscore</bibkey>
    </paper>
    <paper id="246">
      <title><fixed-case>HPT</fixed-case>: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification</title>
      <author><first>Zihan</first><last>Wang</last></author>
      <author><first>Peiyi</first><last>Wang</last></author>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Binghuai</first><last>Lin</last></author>
      <author><first>Yunbo</first><last>Cao</last></author>
      <author><first>Zhifang</first><last>Sui</last></author>
      <author><first>Houfeng</first><last>Wang</last></author>
      <pages>3740-3751</pages>
      <abstract>Hierarchical text classification (HTC) is a challenging subtask of multi-label classification due to its complex label hierarchy.Recently, the pretrained language models (PLM)have been widely adopted in HTC through a fine-tuning paradigm. However, in this paradigm, there exists a huge gap between the classification tasks with sophisticated label hierarchy and the masked language model (MLM) pretraining tasks of PLMs and thus the potential of PLMs cannot be fully tapped.To bridge the gap, in this paper, we propose HPT, a Hierarchy-aware Prompt Tuning method to handle HTC from a multi-label MLM perspective.Specifically, we construct a dynamic virtual template and label words that take the form of soft prompts to fuse the label hierarchy knowledge and introduce a zero-bounded multi-label cross-entropy loss to harmonize the objectives of HTC and MLM.Extensive experiments show HPT achieves state-of-the-art performances on 3 popular HTC datasets and is adept at handling the imbalance and low resource situations. Our code is available at https://github.com/wzh9969/HPT.</abstract>
      <url hash="9a77a01d">2022.emnlp-main.246</url>
      <bibkey>wang-etal-2022-hpt</bibkey>
    </paper>
    <paper id="247">
      <title>Not to Overfit or Underfit the Source Domains? An Empirical Study of Domain Generalization in Question Answering</title>
      <author><first>Md Arafat</first><last>Sultan</last></author>
      <author><first>Avi</first><last>Sil</last></author>
      <author><first>Radu</first><last>Florian</last></author>
      <pages>3752-3761</pages>
      <abstract>Machine learning models are prone to overfitting their training (source) domains, which is commonly believed to be the reason why they falter in novel target domains. Here we examine the contrasting view that multi-source domain generalization (DG) is first and foremost a problem of mitigating source domain underfitting: models not adequately learning the signal already present in their multi-domain training data. Experiments on a reading comprehension DG benchmark show that as a model learns its source domains better—using familiar methods such as knowledge distillation (KD) from a bigger model—its zero-shot out-of-domain utility improves at an even faster pace. Improved source domain learning also demonstrates superior out-of-domain generalization over three popular existing DG approaches that aim to limit overfitting. Our implementation of KD-based domain generalization is available via PrimeQA at: https://ibm.biz/domain-generalization-with-kd.</abstract>
      <url hash="eb6bc124">2022.emnlp-main.247</url>
      <bibkey>sultan-etal-2022-overfit</bibkey>
    </paper>
    <paper id="248">
      <title>Neural Theory-of-Mind? On the Limits of Social Intelligence in Large <fixed-case>LM</fixed-case>s</title>
      <author><first>Maarten</first><last>Sap</last></author>
      <author><first>Ronan</first><last>Le Bras</last></author>
      <author><first>Daniel</first><last>Fried</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>3762-3780</pages>
      <abstract>Social intelligence and Theory of Mind (TOM), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allows humans to effectively navigate and understand everyday social interactions. As NLP systems are used in increasingly complex social situations, their ability to grasp social dynamics becomes crucial.In this work, we examine the open question of social intelligence and Theory of Mind in modern NLP systems from an empirical and theorybased perspective. We show that one of today’s largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et al., 2019), which measure models’ ability to understand intents and reactions of participants of social interactions, and ToMi (Le, Boureau, and Nickel, 2019), which measures whether models can infer mental states and realities of participants of situations.Our results show that models struggle substantially at these Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on SocialIQa and ToMi, respectively. To conclude, we draw on theories from pragmatics to contextualize this shortcoming of large language models, by examining the limitations stemming from their data, neural architecture, and training paradigms. Challenging the prevalent narrative that only scale is needed, we posit that person-centric NLP approaches might be more effective towards neural Theory of Mind.</abstract>
      <url hash="908fcc1a">2022.emnlp-main.248</url>
      <bibkey>sap-etal-2022-neural</bibkey>
    </paper>
    <paper id="249">
      <title>Improving Passage Retrieval with Zero-Shot Question Generation</title>
      <author><first>Devendra</first><last>Sachan</last></author>
      <author><first>Mike</first><last>Lewis</last></author>
      <author><first>Mandar</first><last>Joshi</last></author>
      <author><first>Armen</first><last>Aghajanyan</last></author>
      <author><first>Wen-tau</first><last>Yih</last></author>
      <author><first>Joelle</first><last>Pineau</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <pages>3781-3797</pages>
      <abstract>We propose a simple and effective re-ranking method for improving passage retrieval in open question answering. The re-ranker re-scores retrieved passages with a zero-shot question generation model, which uses a pre-trained language model to compute the probability of the input question conditioned on a retrieved passage. This approach can be applied on top of any retrieval method (e.g. neural or keyword-based), does not require any domain- or task-specific training (and therefore is expected to generalize better to data distribution shifts), and provides rich cross-attention between query and passage (i.e. it must explain every token in the question). When evaluated on a number of open-domain retrieval datasets, our re-ranker improves strong unsupervised retrieval models by 6%-18% absolute and strong supervised models by up to 12% in terms of top-20 passage retrieval accuracy. We also obtain new state-of-the-art results on full open-domain question answering by simply adding the new re-ranker to existing models with no further changes.</abstract>
      <url hash="0b4bb457">2022.emnlp-main.249</url>
      <bibkey>sachan-etal-2022-improving</bibkey>
    </paper>
    <paper id="250">
      <title>Summarizing Community-based Question-Answer Pairs</title>
      <author><first>Ting-Yao</first><last>Hsu</last></author>
      <author><first>Yoshi</first><last>Suhara</last></author>
      <author><first>Xiaolan</first><last>Wang</last></author>
      <pages>3798-3808</pages>
      <abstract>Community-based Question Answering (CQA), which allows users to acquire their desired information, has increasingly become an essential component of online services in various domains such as E-commerce, travel, and dining. However, an overwhelming number of CQA pairs makes it difficult for users without particular intent to find useful information spread over CQA pairs. To help users quickly digest the key information, we propose the novel CQA summarization task that aims to create a concise summary from CQA pairs. To this end, we first design a multi-stage data annotation process and create a benchmark dataset, COQASUM, based on the Amazon QA corpus. We then compare a collection of extractive and abstractive summarization methods and establish a strong baseline approach DedupLED for the CQA summarization task. Our experiment further confirms two key challenges, sentence-type transfer and deduplication removal, towards the CQA summarization task. Our data and code are publicly available.</abstract>
      <url hash="ab1a83fc">2022.emnlp-main.250</url>
      <bibkey>hsu-etal-2022-summarizing</bibkey>
    </paper>
    <paper id="251">
      <title>Logical Reasoning with Span-Level Predictions for Interpretable and Robust <fixed-case>NLI</fixed-case> Models</title>
      <author><first>Joe</first><last>Stacey</last></author>
      <author><first>Pasquale</first><last>Minervini</last></author>
      <author><first>Haim</first><last>Dubossarsky</last></author>
      <author><first>Marek</first><last>Rei</last></author>
      <pages>3809-3823</pages>
      <abstract>Current Natural Language Inference (NLI) models achieve impressive results, sometimes outperforming humans when evaluating on in-distribution test sets. However, as these models are known to learn from annotation artefacts and dataset biases, it is unclear to what extent the models are learning the task of NLI instead of learning from shallow heuristics in their training data.We address this issue by introducing a logical reasoning framework for NLI, creating highly transparent model decisions that are based on logical rules. Unlike prior work, we show that improved interpretability can be achieved without decreasing the predictive accuracy. We almost fully retain performance on SNLI, while also identifying the exact hypothesis spans that are responsible for each model prediction.Using the e-SNLI human explanations, we verify that our model makes sensible decisions at a span level, despite not using any span labels during training. We can further improve model performance and the span-level decisions by using the e-SNLI explanations during training. Finally, our model is more robust in a reduced data setting. When training with only 1,000 examples, out-of-distribution performance improves on the MNLI matched and mismatched validation sets by 13% and 16% relative to the baseline. Training with fewer observations yields further improvements, both in-distribution and out-of-distribution.</abstract>
      <url hash="fbd76e63">2022.emnlp-main.251</url>
      <bibkey>stacey-etal-2022-logical</bibkey>
    </paper>
    <paper id="252">
      <title>How to disagree well: Investigating the dispute tactics used on <fixed-case>W</fixed-case>ikipedia</title>
      <author><first>Christine</first><last>De Kock</last></author>
      <author><first>Andreas</first><last>Vlachos</last></author>
      <pages>3824-3837</pages>
      <abstract>Disagreements are frequently studied from the perspective of either detecting toxicity or analysing argument structure. We propose a framework of dispute tactics which unifies these two perspectives, as well as other dialogue acts which play a role in resolving disputes, such as asking questions and providing clarification. This framework includes a preferential ordering among rebuttal-type tactics, ranging from ad hominem attacks to refuting the central argument. Using this framework, we annotate 213 disagreements (3,865 utterances) from Wikipedia Talk pages. This allows us to investigate research questions around the tactics used in disagreements; for instance, we provide empirical validation of the approach to disagreement recommended by Wikipedia. We develop models for multilabel prediction of dispute tactics in an utterance, achieving the best performance with a transformer-based label powerset model. Adding an auxiliary task to incorporate the ordering of rebuttal tactics further yields a statistically significant increase. Finally, we show that these annotations can be used to provide useful additional signals to improve performance on the task of predicting escalation.</abstract>
      <url hash="dd8122cd">2022.emnlp-main.252</url>
      <bibkey>de-kock-vlachos-2022-disagree</bibkey>
    </paper>
    <paper id="253">
      <title>Chapter Ordering in Novels</title>
      <author><first>Allen</first><last>Kim</last></author>
      <author><first>Steve</first><last>Skiena</last></author>
      <pages>3838-3848</pages>
      <abstract>Understanding narrative flow and text coherence in long-form documents (novels) remains an open problem in NLP.To gain insight, we explore the task of chapter ordering, reconstructing the original order of chapters in novel given a random permutation of the text. This can be seen as extending the well-known sentence ordering task to vastly larger documents: our task deals with over 9,000 novels with an average of twenty chapters each, versus standard sentence ordering datasets averaging only 5-8 sentences. We formulate the task of reconstructing order as a constraint solving problem, using minimum feedback arc set and traveling salesman problem optimization criteria, where the weights of the graph are generated based on models for character occurrences and chapter boundary detection, using relational chapter scores derived from RoBERTa. Our best methods yield a Spearman correlation of 0.59 on this novel and challenging task, substantially above baseline.</abstract>
      <url hash="b53f0f31">2022.emnlp-main.253</url>
      <bibkey>kim-skiena-2022-chapter</bibkey>
    </paper>
    <paper id="254">
      <title>Open-ended Knowledge Tracing for Computer Science Education</title>
      <author><first>Naiming</first><last>Liu</last></author>
      <author><first>Zichao</first><last>Wang</last></author>
      <author><first>Richard</first><last>Baraniuk</last></author>
      <author><first>Andrew</first><last>Lan</last></author>
      <pages>3849-3862</pages>
      <abstract>In educational applications, knowledge tracing refers to the problem of estimating students’ time-varying concept/skill mastery level from their past responses to questions and predicting their future performance.One key limitation of most existing knowledge tracing methods is that they treat student responses to questions as binary-valued, i.e., whether they are correct or incorrect. Response correctness analysis/prediction is straightforward, but it ignores important information regarding mastery, especially for open-ended questions.In contrast, exact student responses can provide much more information.In this paper, we conduct the first exploration int open-ended knowledge tracing (OKT) by studying the new task of predicting students’ exact open-ended responses to questions.Our work is grounded in the domain of computer science education with programming questions. We develop an initial solution to the OKT problem, a student knowledge-guided code generation approach, that combines program synthesis methods using language models with student knowledge tracing methods. We also conduct a series of quantitative and qualitative experiments on a real-world student code dataset to validate and demonstrate the promise of OKT.</abstract>
      <url hash="4c73486c">2022.emnlp-main.254</url>
      <bibkey>liu-etal-2022-open</bibkey>
    </paper>
    <paper id="255">
      <title>Logical Neural Networks for Knowledge Base Completion with Embeddings &amp; Rules</title>
      <author><first>Prithviraj</first><last>Sen</last></author>
      <author><first>Breno William</first><last>Carvalho</last></author>
      <author><first>Ibrahim</first><last>Abdelaziz</last></author>
      <author><first>Pavan</first><last>Kapanipathi</last></author>
      <author><first>Salim</first><last>Roukos</last></author>
      <author><first>Alexander</first><last>Gray</last></author>
      <pages>3863-3875</pages>
      <abstract>Knowledge base completion (KBC) has benefitted greatly by learning explainable rules in an human-interpretable dialect such as first-order logic. Rule-based KBC has so far, mainly focussed on learning one of two types of rules: conjunction-of-disjunctions and disjunction-of-conjunctions. We qualitatively show, via examples, that one of these has an advantage over the other when it comes to achieving high quality KBC. To the best of our knowledge, we are the first to propose learning both kinds of rules within a common framework. To this end, we propose to utilize logical neural networks (LNN), a powerful neuro-symbolic AI framework that can express both kinds of rules and learn these end-to-end using gradient-based optimization. Our in-depth experiments show that our LNN-based approach to learning rules for KBC leads to roughly 10% relative improvements, if not more, over SotA rule-based KBC methods. Moreover, by showing how to combine our proposed methods with knowledge graph embeddings we further achieve an additional 7.5% relative improvement.</abstract>
      <url hash="f33a4999">2022.emnlp-main.255</url>
      <bibkey>sen-etal-2022-logical</bibkey>
    </paper>
    <paper id="256">
      <title><fixed-case>M</fixed-case>ed<fixed-case>CLIP</fixed-case>: Contrastive Learning from Unpaired Medical Images and Text</title>
      <author><first>Zifeng</first><last>Wang</last></author>
      <author><first>Zhenbang</first><last>Wu</last></author>
      <author><first>Dinesh</first><last>Agarwal</last></author>
      <author><first>Jimeng</first><last>Sun</last></author>
      <pages>3876-3887</pages>
      <abstract>Existing vision-text contrastive learning like CLIP aims to match the paired image and caption embeddings while pushing others apart, which improves representation transferability and supports zero-shot prediction. However, medical image-text datasets are orders of magnitude below the general images and captions from the internet. Moreover, previous methods encounter many false negatives, i.e., images and reports from separate patients probably carry the same semantics but are wrongly treated as negatives. In this paper, we decouple images and texts for multimodal contrastive learning, thus scaling the usable training data in a combinatorial magnitude with low cost. We also propose to replace the InfoNCE loss with semantic matching loss based on medical knowledge to eliminate false negatives in contrastive learning. We prove that MedCLIP is a simple yet effective framework: it outperforms state-of-the-art methods on zero-shot prediction, supervised classification, and image-text retrieval. Surprisingly, we observe that with only 20K pre-training data, MedCLIP wins over the state-of-the-art method (using 200K data). The code is available at https://github.com/RyanWangZf/MedCLIP.</abstract>
      <url hash="b2fcd91e">2022.emnlp-main.256</url>
      <bibkey>wang-etal-2022-medclip</bibkey>
    </paper>
    <paper id="257">
      <title><fixed-case>GA</fixed-case>-<fixed-case>SAM</fixed-case>: Gradient-Strength based Adaptive Sharpness-Aware Minimization for Improved Generalization</title>
      <author><first>Zhiyuan</first><last>Zhang</last></author>
      <author><first>Ruixuan</first><last>Luo</last></author>
      <author><first>Qi</first><last>Su</last></author>
      <author><first>Xu</first><last>Sun</last></author>
      <pages>3888-3903</pages>
      <abstract>Recently, Sharpness-Aware Minimization (SAM) algorithm has shown state-of-the-art generalization abilities in vision tasks. It demonstrates that flat minima tend to imply better generalization abilities. However, it has some difficulty implying SAM to some natural language tasks, especially to models with drastic gradient changes, such as RNNs. In this work, we analyze the relation between the flatness of the local minimum and its generalization ability from a novel and straightforward theoretical perspective. We propose that the shift of the training and test distributions can be equivalently seen as a virtual parameter corruption or perturbation, which can explain why flat minima that are robust against parameter corruptions or perturbations have better generalization performances. On its basis, we propose a Gradient-Strength based Adaptive Sharpness-Aware Minimization (GA-SAM) algorithm to help to learn algorithms find flat minima that generalize better. Results in various language benchmarks validate the effectiveness of the proposed GA-SAM algorithm on natural language tasks.</abstract>
      <url hash="e9cdae78">2022.emnlp-main.257</url>
      <bibkey>zhang-etal-2022-ga</bibkey>
    </paper>
    <paper id="258">
      <title>Sparse Teachers Can Be Dense with Knowledge</title>
      <author><first>Yi</first><last>Yang</last></author>
      <author><first>Chen</first><last>Zhang</last></author>
      <author><first>Dawei</first><last>Song</last></author>
      <pages>3904-3915</pages>
      <abstract>Recent advances in distilling pretrained language models have discovered that, besides the expressiveness of knowledge, the student-friendliness should be taken into consideration to realize a truly knowledgeable teacher. Based on a pilot study, we find that over-parameterized teachers can produce expressive yet student-unfriendly knowledge and are thus limited in overall knowledgeableness. To remove the parameters that result in student-unfriendliness, we propose a sparse teacher trick under the guidance of an overall knowledgeable score for each teacher parameter. The knowledgeable score is essentially an interpolation of the expressiveness and student-friendliness scores. The aim is to ensure that the expressive parameters are retained while the student-unfriendly ones are removed. Extensive experiments on the GLUE benchmark show that the proposed sparse teachers can be dense with knowledge and lead to students with compelling performance in comparison with a series of competitive baselines.</abstract>
      <url hash="4c4f6659">2022.emnlp-main.258</url>
      <bibkey>yang-etal-2022-sparse</bibkey>
    </paper>
    <paper id="259">
      <title><fixed-case>BBT</fixed-case>v2: Towards a Gradient-Free Future with Large Language Models</title>
      <author><first>Tianxiang</first><last>Sun</last></author>
      <author><first>Zhengfu</first><last>He</last></author>
      <author><first>Hong</first><last>Qian</last></author>
      <author><first>Yunhua</first><last>Zhou</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <pages>3916-3930</pages>
      <abstract>Most downstream adaptation methods tune all or part of the parameters of pre-trained models (PTMs) through gradient descent, where the tuning cost increases linearly with the growth of the model size.By contrast, gradient-free methods only require the forward computation of the PTM to tune the prompt, retaining the benefits of efficient tuning and deployment.Though, past work on gradient-free tuning often introduces gradient descent to seek a good initialization of prompt and lacks versatility across tasks and PTMs.In this paper, we present BBTv2, an improved version of Black-Box Tuning, to drive PTMs for few-shot learning.We prepend continuous prompts to every layer of the PTM and propose a divide-and-conquer gradient-free algorithm to optimize the prompts at different layers alternately.Extensive experiments across various tasks and PTMs show that BBTv2 can achieve comparable performance to full model tuning and state-of-the-art parameter-efficient methods (e.g., Adapter, LoRA, BitFit, etc.) under few-shot settings while maintaining much fewer tunable parameters.</abstract>
      <url hash="00666da6">2022.emnlp-main.259</url>
      <bibkey>sun-etal-2022-bbtv2</bibkey>
    </paper>
    <paper id="260">
      <title>Passage-Mask: A Learnable Regularization Strategy for Retriever-Reader Models</title>
      <author><first>Shujian</first><last>Zhang</last></author>
      <author><first>Chengyue</first><last>Gong</last></author>
      <author><first>Xingchao</first><last>Liu</last></author>
      <pages>3931-3943</pages>
      <abstract>Retriever-reader models achieve competitive performance across many different NLP tasks such as open question answering and dialogue conversations. In this work, we notice these models easily overfit the top-rank retrieval passages and standard training fails to reason over the entire retrieval passages. We introduce a learnable passage mask mechanism which desensitizes the impact from the top-rank retrieval passages and prevents the model from overfitting. Controlling the gradient variance with fewer mask candidates and selecting the mask candidates with one-shot bi-level optimization, our learnable regularization strategy enforces the answer generation to focus on the entire retrieval passages. Experiments on different tasks across open question answering, dialogue conversation, and fact verification show that our method consistently outperforms its baselines. Extensive experiments and ablation studies demonstrate that our method can be general, effective, and beneficial for many NLP tasks.</abstract>
      <url hash="b09f521d">2022.emnlp-main.260</url>
      <bibkey>zhang-etal-2022-passage</bibkey>
    </paper>
    <paper id="261">
      <title>Mixed-effects transformers for hierarchical adaptation</title>
      <author><first>Julia</first><last>White</last></author>
      <author><first>Noah</first><last>Goodman</last></author>
      <author><first>Robert</first><last>Hawkins</last></author>
      <pages>3944-3954</pages>
      <abstract>Language differs dramatically from context to context. To some degree, large language models like GPT-3 account for such variation by conditioning on strings of initial input text, or prompts. However, prompting can be ineffective when contexts are sparse, out-of-sample, or extra-textual. In this paper, we introduce the mixed-effects transformer (MET), a novel approach for learning hierarchically-structured prefixes— lightweight modules prepended to an input sequence— to account for structured variation in language use. Specifically, we show how the popular class of mixed-effects regression models may be extended to transformer-based architectures using a regularized prefix-tuning procedure with dropout. We evaluate this approach on several domain-adaptation benchmarks, finding that it learns contextual variation from minimal data while generalizing well to unseen contexts.</abstract>
      <url hash="9ad4b03d">2022.emnlp-main.261</url>
      <bibkey>white-etal-2022-mixed</bibkey>
    </paper>
    <paper id="262">
      <title>On Measuring the Intrinsic Few-Shot Hardness of Datasets</title>
      <author><first>Xinran</first><last>Zhao</last></author>
      <author><first>Shikhar</first><last>Murty</last></author>
      <author><first>Christopher</first><last>Manning</last></author>
      <pages>3955-3963</pages>
      <abstract>While advances in pre-training have led to dramatic improvements in few-shot learning of NLP tasks, there is limited understanding of what drives successful few-shot adaptation in datasets. In particular, given a new dataset and a pre-trained model, what properties of the dataset make it few-shot learnable, and are these properties independent of the specific adaptation techniques used? We consider an extensive set of recent few-shot learning methods and show that their performance across a large number of datasets is highly correlated, showing that few-shot hardness may be intrinsic to datasets, for a given pre-trained model. To estimate intrinsic few-shot hardness, we then propose a simple and lightweight metric called Spread that captures the intuition that few-shot learning is made possible by exploiting feature-space invariances between training and test samples. Our metric better accounts for few-shot hardness compared to existing notions of hardness and is ~8-100x faster to compute.</abstract>
      <url hash="84ea96db">2022.emnlp-main.262</url>
      <bibkey>zhao-etal-2022-measuring</bibkey>
    </paper>
    <paper id="263">
      <title>Group is better than individual: Exploiting Label Topologies and Label Relations for Joint Multiple Intent Detection and Slot Filling</title>
      <author><first>Bowen</first><last>Xing</last></author>
      <author><first>Ivor</first><last>Tsang</last></author>
      <pages>3964-3975</pages>
      <abstract>Recent joint multiple intent detection and slot filling models employ label embeddings to achieve the semantics-label interactions.However, they treat all labels and label embeddings as uncorrelated individuals, ignoring the dependencies among them. Besides, they conduct the decoding for the two tasks independently, without leveraging the correlations between them.Therefore, in this paper, we first construct a Heterogeneous Label Graph (HLG) containing two kinds of topologies: (1) statistical dependencies based on labels’ co-occurrence patterns and hierarchies in slot labels; (2) rich relations among the label nodes.Then we propose a novel model termed ReLa-Net.It can capture beneficial correlations among the labels from HLG.The label correlations are leveraged to enhance semantic-label interactions. Moreover, we also propose the label-aware inter-dependent decoding mechanism to further exploit the label correlations for decoding. Experiment results show that our ReLa-Net significantly outperforms previous models.Remarkably, ReLa-Net surpasses the previous best model by over 20% in terms of overall accuracy on MixATIS dataset.</abstract>
      <url hash="bf25c7ec">2022.emnlp-main.263</url>
      <bibkey>xing-tsang-2022-group</bibkey>
    </paper>
    <paper id="264">
      <title>An Empirical Study on Finding Spans</title>
      <author><first>Weiwei</first><last>Gu</last></author>
      <author><first>Boyuan</first><last>Zheng</last></author>
      <author><first>Yunmo</first><last>Chen</last></author>
      <author><first>Tongfei</first><last>Chen</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>3976-3983</pages>
      <abstract>We present an empirical study on methods for span finding, the selection of consecutive tokens in text for some downstream tasks. We focus on approaches that can be employed in training end-to-end information extraction systems, and find there is no definitive solution without considering task properties, and provide our observations to help with future design choices: 1) a tagging approach often yields higher precision while span enumeration and boundary prediction provide higher recall; 2) span type information can benefit a boundary prediction approach; 3) additional contextualization does not help span finding in most cases.</abstract>
      <url hash="da41712c">2022.emnlp-main.264</url>
      <bibkey>gu-etal-2022-empirical</bibkey>
    </paper>
    <paper id="265">
      <title><fixed-case>MGD</fixed-case>oc: Pre-training with Multi-granular Hierarchy for Document Image Understanding</title>
      <author><first>Zilong</first><last>Wang</last></author>
      <author><first>Jiuxiang</first><last>Gu</last></author>
      <author><first>Chris</first><last>Tensmeyer</last></author>
      <author><first>Nikolaos</first><last>Barmpalios</last></author>
      <author><first>Ani</first><last>Nenkova</last></author>
      <author><first>Tong</first><last>Sun</last></author>
      <author><first>Jingbo</first><last>Shang</last></author>
      <author><first>Vlad</first><last>Morariu</last></author>
      <pages>3984-3993</pages>
      <abstract>Document images are a ubiquitous source of data where the text is organized in a complex hierarchical structure ranging from fine granularity (e.g., words), medium granularity (e.g., regions such as paragraphs or figures), to coarse granularity (e.g., the whole page). The spatial hierarchical relationships between content at different levels of granularity are crucial for document image understanding tasks. Existing methods learn features from either word-level or region-level but fail to consider both simultaneously. Word-level models are restricted by the fact that they originate from pure-text language models, which only encode the word-level context. In contrast, region-level models attempt to encode regions corresponding to paragraphs or text blocks into a single embedding, but they perform worse with additional word-level features. To deal with these issues, we propose MGDoc, a new multi-modal multi-granular pre-training framework that encodes page-level, region-level, and word-level information at the same time. MGDoc uses a unified text-visual encoder to obtain multi-modal features across different granularities, which makes it possible to project the multi-granular features into the same hyperspace. To model the region-word correlation, we design a cross-granular attention mechanism and specific pre-training tasks for our model to reinforce the model of learning the hierarchy between regions and words. Experiments demonstrate that our proposed model can learn better features that perform well across granularities and lead to improvements in downstream tasks.</abstract>
      <url hash="21e47bea">2022.emnlp-main.265</url>
      <bibkey>wang-etal-2022-mgdoc</bibkey>
    </paper>
    <paper id="266">
      <title>Understanding Jargon: Combining Extraction and Generation for Definition Modeling</title>
      <author><first>Jie</first><last>Huang</last></author>
      <author><first>Hanyin</first><last>Shao</last></author>
      <author><first>Kevin Chen-Chuan</first><last>Chang</last></author>
      <author><first>Jinjun</first><last>Xiong</last></author>
      <author><first>Wen-mei</first><last>Hwu</last></author>
      <pages>3994-4004</pages>
      <abstract>Can machines know what twin prime is? From the composition of this phrase, machines may guess twin prime is a certain kind of prime, but it is still difficult to deduce exactly what twin stands for without additional knowledge. Here, twin prime is a jargon - a specialized term used by experts in a particular field. Explaining jargon is challenging since it usually requires domain knowledge to understand. Recently, there is an increasing interest in extracting and generating definitions of words automatically. However, existing approaches, either extraction or generation, perform poorly on jargon. In this paper, we propose to combine extraction and generation for jargon definition modeling: first extract self- and correlative definitional information of target jargon from the Web and then generate the final definitions by incorporating the extracted definitional information. Our framework is remarkably simple but effective: experiments demonstrate our method can generate high-quality definitions for jargon and outperform state-of-the-art models significantly, e.g., BLEU score from 8.76 to 22.66 and human-annotated score from 2.34 to 4.04.</abstract>
      <url hash="bdefe849">2022.emnlp-main.266</url>
      <bibkey>huang-etal-2022-understanding</bibkey>
    </paper>
    <paper id="267">
      <title><fixed-case>P</fixed-case>rosocial<fixed-case>D</fixed-case>ialog: A Prosocial Backbone for Conversational Agents</title>
      <author><first>Hyunwoo</first><last>Kim</last></author>
      <author><first>Youngjae</first><last>Yu</last></author>
      <author><first>Liwei</first><last>Jiang</last></author>
      <author><first>Ximing</first><last>Lu</last></author>
      <author><first>Daniel</first><last>Khashabi</last></author>
      <author><first>Gunhee</first><last>Kim</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <author><first>Maarten</first><last>Sap</last></author>
      <pages>4005-4029</pages>
      <abstract>Most existing dialogue systems fail to respond properly to potentially unsafe user utterances by either ignoring or passively agreeing with them. To address this issue, we introduce ProsocialDialog, the first large-scale multi-turn dialogue dataset to teach conversational agents to respond to problematic content following social norms. Covering diverse unethical, problematic, biased, and toxic situations, ProsocialDialog contains responses that encourage prosocial behavior, grounded in commonsense social rules (i.e., rules-of-thumb, RoTs). Created via a human-AI collaborative framework, ProsocialDialog consists of 58K dialogues, with 331K utterances, 160K unique RoTs, and 497K dialogue safety labels accompanied by free-form rationales.With this dataset, we introduce a dialogue safety detection module, Canary, capable of generating RoTs given conversational context, and a socially-informed dialogue agent, Prost. Empirical results show that Prost generates more socially acceptable dialogues compared to other state-of-the-art language and dialogue models in both in-domain and out-of-domain settings. Additionally, Canary effectively guides conversational agents and off-the-shelf language models to generate significantly more prosocial responses. Our work highlights the promise and importance of creating and steering conversational AI to be socially responsible.</abstract>
      <url hash="043b10a9">2022.emnlp-main.267</url>
      <bibkey>kim-etal-2022-prosocialdialog</bibkey>
    </paper>
    <paper id="268">
      <title>Exploiting Global and Local Hierarchies for Hierarchical Text Classification</title>
      <author><first>Ting</first><last>Jiang</last></author>
      <author><first>Deqing</first><last>Wang</last></author>
      <author><first>Leilei</first><last>Sun</last></author>
      <author><first>Zhongzhi</first><last>Chen</last></author>
      <author><first>Fuzhen</first><last>Zhuang</last></author>
      <author><first>Qinghong</first><last>Yang</last></author>
      <pages>4030-4039</pages>
      <abstract>Hierarchical text classification aims to leverage label hierarchy in multi-label text classification. Existing methods encode label hierarchy in a global view, where label hierarchy is treated as the static hierarchical structure containing all labels. Since global hierarchy is static and irrelevant to text samples, it makes these methods hard to exploit hierarchical information. Contrary to global hierarchy, local hierarchy as a structured labels hierarchy corresponding to each text sample. It is dynamic and relevant to text samples, which is ignored in previous methods. To exploit global and local hierarchies, we propose Hierarchy-guided BERT with Global and Local hierarchies (HBGL), which utilizes the large-scale parameters and prior language knowledge of BERT to model both global and local hierarchies. Moreover, HBGL avoids the intentional fusion of semantic and hierarchical modules by directly modeling semantic and hierarchical information with BERT. Compared with the state-of-the-art method HGCLR, our method achieves significant improvement on three benchmark datasets.</abstract>
      <url hash="705c1730">2022.emnlp-main.268</url>
      <bibkey>jiang-etal-2022-exploiting</bibkey>
    </paper>
    <paper id="269">
      <title>Semantic-aware Contrastive Learning for More Accurate Semantic Parsing</title>
      <author><first>Shan</first><last>Wu</last></author>
      <author><first>Chunlei</first><last>Xin</last></author>
      <author><first>Bo</first><last>Chen</last></author>
      <author><first>Xianpei</first><last>Han</last></author>
      <author><first>Le</first><last>Sun</last></author>
      <pages>4040-4052</pages>
      <abstract>Since the meaning representations are detailed and accurate annotations which express fine-grained sequence-level semtantics, it is usually hard to train discriminative semantic parsers via Maximum Likelihood Estimation (MLE) in an autoregressive fashion. In this paper, we propose a semantic-aware contrastive learning algorithm, which can learn to distinguish fine-grained meaning representations and take the overall sequence-level semantic into consideration. Specifically, a multi-level online sampling algorithm is proposed to sample confusing and diverse instances. Three semantic-aware similarity functions are designed to accurately measure the distance between meaning representations as a whole. And a ranked contrastive loss is proposed to pull the representations of the semantic-identical instances together and push negative instances away. Experiments on two standard datasets show that our approach achieves significant improvements over MLE baselines and gets state-of-the-art performances by simply applying semantic-aware contrastive learning on a vanilla Seq2Seq model.</abstract>
      <url hash="4cf60ff6">2022.emnlp-main.269</url>
      <bibkey>wu-etal-2022-semantic</bibkey>
    </paper>
    <paper id="270">
      <title>Scientific Paper Extractive Summarization Enhanced by Citation Graphs</title>
      <author><first>Xiuying</first><last>Chen</last></author>
      <author><first>Mingzhe</first><last>Li</last></author>
      <author><first>Shen</first><last>Gao</last></author>
      <author><first>Rui</first><last>Yan</last></author>
      <author><first>Xin</first><last>Gao</last></author>
      <author><first>Xiangliang</first><last>Zhang</last></author>
      <pages>4053-4062</pages>
      <abstract>In a citation graph, adjacent paper nodes share related scientific terms and topics. The graph thus conveys unique structure information of document-level relatedness that can be utilized in the paper summarization task, for exploring beyond the intra-document information.In this work, we focus on leveraging citation graphs to improve scientific paper extractive summarization under different settings.We first propose a Multi-granularity Unsupervised Summarization model (MUS) as a simple and low-cost solution to the task.MUS finetunes a pre-trained encoder model on the citation graph by link prediction tasks.Then, the abstract sentences are extracted from the corresponding paper considering multi-granularity information.Preliminary results demonstrate that citation graph is helpful even in a simple unsupervised framework.Motivated by this, we next propose a Graph-based Supervised Summarizationmodel (GSS) to achieve more accurate results on the task when large-scale labeled data are available.Apart from employing the link prediction as an auxiliary task, GSS introduces a gated sentence encoder and a graph information fusion module to take advantage of the graph information to polish the sentence representation.Experiments on a public benchmark dataset show that MUS and GSS bring substantial improvements over the prior state-of-the-art model.</abstract>
      <url hash="d2473c7e">2022.emnlp-main.270</url>
      <bibkey>chen-etal-2022-scientific</bibkey>
    </paper>
    <paper id="271">
      <title>Hardness-guided domain adaptation to recognise biomedical named entities under low-resource scenarios</title>
      <author><first>Ngoc Dang</first><last>Nguyen</last></author>
      <author><first>Lan</first><last>Du</last></author>
      <author><first>Wray</first><last>Buntine</last></author>
      <author><first>Changyou</first><last>Chen</last></author>
      <author><first>Richard</first><last>Beare</last></author>
      <pages>4063-4071</pages>
      <abstract>Domain adaptation is an effective solution to data scarcity in low-resource scenarios. However, when applied to token-level tasks such as bioNER, domain adaptation methods often suffer from the challenging linguistic characteristics that clinical narratives possess, which leads to unsatsifactory performance. In this paper, we present a simple yet effective hardness-guided domain adaptation framework for bioNER tasks that can effectively leverage the domain hardness information to improve the adaptability of the learnt model in the low-resource scenarios. Experimental results on biomedical datasets show that our model can achieve significant performance improvement over the recently published state-of-the-art (SOTA) MetaNER model.</abstract>
      <url hash="1e67dc4e">2022.emnlp-main.271</url>
      <bibkey>nguyen-etal-2022-hardness</bibkey>
    </paper>
    <paper id="272">
      <title>Syntactic Multi-view Learning for Open Information Extraction</title>
      <author><first>Kuicai</first><last>Dong</last></author>
      <author><first>Aixin</first><last>Sun</last></author>
      <author><first>Jung-Jae</first><last>Kim</last></author>
      <author><first>Xiaoli</first><last>Li</last></author>
      <pages>4072-4083</pages>
      <abstract>Open Information Extraction (OpenIE) aims to extract relational tuples from open-domain sentences. Traditional rule-based or statistical models were developed based on syntactic structure of sentence, identified by syntactic parsers. However, previous neural OpenIE models under-explored the useful syntactic information. In this paper, we model both constituency and dependency trees into word-level graphs, and enable neural OpenIE to learn from the syntactic structures. To better fuse heterogeneous information from the two graphs, we adopt multi-view learning to capture multiple relationships from them. Finally, the finetuned constituency and dependency representations are aggregated with sentential semantic representations for tuple generation. Experiments show that both constituency and dependency information, and the multi-view learning are effective.</abstract>
      <url hash="a11c39b2">2022.emnlp-main.272</url>
      <bibkey>dong-etal-2022-syntactic</bibkey>
    </paper>
    <paper id="273">
      <title><fixed-case>TRIPS</fixed-case>: Efficient Vision-and-Language Pre-training with Text-Relevant Image Patch Selection</title>
      <author><first>Chaoya</first><last>Jiang</last></author>
      <author><first>Haiyang</first><last>Xu</last></author>
      <author><first>Chenliang</first><last>Li</last></author>
      <author><first>Ming</first><last>Yan</last></author>
      <author><first>Wei</first><last>Ye</last></author>
      <author><first>Shikun</first><last>Zhang</last></author>
      <author><first>Bin</first><last>Bi</last></author>
      <author><first>Songfang</first><last>Huang</last></author>
      <pages>4084-4096</pages>
      <abstract>Vision Transformers (ViTs) have been widely used in large-scale Vision and Language Pre-training (VLP) models. Though previous VLP works have proved the effectiveness of ViTs, they still suffer from computational efficiency brought by the long visual sequence. To tackle this problem, in this paper, we propose an efficient vision-and-language pre-training model with Text-Relevant Image Patch Selection, namely TRIPS, which reduces the visual sequence progressively with a text-guided patch-selection layer in the visual backbone for efficient training and inference. The patch-selection layer can dynamically compute text-dependent visual attention to identify the attentive image tokens with text guidance and fuse inattentive ones in an end-to-end manner. Meanwhile, TRIPS does not introduce extra parameters to ViTs. Experimental results on a variety of popular benchmark datasets demonstrate that TRIPS gain a speedup of 40% over previous similar VLP models, yet with competitive or better downstream task performance.</abstract>
      <url hash="5e237d72">2022.emnlp-main.273</url>
      <bibkey>jiang-etal-2022-trips</bibkey>
    </paper>
    <paper id="274">
      <title><fixed-case>CG</fixed-case>o<fixed-case>D</fixed-case>ial: A Large-Scale Benchmark for <fixed-case>C</fixed-case>hinese Goal-oriented Dialog Evaluation</title>
      <author><first>Yinpei</first><last>Dai</last></author>
      <author><first>Wanwei</first><last>He</last></author>
      <author><first>Bowen</first><last>Li</last></author>
      <author><first>Yuchuan</first><last>Wu</last></author>
      <author><first>Zheng</first><last>Cao</last></author>
      <author><first>Zhongqi</first><last>An</last></author>
      <author><first>Jian</first><last>Sun</last></author>
      <author><first>Yongbin</first><last>Li</last></author>
      <pages>4097-4111</pages>
      <abstract>Practical dialog systems need to deal with various knowledge sources, noisy user expressions, and the shortage of annotated data. To better solve the above problems, we propose CGoDial, a new challenging and comprehensive Chinese benchmark for multi-domain Goal-oriented Dialog evaluation. It contains 96,763 dialog sessions, and 574,949 dialog turns totally, covering three datasets with different knowledge sources: 1) a slot-based dialog (SBD) dataset with table-formed knowledge, 2) a flow-based dialog (FBD) dataset with tree-formed knowledge, and a retrieval-based dialog (RBD) dataset with candidate-formed knowledge. To bridge the gap between academic benchmarks and spoken dialog scenarios, we either collect data from real conversations or add spoken features to existing datasets via crowd-sourcing. The proposed experimental settings include the combinations of training with either the entire training set or a few-shot training set, and testing with either the standard test set or a hard test subset, which can assess model capabilities in terms of general prediction, fast adaptability and reliable robustness.</abstract>
      <url hash="e67cddd3">2022.emnlp-main.274</url>
      <bibkey>dai-etal-2022-cgodial</bibkey>
    </paper>
    <paper id="275">
      <title>Kernel-Whitening: Overcome Dataset Bias with Isotropic Sentence Embedding</title>
      <author><first>SongYang</first><last>Gao</last></author>
      <author><first>Shihan</first><last>Dou</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <pages>4112-4122</pages>
      <abstract>Dataset bias has attracted increasing attention recently for its detrimental effect on the generalization ability of fine-tuned models. The current mainstream solution is designing an additional shallow model to pre-identify biased instances. However, such two-stage methods scale up the computational complexity of training process and obstruct valid feature information while mitigating bias.To address this issue, we utilize the representation normalization method which aims at disentangling the correlations between features of encoded sentences. We find it also promising in eliminating the bias problem by providing isotropic data distribution. We further propose Kernel-Whitening, a Nystrom kernel approximation method to achieve more thorough debiasing on nonlinear spurious correlations. Our framework is end-to-end with similar time consumption to fine-tuning. Experiments show that Kernel-Whitening significantly improves the performance of BERT on out-of-distribution datasets while maintaining in-distribution accuracy.</abstract>
      <url hash="4cd47c8f">2022.emnlp-main.275</url>
      <bibkey>gao-etal-2022-kernel</bibkey>
    </paper>
    <paper id="276">
      <title>A Unified Positive-Unlabeled Learning Framework for Document-Level Relation Extraction with Different Levels of Labeling</title>
      <author><first>Ye</first><last>Wang</last></author>
      <author><first>Xinxin</first><last>Liu</last></author>
      <author><first>Wenxin</first><last>Hu</last></author>
      <author><first>Tao</first><last>Zhang</last></author>
      <pages>4123-4135</pages>
      <abstract>Document-level relation extraction (RE) aims to identify relations between entities across multiple sentences. Most previous methods focused on document-level RE under full supervision. However, in real-world scenario, it is expensive and difficult to completely label all relations in a document because the number of entity pairs in document-level RE grows quadratically with the number of entities. To solve the common incomplete labeling problem, we propose a unified positive-unlabeled learning framework - shift and squared ranking loss positive-unlabeled (SSR-PU) learning. We use positive-unlabeled (PU) learning on document-level RE for the first time. Considering that labeled data of a dataset may lead to prior shift of unlabeled data, we introduce a PU learning under prior shift of training data. Also, using none-class score as an adaptive threshold, we propose squared ranking loss and prove its Bayesian consistency with multi-label ranking metrics. Extensive experiments demonstrate that our method achieves an improvement of about 14 F1 points relative to the previous baseline with incomplete labeling. In addition, it outperforms previous state-of-the-art results under both fully supervised and extremely unlabeled settings as well.</abstract>
      <url hash="3e06f137">2022.emnlp-main.276</url>
      <bibkey>wang-etal-2022-unified</bibkey>
    </paper>
    <paper id="277">
      <title>Automatic Generation of Socratic Subquestions for Teaching Math Word Problems</title>
      <author><first>Kumar</first><last>Shridhar</last></author>
      <author><first>Jakub</first><last>Macina</last></author>
      <author><first>Mennatallah</first><last>El-Assady</last></author>
      <author><first>Tanmay</first><last>Sinha</last></author>
      <author><first>Manu</first><last>Kapur</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last></author>
      <pages>4136-4149</pages>
      <abstract>Socratic questioning is an educational method that allows students to discover answers to complex problems by asking them a series of thoughtful questions. Generation of didactically sound questions is challenging, requiring understanding of the reasoning process involved in the problem. We hypothesize that such questioning strategy can not only enhance the human performance, but also assist the math word problem (MWP) solvers.In this work, we explore the ability of large language models (LMs) in generating sequential questions for guiding math word problem-solving. We propose various guided question generation schemes based on input conditioning and reinforcement learning.On both automatic and human quality evaluations, we find that LMs constrained with desirable question properties generate superior questions and improve the overall performance of a math word problem solver. We conduct a preliminary user study to examine the potential value of such question generation models in the education domain. Results suggest that the difficulty level of problems plays an important role in determining whether questioning improves or hinders human performance. We discuss the future of using such questioning strategies in education.</abstract>
      <url hash="d008a37e">2022.emnlp-main.277</url>
      <bibkey>shridhar-etal-2022-automatic</bibkey>
    </paper>
    <paper id="278">
      <title>Mixture of Attention Heads: Selecting Attention Heads Per Token</title>
      <author><first>Xiaofeng</first><last>Zhang</last></author>
      <author><first>Yikang</first><last>Shen</last></author>
      <author><first>Zeyu</first><last>Huang</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Wenge</first><last>Rong</last></author>
      <author><first>Zhang</first><last>Xiong</last></author>
      <pages>4150-4162</pages>
      <abstract>Mixture-of-Experts (MoE) networks have been proposed as an efficient way to scale up model capacity and implement conditional computing. However, the study of MoE components mostly focused on the feedforward layer in Transformer architecture. This paper proposes the Mixture of Attention Heads (MoA), a new architecture that combines multi-head attention with the MoE mechanism. MoA includes a set of attention heads that each has its own set of parameters. Given an input, a router dynamically selects a subset of k attention heads per token. This conditional computation schema allows MoA to achieve stronger performance than the standard multi-head attention layer. Furthermore, the sparsely gated MoA can easily scale up the number of attention heads and the number of parameters while preserving computational efficiency. Despite performance improvements, MoA also automatically differentiates heads’ utilities, providing a new perspective to discuss the model’s interpretability. We conducted experiments on several important tasks, including Machine Translation and Masked Language Modeling. Experiments have shown promising results on several tasks against strong baselines that involve large and very deep models.</abstract>
      <url hash="73e09f8d">2022.emnlp-main.278</url>
      <bibkey>zhang-etal-2022-mixture</bibkey>
    </paper>
    <paper id="279">
      <title>The Optimal <fixed-case>BERT</fixed-case> Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models</title>
      <author><first>Eldar</first><last>Kurtic</last></author>
      <author><first>Daniel</first><last>Campos</last></author>
      <author><first>Tuan</first><last>Nguyen</last></author>
      <author><first>Elias</first><last>Frantar</last></author>
      <author><first>Mark</first><last>Kurtz</last></author>
      <author><first>Benjamin</first><last>Fineran</last></author>
      <author><first>Michael</first><last>Goin</last></author>
      <author><first>Dan</first><last>Alistarh</last></author>
      <pages>4163-4181</pages>
      <abstract>In this paper, we consider the problem of sparsifying BERT models, which are a key building block for natural language processing, in order to reduce their storage and computational cost. We introduce the Optimal BERT Surgeon (oBERT), an efficient and accurate pruning method based on approximate second-order information, which we show to yield state-of-the-art results in both stages of language tasks: pre-training and fine-tuning. Specifically, oBERT extends existing work on second-order pruning by allowing for pruning weight blocks, and is the first such method that is applicable at BERT scale. Second, we investigate compounding compression approaches to obtain highly compressed but accurate models for deployment on edge devices. These models significantly push boundaries of the current state-of-the-art sparse BERT models with respect to all metrics: model size, inference speed and task accuracy. For example, relative to the dense BERT-base, we obtain 10x model size compression with &lt; 1% accuracy drop, 10x CPU-inference speedup with &lt; 2% accuracy drop, and 29x CPU-inference speedup with &lt; 7.5% accuracy drop. Our code, fully integrated with Transformers and SparseML, is available at https://github.com/neuralmagic/sparseml/tree/main/research/optimal_BERT_surgeon_oBERT.</abstract>
      <url hash="deb8e075">2022.emnlp-main.279</url>
      <bibkey>kurtic-etal-2022-optimal</bibkey>
    </paper>
    <paper id="280">
      <title>Information-Theoretic Text Hallucination Reduction for Video-grounded Dialogue</title>
      <author><first>Sunjae</first><last>Yoon</last></author>
      <author><first>Eunseop</first><last>Yoon</last></author>
      <author><first>Hee Suk</first><last>Yoon</last></author>
      <author><first>Junyeong</first><last>Kim</last></author>
      <author><first>Chang</first><last>Yoo</last></author>
      <pages>4182-4193</pages>
      <abstract>Video-grounded Dialogue (VGD) aims to decode an answer sentence to a question regarding a given video and dialogue context. Despite the recent success of multi-modal reasoning to generate answer sentences, existing dialogue systems still suffer from a text hallucination problem, which denotes indiscriminate text-copying from input texts without an understanding of the question. This is due to learning spurious correlations from the fact that answer sentences in the dataset usually include the words of input texts, thus the VGD system excessively relies on copying words from input texts by hoping those words to overlap with ground-truth texts. Hence, we design Text Hallucination Mitigating (THAM) framework, which incorporates Text Hallucination Regularization (THR) loss derived from the proposed information-theoretic text hallucination measurement approach. Applying THAM with current dialogue systems validates the effectiveness on VGD benchmarks (i.e., AVSD@DSTC7 and AVSD@DSTC8) and shows enhanced interpretability.</abstract>
      <url hash="3b5a2534">2022.emnlp-main.280</url>
      <bibkey>yoon-etal-2022-information</bibkey>
    </paper>
    <paper id="281">
      <title><fixed-case>DSM</fixed-case>: Question Generation over Knowledge Base via Modeling Diverse Subgraphs with Meta-learner</title>
      <author><first>Shasha</first><last>Guo</last></author>
      <author><first>Jing</first><last>Zhang</last></author>
      <author><first>Yanling</first><last>Wang</last></author>
      <author><first>Qianyi</first><last>Zhang</last></author>
      <author><first>Cuiping</first><last>Li</last></author>
      <author><first>Hong</first><last>Chen</last></author>
      <pages>4194-4207</pages>
      <abstract>Existing methods on knowledge base question generation (KBQG) learn a one-size-fits-all model by training together all subgraphs without distinguishing the diverse semantics of subgraphs. In this work, we show that making use of the past experience on semantically similar subgraphs can reduce the learning difficulty and promote the performance of KBQG models. To achieve this, we propose a novel approach to model diverse subgraphs with meta-learner (DSM). Specifically, we devise a graph contrastive learning-based retriever to identify semantically similar subgraphs, so that we can construct the semantics-aware learning tasks for the meta-learner to learn semantics-specific and semantics-agnostic knowledge on and across these tasks. Extensive experiments on two widely-adopted benchmarks for KBQG show that DSM derives new state-of-the-art performance and benefits the question answering tasks as a means of data augmentation.</abstract>
      <url hash="7a542e70">2022.emnlp-main.281</url>
      <bibkey>guo-etal-2022-dsm</bibkey>
    </paper>
    <paper id="282">
      <title><fixed-case>R</fixed-case>el<fixed-case>U</fixed-case>-Net: Syntax-aware Graph <fixed-case>U</fixed-case>-Net for Relational Triple Extraction</title>
      <author><first>Yunqi</first><last>Zhang</last></author>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Yongfeng</first><last>Huang</last></author>
      <pages>4208-4217</pages>
      <abstract>Relational triple extraction is a critical task for natural language processing. Existing methods mainly focused on capturing semantic information, but suffered from ignoring the syntactic structure of the sentence, which is proved in the relation classification task to contain rich relational information. This is due to the absence of entity locations, which is the prerequisite for pruning noisy edges from the dependency tree, when extracting relational triples. In this paper, we propose a unified framework to tackle this challenge and incorporate syntactic information for relational triple extraction. First, we propose to automatically contract the dependency tree into a core relational topology and eliminate redundant information with graph pooling operations. Then, we propose a symmetrical expanding path with graph unpooling operations to fuse the contracted core syntactic interactions with the original sentence context. We also propose a bipartite graph matching objective function to capture the reflections between the core topology and golden relational facts. Since our model shares similar contracting and expanding paths with encoder-decoder models like U-Net, we name our model as Relation U-Net (RelU-Net). We conduct experiments on several datasets and the results prove the effectiveness of our method.</abstract>
      <url hash="53407464">2022.emnlp-main.282</url>
      <bibkey>zhang-etal-2022-relu</bibkey>
    </paper>
    <paper id="283">
      <title>Evidence &gt; Intuition: Transferability Estimation for Encoder Selection</title>
      <author><first>Elisa</first><last>Bassignana</last></author>
      <author><first>Max</first><last>Müller-Eberstein</last></author>
      <author><first>Mike</first><last>Zhang</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>4218-4227</pages>
      <abstract>With the increase in availability of large pre-trained language models (LMs) in Natural Language Processing (NLP), it becomes critical to assess their fit for a specific target task a priori—as fine-tuning the entire space of available LMs is computationally prohibitive and unsustainable. However, encoder transferability estimation has received little to no attention in NLP. In this paper, we propose to generate quantitative evidence to predict which LM, out of a pool of models, will perform best on a target task without having to fine-tune all candidates. We provide a comprehensive study on LM ranking for 10 NLP tasks spanning the two fundamental problem types of classification and structured prediction. We adopt the state-of-the-art Logarithm of Maximum Evidence (LogME) measure from Computer Vision (CV) and find that it positively correlates with final LM performance in 94% of the setups.In the first study of its kind, we further compare transferability measures with the de facto standard of human practitioner ranking, finding that evidence from quantitative metrics is more robust than pure intuition and can help identify unexpected LM candidates.</abstract>
      <url hash="bd424460">2022.emnlp-main.283</url>
      <bibkey>bassignana-etal-2022-evidence</bibkey>
    </paper>
    <paper id="284">
      <title>Chunk-based Nearest Neighbor Machine Translation</title>
      <author><first>Pedro Henrique</first><last>Martins</last></author>
      <author><first>Zita</first><last>Marinho</last></author>
      <author><first>André F. T.</first><last>Martins</last></author>
      <pages>4228-4245</pages>
      <abstract>Semi-parametric models, which augment generation with retrieval, have led to impressive results in language modeling and machine translation, due to their ability to retrieve fine-grained information from a datastore of examples. One of the most prominent approaches, kNN-MT, exhibits strong domain adaptation capabilities by retrieving tokens from domain-specific datastores (Khandelwal et al., 2021). However, kNN-MT requires an expensive retrieval operation for every single generated token, leading to a very low decoding speed (around 8 times slower than a parametric model). In this paper, we introduce a chunk-based kNN-MT model which retrieves chunks of tokens from the datastore, instead of a single token. We propose several strategies for incorporating the retrieved chunks into the generation process, and for selecting the steps at which the model needs to search for neighbors in the datastore. Experiments on machine translation in two settings, static and “on-the-fly” domain adaptation, show that the chunk-based kNN-MT model leads to significant speed-ups (up to 4 times) with only a small drop in translation quality.</abstract>
      <url hash="cfa5d517">2022.emnlp-main.284</url>
      <bibkey>martins-etal-2022-chunk</bibkey>
    </paper>
    <paper id="285">
      <title><fixed-case>F</fixed-case>i<fixed-case>E</fixed-case>: Building a Global Probability Space by Leveraging Early Fusion in Encoder for Open-Domain Question Answering</title>
      <author><first>Akhil</first><last>Kedia</last></author>
      <author><first>Mohd Abbas</first><last>Zaidi</last></author>
      <author><first>Haejun</first><last>Lee</last></author>
      <pages>4246-4260</pages>
      <abstract>Generative models have recently started to outperform extractive models in Open Domain Question Answering, largely by leveraging their decoder to attend over multiple encoded passages and combining their information. However, generative models tend to be larger than extractive models due to the need for a decoder, run slower during inference due to auto-regressive decoder beam search, and their generated output often suffers from hallucinations. We propose to extend transformer encoders with the ability to fuse information from multiple passages, using global representation to provide cross-sample attention over all tokens across samples. Furthermore, we propose an alternative answer span probability calculation to better aggregate answer scores in the global space of all samples. Using our proposed method, we outperform the current state-of-the-art method by 2.5 Exact Match score on the Natural Question dataset while using only 25% of parameters and 35% of the latency during inference, and 4.4 Exact Match on WebQuestions dataset. When coupled with synthetic data augmentation, we outperform larger models on the TriviaQA dataset as well. The latency and parameter savings of our method make it particularly attractive for open-domain question answering, as these models are often compute-intensive.</abstract>
      <url hash="56c92f53">2022.emnlp-main.285</url>
      <bibkey>kedia-etal-2022-fie</bibkey>
    </paper>
    <paper id="286">
      <title>Inductive Relation Prediction with Logical Reasoning Using Contrastive Representations</title>
      <author><first>Yudai</first><last>Pan</last></author>
      <author><first>Jun</first><last>Liu</last></author>
      <author><first>Lingling</first><last>Zhang</last></author>
      <author><first>Tianzhe</first><last>Zhao</last></author>
      <author><first>Qika</first><last>Lin</last></author>
      <author><first>Xin</first><last>Hu</last></author>
      <author><first>Qianying</first><last>Wang</last></author>
      <pages>4261-4274</pages>
      <abstract>Relation prediction in knowledge graphs (KGs) aims at predicting missing relations in incomplete triples, whereas the dominant embedding paradigm has a restriction on handling unseen entities during testing. In the real-world scenario, the inductive setting is more common because entities in the training process are finite. Previous methods capture an inductive ability by implicit logic in KGs. However, it would be challenging to preciously acquire entity-independent relational semantics of compositional logic rules and to deal with the deficient supervision of logic caused by the scarcity of relational semantics. To this end, we propose a novel graph convolutional network (GCN)-based model LogCo with logical reasoning by contrastive representations. LogCo firstly extracts enclosing subgraphs and relational paths between two entities to supply the entity-independence. Then a contrastive strategy for relational path instances and the subgraph is proposed for the issue of deficient supervision. The contrastive representations are learned for a joint training regime. Finally, prediction results and logic rules for reasoning are attained. Comprehensive experiments on twelve inductive datasets show that LogCo achieves outstanding performance comparing with state-of-the-art inductive relation prediction baselines.</abstract>
      <url hash="a447878f">2022.emnlp-main.286</url>
      <bibkey>pan-etal-2022-inductive</bibkey>
    </paper>
    <paper id="287">
      <title>Improving <fixed-case>C</fixed-case>hinese Spelling Check by Character Pronunciation Prediction: The Effects of Adaptivity and Granularity</title>
      <author><first>Jiahao</first><last>Li</last></author>
      <author><first>Quan</first><last>Wang</last></author>
      <author><first>Zhendong</first><last>Mao</last></author>
      <author><first>Junbo</first><last>Guo</last></author>
      <author><first>Yanyan</first><last>Yang</last></author>
      <author><first>Yongdong</first><last>Zhang</last></author>
      <pages>4275-4286</pages>
      <abstract>Chinese spelling check (CSC) is a fundamental NLP task that detects and corrects spelling errors in Chinese texts. As most of these spelling errors are caused by phonetic similarity, effectively modeling the pronunciation of Chinese characters is a key factor for CSC. In this paper, we consider introducing an auxiliary task of Chinese pronunciation prediction (CPP) to improve CSC, and, for the first time, systematically discuss the adaptivity and granularity of this auxiliary task. We propose SCOPE which builds upon a shared encoder two parallel decoders, one for the primary CSC task and the other for a fine-grained auxiliary CPP task, with a novel adaptive weighting scheme to balance the two tasks. In addition, we design a delicate iterative correction strategy for further improvements during inference. Empirical evaluation shows that SCOPE achieves new state-of-the-art on three CSC benchmarks, demonstrating the effectiveness and superiority of the auxiliary CPP task. Comprehensive ablation studies further verify the positive effects of adaptivity and granularity of the task.</abstract>
      <url hash="ae8a1115">2022.emnlp-main.287</url>
      <bibkey>li-etal-2022-improving-chinese</bibkey>
    </paper>
    <paper id="288">
      <title><fixed-case>MT</fixed-case>-<fixed-case>G</fixed-case>en<fixed-case>E</fixed-case>val: A Counterfactual and Contextual Dataset for Evaluating Gender Accuracy in Machine Translation</title>
      <author><first>Anna</first><last>Currey</last></author>
      <author><first>Maria</first><last>Nadejde</last></author>
      <author><first>Raghavendra Reddy</first><last>Pappagari</last></author>
      <author><first>Mia</first><last>Mayer</last></author>
      <author><first>Stanislas</first><last>Lauly</last></author>
      <author><first>Xing</first><last>Niu</last></author>
      <author><first>Benjamin</first><last>Hsu</last></author>
      <author><first>Georgiana</first><last>Dinu</last></author>
      <pages>4287-4299</pages>
      <abstract>As generic machine translation (MT) quality has improved, the need for targeted benchmarks that explore fine-grained aspects of quality has increased. In particular, gender accuracy in translation can have implications in terms of output fluency, translation accuracy, and ethics. In this paper, we introduce MT-GenEval, a benchmark for evaluating gender accuracy in translation from English into eight widely-spoken languages. MT-GenEval complements existing benchmarks by providing realistic, gender-balanced, counterfactual data in eight language pairs where the gender of individuals is unambiguous in the input segment, including multi-sentence segments requiring inter-sentential gender agreement. Our data and code is publicly available under a CC BY SA 3.0 license.</abstract>
      <url hash="c31b65a8">2022.emnlp-main.288</url>
      <bibkey>currey-etal-2022-mt</bibkey>
    </paper>
    <paper id="289">
      <title>A Span-level Bidirectional Network for Aspect Sentiment Triplet Extraction</title>
      <author><first>Yuqi</first><last>Chen</last></author>
      <author><first>Chen</first><last>Keming</last></author>
      <author><first>Xian</first><last>Sun</last></author>
      <author><first>Zequn</first><last>Zhang</last></author>
      <pages>4300-4309</pages>
      <abstract>Aspect Sentiment Triplet Extraction (ASTE) is a new fine-grained sentiment analysis task that aims to extract triplets of aspect terms, sentiments, and opinion terms from review sentences. Recently, span-level models achieve gratifying results on ASTE task by taking advantage of the predictions of all possible spans. Since all possible spans significantly increases the number of potential aspect and opinion candidates, it is crucial and challenging to efficiently extract the triplet elements among them. In this paper, we present a span-level bidirectional network which utilizes all possible spans as input and extracts triplets from spans bidirectionally. Specifically, we devise both the aspect decoder and opinion decoder to decode the span representations and extract triples from aspect-to-opinion and opinion-to-aspect directions. With these two decoders complementing with each other, the whole network can extract triplets from spans more comprehensively. Moreover, considering that mutual exclusion cannot be guaranteed between the spans, we design a similar span separation loss to facilitate the downstream task of distinguishing the correct span by expanding the KL divergence of similar spans during the training process; in the inference process, we adopt an inference strategy to remove conflicting triplets from the results base on their confidence scores. Experimental results show that our framework not only significantly outperforms state-of-the-art methods, but achieves better performance in predicting triplets with multi-token entities and extracting triplets in sentences contain multi-triplets.</abstract>
      <url hash="9e794d53">2022.emnlp-main.289</url>
      <bibkey>chen-etal-2022-span</bibkey>
    </paper>
    <paper id="290">
      <title>On the Calibration of Massively Multilingual Language Models</title>
      <author><first>Kabir</first><last>Ahuja</last></author>
      <author><first>Sunayana</first><last>Sitaram</last></author>
      <author><first>Sandipan</first><last>Dandapat</last></author>
      <author><first>Monojit</first><last>Choudhury</last></author>
      <pages>4310-4323</pages>
      <abstract>Massively Multilingual Language Models (MMLMs) have recently gained popularity due to their surprising effectiveness in cross-lingual transfer. While there has been much work in evaluating these models for their performance on a variety of tasks and languages, little attention has been paid on how well calibrated these models are with respect to the confidence in their predictions. We first investigate the calibration of MMLMs in the zero-shot setting and observe a clear case of miscalibration in low-resource languages or those which are typologically diverse from English. Next, we empirically show that calibration methods like temperature scaling and label smoothing do reasonably well in improving calibration in the zero-shot scenario. We also find that few-shot examples in the language can further help reduce calibration errors, often substantially. Overall, our work contributes towards building more reliable multilingual models by highlighting the issue of their miscalibration, understanding what language and model-specific factors influence it, and pointing out the strategies to improve the same.</abstract>
      <url hash="dad30117">2022.emnlp-main.290</url>
      <bibkey>ahuja-etal-2022-calibration</bibkey>
    </paper>
    <paper id="291">
      <title>Momentum Contrastive Pre-training for Question Answering</title>
      <author><first>Minda</first><last>Hu</last></author>
      <author><first>Muzhi</first><last>Li</last></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Irwin</first><last>King</last></author>
      <pages>4324-4330</pages>
      <abstract>Existing pre-training methods for extractive Question Answering (QA) generate cloze-like queries different from natural questions in syntax structure, which could overfit pre-trained models to simple keyword matching. In order to address this problem, we propose a novel Momentum Contrastive pRe-training fOr queStion anSwering (MCROSS) method for extractive QA. Specifically, MCROSS introduces a momentum contrastive learning framework to align the answer probability between cloze-like and natural query-passage sample pairs. Hence, the pre-trained models can better transfer the knowledge learned in cloze-like samples to answering natural questions. Experimental results on three benchmarking QA datasets show that our method achieves noticeable improvement compared with all baselines in both supervised and zero-shot scenarios.</abstract>
      <url hash="9f59252a">2022.emnlp-main.291</url>
      <bibkey>hu-etal-2022-momentum</bibkey>
    </paper>
    <paper id="292">
      <title>A Second Wave of <fixed-case>UD</fixed-case> <fixed-case>H</fixed-case>ebrew Treebanking and Cross-Domain Parsing</title>
      <author><first>Amir</first><last>Zeldes</last></author>
      <author><first>Nick</first><last>Howell</last></author>
      <author><first>Noam</first><last>Ordan</last></author>
      <author><first>Yifat</first><last>Ben Moshe</last></author>
      <pages>4331-4344</pages>
      <abstract>Foundational Hebrew NLP tasks such as segmentation, tagging and parsing, have relied to date on various versions of the Hebrew Treebank (HTB, Sima’an et al. 2001). However, the data in HTB, a single-source newswire corpus, is now over 30 years old, and does not cover many aspects of contemporary Hebrew on the web. This paper presents a new, freely available UD treebank of Hebrew stratified from a range of topics selected from Hebrew Wikipedia. In addition to introducing the corpus and evaluating the quality of its annotations, we deploy automatic validation tools based on grew (Guillaume, 2021), and conduct the first cross domain parsing experiments in Hebrew. We obtain new state-of-the-art (SOTA) results on UD NLP tasks, using a combination of the latest language modelling and some incremental improvements to existing transformer based approaches. We also release a new version of the UD HTB matching annotation scheme updates from our new corpus.</abstract>
      <url hash="aef3423b">2022.emnlp-main.292</url>
      <bibkey>zeldes-etal-2022-second</bibkey>
    </paper>
    <paper id="293">
      <title>Finding Dataset Shortcuts with Grammar Induction</title>
      <author><first>Dan</first><last>Friedman</last></author>
      <author><first>Alexander</first><last>Wettig</last></author>
      <author><first>Danqi</first><last>Chen</last></author>
      <pages>4345-4363</pages>
      <abstract>Many NLP datasets have been found to contain shortcuts: simple decision rules that achieve surprisingly high accuracy. However, it is difficult to discover shortcuts automatically. Prior work on automatic shortcut detection has focused on enumerating features like unigrams or bigrams, which can find only low-level shortcuts, or relied on post-hoc model interpretability methods like saliency maps, which reveal qualitative patterns without a clear statistical interpretation. In this work, we propose to use probabilistic grammars to characterize and discover shortcuts in NLP datasets. Specifically, we use a context-free grammar to model patterns in sentence classification datasets and use a synchronous context-free grammar to model datasets involving sentence pairs. The resulting grammars reveal interesting shortcut features in a number of datasets, including both simple and high-level features, and automatically identify groups of test examples on which conventional classifiers fail. Finally, we show that the features we discover can be used to generate diagnostic contrast examples and incorporated into standard robust optimization methods to improve worst-group accuracy.</abstract>
      <url hash="8a0a02ff">2022.emnlp-main.293</url>
      <bibkey>friedman-etal-2022-finding</bibkey>
    </paper>
    <paper id="294">
      <title>Retrieval Augmentation for Commonsense Reasoning: A Unified Approach</title>
      <author><first>Wenhao</first><last>Yu</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Zhihan</first><last>Zhang</last></author>
      <author><first>Shuohang</first><last>Wang</last></author>
      <author><first>Zhuosheng</first><last>Zhang</last></author>
      <author><first>Yuwei</first><last>Fang</last></author>
      <author><first>Meng</first><last>Jiang</last></author>
      <pages>4364-4377</pages>
      <abstract>A common thread of retrieval-augmented methods in the existing literature focuses on retrieving encyclopedic knowledge, such as Wikipedia, which facilitates well-defined entity and relation spaces that can be modeled. However, applying such methods to commonsense reasoning tasks faces two unique challenges, i.e., the lack of a general large-scale corpus for retrieval and a corresponding effective commonsense retriever. In this paper, we systematically investigate how to leverage commonsense knowledge retrieval to improve commonsense reasoning tasks. We proposed a unified framework of retrieval-augmented commonsense reasoning (called RACo), including a newly constructed commonsense corpus with over 20 million documents and novel strategies for training a commonsense retriever. We conducted experiments on four different commonsense reasoning tasks. Extensive evaluation results showed that our proposed RACo can significantly outperform other knowledge-enhanced method counterparts, achieving new SoTA performance on the CommonGen and CREAK leaderboards.</abstract>
      <url hash="3fa25878">2022.emnlp-main.294</url>
      <bibkey>yu-etal-2022-retrieval</bibkey>
    </paper>
    <paper id="295">
      <title>Open World Classification with Adaptive Negative Samples</title>
      <author><first>Ke</first><last>Bai</last></author>
      <author><first>Guoyin</first><last>Wang</last></author>
      <author><first>Jiwei</first><last>Li</last></author>
      <author><first>Sunghyun</first><last>Park</last></author>
      <author><first>Sungjin</first><last>Lee</last></author>
      <author><first>Puyang</first><last>Xu</last></author>
      <author><first>Ricardo</first><last>Henao</last></author>
      <author><first>Lawrence</first><last>Carin</last></author>
      <pages>4378-4392</pages>
      <abstract>Open world classification is a task in natural language processing with key practical relevance and impact.Since the open or unknown category data only manifests in the inference phase, finding a model with a suitable decision boundary accommodating for the identification of known classes and discrimination of the open category is challenging.The performance of existing models is limited by the lack of effective open category data during the training stage or the lack of a good mechanism to learn appropriate decision boundaries.We propose an approach based on Adaptive Negative Samples (ANS) designed to generate effective synthetic open category samples in the training stage and without requiring any prior knowledge or external datasets.Empirically, we find a significant advantage in using auxiliary one-versus-rest binary classifiers, which effectively utilize the generated negative samples and avoid the complex threshold-seeking stage in previous works.Extensive experiments on three benchmark datasets show that ANS achieves significant improvements over state-of-the-art methods.</abstract>
      <url hash="94d64af7">2022.emnlp-main.295</url>
      <bibkey>bai-etal-2022-open</bibkey>
    </paper>
    <paper id="296">
      <title>Re3: Generating Longer Stories With Recursive Reprompting and Revision</title>
      <author><first>Kevin</first><last>Yang</last></author>
      <author><first>Yuandong</first><last>Tian</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <author><first>Dan</first><last>Klein</last></author>
      <pages>4393-4479</pages>
      <abstract>We consider the problem of automatically generating longer stories of over two thousand words. Compared to prior work on shorter stories, long-range plot coherence and relevance are more central challenges here. We propose the Recursive Reprompting and Revision framework (Re3) to address these challenges by (a) prompting a general-purpose language model to construct a structured overarching plan, and (b) generating story passages by repeatedly injecting contextual information from both the plan and current story state into a language model prompt. We then revise by (c) reranking different continuations for plot coherence and premise relevance, and finally (d) editing the best continuation for factual consistency. Compared to similar-length stories generated directly from the same base model, human evaluators judged substantially more of Re3’s stories as having a coherent overarching plot (by 14% absolute increase), and relevant to the given initial premise (by 20%).</abstract>
      <url hash="3bd78236">2022.emnlp-main.296</url>
      <bibkey>yang-etal-2022-re3</bibkey>
    </paper>
    <paper id="297">
      <title>Does Joint Training Really Help Cascaded Speech Translation?</title>
      <author><first>Viet Anh Khoa</first><last>Tran</last></author>
      <author><first>David</first><last>Thulke</last></author>
      <author><first>Yingbo</first><last>Gao</last></author>
      <author><first>Christian</first><last>Herold</last></author>
      <author><first>Hermann</first><last>Ney</last></author>
      <pages>4480-4487</pages>
      <abstract>Currently, in speech translation, the straightforward approach - cascading a recognition system with a translation system - delivers state-of-the-art results.However, fundamental challenges such as error propagation from the automatic speech recognition system still remain.To mitigate these problems, recently, people turn their attention to direct data and propose various joint training methods.In this work, we seek to answer the question of whether joint training really helps cascaded speech translation.We review recent papers on the topic and also investigate a joint training criterion by marginalizing the transcription posterior probabilities.Our findings show that a strong cascaded baseline can diminish any improvements obtained using joint training, and we suggest alternatives to joint training.We hope this work can serve as a refresher of the current speech translation landscape, and motivate research in finding more efficient and creative ways to utilize the direct data for speech translation.</abstract>
      <url hash="6cacfc41">2022.emnlp-main.297</url>
      <bibkey>tran-etal-2022-joint</bibkey>
    </paper>
    <paper id="298">
      <title><fixed-case>M</fixed-case>asakha<fixed-case>NER</fixed-case> 2.0: <fixed-case>A</fixed-case>frica-centric Transfer Learning for Named Entity Recognition</title>
      <author><first>David</first><last>Adelani</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <author><first>Sebastian</first><last>Ruder</last></author>
      <author><first>Shruti</first><last>Rijhwani</last></author>
      <author><first>Michael</first><last>Beukman</last></author>
      <author><first>Chester</first><last>Palen-Michel</last></author>
      <author><first>Constantine</first><last>Lignos</last></author>
      <author><first>Jesujoba</first><last>Alabi</last></author>
      <author><first>Shamsuddeen</first><last>Muhammad</last></author>
      <author><first>Peter</first><last>Nabende</last></author>
      <author><first>Cheikh M. Bamba</first><last>Dione</last></author>
      <author><first>Andiswa</first><last>Bukula</last></author>
      <author><first>Rooweither</first><last>Mabuya</last></author>
      <author><first>Bonaventure F. P.</first><last>Dossou</last></author>
      <author><first>Blessing</first><last>Sibanda</last></author>
      <author><first>Happy</first><last>Buzaaba</last></author>
      <author><first>Jonathan</first><last>Mukiibi</last></author>
      <author><first>Godson</first><last>Kalipe</last></author>
      <author><first>Derguene</first><last>Mbaye</last></author>
      <author><first>Amelia</first><last>Taylor</last></author>
      <author><first>Fatoumata</first><last>Kabore</last></author>
      <author><first>Chris Chinenye</first><last>Emezue</last></author>
      <author><first>Anuoluwapo</first><last>Aremu</last></author>
      <author><first>Perez</first><last>Ogayo</last></author>
      <author><first>Catherine</first><last>Gitau</last></author>
      <author><first>Edwin</first><last>Munkoh-Buabeng</last></author>
      <author><first>Victoire</first><last>Memdjokam Koagne</last></author>
      <author><first>Allahsera Auguste</first><last>Tapo</last></author>
      <author><first>Tebogo</first><last>Macucwa</last></author>
      <author><first>Vukosi</first><last>Marivate</last></author>
      <author><first>Mboning Tchiaze</first><last>Elvis</last></author>
      <author><first>Tajuddeen</first><last>Gwadabe</last></author>
      <author><first>Tosin</first><last>Adewumi</last></author>
      <author><first>Orevaoghene</first><last>Ahia</last></author>
      <author><first>Joyce</first><last>Nakatumba-Nabende</last></author>
      <pages>4488-4508</pages>
      <abstract>African languages are spoken by over a billion people, but they are under-represented in NLP research and development. Multiple challenges exist, including the limited availability of annotated training and evaluation datasets as well as the lack of understanding of which settings, languages, and recently proposed methods like cross-lingual transfer will be effective. In this paper, we aim to move towards solutions for these challenges, focusing on the task of named entity recognition (NER). We present the creation of the largest to-date human-annotated NER dataset for 20 African languages. We study the behaviour of state-of-the-art cross-lingual transfer methods in an Africa-centric setting, empirically demonstrating that the choice of source transfer language significantly affects performance. While much previous work defaults to using English as the source language, our results show that choosing the best transfer language improves zero-shot F1 scores by an average of 14% over 20 languages as compared to using English.</abstract>
      <url hash="a1a9b4ef">2022.emnlp-main.298</url>
      <bibkey>adelani-etal-2022-masakhaner</bibkey>
    </paper>
    <paper id="299">
      <title>Ethics consideration sections in natural language processing papers</title>
      <author><first>Luciana</first><last>Benotti</last></author>
      <author><first>Patrick</first><last>Blackburn</last></author>
      <pages>4509-4516</pages>
      <abstract>In this paper, we present the results of a manual classification of all ethical consideration sections for ACL 2021. We also compare how many papers had an ethics consideration section per track and per world region in ACL 2021. We classified papers according to the ethical issues covered (research benefits, potential harms, and vulnerable groups affected) and whether the paper was marked as requiring ethics review by at least one reviewer. Moreover, we discuss recurring obstacles we have observed (highlighting some interesting texts we found along the way) and conclude with three suggestions. We think that this paper may be useful for anyone who needs to write — or review — an ethics section and would like to get an overview of what others have done.</abstract>
      <url hash="0b089886">2022.emnlp-main.299</url>
      <bibkey>benotti-blackburn-2022-ethics</bibkey>
    </paper>
    <paper id="300">
      <title>Continued Pretraining for Better Zero- and Few-Shot Promptability</title>
      <author><first>Zhaofeng</first><last>Wu</last></author>
      <author><first>Robert L</first><last>Logan IV</last></author>
      <author><first>Pete</first><last>Walsh</last></author>
      <author><first>Akshita</first><last>Bhagia</last></author>
      <author><first>Dirk</first><last>Groeneveld</last></author>
      <author><first>Sameer</first><last>Singh</last></author>
      <author><first>Iz</first><last>Beltagy</last></author>
      <pages>4517-4531</pages>
      <abstract>Recently introduced language model prompting methods can achieve high accuracy in zero- and few-shot settings while requiring few to no learned task-specific parameters. Nevertheless, these methods still often trail behind full model finetuning. In this work, we investigate if a dedicated continued pretraining stage could improve “promptability”, i.e., zero-shot performance with natural language prompts or few-shot performance with prompt tuning. We reveal settings where existing continued pretraining methods lack promptability. We also identify current methodological gaps, which we fill with thorough large-scale experiments. We demonstrate that a simple recipe, continued pretraining that incorporates a trainable prompt during multi-task learning, leads to improved promptability in both zero- and few-shot settings compared to existing methods, up to 31% relative. On the other hand, we find that continued pretraining using MAML-style meta-learning, a method that directly optimizes few-shot promptability, yields subpar performance. We validate our findings with two prompt tuning methods, and, based on our results, we provide concrete recommendations to optimize promptability for different use cases.</abstract>
      <url hash="3d94d147">2022.emnlp-main.300</url>
      <bibkey>wu-etal-2022-continued</bibkey>
    </paper>
    <paper id="301">
      <title>Less is More: Summary of Long Instructions is Better for Program Synthesis</title>
      <author><first>Kirby</first><last>Kuznia</last></author>
      <author><first>Swaroop</first><last>Mishra</last></author>
      <author><first>Mihir</first><last>Parmar</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <pages>4532-4552</pages>
      <abstract>Despite the success of large pre-trained language models (LMs) such as Codex, they show below-par performance on the larger and more complicated programming related questions. We show that LMs benefit from the summarized version of complicated questions. Our findings show that superfluous information often present in problem description such as human characters, background stories, and names (which are included to help humans in understanding a task) does not help models in understanding a task. To this extent, we create a meta-dataset from the frequently used APPS dataset and the newly created CodeContests dataset for the program synthesis task. Our meta-dataset consists of human and synthesized summaries of the long and complicated programming questions. Experimental results on Codex show that our proposed approach outperforms baseline by 8.13% on the APPS dataset and 11.88% on the CodeContests dataset on an average in terms of strict accuracy. Our analysis shows that summaries significantly improve performance for introductory (9.86%) and interview (11.48%) related programming questions. However, it shows improvement by a small margin ( 2%) for competitive programming questions, implying the scope for future research direction.</abstract>
      <url hash="e6043c61">2022.emnlp-main.301</url>
      <bibkey>kuznia-etal-2022-less</bibkey>
    </paper>
    <paper id="302">
      <title>Is a Question Decomposition Unit All We Need?</title>
      <author><first>Pruthvi</first><last>Patel</last></author>
      <author><first>Swaroop</first><last>Mishra</last></author>
      <author><first>Mihir</first><last>Parmar</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <pages>4553-4569</pages>
      <abstract>Large Language Models (LMs) have achieved state-of-the-art performance on many Natural Language Processing (NLP) benchmarks. With the growing number of new benchmarks, we build bigger and more complex LMs. However, building new LMs may not be an ideal option owing to the cost, time and environmental impact associated with it. We explore an alternative route: can we modify data by expressing it in terms of the model’s strengths, so that a question becomes easier for models to answer? We investigate if humans can decompose a hard question into a set of simpler questions that are relatively easier for models to solve. We analyze a range of datasets involving various forms of reasoning and find that it is indeed possible to significantly improve model performance (24% for GPT3 and 29% for RoBERTa-SQuAD along with a symbolic calculator) via decomposition. Our approach provides a viable option to involve people in NLP research in a meaningful way. Our findings indicate that Human-in-the-loop Question Decomposition (HQD) can potentially provide an alternate path to building large LMs.</abstract>
      <url hash="5b9f9580">2022.emnlp-main.302</url>
      <bibkey>patel-etal-2022-question</bibkey>
    </paper>
    <paper id="303">
      <title>Discourse-Aware Soft Prompting for Text Generation</title>
      <author><first>Marjan</first><last>Ghazvininejad</last></author>
      <author><first>Vladimir</first><last>Karpukhin</last></author>
      <author><first>Vera</first><last>Gor</last></author>
      <author><first>Asli</first><last>Celikyilmaz</last></author>
      <pages>4570-4589</pages>
      <abstract>Current efficient fine-tuning methods(e.g., adapters, prefix-tuning, etc.) have optimized conditional text generation via training a small set of extra parameters of the neural language model, while freezing the rest for efficiency. While showing strong performance on some generation tasks, they don’t generalize across all generation tasks. We show that soft-prompt based conditional text generation can be improved with simple and efficient methods that simulate modeling the discourse structure of human written text.We investigate two design choices: First, we apply hierarchical blocking on the prefix parameters to simulate a higher-level discourse structure of human written text. Second, we apply attention sparsity on the prefix parameters at different layers of the network and learn sparse transformations on the softmax-function. We show that structured design of prefix parameters yields more coherent, faithful and relevant generations than the baseline prefix-tuning on all generation tasks.</abstract>
      <url hash="f45eb2d5">2022.emnlp-main.303</url>
      <bibkey>ghazvininejad-etal-2022-discourse</bibkey>
    </paper>
    <paper id="304">
      <title><fixed-case>E</fixed-case>x<fixed-case>PUN</fixed-case>ations: Augmenting Puns with Keywords and Explanations</title>
      <author><first>Jiao</first><last>Sun</last></author>
      <author><first>Anjali</first><last>Narayan-Chen</last></author>
      <author><first>Shereen</first><last>Oraby</last></author>
      <author><first>Alessandra</first><last>Cervone</last></author>
      <author><first>Tagyoung</first><last>Chung</last></author>
      <author><first>Jing</first><last>Huang</last></author>
      <author id="yang-liu-microsoft"><first>Yang</first><last>Liu</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>4590-4605</pages>
      <abstract>The tasks of humor understanding and generation are challenging and subjective even for humans, requiring commonsense and real-world knowledge to master. Puns, in particular, add the challenge of fusing that knowledge with the ability to interpret lexical-semantic ambiguity. In this paper, we present the ExPUNations (ExPUN) dataset, in which we augment an existing dataset of puns with detailed crowdsourced annotations of keywords denoting the most distinctive words that make the text funny, pun explanations describing why the text is funny, and fine-grained funniness ratings. This is the first humor dataset with such extensive and fine-grained annotations specifically for puns. Based on these annotations, we propose two tasks: explanation generation to aid with pun classification and keyword-conditioned pun generation, to challenge the current state-of-the-art natural language understanding and generation models’ ability to understand and generate humor. We showcase that the annotated keywords we collect are helpful for generating better novel humorous texts in human evaluation, and that our natural language explanations can be leveraged to improve both the accuracy and robustness of humor classifiers.</abstract>
      <url hash="b038fdba">2022.emnlp-main.304</url>
      <bibkey>sun-etal-2022-expunations</bibkey>
    </paper>
    <paper id="305">
      <title><fixed-case>SLING</fixed-case>: <fixed-case>S</fixed-case>ino Linguistic Evaluation of Large Language Models</title>
      <author><first>Yixiao</first><last>Song</last></author>
      <author><first>Kalpesh</first><last>Krishna</last></author>
      <author><first>Rajesh</first><last>Bhatt</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <pages>4606-4634</pages>
      <abstract>To understand what kinds of linguistic knowledge are encoded by pretrained Chinese language models (LMs), we introduce the benchmark of Sino LINGuistics (SLING), which consists of 38K minimal sentence pairs in Mandarin Chinese grouped into 9 high-level linguistic phenomena. Each pair demonstrates the acceptability contrast of a specific syntactic or semantic phenomenon (e.g., The keys are lost vs. The keys is lost), and an LM should assign lower perplexity to the acceptable sentence. In contrast to the CLiMP dataset (Xiang et al., 2021), which also contains Chinese minimal pairs and was created by translating the vocabulary of the English BLiMP dataset, the minimal pairs in SLING are derived primarily by applying syntactic and lexical transformations to naturally-occurring, linguist-annotated sentences from the Chinese Treebank 9.0, thus addressing severe issues in CLiMP’s data generation process. We test 18 publicly available pretrained monolingual (e.g., BERT-base-zh, CPM) and multi-lingual (e.g., mT5, XLM) language models on SLING. Our experiments show that the average accuracy for LMs is far below human performance (69.7% vs. 97.1%), while BERT-base-zh achieves the highest accuracy (84.8%) of all tested LMs, even much larger ones. Additionally, we find that most LMs have a strong gender and number (singular/plural) bias, and they perform better on local phenomena than hierarchical ones.</abstract>
      <url hash="f2d99667">2022.emnlp-main.305</url>
      <bibkey>song-etal-2022-sling</bibkey>
    </paper>
    <paper id="306">
      <title>Context-Situated Pun Generation</title>
      <author><first>Jiao</first><last>Sun</last></author>
      <author><first>Anjali</first><last>Narayan-Chen</last></author>
      <author><first>Shereen</first><last>Oraby</last></author>
      <author><first>Shuyang</first><last>Gao</last></author>
      <author><first>Tagyoung</first><last>Chung</last></author>
      <author><first>Jing</first><last>Huang</last></author>
      <author id="yang-liu-ict"><first>Yang</first><last>Liu</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>4635-4648</pages>
      <abstract>Previous work on pun generation commonly begins with a given pun word (a pair of homophones for heterographic pun generation and a polyseme for homographic pun generation) and seeks to generate an appropriate pun. While this may enable efficient pun generation, we believe that a pun is most entertaining if it fits appropriately within a given context, e.g., a given situation or dialogue. In this work, we propose a new task, context-situated pun generation, where a specific context represented by a set of keywords is provided, and the task is to first identify suitable pun words that are appropriate for the context, then generate puns based on the context keywords and the identified pun words. We collect a new dataset, CUP (Context-sitUated Pun), containing 4.5k tuples of context words and pun pairs. Based on the new data and setup, we propose a pipeline system for context-situated pun generation, including a pun word retrieval module that identifies suitable pun words for a given context, and a pun generation module that generates puns from context keywords and pun words. Human evaluation shows that 69% of our top retrieved pun words can be used to generate context-situated puns, and our generation module yields successful puns 31% of the time given a plausible tuple of context words and pun pair, almost tripling the yield of a state-of-the-art pun generation model. With an end-to-end evaluation, our pipeline system with the top-1 retrieved pun pair for a given context can generate successful puns 40% of the time, better than all other modeling variations but 32% lower than the human success rate. This highlights the difficulty of the task, and encourages more research in this direction.</abstract>
      <url hash="e365ce00">2022.emnlp-main.306</url>
      <bibkey>sun-etal-2022-context</bibkey>
    </paper>
    <paper id="307">
      <title>Retrieval-Augmented Generative Question Answering for Event Argument Extraction</title>
      <author><first>Xinya</first><last>Du</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <pages>4649-4666</pages>
      <abstract>Event argument extraction has long been studied as a sequential prediction problem with extractive-based methods, tackling each argument in isolation. Although recent work proposes generation-based methods to capture cross-argument dependency, they require generating and post-processing a complicated target sequence (template). Motivated by these observations and recent pretrained language models’ capabilities of learning from demonstrations. We propose a retrieval-augmented generative QA model (R-GQA) for event argument extraction. It retrieves the most similar QA pair and augments it as prompt to the current example’s context, then decodes the arguments as answers. Our approach outperforms substantially prior methods across various settings (i.e. fully supervised, domain transfer, and fewshot learning). Finally, we propose a clustering-based sampling strategy (JointEnc) and conduct a thorough analysis of how different strategies influence the few-shot learning performances.</abstract>
      <url hash="8963c8bf">2022.emnlp-main.307</url>
      <bibkey>du-ji-2022-retrieval</bibkey>
    </paper>
    <paper id="308">
      <title>Concadia: Towards Image-Based Text Generation with a Purpose</title>
      <author><first>Elisa</first><last>Kreiss</last></author>
      <author><first>Fei</first><last>Fang</last></author>
      <author><first>Noah</first><last>Goodman</last></author>
      <author><first>Christopher</first><last>Potts</last></author>
      <pages>4667-4684</pages>
      <abstract>Current deep learning models often achieve excellent results on benchmark image-to-text datasets but fail to generate texts that are useful in practice. We argue that to close this gap, it is vital to distinguish descriptions from captions based on their distinct communicative roles. Descriptions focus on visual features and are meant to replace an image (often to increase accessibility), whereas captions appear alongside an image to supply additional information. To motivate this distinction and help people put it into practice, we introduce the publicly available Wikipedia-based dataset Concadia consisting of 96,918 images with corresponding English-language descriptions, captions, and surrounding context. Using insights from Concadia, models trained on it, and a preregistered human-subjects experiment with human- and model-generated texts, we characterize the commonalities and differences between descriptions and captions. In addition, we show that, for generating both descriptions and captions, it is useful to augment image-to-text models with representations of the textual context in which the image appeared.</abstract>
      <url hash="890a49a5">2022.emnlp-main.308</url>
      <bibkey>kreiss-etal-2022-concadia</bibkey>
    </paper>
    <paper id="309">
      <title>Context Matters for Image Descriptions for Accessibility: Challenges for Referenceless Evaluation Metrics</title>
      <author><first>Elisa</first><last>Kreiss</last></author>
      <author><first>Cynthia</first><last>Bennett</last></author>
      <author><first>Shayan</first><last>Hooshmand</last></author>
      <author><first>Eric</first><last>Zelikman</last></author>
      <author><first>Meredith</first><last>Ringel Morris</last></author>
      <author><first>Christopher</first><last>Potts</last></author>
      <pages>4685-4697</pages>
      <abstract>Few images on the Web receive alt-text descriptions that would make them accessible to blind and low vision (BLV) users. Image-based NLG systems have progressed to the point where they can begin to address this persistent societal problem, but these systems will not be fully successful unless we evaluate them on metrics that guide their development correctly. Here, we argue against current referenceless metrics – those that don’t rely on human-generated ground-truth descriptions – on the grounds that they do not align with the needs of BLV users. The fundamental shortcoming of these metrics is that they do not take context into account, whereas contextual information is highly valued by BLV users. To substantiate these claims, we present a study with BLV participants who rated descriptions along a variety of dimensions. An in-depth analysis reveals that the lack of context-awareness makes current referenceless metrics inadequate for advancing image accessibility. As a proof-of-concept, we provide a contextual version of the referenceless metric CLIPScore which begins to address the disconnect to the BLV data.</abstract>
      <url hash="1d8861ec">2022.emnlp-main.309</url>
      <bibkey>kreiss-etal-2022-context</bibkey>
    </paper>
    <paper id="310">
      <title><fixed-case>M</fixed-case>eta<fixed-case>L</fixed-case>ogic: Logical Reasoning Explanations with Fine-Grained Structure</title>
      <author><first>Yinya</first><last>Huang</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Ruixin</first><last>Hong</last></author>
      <author><first>Xiaodan</first><last>Liang</last></author>
      <author><first>Changshui</first><last>Zhang</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <pages>4698-4724</pages>
      <abstract>In this paper, we propose a comprehensive benchmark to investigate models’ logical reasoning capabilities in complex real-life scenarios. Current explanation datasets often employ synthetic data with simple reasoning structures. Therefore, it cannot express more complex reasoning processes, such as the rebuttal to a reasoning step and the degree of certainty of the evidence. To this end, we propose a comprehensive logical reasoning explanation form. Based on the multi-hop chain of reasoning, the explanation form includes three main components: (1) The condition of rebuttal that the reasoning node can be challenged; (2) Logical formulae that uncover the internal texture of reasoning nodes; (3) Reasoning strength indicated by degrees of certainty. The fine-grained structure conforms to the real logical reasoning scenario, better fitting the human cognitive process but, simultaneously, is more challenging for the current models. We evaluate the current best models’ performance on this new explanation form. The experimental results show that generating reasoning graphs remains a challenging task for current models, even with the help of giant pre-trained language models.</abstract>
      <url hash="0c87ccc8">2022.emnlp-main.310</url>
      <bibkey>huang-etal-2022-metalogic</bibkey>
    </paper>
    <paper id="311">
      <title>Explicit Query Rewriting for Conversational Dense Retrieval</title>
      <author><first>Hongjin</first><last>Qian</last></author>
      <author><first>Zhicheng</first><last>Dou</last></author>
      <pages>4725-4737</pages>
      <abstract>In a conversational search scenario, a query might be context-dependent because some words are referred to previous expressions or omitted. Previous works tackle the issue by either reformulating the query into a self-contained query (query rewriting) or learning a contextualized query embedding from the query context (context modelling). In this paper, we propose a model CRDR that can perform query rewriting and context modelling in a unified framework in which the query rewriting’s supervision signals further enhance the context modelling. Instead of generating a new query, CRDR only performs necessary modifications on the original query, which improves both accuracy and efficiency of query rewriting. In the meantime, the query rewriting benefits the context modelling by explicitly highlighting relevant terms in the query context, which improves the quality of the learned contextualized query embedding. To verify the effectiveness of CRDR, we perform comprehensive experiments on TREC CAsT-19 and TREC CAsT-20 datasets, and the results show that our method outperforms all baseline models in terms of both quality of query rewriting and quality of context-aware ranking.</abstract>
      <url hash="2bdc65eb">2022.emnlp-main.311</url>
      <bibkey>qian-dou-2022-explicit</bibkey>
    </paper>
    <paper id="312">
      <title>Efficient Nearest Neighbor Emotion Classification with <fixed-case>BERT</fixed-case>-whitening</title>
      <author><first>Wenbiao</first><last>Yin</last></author>
      <author><first>Lin</first><last>Shang</last></author>
      <pages>4738-4745</pages>
      <abstract>Retrieval-based methods have been proven effective in many NLP tasks. Previous methods use representations from the pre-trained model for similarity search directly. However, the sentence representations from the pre-trained model like BERT perform poorly in retrieving semantically similar sentences, resulting in poor performance of the retrieval-based methods. In this paper, we propose kNN-EC, a simple and efficient non-parametric emotion classification (EC) method using nearest neighbor retrieval. We use BERT-whitening to get better sentence semantics, ensuring that nearest neighbor retrieval works. Meanwhile, BERT-whitening can also reduce memory storage of datastore and accelerate retrieval speed, solving the efficiency problem of the previous methods. kNN-EC average improves the pre-trained model by 1.17 F1-macro on two emotion classification datasets.</abstract>
      <url hash="27fc7c12">2022.emnlp-main.312</url>
      <bibkey>yin-shang-2022-efficient</bibkey>
    </paper>
    <paper id="313">
      <title><fixed-case>F</fixed-case>ast<fixed-case>C</fixed-case>lass: A Time-Efficient Approach to Weakly-Supervised Text Classification</title>
      <author><first>Tingyu</first><last>Xia</last></author>
      <author><first>Yue</first><last>Wang</last></author>
      <author><first>Yuan</first><last>Tian</last></author>
      <author><first>Yi</first><last>Chang</last></author>
      <pages>4746-4758</pages>
      <abstract>Weakly-supervised text classification aims to train a classifier using only class descriptions and unlabeled data. Recent research shows that keyword-driven methods can achieve state-of-the-art performance on various tasks. However, these methods not only rely on carefully-crafted class descriptions to obtain class-specific keywords but also require substantial amount of unlabeled data and takes a long time to train. This paper proposes FastClass, an efficient weakly-supervised classification approach. It uses dense text representation to retrieve class-relevant documents from external unlabeled corpus and selects an optimal subset to train a classifier. Compared to keyword-driven methods, our approach is less reliant on initial class descriptions as it no longer needs to expand each class description into a set of class-specific keywords.Experiments on a wide range of classification tasks show that the proposed approach frequently outperforms keyword-driven models in terms of classification accuracy and often enjoys orders-of-magnitude faster training speed.</abstract>
      <url hash="beb61fee">2022.emnlp-main.313</url>
      <bibkey>xia-etal-2022-fastclass</bibkey>
    </paper>
    <paper id="314">
      <title>Neural-Symbolic Inference for Robust Autoregressive Graph Parsing via Compositional Uncertainty Quantification</title>
      <author><first>Zi</first><last>Lin</last></author>
      <author><first>Jeremiah</first><last>Liu</last></author>
      <author><first>Jingbo</first><last>Shang</last></author>
      <pages>4759-4776</pages>
      <abstract>Pre-trained seq2seq models excel at graph semantic parsing with rich annotated data, but generalize worse to out-of-distribution (OOD) and long-tail examples. In comparison, symbolic parsers under-perform on population-level metrics, but exhibit unique strength in OOD and tail generalization. In this work, we study compositionality-aware approach to neural-symbolic inference informed by model confidence, performing fine-grained neural-symbolic reasoning at subgraph level (i.e., nodes and edges) and precisely targeting subgraph components with high uncertainty in the neural parser. As a result, the method combines the distinct strength of the neural and symbolic approaches in capturing different aspects of the graph prediction, leading to well-rounded generalization performance both across domains and in the tail. We empirically investigate the approach in the English Resource Grammar (ERG) parsing problem on a diverse suite of standard in-domain and seven OOD corpora. Our approach leads to 35.26% and 35.60% error reduction in aggregated SMATCH score over neural and symbolic approaches respectively, and 14% absolute accuracy gain in key tail linguistic categories over the neural model, outperforming prior state-of-art methods that do not account for compositionality or uncertainty.</abstract>
      <url hash="e869bddc">2022.emnlp-main.314</url>
      <bibkey>lin-etal-2022-neural</bibkey>
    </paper>
    <paper id="315">
      <title>A Speaker-Aware Co-Attention Framework for Medical Dialogue Information Extraction</title>
      <author><first>Yuan</first><last>Xia</last></author>
      <author><first>Zhenhui</first><last>Shi</last></author>
      <author><first>Jingbo</first><last>Zhou</last></author>
      <author><first>Jiayu</first><last>Xu</last></author>
      <author><first>Chao</first><last>Lu</last></author>
      <author><first>Yehui</first><last>Yang</last></author>
      <author><first>Lei</first><last>Wang</last></author>
      <author><first>Haifeng</first><last>Huang</last></author>
      <author><first>Xia</first><last>Zhang</last></author>
      <author><first>Junwei</first><last>Liu</last></author>
      <pages>4777-4786</pages>
      <abstract>With the development of medical digitization, the extraction and structuring of Electronic Medical Records (EMRs) have become challenging but fundamental tasks. How to accurately and automatically extract structured information from medical dialogues is especially difficult because the information needs to be inferred from complex interactions between the doctor and the patient. To this end, in this paper, we propose a speaker-aware co-attention framework for medical dialogue information extraction. To better utilize the pre-trained language representation model to perceive the semantics of the utterance and the candidate item, we develop a speaker-aware dialogue encoder with multi-task learning, which considers the speaker’s identity into account. To deal with complex interactions between different utterances and the correlations between utterances and candidate items, we propose a co-attention fusion network to aggregate the utterance information. We evaluate our framework on the public medical dialogue extraction datasets to demonstrate the superiority of our method, which can outperform the state-of-the-art methods by a large margin. Codes will be publicly available upon acceptance.</abstract>
      <url hash="c1d2d083">2022.emnlp-main.315</url>
      <bibkey>xia-etal-2022-speaker</bibkey>
    </paper>
    <paper id="316">
      <title>Towards Interactivity and Interpretability: A Rationale-based Legal Judgment Prediction Framework</title>
      <author><first>Yiquan</first><last>Wu</last></author>
      <author><first>Yifei</first><last>Liu</last></author>
      <author><first>Weiming</first><last>Lu</last></author>
      <author><first>Yating</first><last>Zhang</last></author>
      <author><first>Jun</first><last>Feng</last></author>
      <author><first>Changlong</first><last>Sun</last></author>
      <author><first>Fei</first><last>Wu</last></author>
      <author><first>Kun</first><last>Kuang</last></author>
      <pages>4787-4799</pages>
      <abstract>Legal judgment prediction (LJP) is a fundamental task in legal AI, which aims to assist the judge to hear the case and determine the judgment. The legal judgment usually consists of the law article, charge, and term of penalty. In the real trial scenario, the judge usually makes the decision step-by-step: first concludes the rationale according to the case’s facts and then determines the judgment. Recently, many models have been proposed and made tremendous progress in LJP, but most of them adopt an end-to-end manner that cannot be manually intervened by the judge for practical use. Moreover, existing models lack interpretability due to the neglect of rationale in the prediction process. Following the judge’s real trial logic, in this paper, we propose a novel Rationale-based Legal Judgment Prediction (RLJP) framework. In the RLJP framework, the LJP process is split into two steps. In the first phase, the model generates the rationales according to the fact description. Then it predicts the judgment based on the fact and the generated rationales. Extensive experiments on a real-world dataset show RLJP achieves the best results compared to the state-of-the-art models. Meanwhile, the proposed framework provides good interactivity and interpretability which enables practical use.</abstract>
      <url hash="1ae2867c">2022.emnlp-main.316</url>
      <bibkey>wu-etal-2022-towards</bibkey>
    </paper>
    <paper id="317">
      <title><fixed-case>R</fixed-case>el<fixed-case>CLIP</fixed-case>: Adapting Language-Image Pretraining for Visual Relationship Detection via Relational Contrastive Learning</title>
      <author><first>Yi</first><last>Zhu</last></author>
      <author><first>Zhaoqing</first><last>Zhu</last></author>
      <author><first>Bingqian</first><last>Lin</last></author>
      <author><first>Xiaodan</first><last>Liang</last></author>
      <author><first>Feng</first><last>Zhao</last></author>
      <author><first>Jianzhuang</first><last>Liu</last></author>
      <pages>4800-4810</pages>
      <abstract>Conventional visual relationship detection models only use the numeric ids of relation labels for training, but ignore the semantic correlation between the labels, which leads to severe training biases and harms the generalization ability of representations. In this paper, we introduce compact language information of relation labels for regularizing the representation learning of visual relations. Specifically, we propose a simple yet effective visual Relationship prediction framework that transfers natural language knowledge learned from Contrastive Language-Image Pre-training (CLIP) models to enhance the relationship prediction, termed RelCLIP. Benefiting from the powerful visual-semantic alignment ability of CLIP at image level, we introduce a novel Relational Contrastive Learning (RCL) approach which explores relation-level visual-semantic alignment via learning to match cross-modal relational embeddings. By collaboratively learning the semantic coherence and discrepancy from relation triplets, the model can generate more discriminative and robust representations. Experimental results on the Visual Genome dataset show that RelCLIP achieves significant improvements over strong baselines under full (provide accurate labels) and distant supervision (provide noise labels), demonstrating its powerful generalization ability in learning relationship representations. Code will be available at https://gitee.com/mindspore/models/tree/master/research/cv/RelCLIP.</abstract>
      <url hash="b95f1215">2022.emnlp-main.317</url>
      <bibkey>zhu-etal-2022-relclip</bibkey>
    </paper>
    <paper id="318">
      <title>Candidate Soups: Fusing Candidate Results Improves Translation Quality for Non-Autoregressive Translation</title>
      <author><first>Huanran</first><last>Zheng</last></author>
      <author><first>Wei</first><last>Zhu</last></author>
      <author><first>Pengfei</first><last>Wang</last></author>
      <author><first>Xiaoling</first><last>Wang</last></author>
      <pages>4811-4823</pages>
      <abstract>Non-autoregressive translation (NAT) model achieves a much faster inference speed than the autoregressive translation (AT) model because it can simultaneously predict all tokens during inference. However, its translation quality suffers from degradation compared to AT. And existing NAT methods only focus on improving the NAT model’s performance but do not fully utilize it. In this paper, we propose a simple but effective method called “Candidate Soups,” which can obtain high-quality translations while maintaining the inference speed of NAT models. Unlike previous approaches that pick the individual result and discard the remainders, Candidate Soups (CDS) can fully use the valuable information in the different candidate translations through model uncertainty. Extensive experiments on two benchmarks (WMT’14 EN–DE and WMT’16 EN–RO) demonstrate the effectiveness and generality of our proposed method, which can significantly improve the translation quality of various base models. More notably, our best variant outperforms the AT model on three translation tasks with 7.6× speedup.</abstract>
      <url hash="c1e9397c">2022.emnlp-main.318</url>
      <bibkey>zheng-etal-2022-candidate</bibkey>
    </paper>
    <paper id="319">
      <title>Evaluating Parameter Efficient Learning for Generation</title>
      <author><first>Peng</first><last>Xu</last></author>
      <author><first>Mostofa</first><last>Patwary</last></author>
      <author><first>Shrimai</first><last>Prabhumoye</last></author>
      <author><first>Virginia</first><last>Adams</last></author>
      <author><first>Ryan</first><last>Prenger</last></author>
      <author><first>Wei</first><last>Ping</last></author>
      <author><first>Nayeon</first><last>Lee</last></author>
      <author><first>Mohammad</first><last>Shoeybi</last></author>
      <author><first>Bryan</first><last>Catanzaro</last></author>
      <pages>4824-4833</pages>
      <abstract>Parameter efficient learning methods (PERMs)have recently gained significant attention asthey provide an efficient way for pre-trainedlanguage models (PLMs) to adapt to a downstream task. However, these conclusions aremostly drawn from in-domain evaluations overthe full training set. In this paper, we presentcomparisons between PERMs and finetuningfrom three new perspectives: (1) the effect ofsample and model size to in-domain evaluations, (2) generalization to unseen domains andnew datasets, and (3) the faithfulness of generations. Our results show that for in-domainsettings (a) there is a cross point of samplesize for which PERMs will perform better thanfinetuning when training with fewer samples,and (b) larger PLMs have larger cross points.For cross-domain and cross-dataset cases, weshow that (a) Adapter (Houlsby et al., 2019)performs the best amongst all the PERMs studied here, and (b) it outperforms finetuning ifthe task dataset is below a certain size. Wealso compare the faithfulness of generationsand show that PERMs can achieve better faithfulness score than finetuning, especially forsmall training set, by as much as 6%. Finally,we apply Adapter to MT-NLG 530b (Smithet al., 2022) and achieve new state-of-the-artresults on Xsum (Narayan et al., 2018) for allROUGE scores (ROUGE-1 49.17, ROUGE-227.20, ROUGE-L 40.98).</abstract>
      <url hash="93c4e4b1">2022.emnlp-main.319</url>
      <bibkey>xu-etal-2022-evaluating</bibkey>
    </paper>
    <paper id="320">
      <title><fixed-case>M</fixed-case>c<fixed-case>Q</fixed-case>ueen: a Benchmark for Multimodal Conversational Query Rewrite</title>
      <author><first>Yifei</first><last>Yuan</last></author>
      <author><first>Chen</first><last>Shi</last></author>
      <author><first>Runze</first><last>Wang</last></author>
      <author><first>Liyi</first><last>Chen</last></author>
      <author><first>Feijun</first><last>Jiang</last></author>
      <author><first>Yuan</first><last>You</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <pages>4834-4844</pages>
      <abstract>The task of query rewrite aims to convert an in-context query to its fully-specified version where ellipsis and coreference are completed and referred-back according to the history context. Although much progress has been made, less efforts have been paid to real scenario conversations that involve drawing information from more than one modalities. In this paper, we propose the task of multimodal conversational query rewrite (McQR), which performs query rewrite under the multimodal visual conversation setting. We collect a large-scale dataset named McQueen based on manual annotation, which contains 15k visual conversations and over 80k queries where each one is associated with a fully-specified rewrite version. In addition, for entities appearing in the rewrite, we provide the corresponding image box annotation. We then use the McQueen dataset to benchmark a state-of-the-art method for effectively tackling the McQR task, which is based on a multimodal pre-trained model with pointer generator. Extensive experiments are performed to demonstrate the effectiveness of our model on this task.</abstract>
      <url hash="491a09a6">2022.emnlp-main.320</url>
      <bibkey>yuan-etal-2022-mcqueen</bibkey>
    </paper>
    <paper id="321">
      <title>Self-supervised Graph Masking Pre-training for Graph-to-Text Generation</title>
      <author><first>Jiuzhou</first><last>Han</last></author>
      <author><first>Ehsan</first><last>Shareghi</last></author>
      <pages>4845-4853</pages>
      <abstract>Large-scale pre-trained language models (PLMs) have advanced Graph-to-Text (G2T) generation by processing the linearised version of a graph. However, the linearisation is known to ignore the structural information. Additionally, PLMs are typically pre-trained on free text which introduces domain mismatch between pre-training and downstream G2T generation tasks. To address these shortcomings, we propose graph masking pre-training strategies that neither require supervision signals nor adjust the architecture of the underlying pre-trained encoder-decoder model. When used with a pre-trained T5, our approach achieves new state-of-the-art results on WebNLG+2020 and EventNarrative G2T generation datasets. Our method also shows to be very effective in the low-resource setting.</abstract>
      <url hash="33c310e7">2022.emnlp-main.321</url>
      <bibkey>han-shareghi-2022-self</bibkey>
    </paper>
    <paper id="322">
      <title>Improving Stability of Fine-Tuning Pretrained Language Models via Component-Wise Gradient Norm Clipping</title>
      <author><first>Chenghao</first><last>Yang</last></author>
      <author><first>Xuezhe</first><last>Ma</last></author>
      <pages>4854-4859</pages>
      <abstract>Fine-tuning over large pretrained language models (PLMs) has established many state-of-the-art results. Despite its superior performance, such fine-tuning can be unstable, resulting in significant variance in performance and potential risks for practical applications. Previous works have attributed such instability to the catastrophic forgetting problem in the top layers of PLMs, which indicates iteratively fine-tuning layers in a top-down manner is a promising solution. In this paper, we first point out that this method does not always work out due to the different convergence speeds of different layers/modules. Inspired by this observation, we propose a simple component-wise gradient norm clipping method to adjust the convergence speed for different components. Experiment results demonstrate that our method achieves consistent improvements in terms of generalization performance, convergence speed, and training stability. The codebase can be found at https://github.com/yangalan123/FineTuningStability.</abstract>
      <url hash="57ec43d1">2022.emnlp-main.322</url>
      <bibkey>yang-ma-2022-improving</bibkey>
    </paper>
    <paper id="323">
      <title>Differentially Private Language Models for Secure Data Sharing</title>
      <author><first>Justus</first><last>Mattern</last></author>
      <author><first>Zhijing</first><last>Jin</last></author>
      <author><first>Benjamin</first><last>Weggenmann</last></author>
      <author><first>Bernhard</first><last>Schoelkopf</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last></author>
      <pages>4860-4873</pages>
      <abstract>To protect the privacy of individuals whose data is being shared, it is of high importance to develop methods allowing researchers and companies to release textual data while providing formal privacy guarantees to its originators. In the field of NLP, substantial efforts have been directed at building mechanisms following the framework of local differential privacy, thereby anonymizing individual text samples before releasing them. In practice, these approaches are often dissatisfying in terms of the quality of their output language due to the strong noise required for local differential privacy. In this paper, we approach the problem at hand using global differential privacy, particularly by training a generative language model in a differentially private manner and consequently sampling data from it. Using natural language prompts and a new prompt-mismatch loss, we are able to create highly accurate and fluent textual datasets taking on specific desired attributes such as sentiment or topic and resembling statistical properties of the training data. We perform thorough experiments indicating that our synthetic datasets do not leak information from our original data and are of high language quality and highly suitable for training models for further analysis on real-world data. Notably, we also demonstrate that training classifiers on private synthetic data outperforms directly training classifiers with DP-SGD.</abstract>
      <url hash="90479fef">2022.emnlp-main.323</url>
      <bibkey>mattern-etal-2022-differentially</bibkey>
    </paper>
    <paper id="324">
      <title>Conditional set generation using Seq2seq models</title>
      <author><first>Aman</first><last>Madaan</last></author>
      <author><first>Dheeraj</first><last>Rajagopal</last></author>
      <author><first>Niket</first><last>Tandon</last></author>
      <author><first>Yiming</first><last>Yang</last></author>
      <author><first>Antoine</first><last>Bosselut</last></author>
      <pages>4874-4896</pages>
      <abstract>Conditional set generation learns a mapping from an input sequence of tokens to a set. Several NLP tasks, such as entity typing and dialogue emotion tagging, are instances of set generation. Seq2Seq models are a popular choice to model set generation but they treat a set as a sequence and do not fully leverage its key properties, namely order-invariance and cardinality. We propose a novel algorithm for effectively sampling informative orders over the combinatorial space of label orders. Further, we jointly model the set cardinality and output by listing the set size as the first element and taking advantage of the autoregressive factorization used by Seq2Seq models. Our method is a model-independent data augmentation approach that endows any Seq2Seq model with the signals of order-invariance and cardinality. Training a Seq2Seq model on this new augmented data (without any additional annotations), gets an average relative improvement of 20% for four benchmarks datasets across models spanning from BART-base, T5-11B, and GPT-3. We will release all code and data upon acceptance.</abstract>
      <url hash="746c9005">2022.emnlp-main.324</url>
      <bibkey>madaan-etal-2022-conditional</bibkey>
    </paper>
    <paper id="325">
      <title>Analyzing and Evaluating Faithfulness in Dialogue Summarization</title>
      <author><first>Bin</first><last>Wang</last></author>
      <author><first>Chen</first><last>Zhang</last></author>
      <author><first>Yan</first><last>Zhang</last></author>
      <author><first>Yiming</first><last>Chen</last></author>
      <author><first>Haizhou</first><last>Li</last></author>
      <pages>4897-4908</pages>
      <abstract>Dialogue summarization is abstractive in nature, making it suffer from factual errors. The factual correctness of summaries has the highest priority before practical applications. Many efforts have been made to improve faithfulness in text summarization. However, there is a lack of systematic study on dialogue summarization systems. In this work, we first perform the fine-grained human analysis on the faithfulness of dialogue summaries and observe that over 35% of generated summaries are faithfully inconsistent respective the source dialogues. Furthermore, we present a new model-level faithfulness evaluation method. It examines generation models with multi-choice questions created by rule-based transformations. Experimental results show that our evaluation schema is a strong proxy for the factual correctness of summarization models. The human-annotated faithfulness samples and the evaluation toolkit are released to facilitate future research toward faithful dialogue summarization.</abstract>
      <url hash="bafba617">2022.emnlp-main.325</url>
      <bibkey>wang-etal-2022-analyzing</bibkey>
    </paper>
    <paper id="326">
      <title>Twist Decoding: Diverse Generators Guide Each Other</title>
      <author><first>Jungo</first><last>Kasai</last></author>
      <author><first>Keisuke</first><last>Sakaguchi</last></author>
      <author><first>Ronan</first><last>Le Bras</last></author>
      <author><first>Hao</first><last>Peng</last></author>
      <author><first>Ximing</first><last>Lu</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <pages>4909-4923</pages>
      <abstract>Many language generation models are now available for a wide range of generation tasks, including machine translation and summarization. Combining such diverse models may lead to further progress, but ensembling generation models is challenging during inference: conventional ensembling methods (e.g., shallow fusion) require that the models share vocabulary/tokenization schemes. We introduce Twist decoding, a simple and general text generation algorithm that benefits from diverse models at inference time. Our method does not assume the vocabulary, tokenization or even generation order is shared. Our extensive evaluations on machine translation and scientific paper summarization demonstrate that Twist decoding substantially outperforms each model decoded in isolation over various scenarios, including cases where domain-specific and general-purpose models are both available. Twist decoding also consistently outperforms the popular reranking heuristic where output candidates from one model are rescored by another. We hope that our work will encourage researchers and practitioners to examine generation models collectively, not just independently, and to seek out models with complementary strengths to the currently available models.</abstract>
      <url hash="5755c29f">2022.emnlp-main.326</url>
      <bibkey>kasai-etal-2022-twist</bibkey>
    </paper>
    <paper id="327">
      <title>Exploring Representation-level Augmentation for Code Search</title>
      <author><first>Haochen</first><last>Li</last></author>
      <author><first>Chunyan</first><last>Miao</last></author>
      <author><first>Cyril</first><last>Leung</last></author>
      <author><first>Yanxian</first><last>Huang</last></author>
      <author><first>Yuan</first><last>Huang</last></author>
      <author><first>Hongyu</first><last>Zhang</last></author>
      <author><first>Yanlin</first><last>Wang</last></author>
      <pages>4924-4936</pages>
      <abstract>Code search, which aims at retrieving the most relevant code fragment for a given natural language query, is a common activity in software development practice. Recently, contrastive learning is widely used in code search research, where many data augmentation approaches for source code (e.g., semantic-preserving program transformation) are proposed to learn better representations. However, these augmentations are at the raw-data level, which requires additional code analysis in the preprocessing stage and additional training cost in the training stage. In this paper, we explore augmentation methods that augment data (both code and query) at representation level which does not require additional data processing and training, and based on this we propose a general format of representation-level augmentation that unifies existing methods. Then, we propose three new augmentation methods (linear extrapolation, binary interpolation, and Gaussian scaling) based on the general format. Furthermore, we theoretically analyze the advantages of the proposed augmentation methods over traditional contrastive learning methods on code search. We experimentally evaluate the proposed representation-level augmentation methods with state-of-the-art code search models on a large-scale public dataset consisting of six programming languages. The experimental results show that our approach can consistently boost the performance of the studied code search models.</abstract>
      <url hash="288caa4b">2022.emnlp-main.327</url>
      <bibkey>li-etal-2022-exploring-representation</bibkey>
    </paper>
    <paper id="328">
      <title>Learning Semantic Textual Similarity via Topic-informed Discrete Latent Variables</title>
      <author><first>Erxin</first><last>Yu</last></author>
      <author><first>Lan</first><last>Du</last></author>
      <author><first>Yuan</first><last>Jin</last></author>
      <author><first>Zhepei</first><last>Wei</last></author>
      <author><first>Yi</first><last>Chang</last></author>
      <pages>4937-4948</pages>
      <abstract>Recently, discrete latent variable models have received a surge of interest in both Natural Language Processing (NLP) and Computer Vision (CV), attributed to their comparable performance to the continuous counterparts in representation learning, while being more interpretable in their predictions. In this paper, we develop a topic-informed discrete latent variable model for semantic textual similarity, which learns a shared latent space for sentence-pair representation via vector quantization. Compared with previous models limited to local semantic contexts, our model can explore richer semantic information via topic modeling. We further boost the performance of semantic similarity by injecting the quantized representation into a transformer-based language model with a well-designed semantic-driven attention mechanism. We demonstrate, through extensive experiments across various English language datasets, that our model is able to surpass several strong neural baselines in semantic textual similarity tasks.</abstract>
      <url hash="eec78e28">2022.emnlp-main.328</url>
      <bibkey>yu-etal-2022-learning</bibkey>
    </paper>
    <paper id="329">
      <title><fixed-case>STRUDEL</fixed-case>: Structured Dialogue Summarization for Dialogue Comprehension</title>
      <author><first>Borui</first><last>Wang</last></author>
      <author><first>Chengcheng</first><last>Feng</last></author>
      <author><first>Arjun</first><last>Nair</last></author>
      <author><first>Madelyn</first><last>Mao</last></author>
      <author><first>Jai</first><last>Desai</last></author>
      <author><first>Asli</first><last>Celikyilmaz</last></author>
      <author><first>Haoran</first><last>Li</last></author>
      <author><first>Yashar</first><last>Mehdad</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <pages>4949-4958</pages>
      <abstract>Abstractive dialogue summarization has long been viewed as an important standalone task in natural language processing, but no previous work has explored the possibility of whether abstractive dialogue summarization can also be used as a means to boost an NLP system’s performance on other important dialogue comprehension tasks. In this paper, we propose a novel type of dialogue summarization task - STRUctured DiaLoguE Summarization (STRUDEL) - that can help pre-trained language models to better understand dialogues and improve their performance on important dialogue comprehension tasks. In contrast to the holistic approach taken by the traditional free-form abstractive summarization task for dialogues, STRUDEL aims to decompose and imitate the hierarchical, systematic and structured mental process that we human beings usually go through when understanding and analyzing dialogues, and thus has the advantage of being more focused, specific and instructive for dialogue comprehension models to learn from. We further introduce a new STRUDEL dialogue comprehension modeling framework that integrates STRUDEL into a dialogue reasoning module over transformer encoder language models to improve their dialogue comprehension ability. In our empirical experiments on two important downstream dialogue comprehension tasks - dialogue question answering and dialogue response prediction - we demonstrate that our STRUDEL dialogue comprehension models can significantly improve the dialogue comprehension performance of transformer encoder language models.</abstract>
      <url hash="e8b10683">2022.emnlp-main.329</url>
      <bibkey>wang-etal-2022-strudel</bibkey>
    </paper>
    <paper id="330">
      <title>Competency-Aware Neural Machine Translation: Can Machine Translation Know its Own Translation Quality?</title>
      <author><first>Pei</first><last>Zhang</last></author>
      <author><first>Baosong</first><last>Yang</last></author>
      <author><first>Hao-Ran</first><last>Wei</last></author>
      <author><first>Dayiheng</first><last>Liu</last></author>
      <author><first>Kai</first><last>Fan</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <author><first>Jun</first><last>Xie</last></author>
      <pages>4959-4970</pages>
      <abstract>Neural machine translation (NMT) is often criticized for failures that happenwithout awareness. The lack of competency awareness makes NMT untrustworthy. This is in sharp contrast to human translators who give feedback or conduct further investigations whenever they are in doubt about predictions. To fill this gap, we propose a novel competency-aware NMT by extending conventional NMT with a self-estimator, offering abilities to translate a source sentence and estimate its competency.The self-estimator encodes the information of the decoding procedure and then examines whether it can reconstruct the original semantics of the source sentence. Experimental results on four translation tasks demonstrate that the proposed method not only carries out translation tasks intact but also delivers outstanding performance on quality estimation.Without depending on any reference or annotated data typically required by state-of-the-art metric and quality estimation methods, our model yields an even higher correlation with human quality judgments than a variety of aforementioned methods, such as BLEURT, COMET, and BERTScore. Quantitative and qualitative analyses show better robustness of competency awareness in our model.</abstract>
      <url hash="0b404cd1">2022.emnlp-main.330</url>
      <bibkey>zhang-etal-2022-competency</bibkey>
    </paper>
    <paper id="331">
      <title><fixed-case>PASTA</fixed-case>: Table-Operations Aware Fact Verification via Sentence-Table Cloze Pre-training</title>
      <author><first>Zihui</first><last>Gu</last></author>
      <author><first>Ju</first><last>Fan</last></author>
      <author><first>Nan</first><last>Tang</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <author><first>Xiaoman</first><last>Zhao</last></author>
      <author><first>Xiaoyong</first><last>Du</last></author>
      <pages>4971-4983</pages>
      <abstract>Fact verification has attracted a lot of attention recently, e.g., in journalism, marketing, and policymaking, as misinformation and dis- information can sway one’s opinion and affect one’s actions. While fact-checking is a hard task in general, in many cases, false statements can be easily debunked based on analytics over tables with reliable information. Hence, table- based fact verification has recently emerged as an important and growing research area. Yet, progress has been limited due to the lack of datasets that can be used to pre-train language models (LMs) to be aware of common table operations, such as aggregating a column or comparing tuples. To bridge this gap, this paper introduces PASTA for table-based fact verification via pre-training with synthesized sentence–table cloze questions. In particular, we design six types of common sentence–table cloze tasks, including Filter, Aggregation, Superlative, Comparative, Ordinal, and Unique, based on which we synthesize a large corpus consisting of 1.2 million sentence–table pairs from WikiTables. PASTA uses a recent pre-trained LM, DeBERTaV3, and further pre- trains it on our corpus. Our experimental results show that PASTA achieves new state-of-the-art (SOTA) performance on two table-based fact verification datasets TabFact and SEM-TAB- FACTS. In particular, on the complex set of TabFact, which contains multiple operations, PASTA largely outperforms previous SOTA by 4.7% (85.6% vs. 80.9%), and the gap between PASTA and human performance on the small test set is narrowed to just 1.5% (90.6% vs. 92.1%).</abstract>
      <url hash="8112a4de">2022.emnlp-main.331</url>
      <bibkey>gu-etal-2022-pasta</bibkey>
    </paper>
    <paper id="332">
      <title>Sentiment-Aware Word and Sentence Level Pre-training for Sentiment Analysis</title>
      <author><first>Shuai</first><last>Fan</last></author>
      <author><first>Chen</first><last>Lin</last></author>
      <author><first>Haonan</first><last>Li</last></author>
      <author><first>Zhenghao</first><last>Lin</last></author>
      <author><first>Jinsong</first><last>Su</last></author>
      <author><first>Hang</first><last>Zhang</last></author>
      <author><first>Yeyun</first><last>Gong</last></author>
      <author><first>JIan</first><last>Guo</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <pages>4984-4994</pages>
      <abstract>Most existing pre-trained language representation models (PLMs) are sub-optimal in sentiment analysis tasks, as they capture the sentiment information from word-level while under-considering sentence-level information. In this paper, we propose SentiWSP, a novel Sentiment-aware pre-trained language model with combined Word-level and Sentence-level Pre-training tasks.The word level pre-training task detects replaced sentiment words, via a generator-discriminator framework, to enhance the PLM’s knowledge about sentiment words.The sentence level pre-training task further strengthens the discriminator via a contrastive learning framework, with similar sentences as negative samples, to encode sentiments in a sentence.Extensive experimental results show that SentiWSP achieves new state-of-the-art performance on various sentence-level and aspect-level sentiment classification benchmarks. We have made our code and model publicly available at https://github.com/XMUDM/SentiWSP.</abstract>
      <url hash="dd79f6de">2022.emnlp-main.332</url>
      <bibkey>fan-etal-2022-sentiment</bibkey>
    </paper>
    <paper id="333">
      <title>Towards Multi-Modal Sarcasm Detection via Hierarchical Congruity Modeling with Knowledge Enhancement</title>
      <author><first>Hui</first><last>Liu</last></author>
      <author><first>Wenya</first><last>Wang</last></author>
      <author><first>Haoliang</first><last>Li</last></author>
      <pages>4995-5006</pages>
      <abstract>Sarcasm is a linguistic phenomenon indicating a discrepancy between literal meanings and implied intentions. Due to its sophisticated nature, it is usually difficult to be detected from the text itself. As a result, multi-modal sarcasm detection has received more and more attention in both academia and industries. However, most existing techniques only modeled the atomic-level inconsistencies between the text input and its accompanying image, ignoring more complex compositions for both modalities. Moreover, they neglected the rich information contained in external knowledge, e.g., image captions. In this paper, we propose a novel hierarchical framework for sarcasm detection by exploring both the atomic-level congruity based on multi-head cross attentions and the composition-level congruity based on graph neural networks, where a post with low congruity can be identified as sarcasm. In addition, we exploit the effect of various knowledge resources for sarcasm detection. Evaluation results on a public multi-modal sarcasm detection dataset based on Twitter demonstrate the superiority of our proposed model.</abstract>
      <url hash="998db871">2022.emnlp-main.333</url>
      <bibkey>liu-etal-2022-towards-multi-modal</bibkey>
    </paper>
    <paper id="334">
      <title>Efficiently Tuned Parameters Are Task Embeddings</title>
      <author><first>Wangchunshu</first><last>Zhou</last></author>
      <author><first>Canwen</first><last>Xu</last></author>
      <author><first>Julian</first><last>McAuley</last></author>
      <pages>5007-5014</pages>
      <abstract>Intermediate-task transfer can benefit a wide range of NLP tasks with properly selected source datasets. However, it is computationally infeasible to experiment with all intermediate transfer combinations, making choosing a useful source task a challenging problem. In this paper, we anticipate that task-specific parameters updated in parameter-efficient tuning methods are likely to encode task-specific information. Therefore, such parameters can be predictive for inter-task transferability. Thus, we propose to exploit these efficiently tuned parameters as off-the-shelf task embeddings for the efficient selection of source datasets for intermediate-task transfer. We experiment with 11 text classification tasks and 11 question answering tasks. Experimental results show that our approach consistently outperforms existing inter-task transferability prediction methods while being conceptually simple and computationally efficient. Our analysis also reveals that the ability of efficiently tuned parameters on transferability prediction is disentangled with their in-task performance. This allows us to use parameters from early checkpoints as task embeddings to further improve efficiency.</abstract>
      <url hash="5e8cdfa6">2022.emnlp-main.334</url>
      <bibkey>zhou-etal-2022-efficiently</bibkey>
    </paper>
    <paper id="335">
      <title><fixed-case>COPEN</fixed-case>: Probing Conceptual Knowledge in Pre-trained Language Models</title>
      <author><first>Hao</first><last>Peng</last></author>
      <author><first>Xiaozhi</first><last>Wang</last></author>
      <author><first>Shengding</first><last>Hu</last></author>
      <author><first>Hailong</first><last>Jin</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>5015-5035</pages>
      <abstract>Conceptual knowledge is fundamental to human cognition and knowledge bases. However, existing knowledge probing works only focus on evaluating factual knowledge of pre-trained language models (PLMs) and ignore conceptual knowledge. Since conceptual knowledge often appears as implicit commonsense behind texts, designing probes for conceptual knowledge is hard. Inspired by knowledge representation schemata, we comprehensively evaluate conceptual knowledge of PLMs by designing three tasks to probe whether PLMs organize entities by conceptual similarities, learn conceptual properties, and conceptualize entities in contexts, respectively. For the tasks, we collect and annotate 24k data instances covering 393 concepts, which is COPEN, a COnceptual knowledge Probing bENchmark. Extensive experiments on different sizes and types of PLMs show that existing PLMs systematically lack conceptual knowledge and suffer from various spurious correlations. We believe this is a critical bottleneck for realizing human-like cognition in PLMs. COPEN and our codes are publicly released at https://github.com/THU-KEG/COPEN.</abstract>
      <url hash="4b66f01a">2022.emnlp-main.335</url>
      <bibkey>peng-etal-2022-copen</bibkey>
    </paper>
    <paper id="336">
      <title>Capturing Global Structural Information in Long Document Question Answering with Compressive Graph Selector Network</title>
      <author><first>Yuxiang</first><last>Nie</last></author>
      <author><first>Heyan</first><last>Huang</last></author>
      <author><first>Wei</first><last>Wei</last></author>
      <author><first>Xian-Ling</first><last>Mao</last></author>
      <pages>5036-5047</pages>
      <abstract>Long document question answering is a challenging task due to its demands for complex reasoning over long text. Previous works usually take long documents as non-structured flat texts or only consider the local structure in long documents. However, these methods usually ignore the global structure of the long document, which is essential for long-range understanding. To tackle this problem, we propose Compressive Graph Selector Network (CGSN) to capture the global structure in a compressive and iterative manner. The proposed model mainly focuses on the evidence selection phase of long document question answering. Specifically, it consists of three modules: local graph network, global graph network and evidence memory network. Firstly, the local graph network builds the graph structure of the chunked segment in token, sentence, paragraph and segment levels to capture the short-term dependency of the text. Secondly, the global graph network selectively receives the information of each level from the local graph, compresses them into the global graph nodes and applies graph attention to the global graph nodes to build the long-range reasoning over the entire text in an iterative way. Thirdly, the evidence memory network is designed to alleviate the redundancy problem in the evidence selection by saving the selected result in the previous steps. Extensive experiments show that the proposed model outperforms previous methods on two datasets.</abstract>
      <url hash="068cfd2b">2022.emnlp-main.336</url>
      <bibkey>nie-etal-2022-capturing</bibkey>
    </paper>
    <paper id="337">
      <title>Structural generalization is hard for sequence-to-sequence models</title>
      <author><first>Yuekun</first><last>Yao</last></author>
      <author><first>Alexander</first><last>Koller</last></author>
      <pages>5048-5062</pages>
      <abstract>Sequence-to-sequence (seq2seq) models have been successful across many NLP tasks,including ones that require predicting linguistic structure. However, recent work on compositional generalization has shown that seq2seq models achieve very low accuracy in generalizing to linguistic structures that were not seen in training. We present new evidence that this is a general limitation of seq2seq models that is present not just in semantic parsing, but also in syntactic parsing and in text-to-text tasks, and that this limitation can often be overcome by neurosymbolic models that have linguistic knowledge built in. We further report on some experiments that give initial answers on the reasons for these limitations.</abstract>
      <url hash="e96cd8a1">2022.emnlp-main.337</url>
      <bibkey>yao-koller-2022-structural</bibkey>
    </paper>
    <paper id="338">
      <title>Contrastive Learning enhanced Author-Style Headline Generation</title>
      <author><first>Hui</first><last>Liu</last></author>
      <author><first>Weidong</first><last>Guo</last></author>
      <author><first>Yige</first><last>Chen</last></author>
      <author><first>Xiangyang</first><last>Li</last></author>
      <pages>5063-5072</pages>
      <abstract>Headline generation is a task of generating an appropriate headline for a given article, which can be further used for machine-aided writing or enhancing the click-through ratio. Current works only use the article itself in the generation, but have not taken the writing style of headlines into consideration. In this paper, we propose a novel Seq2Seq model called CLH3G (Contrastive Learning enhanced Historical Headlines based Headline Generation) which can use the historical headlines of the articles that the author wrote in the past to improve the headline generation of current articles. By taking historical headlines into account, we can integrate the stylistic features of the author into our model, and generate a headline not only appropriate for the article, but also consistent with the author’s style. In order to efficiently learn the stylistic features of the author, we further introduce a contrastive learning based auxiliary task for the encoder of our model. Besides, we propose two methods to use the learned stylistic features to guide both the pointer and the decoder during the generation. Experimental results show that historical headlines of the same user can improve the headline generation significantly, and both the contrastive learning module and the two style features fusion methods can further boost the performance.</abstract>
      <url hash="ca0a7536">2022.emnlp-main.338</url>
      <bibkey>liu-etal-2022-contrastive</bibkey>
    </paper>
    <paper id="339">
      <title>Multi-Granularity Optimization for Non-Autoregressive Translation</title>
      <author><first>Yafu</first><last>Li</last></author>
      <author><first>Leyang</first><last>Cui</last></author>
      <author><first>Yongjing</first><last>Yin</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>5073-5084</pages>
      <abstract>Despite low latency, non-autoregressive machine translation (NAT) suffers severe performance deterioration due to the naive independence assumption. This assumption is further strengthened by cross-entropy loss, which encourages a strict match between the hypothesis and the reference token by token. To alleviate this issue, we propose multi-granularity optimization for NAT, which collects model behaviours on translation segments of various granularities and integrates feedback for backpropagation. Experiments on four WMT benchmarks show that the proposed method significantly outperforms the baseline models trained with cross-entropy loss, and achieves the best performance on WMT’16 En⇔Ro and highly competitive results on WMT’14 En⇔De for fully non-autoregressive translation.</abstract>
      <url hash="30b3e93a">2022.emnlp-main.339</url>
      <bibkey>li-etal-2022-multi-granularity</bibkey>
    </paper>
    <paper id="340">
      <title>Super-<fixed-case>N</fixed-case>atural<fixed-case>I</fixed-case>nstructions: Generalization via Declarative Instructions on 1600+ <fixed-case>NLP</fixed-case> Tasks</title>
      <author><first>Yizhong</first><last>Wang</last></author>
      <author><first>Swaroop</first><last>Mishra</last></author>
      <author><first>Pegah</first><last>Alipoormolabashi</last></author>
      <author><first>Yeganeh</first><last>Kordi</last></author>
      <author><first>Amirreza</first><last>Mirzaei</last></author>
      <author><first>Atharva</first><last>Naik</last></author>
      <author><first>Arjun</first><last>Ashok</last></author>
      <author><first>Arut Selvan</first><last>Dhanasekaran</last></author>
      <author><first>Anjana</first><last>Arunkumar</last></author>
      <author><first>David</first><last>Stap</last></author>
      <author><first>Eshaan</first><last>Pathak</last></author>
      <author><first>Giannis</first><last>Karamanolakis</last></author>
      <author><first>Haizhi</first><last>Lai</last></author>
      <author><first>Ishan</first><last>Purohit</last></author>
      <author><first>Ishani</first><last>Mondal</last></author>
      <author><first>Jacob</first><last>Anderson</last></author>
      <author><first>Kirby</first><last>Kuznia</last></author>
      <author><first>Krima</first><last>Doshi</last></author>
      <author><first>Kuntal Kumar</first><last>Pal</last></author>
      <author><first>Maitreya</first><last>Patel</last></author>
      <author><first>Mehrad</first><last>Moradshahi</last></author>
      <author><first>Mihir</first><last>Parmar</last></author>
      <author><first>Mirali</first><last>Purohit</last></author>
      <author><first>Neeraj</first><last>Varshney</last></author>
      <author><first>Phani Rohitha</first><last>Kaza</last></author>
      <author><first>Pulkit</first><last>Verma</last></author>
      <author><first>Ravsehaj Singh</first><last>Puri</last></author>
      <author><first>Rushang</first><last>Karia</last></author>
      <author><first>Savan</first><last>Doshi</last></author>
      <author><first>Shailaja Keyur</first><last>Sampat</last></author>
      <author><first>Siddhartha</first><last>Mishra</last></author>
      <author><first>Sujan</first><last>Reddy A</last></author>
      <author><first>Sumanta</first><last>Patro</last></author>
      <author><first>Tanay</first><last>Dixit</last></author>
      <author><first>Xudong</first><last>Shen</last></author>
      <pages>5085-5109</pages>
      <abstract>How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions—training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones.Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.</abstract>
      <url hash="00f6c5d6">2022.emnlp-main.340</url>
      <bibkey>wang-etal-2022-super</bibkey>
    </paper>
    <paper id="341">
      <title><fixed-case>M</fixed-case>eta<fixed-case>F</fixed-case>ill: Text Infilling for Meta-Path Generation on Heterogeneous Information Networks</title>
      <author><first>Zequn</first><last>Liu</last></author>
      <author><first>Kefei</first><last>Duan</last></author>
      <author><first>Junwei</first><last>Yang</last></author>
      <author><first>Hanwen</first><last>Xu</last></author>
      <author><first>Ming</first><last>Zhang</last></author>
      <author><first>Sheng</first><last>Wang</last></author>
      <pages>5110-5122</pages>
      <abstract>Heterogeneous information network (HIN) is essential to study complicated networks containing multiple edge types and node types. Meta-path, a sequence of node types and edge types, is the core technique to embed HINs. Since manually curating meta-paths is time-consuming, there is a pressing need to develop automated meta-path generation approaches. Existing meta-path generation approaches cannot fully exploit the rich textual information in HINs, such as node names and edge type names. To address this problem, we propose MetaFill, a text-infilling-based approach for meta-path generation. The key idea of MetaFill is to formulate meta-path identification problem as a word sequence infilling problem, which can be advanced by pretrained language models (PLMs). We observed the superior performance of MetaFill against existing meta-path generation methods and graph embedding methods that do not leverage meta-paths in both link prediction and node classification on two real-world HIN datasets. We further demonstrated how MetaFill can accurately classify edges in the zero-shot setting, where existing approaches cannot generate any meta-paths. MetaFill exploits PLMs to generate meta-paths for graph embedding, opening up new avenues for language model applications in graph analysis.</abstract>
      <url hash="b023486d">2022.emnlp-main.341</url>
      <bibkey>liu-etal-2022-metafill</bibkey>
    </paper>
    <paper id="342">
      <title><fixed-case>DRLK</fixed-case>: Dynamic Hierarchical Reasoning with Language Model and Knowledge Graph for Question Answering</title>
      <author><first>Miao</first><last>Zhang</last></author>
      <author><first>Rufeng</first><last>Dai</last></author>
      <author><first>Ming</first><last>Dong</last></author>
      <author><first>Tingting</first><last>He</last></author>
      <pages>5123-5133</pages>
      <abstract>In recent years, Graph Neural Network (GNN) approaches with enhanced knowledge graphs (KG) perform well in question answering (QA) tasks. One critical challenge is how to effectively utilize interactions between the QA context and KG. However, existing work only adopts the identical QA context representation to interact with multiple layers of KG, which results in a restricted interaction. In this paper, we propose DRLK (Dynamic Hierarchical Reasoning with Language Model and Knowledge Graphs), a novel model that utilizes dynamic hierarchical interactions between the QA context and KG for reasoning. DRLK extracts dynamic hierarchical features in the QA context, and performs inter-layer and intra-layer interactions on each iteration, allowing the KG representation to be grounded with the hierarchical features of the QA context. We conduct extensive experiments on four benchmark datasets in medical QA and commonsense reasoning. The experimental results demonstrate that DRLK achieves state-of-the-art performances on two benchmark datasets and performs competitively on the others.</abstract>
      <url hash="72bfe1df">2022.emnlp-main.342</url>
      <bibkey>zhang-etal-2022-drlk</bibkey>
    </paper>
    <paper id="343">
      <title><fixed-case>AEG</fixed-case>: Argumentative Essay Generation via A Dual-Decoder Model with Content Planning</title>
      <author><first>Jianzhu</first><last>Bao</last></author>
      <author><first>Yasheng</first><last>Wang</last></author>
      <author><first>Yitong</first><last>Li</last></author>
      <author><first>Fei</first><last>Mi</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <pages>5134-5148</pages>
      <abstract>Argument generation is an important but challenging task in computational argumentation.Existing studies have mainly focused on generating individual short arguments, while research on generating long and coherent argumentative essays is still under-explored.In this paper, we propose a new task, Argumentative Essay Generation (AEG).Given a writing prompt, the goal of AEG is to automatically generate an argumentative essay with strong persuasiveness.We construct a large-scale dataset, ArgEssay, for this new task and establish a strong model based on a dual-decoder Transformer architecture.Our proposed model contains two decoders, a planning decoder (PD) and a writing decoder (WD), where PD is used to generate a sequence for essay content planning and WD incorporates the planning information to write an essay.Further, we pre-train this model on a large news dataset to enhance the plan-and-write paradigm.Automatic and human evaluation results show that our model can generate more coherent and persuasive essays with higher diversity and less repetition compared to several baselines.</abstract>
      <url hash="fd687121">2022.emnlp-main.343</url>
      <bibkey>bao-etal-2022-aeg</bibkey>
    </paper>
    <paper id="344">
      <title><fixed-case>B</fixed-case>ots<fixed-case>T</fixed-case>alk: Machine-sourced Framework for Automatic Curation of Large-scale Multi-skill Dialogue Datasets</title>
      <author><first>Minju</first><last>Kim</last></author>
      <author><first>Chaehyeong</first><last>Kim</last></author>
      <author><first>Yong Ho</first><last>Song</last></author>
      <author><first>Seung-won</first><last>Hwang</last></author>
      <author><first>Jinyoung</first><last>Yeo</last></author>
      <pages>5149-5170</pages>
      <abstract>To build open-domain chatbots that are able to use diverse communicative skills, we propose a novel framework BotsTalk, where multiple agents grounded to the specific target skills participate in a conversation to automatically annotate multi-skill dialogues. We further present Blended Skill BotsTalk (BSBT), a large-scale multi-skill dialogue dataset comprising 300K conversations. Through extensive experiments, we demonstrate that our dataset can be effective for multi-skill dialogue systems which require an understanding of skill blending as well as skill grounding. Our code and data are available at https://github.com/convei-lab/BotsTalk.</abstract>
      <url hash="d39af69c">2022.emnlp-main.344</url>
      <bibkey>kim-etal-2022-botstalk</bibkey>
    </paper>
    <paper id="345">
      <title>Wider &amp; Closer: Mixture of Short-channel Distillers for Zero-shot Cross-lingual Named Entity Recognition</title>
      <author><first>Jun-Yu</first><last>Ma</last></author>
      <author><first>Beiduo</first><last>Chen</last></author>
      <author><first>Jia-Chen</first><last>Gu</last></author>
      <author><first>Zhenhua</first><last>Ling</last></author>
      <author><first>Wu</first><last>Guo</last></author>
      <author><first>Quan</first><last>Liu</last></author>
      <author><first>Zhigang</first><last>Chen</last></author>
      <author><first>Cong</first><last>Liu</last></author>
      <pages>5171-5183</pages>
      <abstract>Zero-shot cross-lingual named entity recognition (NER) aims at transferring knowledge from annotated and rich-resource data in source languages to unlabeled and lean-resource data in target languages. Existing mainstream methods based on the teacher-student distillation framework ignore the rich and complementary information lying in the intermediate layers of pre-trained language models, and domain-invariant information is easily lost during transfer. In this study, a mixture of short-channel distillers (MSD) method is proposed to fully interact the rich hierarchical information in the teacher model and to transfer knowledge to the student model sufficiently and efficiently. Concretely, a multi-channel distillation framework is designed for sufficient information transfer by aggregating multiple distillers as a mixture. Besides, an unsupervised method adopting parallel domain adaptation is proposed to shorten the channels between the teacher and student models to preserve domain-invariant features. Experiments on four datasets across nine languages demonstrate that the proposed method achieves new state-of-the-art performance on zero-shot cross-lingual NER and shows great generalization and compatibility across languages and fields.</abstract>
      <url hash="cd5e66c9">2022.emnlp-main.345</url>
      <bibkey>ma-etal-2022-wider</bibkey>
    </paper>
    <paper id="346">
      <title>An Efficient Memory-Augmented Transformer for Knowledge-Intensive <fixed-case>NLP</fixed-case> Tasks</title>
      <author><first>Yuxiang</first><last>Wu</last></author>
      <author><first>Yu</first><last>Zhao</last></author>
      <author><first>Baotian</first><last>Hu</last></author>
      <author><first>Pasquale</first><last>Minervini</last></author>
      <author><first>Pontus</first><last>Stenetorp</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <pages>5184-5196</pages>
      <abstract>Access to external knowledge is essential for many natural language processing tasks, such as question answering and dialogue. Existing methods often rely on a parametric model that stores knowledge in its parameters, or use a retrieval-augmented model that has access to an external knowledge source. Parametric and retrieval-augmented models have complementary strengths in terms of computational efficiency and predictive accuracy. To combine the strength of both approaches, we propose the Efficient Memory-Augmented Transformer (EMAT) – it encodes external knowledge into a key-value memory and exploits the fast maximum inner product search for memory querying. We also introduce pre-training tasks that allow EMAT to encode informative key-value representations, and to learn an implicit strategy to integrate multiple memory slots into the transformer. Experiments on various knowledge-intensive tasks such as question answering and dialogue datasets show that, simply augmenting parametric models (T5-base) using our method produces more accurate results (e.g., 25.8 → 44.3 EM on NQ) while retaining a high throughput (e.g., 1000 queries/s on NQ). Compared to retrieval-augmented models, EMAT runs substantially faster across the board and produces more accurate results on WoW and ELI5.</abstract>
      <url hash="56cafe1d">2022.emnlp-main.346</url>
      <bibkey>wu-etal-2022-efficient</bibkey>
    </paper>
    <paper id="347">
      <title>Supervised Prototypical Contrastive Learning for Emotion Recognition in Conversation</title>
      <author><first>Xiaohui</first><last>Song</last></author>
      <author><first>Longtao</first><last>Huang</last></author>
      <author><first>Hui</first><last>Xue</last></author>
      <author><first>Songlin</first><last>Hu</last></author>
      <pages>5197-5206</pages>
      <abstract>Capturing emotions within a conversation plays an essential role in modern dialogue systems. However, the weak correlation between emotions and semantics brings many challenges to emotion recognition in conversation (ERC). Even semantically similar utterances, the emotion may vary drastically depending on contexts or speakers. In this paper, we propose a Supervised Prototypical Contrastive Learning (SPCL) loss for the ERC task. Leveraging the Prototypical Network, the SPCL targets at solving the imbalanced classification problem through contrastive learning and does not require a large batch size. Meanwhile, we design a difficulty measure function based on the distance between classes and introduce curriculum learning to alleviate the impact of extreme samples. We achieve state-of-the-art results on three widely used benchmarks. Further, we conduct analytical experiments to demonstrate the effectiveness of our proposed SPCL and curriculum learning strategy.</abstract>
      <url hash="9e98e235">2022.emnlp-main.347</url>
      <bibkey>song-etal-2022-supervised</bibkey>
    </paper>
    <paper id="348">
      <title><fixed-case>R</fixed-case>u<fixed-case>C</fixed-case>o<fixed-case>LA</fixed-case>: <fixed-case>R</fixed-case>ussian Corpus of Linguistic Acceptability</title>
      <author><first>Vladislav</first><last>Mikhailov</last></author>
      <author><first>Tatiana</first><last>Shamardina</last></author>
      <author><first>Max</first><last>Ryabinin</last></author>
      <author><first>Alena</first><last>Pestova</last></author>
      <author><first>Ivan</first><last>Smurov</last></author>
      <author><first>Ekaterina</first><last>Artemova</last></author>
      <pages>5207-5227</pages>
      <abstract>Linguistic acceptability (LA) attracts the attention of the research community due to its many uses, such as testing the grammatical knowledge of language models and filtering implausible texts with acceptability classifiers.However, the application scope of LA in languages other than English is limited due to the lack of high-quality resources.To this end, we introduce the Russian Corpus of Linguistic Acceptability (RuCoLA), built from the ground up under the well-established binary LA approach. RuCoLA consists of 9.8k in-domain sentences from linguistic publications and 3.6k out-of-domain sentences produced by generative models. The out-of-domain set is created to facilitate the practical use of acceptability for improving language generation.Our paper describes the data collection protocol and presents a fine-grained analysis of acceptability classification experiments with a range of baseline approaches.In particular, we demonstrate that the most widely used language models still fall behind humans by a large margin, especially when detecting morphological and semantic errors. We release RuCoLA, the code of experiments, and a public leaderboard to assess the linguistic competence of language models for Russian.</abstract>
      <url hash="6884543b">2022.emnlp-main.348</url>
      <bibkey>mikhailov-etal-2022-rucola</bibkey>
    </paper>
    <paper id="349">
      <title>Complex Hyperbolic Knowledge Graph Embeddings with Fast <fixed-case>F</fixed-case>ourier Transform</title>
      <author><first>Huiru</first><last>Xiao</last></author>
      <author><first>Xin</first><last>Liu</last></author>
      <author><first>Yangqiu</first><last>Song</last></author>
      <author><first>Ginny</first><last>Wong</last></author>
      <author><first>Simon</first><last>See</last></author>
      <pages>5228-5239</pages>
      <abstract>The choice of geometric space for knowledge graph (KG) embeddings can have significant effects on the performance of KG completion tasks. The hyperbolic geometry has been shown to capture the hierarchical patterns due to its tree-like metrics, which addressed the limitations of the Euclidean embedding models. Recent explorations of the complex hyperbolic geometry further improved the hyperbolic embeddings for capturing a variety of hierarchical structures. However, the performance of the hyperbolic KG embedding models for non-transitive relations is still unpromising, while the complex hyperbolic embeddings do not deal with multi-relations. This paper aims to utilize the representation capacity of the complex hyperbolic geometry in multi-relational KG embeddings. To apply the geometric transformations which account for different relations and the attention mechanism in the complex hyperbolic space, we propose to use the fast Fourier transform (FFT) as the conversion between the real and complex hyperbolic space. Constructing the attention-based transformations in the complex space is very challenging, while the proposed Fourier transform-based complex hyperbolic approaches provide a simple and effective solution. Experimental results show that our methods outperform the baselines, including the Euclidean and the real hyperbolic embedding models.</abstract>
      <url hash="4d883663">2022.emnlp-main.349</url>
      <bibkey>xiao-etal-2022-complex</bibkey>
    </paper>
    <paper id="350">
      <title>Towards Knowledge-Intensive Text-to-<fixed-case>SQL</fixed-case> Semantic Parsing with Formulaic Knowledge</title>
      <author><first>Longxu</first><last>Dou</last></author>
      <author><first>Yan</first><last>Gao</last></author>
      <author><first>Xuqi</first><last>Liu</last></author>
      <author><first>Mingyang</first><last>Pan</last></author>
      <author><first>Dingzirui</first><last>Wang</last></author>
      <author><first>Wanxiang</first><last>Che</last></author>
      <author><first>Dechen</first><last>Zhan</last></author>
      <author><first>Min-Yen</first><last>Kan</last></author>
      <author><first>Jian-Guang</first><last>Lou</last></author>
      <pages>5240-5253</pages>
      <abstract>In this paper, we study the problem of knowledge-intensive text-to-SQL, in which domain knowledge is necessary to parse expert questions into SQL queries over domain-specific tables. We formalize this scenario by building a new benchmark KnowSQL consisting of domain-specific questions covering various domains. We then address this problem by representing formulaic knowledge rather than by annotating additional data examples. More concretely, we construct a formulaic knowledge bank as a domain knowledge base and propose a framework (ReGrouP) to leverage this formulaic knowledge during parsing. Experiments using ReGrouP demonstrate a significant 28.2% improvement overall on KnowSQL.</abstract>
      <url hash="424b5523">2022.emnlp-main.350</url>
      <bibkey>dou-etal-2022-towards</bibkey>
    </paper>
    <paper id="351">
      <title>Should We Ban <fixed-case>E</fixed-case>nglish <fixed-case>NLP</fixed-case> for a Year?</title>
      <author><first>Anders</first><last>Søgaard</last></author>
      <pages>5254-5260</pages>
      <abstract>Around two thirds of NLP research at top venues is devoted exclusively to developing technology for speakers of English, most speech data comes from young urban speakers, and most texts used to train language models come from male writers. These biases feed into consumer technologies to widen existing inequality gaps, not only within, but also across, societies. Many have argued that it is almost impossible to mitigate inequality amplification. I argue that, on the contrary, it is quite simple to do so, and that counter-measures would have little-to-no negative impact, except for, perhaps, in the very short term.</abstract>
      <url hash="e83c6c43">2022.emnlp-main.351</url>
      <bibkey>sogaard-2022-ban</bibkey>
    </paper>
    <paper id="352">
      <title><fixed-case>L</fixed-case>ittle<fixed-case>B</fixed-case>ird: Efficient Faster &amp; Longer Transformer for Question Answering</title>
      <author><first>Minchul</first><last>Lee</last></author>
      <author><first>Kijong</first><last>Han</last></author>
      <author><first>Myeong Cheol</first><last>Shin</last></author>
      <pages>5261-5277</pages>
      <abstract>BERT has shown a lot of sucess in a wide variety of NLP tasks. But it has a limitation dealing with long inputs due to its attention mechanism. Longformer, ETC and BigBird addressed this issue and effectively solved the quadratic dependency problem.However we find that these models are not sufficient, and propose LittleBird, a novel model based on BigBird with improved speed and memory footprint while maintaining accuracy.In particular, we devise a more flexible and efficient position representation method based on Attention with Linear Biases(ALiBi). We also show that replacing the method of global information represented in the BigBird with pack and unpack attention is more effective.The proposed model can work on long inputs even after being pre-trained on short inputs, and can be trained efficiently reusing existing pre-trained language model for short inputs. This is a significant benefit for low-resource languages where large amounts of long text data are difficult to obtain.As a result, our experiments show that LittleBird works very well in a variety of languages, achieving high performance in question answering tasks, particularly in KorQuAD2.0, Korean Question Answering Dataset for long paragraphs.</abstract>
      <url hash="455a3258">2022.emnlp-main.352</url>
      <bibkey>lee-etal-2022-littlebird</bibkey>
    </paper>
    <paper id="353">
      <title><fixed-case>W</fixed-case>e<fixed-case>TS</fixed-case>: A Benchmark for Translation Suggestion</title>
      <author><first>Zhen</first><last>Yang</last></author>
      <author><first>Fandong</first><last>Meng</last></author>
      <author><first>Yingxue</first><last>Zhang</last></author>
      <author><first>Ernan</first><last>Li</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>5278-5290</pages>
      <abstract>Translation suggestion (TS), which provides alternatives for specific words or phrases given the entire documents generated by machine translation (MT), has been proven to play a significant role in post-editing (PE). There are two main pitfalls for existing researches in this line. First, most conventional works only focus on the overall performance of PE but ignore the exact performance of TS, which makes the progress of PE sluggish and less explainable; Second, as no publicly available golden dataset exists to support in-depth research for TS, almost all of the previous works conduct experiments on their in-house datasets or the noisy datasets built automatically, which makes their experiments hard to be reproduced and compared. To break these limitations mentioned above and spur the research in TS, we create a benchmark dataset, called <i>WeTS</i>, which is a golden corpus annotated by expert translators on four translation directions. Apart from the golden corpus, we also propose several methods to generate synthetic corpora which can be used to improve the performance substantially through pre-training. As for the model, we propose the segment-aware self-attention based Transformer for TS. Experimental results show that our approach achieves the best results on all four directions, including English-to-German, German-to-English, Chinese-to-English, and English-to-Chinese.</abstract>
      <url hash="f21d4233">2022.emnlp-main.353</url>
      <bibkey>yang-etal-2022-wets</bibkey>
    </paper>
    <paper id="354">
      <title>Discrete Cross-Modal Alignment Enables Zero-Shot Speech Translation</title>
      <author><first>Chen</first><last>Wang</last></author>
      <author><first>Yuchen</first><last>Liu</last></author>
      <author><first>Boxing</first><last>Chen</last></author>
      <author><first>Jiajun</first><last>Zhang</last></author>
      <author><first>Wei</first><last>Luo</last></author>
      <author><first>Zhongqiang</first><last>Huang</last></author>
      <author><first>Chengqing</first><last>Zong</last></author>
      <pages>5291-5302</pages>
      <abstract>End-to-end Speech Translation (ST) aims at translating the source language speech into target language text without generating the intermediate transcriptions. However, the training of end-to-end methods relies on parallel ST data, which are difficult and expensive to obtain. Fortunately, the supervised data for automatic speech recognition (ASR) and machine translation (MT) are usually more accessible, making zero-shot speech translation a potential direction. Existing zero-shot methods fail to align the two modalities of speech and text into a shared semantic space, resulting in much worse performance compared to the supervised ST methods. In order to enable zero-shot ST, we propose a novel Discrete Cross-Modal Alignment (DCMA) method that employs a shared discrete vocabulary space to accommodate and match both modalities of speech and text. Specifically, we introduce a vector quantization module to discretize the continuous representations of speech and text into a finite set of virtual tokens, and use ASR data to map corresponding speech and text to the same virtual token in a shared codebook. This way, source language speech can be embedded in the same semantic space as the source language text, which can be then transformed into target language text with an MT module. Experiments on multiple language pairs demonstrate that our zero-shot ST method significantly improves the SOTA, and even performers on par with the strong supervised ST baselines.</abstract>
      <url hash="7c92b620">2022.emnlp-main.354</url>
      <bibkey>wang-etal-2022-discrete</bibkey>
    </paper>
    <paper id="355">
      <title>Abstractive Summarization Guided by Latent Hierarchical Document Structure</title>
      <author><first>Yifu</first><last>Qiu</last></author>
      <author><first>Shay B.</first><last>Cohen</last></author>
      <pages>5303-5317</pages>
      <abstract>Sequential abstractive neural summarizers often do not use the underlying structure in the input article or dependencies between the input sentences. This structure is essential to integrate and consolidate information from different parts of the text. To address this shortcoming, we propose a hierarchy-aware graph neural network (HierGNN) which captures such dependencies through three main steps: 1) learning a hierarchical document structure through a latent structure tree learned by a sparse matrix-tree computation; 2) propagating sentence information over this structure using a novel message-passing node propagation mechanism to identify salient information; 3) using graph-level attention to concentrate the decoder on salient information. Experiments confirm HierGNN improves strong sequence models such as BART, with a 0.55 and 0.75 margin in average ROUGE-1/2/L for CNN/DM and XSum. Further human evaluation demonstrates that summaries produced by our model are more relevant and less redundant than the baselines, into which HierGNN is incorporated. We also find HierGNN synthesizes summaries by fusing multiple source sentences more, rather than compressing a single source sentence, and that it processes long inputs more effectively.</abstract>
      <url hash="ca2e51dc">2022.emnlp-main.355</url>
      <bibkey>qiu-cohen-2022-abstractive</bibkey>
    </paper>
    <paper id="356">
      <title>Explainable Question Answering based on Semantic Graph by Global Differentiable Learning and Dynamic Adaptive Reasoning</title>
      <author><first>Jianguo</first><last>Mao</last></author>
      <author><first>Wenbin</first><last>Jiang</last></author>
      <author><first>Xiangdong</first><last>Wang</last></author>
      <author><first>Hong</first><last>Liu</last></author>
      <author><first>Yu</first><last>Xia</last></author>
      <author><first>Yajuan</first><last>Lyu</last></author>
      <author><first>QiaoQiao</first><last>She</last></author>
      <pages>5318-5325</pages>
      <abstract>Multi-hop Question Answering is an agent task for testing the reasoning ability. With the development of pre-trained models, the implicit reasoning ability has been surprisingly improved and can even surpass human performance. However, the nature of the black box hinders the construction of explainable intelligent systems. Several researchers have explored explainable neural-symbolic reasoning methods based on question decomposition techniques. The undifferentiable symbolic operations and the error propagation in the reasoning process lead to poor performance. To alleviate it, we propose a simple yet effective Global Differentiable Learning strategy to explore optimal reasoning paths from the latent probability space so that the model learns to solve intermediate reasoning processes without expert annotations. We further design a Dynamic Adaptive Reasoner to enhance the generalization of unseen questions. Our method achieves 17% improvements in F1-score against BreakRC and shows better interpretability. We take a step forward in building interpretable reasoning methods.</abstract>
      <url hash="596ab8ff">2022.emnlp-main.356</url>
      <bibkey>mao-etal-2022-explainable</bibkey>
    </paper>
    <paper id="357">
      <title><fixed-case>D</fixed-case>u<fixed-case>R</fixed-case>eader-Retrieval: A Large-scale <fixed-case>C</fixed-case>hinese Benchmark for Passage Retrieval from Web Search Engine</title>
      <author><first>Yifu</first><last>Qiu</last></author>
      <author><first>Hongyu</first><last>Li</last></author>
      <author><first>Yingqi</first><last>Qu</last></author>
      <author><first>Ying</first><last>Chen</last></author>
      <author><first>QiaoQiao</first><last>She</last></author>
      <author><first>Jing</first><last>Liu</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <pages>5326-5338</pages>
      <abstract>In this paper, we present DuReader-retrieval, a large-scale Chinese dataset for passage retrieval. DuReader-retrieval contains more than 90K queries and over 8M unique passages from a commercial search engine. To alleviate the shortcomings of other datasets and ensure the quality of our benchmark, we (1) reduce the false negatives in development and test sets by manually annotating results pooled from multiple retrievers, and (2) remove the training queries that are semantically similar to the development and testing queries. Additionally, we provide two out-of-domain testing sets for cross-domain evaluation, as well as a set of human translated queries for for cross-lingual retrieval evaluation. The experiments demonstrate that DuReader-retrieval is challenging and a number of problems remain unsolved, such as the salient phrase mismatch and the syntactic mismatch between queries and paragraphs. These experiments also show that dense retrievers do not generalize well across domains, and cross-lingual retrieval is essentially challenging. DuReader-retrieval is publicly available at https://github.com/baidu/DuReader/tree/master/DuReader-Retrieval.</abstract>
      <url hash="3548d9df">2022.emnlp-main.357</url>
      <bibkey>qiu-etal-2022-dureader</bibkey>
    </paper>
    <paper id="358">
      <title>Pair-Based Joint Encoding with Relational Graph Convolutional Networks for Emotion-Cause Pair Extraction</title>
      <author><first>Junlong</first><last>Liu</last></author>
      <author><first>Xichen</first><last>Shang</last></author>
      <author><first>Qianli</first><last>Ma</last></author>
      <pages>5339-5351</pages>
      <abstract>Emotion-cause pair extraction (ECPE) aims to extract emotion clauses and corresponding cause clauses, which have recently received growing attention. Previous methods sequentially encode features with a specified order. They first encode the emotion and cause features for clause extraction and then combine them for pair extraction. This lead to an imbalance in inter-task feature interaction where features extracted later have no direct contact with the former. To address this issue, we propose a novel **P**air-**B**ased **J**oint **E**ncoding (**PBJE**) network, which generates pairs and clauses features simultaneously in a joint feature encoding manner to model the causal relationship in clauses. PBJE can balance the information flow among emotion clauses, cause clauses and pairs. From a multi-relational perspective, we construct a heterogeneous undirected graph and apply the Relational Graph Convolutional Network (RGCN) to capture the multiplex relationship between clauses and the relationship between pairs and clauses. Experimental results show that PBJE achieves state-of-the-art performance on the Chinese benchmark corpus.</abstract>
      <url hash="f8e7b2c0">2022.emnlp-main.358</url>
      <bibkey>liu-etal-2022-pair</bibkey>
    </paper>
    <paper id="359">
      <title>Affective Knowledge Enhanced Multiple-Graph Fusion Networks for Aspect-based Sentiment Analysis</title>
      <author><first>Siyu</first><last>Tang</last></author>
      <author><first>Heyan</first><last>Chai</last></author>
      <author><first>Ziyi</first><last>Yao</last></author>
      <author><first>Ye</first><last>Ding</last></author>
      <author><first>Cuiyun</first><last>Gao</last></author>
      <author><first>Binxing</first><last>Fang</last></author>
      <author><first>Qing</first><last>Liao</last></author>
      <pages>5352-5362</pages>
      <abstract>Aspect-based sentiment analysis aims to identify sentiment polarity of social media users toward different aspects. Most recent methods adopt the aspect-centric latent tree to connect aspects and their corresponding opinion words, thinking that would facilitate establishing the relationship between aspects and opinion words.However, these methods ignore the roles of syntax dependency relation labels and affective semantic information in determining the sentiment polarity, resulting in the wrong prediction.In this paper, we propose a novel multi-graph fusion network (MGFN) based on latent graph to leverage the richer syntax dependency relation label information and affective semantic information of words.Specifically, we construct a novel syntax-aware latent graph (SaLG) to fully leverage the syntax dependency relation label information to facilitate the learning of sentiment representations. Subsequently, a multi-graph fusion module is proposed to fuse semantic information of surrounding contexts of aspects adaptively. Furthermore, we design an affective refinement strategy to guide the MGFN to capture significant affective clues. Extensive experiments on three datasets demonstrate that our MGFN model outperforms all state-of-the-art methods and verify the effectiveness of our model.</abstract>
      <url hash="6613fa07">2022.emnlp-main.359</url>
      <bibkey>tang-etal-2022-affective</bibkey>
    </paper>
    <paper id="360">
      <title><fixed-case>I</fixed-case>ndic<fixed-case>NLG</fixed-case> Benchmark: Multilingual Datasets for Diverse <fixed-case>NLG</fixed-case> Tasks in <fixed-case>I</fixed-case>ndic Languages</title>
      <author><first>Aman</first><last>Kumar</last></author>
      <author><first>Himani</first><last>Shrotriya</last></author>
      <author><first>Prachi</first><last>Sahu</last></author>
      <author><first>Amogh</first><last>Mishra</last></author>
      <author><first>Raj</first><last>Dabre</last></author>
      <author><first>Ratish</first><last>Puduppully</last></author>
      <author><first>Anoop</first><last>Kunchukuttan</last></author>
      <author><first>Mitesh M.</first><last>Khapra</last></author>
      <author><first>Pratyush</first><last>Kumar</last></author>
      <pages>5363-5394</pages>
      <abstract>Natural Language Generation (NLG) for non-English languages is hampered by the scarcity of datasets in these languages. We present the IndicNLG Benchmark, a collection of datasets for benchmarking NLG for 11 Indic languages. We focus on five diverse tasks, namely, biography generation using Wikipedia infoboxes, news headline generation, sentence summarization, paraphrase generation and, question generation. We describe the created datasets and use them to benchmark the performance of several monolingual and multilingual baselines that leverage pre-trained sequence-to-sequence models. Our results exhibit the strong performance of multilingual language-specific pre-trained models, and the utility of models trained on our dataset for other related NLG tasks. Our dataset creation methods can be easily applied to modest-resource languages as they involve simple steps such as scraping news articles and Wikipedia infoboxes, light cleaning, and pivoting through machine translation data. To the best of our knowledge, the IndicNLG Benchmark is the first NLG benchmark for Indic languages and the most diverse multilingual NLG dataset, with approximately 8M examples across 5 tasks and 11 languages. The datasets and models will be publicly available.</abstract>
      <url hash="8159cc70">2022.emnlp-main.360</url>
      <bibkey>kumar-etal-2022-indicnlg</bibkey>
    </paper>
    <paper id="361">
      <title>Improving Machine Translation with Phrase Pair Injection and Corpus Filtering</title>
      <author><first>Akshay</first><last>Batheja</last></author>
      <author><first>Pushpak</first><last>Bhattacharyya</last></author>
      <pages>5395-5400</pages>
      <abstract>In this paper, we show that the combination of Phrase Pair Injection and Corpus Filtering boosts the performance of Neural Machine Translation (NMT) systems. We extract parallel phrases and sentences from the pseudo-parallel corpus and augment it with the parallel corpus to train the NMT models. With the proposed approach, we observe an improvement in the Machine Translation (MT) system for 3 low-resource language pairs, Hindi-Marathi, English-Marathi, and English-Pashto, and 6 translation directions by up to 2.7 BLEU points, on the FLORES test data. These BLEU score improvements are over the models trained using the whole pseudo-parallel corpus augmented with the parallel corpus.</abstract>
      <url hash="d82889e1">2022.emnlp-main.361</url>
      <bibkey>batheja-bhattacharyya-2022-improving</bibkey>
    </paper>
    <paper id="362">
      <title>An Anchor-based Relative Position Embedding Method for Cross-Modal Tasks</title>
      <author><first>Ya</first><last>Wang</last></author>
      <author><first>Xingwu</first><last>Sun</last></author>
      <author><first>Lian</first><last>Fengzong</last></author>
      <author><first>ZhanHui</first><last>Kang</last></author>
      <author><first>Chengzhong Xu</first><last>Xu</last></author>
      <pages>5401-5413</pages>
      <abstract>Position Embedding (PE) is essential for transformer to capture the sequence ordering of input tokens. Despite its general effectiveness verified in Natural Language Processing (NLP) and Computer Vision (CV), its application in cross-modal tasks remains unexplored and suffers from two challenges: 1) the input text tokens and image patches are not aligned, 2) the encoding space of each modality is different, making it unavailable for feature comparison. In this paper, we propose a unified position embedding method for these problems, called AnChor-basEd Relative Position Embedding (ACE-RPE), in which we first introduce an anchor locating mechanism to bridge the semantic gap and locate anchors from different modalities. Then we conduct the distance calculation of each text token and image patch by computing their shortest paths from the located anchors. Last, we embed the anchor-based distance to guide the computation of cross-attention. In this way, it calculates cross-modal relative position embedding for cross-modal transformer. Benefiting from ACE-RPE, our method obtains new SOTA results on a wide range of benchmarks, such as Image-Text Retrieval on MS-COCO and Flickr30K, Visual Entailment on SNLI-VE, Visual Reasoning on NLVR2 and Weakly-supervised Visual Grounding on RefCOCO+.</abstract>
      <url hash="7f9c688c">2022.emnlp-main.362</url>
      <bibkey>wang-etal-2022-anchor</bibkey>
    </paper>
    <paper id="363">
      <title>Norm-based Noisy Corpora Filtering and Refurbishing in Neural Machine Translation</title>
      <author><first>Yu</first><last>Lu</last></author>
      <author><first>Jiajun</first><last>Zhang</last></author>
      <pages>5414-5425</pages>
      <abstract>Recent advances in neural machine translation depend on massive parallel corpora, which are collected from any open source without much guarantee of quality. It stresses the need for noisy corpora filtering, but existing methods are insufficient to solve this issue. They spend much time ensembling multiple scorers trained on clean bitexts, unavailable for low-resource languages in practice. In this paper, we propose a norm-based noisy corpora filtering and refurbishing method with no external data and costly scorers. The noisy and clean samples are separated based on how much information from the source and target sides the model requires to fit the given translation. For the unparallel sentence, the target-side history translation is much more important than the source context, contrary to the parallel ones. The amount of these two information flows can be measured by norms of source-/target-side context vectors. Moreover, we propose to reuse the discovered noisy data by generating pseudo labels via online knowledge distillation. Extensive experiments show that our proposed filtering method performs comparably with state-of-the-art noisy corpora filtering techniques but is more efficient and easier to operate. Noisy sample refurbishing further enhances the performance by making the most of the given data.</abstract>
      <url hash="5a1ee4bb">2022.emnlp-main.363</url>
      <bibkey>lu-zhang-2022-norm</bibkey>
    </paper>
    <paper id="364">
      <title><fixed-case>T</fixed-case>ele<fixed-case>M</fixed-case>elody: Lyric-to-Melody Generation with a Template-Based Two-Stage Method</title>
      <author><first>Zeqian</first><last>Ju</last></author>
      <author><first>Peiling</first><last>Lu</last></author>
      <author><first>Xu</first><last>Tan</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Chen</first><last>Zhang</last></author>
      <author><first>Songruoyao</first><last>Wu</last></author>
      <author><first>Kejun</first><last>Zhang</last></author>
      <author><first>Xiang-Yang</first><last>Li</last></author>
      <author><first>Tao</first><last>Qin</last></author>
      <author><first>Tie-Yan</first><last>Liu</last></author>
      <pages>5426-5437</pages>
      <abstract>Lyric-to-melody generation is an important task in automatic songwriting. Previous lyric-to-melody generation systems usually adopt end-to-end models that directly generate melodies from lyrics, which suffer from several issues: 1) lack of paired lyric-melody training data; 2) lack of control on generated melodies. In this paper, we develop TeleMelody, a two-stage lyric-to-melody generation system with music template (e.g., tonality, chord progression, rhythm pattern, and cadence) to bridge the gap between lyrics and melodies (i.e., the system consists of a lyric-to-template module and a template-to-melody module). TeleMelody has two advantages. First, it is data efficient. The template-to-melody module is trained in a self-supervised way (i.e., the source template is extracted from the target melody) that does not need any lyric-melody paired data. The lyric-to-template module is made up of some rules and a lyric-to-rhythm model, which is trained with paired lyric-rhythm data that is easier to obtain than paired lyric-melody data. Second, it is controllable. The design of the template ensures that the generated melodies can be controlled by adjusting the musical elements in the template. Both subjective and objective experimental evaluations demonstrate that TeleMelody generates melodies with higher quality, better controllability, and less requirement on paired lyric-melody data than previous generation systems.</abstract>
      <url hash="ecabf7c0">2022.emnlp-main.364</url>
      <bibkey>ju-etal-2022-telemelody</bibkey>
    </paper>
    <paper id="365">
      <title><fixed-case>SEEN</fixed-case>: Structured Event Enhancement Network for Explainable Need Detection of Information Recall Assistance</title>
      <author><first>You-En</first><last>Lin</last></author>
      <author><first>An-Zi</first><last>Yen</last></author>
      <author><first>Hen-Hsen</first><last>Huang</last></author>
      <author><first>Hsin-Hsi</first><last>Chen</last></author>
      <pages>5438-5451</pages>
      <abstract>When recalling life experiences, people often forget or confuse life events, which necessitates information recall services. Previous work on information recall focuses on providing such assistance reactively, i.e., by retrieving the life event of a given query. Proactively detecting the need for information recall services is rarely discussed. In this paper, we use a human-annotated life experience retelling dataset to detect the right time to trigger the information recall service. We propose a pilot model—structured event enhancement network (SEEN) that detects life event inconsistency, additional information in life events, and forgotten events. A fusing mechanism is also proposed to incorporate event graphs of stories and enhance the textual representations. To explain the need detection results, SEEN simultaneously provides support evidence by selecting the related nodes from the event graph. Experimental results show that SEEN achieves promising performance in detecting information needs. In addition, the extracted evidence can be served as complementary information to remind users what events they may want to recall.</abstract>
      <url hash="369a606f">2022.emnlp-main.365</url>
      <bibkey>lin-etal-2022-seen</bibkey>
    </paper>
    <paper id="366">
      <title>Rethinking Style Transformer with Energy-based Interpretation: Adversarial Unsupervised Style Transfer using a Pretrained Model</title>
      <author><first>Hojun</first><last>Cho</last></author>
      <author><first>Dohee</first><last>Kim</last></author>
      <author><first>Seungwoo</first><last>Ryu</last></author>
      <author><first>ChaeHun</first><last>Park</last></author>
      <author><first>Hyungjong</first><last>Noh</last></author>
      <author><first>Jeong-in</first><last>Hwang</last></author>
      <author><first>Minseok</first><last>Choi</last></author>
      <author><first>Edward</first><last>Choi</last></author>
      <author><first>Jaegul</first><last>Choo</last></author>
      <pages>5452-5467</pages>
      <abstract>Style control, content preservation, and fluency determine the quality of text style transfer models. To train on a nonparallel corpus, several existing approaches aim to deceive the style discriminator with an adversarial loss. However, adversarial training significantly degrades fluency compared to the other two metrics. In this work, we explain this phenomenon using energy-based interpretation, and leverage a pretrained language model to improve fluency. Specifically, we propose a novel approach which applies the pretrained language model to the text style transfer framework by restructuring the discriminator and the model itself, allowing the generator and the discriminator to also take advantage of the power of the pretrained model. We evaluated our model on three public benchmarks GYAFC, Amazon, and Yelp and achieved state-of-the-art performance on the overall metrics.</abstract>
      <url hash="212c867b">2022.emnlp-main.366</url>
      <bibkey>cho-etal-2022-rethinking</bibkey>
    </paper>
    <paper id="367">
      <title>Towards Robust k-Nearest-Neighbor Machine Translation</title>
      <author><first>Hui</first><last>Jiang</last></author>
      <author><first>Ziyao</first><last>Lu</last></author>
      <author><first>Fandong</first><last>Meng</last></author>
      <author><first>Chulun</first><last>Zhou</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <author><first>Degen</first><last>Huang</last></author>
      <author><first>Jinsong</first><last>Su</last></author>
      <pages>5468-5477</pages>
      <abstract>k-Nearest-Neighbor Machine Translation (kNN-MT) becomes an important research direction of NMT in recent years. Its main idea is to retrieve useful key-value pairs from an additional datastore to modify translations without updating the NMT model. However, the underlying retrieved noisy pairs will dramatically deteriorate the model performance. In this paper, we conduct a preliminary study and find that this problem results from not fully exploiting the prediction of the NMT model. To alleviate the impact of noise, we propose a confidence-enhanced kNN-MT model with robust training. Concretely, we introduce the NMT confidence to refine the modeling of two important components of kNN-MT: kNN distribution and the interpolation weight. Meanwhile we inject two types of perturbations into the retrieved pairs for robust training. Experimental results on four benchmark datasets demonstrate that our model not only achieves significant improvements over current kNN-MT models, but also exhibits better robustness. Our code is available at https://github.com/DeepLearnXMU/Robust-knn-mt.</abstract>
      <url hash="8a3bf786">2022.emnlp-main.367</url>
      <bibkey>jiang-etal-2022-towards</bibkey>
    </paper>
    <paper id="368">
      <title>Tiny-<fixed-case>N</fixed-case>ews<fixed-case>R</fixed-case>ec: Effective and Efficient <fixed-case>PLM</fixed-case>-based News Recommendation</title>
      <author><first>Yang</first><last>Yu</last></author>
      <author><first>Fangzhao</first><last>Wu</last></author>
      <author><first>Chuhan</first><last>Wu</last></author>
      <author><first>Jingwei</first><last>Yi</last></author>
      <author><first>Qi</first><last>Liu</last></author>
      <pages>5478-5489</pages>
      <abstract>News recommendation is a widely adopted technique to provide personalized news feeds for the user. Recently, pre-trained language models (PLMs) have demonstrated the great capability of natural language understanding and benefited news recommendation via improving news modeling. However, most existing works simply finetune the PLM with the news recommendation task, which may suffer from the known domain shift problem between the pre-training corpus and downstream news texts. Moreover, PLMs usually contain a large volume of parameters and have high computational overhead, which imposes a great burden on low-latency online services. In this paper, we propose Tiny-NewsRec, which can improve both the effectiveness and the efficiency of PLM-based news recommendation. We first design a self-supervised domain-specific post-training method to better adapt the general PLM to the news domain with a contrastive matching task between news titles and news bodies. We further propose a two-stage knowledge distillation method to improve the efficiency of the large PLM-based news recommendation model while maintaining its performance. Multiple teacher models originated from different time steps of our post-training procedure are used to transfer comprehensive knowledge to the student model in both its post-training stage and finetuning stage. Extensive experiments on two real-world datasets validate the effectiveness and efficiency of our method.</abstract>
      <url hash="63eea59c">2022.emnlp-main.368</url>
      <bibkey>yu-etal-2022-tiny</bibkey>
    </paper>
    <paper id="369">
      <title><fixed-case>TABS</fixed-case>: Efficient Textual Adversarial Attack for Pre-trained <fixed-case>NL</fixed-case> Code Model Using Semantic Beam Search</title>
      <author><first>YunSeok</first><last>Choi</last></author>
      <author><first>Hyojun</first><last>Kim</last></author>
      <author><first>Jee-Hyong</first><last>Lee</last></author>
      <pages>5490-5498</pages>
      <abstract>As pre-trained models have shown successful performance in program language processing as well as natural language processing, adversarial attacks on these models also attract attention.However, previous works on black-box adversarial attacks generated adversarial examples in a very inefficient way with simple greedy search. They also failed to find out better adversarial examples because it was hard to reduce the search space without performance loss.In this paper, we propose TABS, an efficient beam search black-box adversarial attack method. We adopt beam search to find out better adversarial examples, and contextual semantic filtering to effectively reduce the search space. Contextual semantic filtering reduces the number of candidate adversarial words considering the surrounding context and the semantic similarity.Our proposed method shows good performance in terms of attack success rate, the number of queries, and semantic similarity in attacking models for two tasks: NL code search classification and retrieval tasks.</abstract>
      <url hash="e060217d">2022.emnlp-main.369</url>
      <bibkey>choi-etal-2022-tabs</bibkey>
    </paper>
    <paper id="370">
      <title>Investigating the Robustness of Natural Language Generation from Logical Forms via Counterfactual Samples</title>
      <author><first>Chengyuan</first><last>Liu</last></author>
      <author><first>Leilei</first><last>Gan</last></author>
      <author><first>Kun</first><last>Kuang</last></author>
      <author><first>Fei</first><last>Wu</last></author>
      <pages>5499-5512</pages>
      <abstract>The aim of Logic2Text is to generate controllable and faithful texts conditioned on tables and logical forms, which not only requires a deep understanding of the tables and logical forms, but also warrants symbolic reasoning over the tables according to the logical forms. State-of-the-art methods based on pre-trained models have achieved remarkable performance on the standard test dataset. However, we question whether these methods really learn how to perform logical reasoning, rather than just relying on the spurious correlations between the headers of the tables and operators of the logical form. To verify this hypothesis, we manually construct a set of counterfactual samples, which modify the original logical forms to generate counterfactual logical forms with rare co-occurred headers and operators and corresponding counterfactual references. SOTA methods give much worse results on these counterfactual samples compared with the results on the original test dataset, which verifies our hypothesis. To deal with this problem, we firstly analyze this bias from a causal perspective, based on which we propose two approaches to reduce the model’s reliance on the shortcut. The first one incorporates the hierarchical structure of the logical forms into the model. The second one exploits automatically generated counterfactual data for training. Automatic and manual experimental results on the original test dataset and counterfactual dataset show that our method is effective to alleviate the spurious correlation. Our work points out the weakness of current methods and takes a further step toward developing Logic2Text models with real logical reasoning ability.</abstract>
      <url hash="2fd5ab97">2022.emnlp-main.370</url>
      <bibkey>liu-etal-2022-investigating</bibkey>
    </paper>
    <paper id="371">
      <title>Helping the Weak Makes You Strong: Simple Multi-Task Learning Improves Non-Autoregressive Translators</title>
      <author><first>Xinyou</first><last>Wang</last></author>
      <author><first>Zaixiang</first><last>Zheng</last></author>
      <author><first>Shujian</first><last>Huang</last></author>
      <pages>5513-5519</pages>
      <abstract>Recently, non-autoregressive (NAR) neural machine translation models have received increasing attention due to their efficient parallel decoding.However, the probabilistic framework of NAR models necessitates conditional independence assumption on target sequences, falling short of characterizing human language data.This drawback results in less informative learning signals for NAR models under conventional MLE training, thereby yielding unsatisfactory accuracy compared to their autoregressive (AR) counterparts.In this paper, we propose a simple and model-agnostic multi-task learning framework to provide more informative learning signals.During training stage, we introduce a set of sufficiently weak AR decoders that solely rely on the information provided by NAR decoder to make prediction, forcing the NAR decoder to become stronger or else it will be unable to support its weak AR partners.Experiments on WMT and IWSLT datasets show that our approach can consistently improve accuracy of multiple NAR baselines without adding any additional decoding overhead.</abstract>
      <url hash="6878a4e9">2022.emnlp-main.371</url>
      <bibkey>wang-etal-2022-helping</bibkey>
    </paper>
    <paper id="372">
      <title><fixed-case>RACE</fixed-case>: Retrieval-augmented Commit Message Generation</title>
      <author><first>Ensheng</first><last>Shi</last></author>
      <author><first>Yanlin</first><last>Wang</last></author>
      <author><first>Wei</first><last>Tao</last></author>
      <author><first>Lun</first><last>Du</last></author>
      <author><first>Hongyu</first><last>Zhang</last></author>
      <author><first>Shi</first><last>Han</last></author>
      <author><first>Dongmei</first><last>Zhang</last></author>
      <author><first>Hongbin</first><last>Sun</last></author>
      <pages>5520-5530</pages>
      <abstract>Commit messages are important for software development and maintenance. Many neural network-based approaches have been proposed and shown promising results on automatic commit message generation. However, the generated commit messages could be repetitive or redundant. In this paper, we propose RACE, a new retrieval-augmented neural commit message generation method, which treats the retrieved similar commit as an exemplar and leverages it to generate an accurate commit message. As the retrieved commit message may not always accurately describe the content/intent of the current code diff, we also propose an exemplar guider, which learns the semantic similarity between the retrieved and current code diff and then guides the generation of commit message based on the similarity. We conduct extensive experiments on a large public dataset with five programming languages. Experimental results show that RACE can outperform all baselines. Furthermore, RACE can boost the performance of existing Seq2Seq models in commit message generation.</abstract>
      <url hash="da29350c">2022.emnlp-main.372</url>
      <bibkey>shi-etal-2022-race</bibkey>
    </paper>
    <paper id="373">
      <title><fixed-case>PLOG</fixed-case>: Table-to-Logic Pretraining for Logical Table-to-Text Generation</title>
      <author><first>Ao</first><last>Liu</last></author>
      <author><first>Haoyu</first><last>Dong</last></author>
      <author><first>Naoaki</first><last>Okazaki</last></author>
      <author><first>Shi</first><last>Han</last></author>
      <author><first>Dongmei</first><last>Zhang</last></author>
      <pages>5531-5546</pages>
      <abstract>Logical table-to-text generation is a task that involves generating logically faithful sentences from tables, which requires models to derive logical-level facts from table records via logical inference. It raises a new challenge on the logical-level content planning of table-to-text models. However, directly learning the logical inference knowledge from table-text pairs is very difficult for neural models because of the ambiguity of natural language and the scarcity of parallel data. Hence even large-scale pre-trained language models present low logical fidelity on logical table-to-text. In this work, we propose a Pretrained Logical Form Generator (PLOG) framework to improve generation fidelity. Specifically, PLOG is first pretrained on a table-to-logical-form generation (table-to-logic) task, then finetuned on downstream table-to-text tasks. The logical forms are formally defined with unambiguous semantics. Hence we can collect a large amount of accurate logical forms from tables without human annotation. In addition, PLOG can learn logical inference from table-logic pairs much more reliably than from table-text pairs. To evaluate our model, we further collect a controlled logical table-to-text dataset CONTLOG based on an existing dataset. On two benchmarks, LOGICNLG and CONTLOG, PLOG outperforms strong baselines by a large margin on the logical fidelity, demonstrating the effectiveness of table-to-logic pretraining.</abstract>
      <url hash="68fc26c6">2022.emnlp-main.373</url>
      <bibkey>liu-etal-2022-plog</bibkey>
    </paper>
    <paper id="374">
      <title><fixed-case>GHAN</fixed-case>: Graph-Based Hierarchical Aggregation Network for Text-Video Retrieval</title>
      <author><first>Yahan</first><last>Yu</last></author>
      <author><first>Bojie</first><last>Hu</last></author>
      <author><first>Yu</first><last>Li</last></author>
      <pages>5547-5557</pages>
      <abstract>Text-video retrieval focuses on two aspects: cross-modality interaction and video-language encoding. Currently, the mainstream approach is to train a joint embedding space for multimodal interactions. However, there are structural and semantic differences between text and video, making this approach challenging for fine-grained understanding. In order to solve this, we propose an end-to-end graph-based hierarchical aggregation network for text-video retrieval according to the hierarchy possessed by text and video. We design a token-level weighted network to refine intra-modality representations and construct a graph-based message passing attention network for global-local alignment across modality. We conduct experiments on the public datasets MSR-VTT-9K, MSR-VTT-7K and MSVD, and achieve Recall@1 of 73.0%, 65.6%, and 64.0% , which is 25.7%, 16.5%, and 14.2% better than the current state-of-the-art model.</abstract>
      <url hash="e570ce28">2022.emnlp-main.374</url>
      <bibkey>yu-etal-2022-ghan</bibkey>
    </paper>
    <paper id="375">
      <title><fixed-case>M</fixed-case>u<fixed-case>RAG</fixed-case>: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text</title>
      <author><first>Wenhu</first><last>Chen</last></author>
      <author><first>Hexiang</first><last>Hu</last></author>
      <author><first>Xi</first><last>Chen</last></author>
      <author><first>Pat</first><last>Verga</last></author>
      <author><first>William</first><last>Cohen</last></author>
      <pages>5558-5570</pages>
      <abstract>While language Models store a massive amount of world knowledge implicitly in their parameters, even very large models often fail to encode information about rare entities and events, while incurring huge computational costs. Recently, retrieval-augmented models, such as REALM, RAG, and RETRO, have incorporated world knowledge into language generation by leveraging an external non-parametric index and have demonstrated impressive performance with constrained model sizes. However, these methods are restricted to retrieving only textual knowledge, neglecting the ubiquitous amount of knowledge in other modalities like images – much of which contains information not covered by any text. To address this limitation, we propose the first Multimodal Retrieval-Augmented Transformer (MuRAG), which accesses an external non-parametric multimodal memory to augment language generation. MuRAG is pre-trained with a mixture of large-scale image-text and text-only corpora using a joint contrastive and generative loss. We perform experiments on two different datasets that require retrieving and reasoning over both images and text to answer a given query: WebQA, and MultimodalQA. Our results show that MuRAG achieves state-of-the-art accuracy, outperforming existing models by 10-20% absolute on both datasets and under both distractor and full-wiki settings.</abstract>
      <url hash="e39384ff">2022.emnlp-main.375</url>
      <bibkey>chen-etal-2022-murag</bibkey>
    </paper>
    <paper id="376">
      <title><fixed-case>PHEE</fixed-case>: A Dataset for Pharmacovigilance Event Extraction from Text</title>
      <author><first>Zhaoyue</first><last>Sun</last></author>
      <author><first>Jiazheng</first><last>Li</last></author>
      <author><first>Gabriele</first><last>Pergola</last></author>
      <author><first>Byron</first><last>Wallace</last></author>
      <author><first>Bino</first><last>John</last></author>
      <author><first>Nigel</first><last>Greene</last></author>
      <author><first>Joseph</first><last>Kim</last></author>
      <author><first>Yulan</first><last>He</last></author>
      <pages>5571-5587</pages>
      <abstract>The primary goal of drug safety researchers and regulators is to promptly identify adverse drug reactions. Doing so may in turn prevent or reduce the harm to patients and ultimately improve public health. Evaluating and monitoring drug safety (i.e., pharmacovigilance) involves analyzing an ever growing collection of spontaneous reports from health professionals, physicians, and pharmacists, and information voluntarily submitted by patients. In this scenario, facilitating analysis of such reports via automation has the potential to rapidly identify safety signals. Unfortunately, public resources for developing natural language models for this task are scant. We present PHEE, a novel dataset for pharmacovigilance comprising over 5000 annotated events from medical case reports and biomedical literature, making it the largest such public dataset to date. We describe the hierarchical event schema designed to provide coarse and fine-grained information about patients’ demographics, treatments and (side) effects. Along with the discussion of the dataset, we present a thorough experimental evaluation of current state-of-the-art approaches for biomedical event extraction, point out their limitations, and highlight open challenges to foster future research in this area.</abstract>
      <url hash="506f2f5b">2022.emnlp-main.376</url>
      <bibkey>sun-etal-2022-phee</bibkey>
    </paper>
    <paper id="377">
      <title><fixed-case>OTS</fixed-case>eq2<fixed-case>S</fixed-case>et: An Optimal Transport Enhanced Sequence-to-Set Model for Extreme Multi-label Text Classification</title>
      <author><first>Jie</first><last>Cao</last></author>
      <author><first>Yin</first><last>Zhang</last></author>
      <pages>5588-5597</pages>
      <abstract>Extreme multi-label text classification (XMTC) is the task of finding the most relevant subset labels from an extremely large-scale label collection. Recently, some deep learning models have achieved state-of-the-art results in XMTC tasks. These models commonly predict scores for all labels by a fully connected layer as the last layer of the model. However, such models can’t predict a relatively complete and variable-length label subset for each document, because they select positive labels relevant to the document by a fixed threshold or take top k labels in descending order of scores. A less popular type of deep learning models called sequence-to-sequence (Seq2Seq) focus on predicting variable-length positive labels in sequence style. However, the labels in XMTC tasks are essentially an unordered set rather than an ordered sequence, the default order of labels restrains Seq2Seq models in training. To address this limitation in Seq2Seq, we propose an autoregressive sequence-to-set model for XMTC tasks named OTSeq2Set. Our model generates predictions in student-forcing scheme and is trained by a loss function based on bipartite matching which enables permutation-invariance. Meanwhile, we use the optimal transport distance as a measurement to force the model to focus on the closest labels in semantic label space. Experiments show that OTSeq2Set outperforms other competitive baselines on 4 benchmark datasets. Especially, on the Wikipedia dataset with 31k labels, it outperforms the state-of-the-art Seq2Seq method by 16.34% in micro-F1 score. The code is available at https://github.com/caojie54/OTSeq2Set.</abstract>
      <url hash="48e6c7d6">2022.emnlp-main.377</url>
      <bibkey>cao-zhang-2022-otseq2set</bibkey>
    </paper>
    <paper id="378">
      <title><fixed-case>S</fixed-case>im<fixed-case>QA</fixed-case>: Detecting Simultaneous <fixed-case>MT</fixed-case> Errors through Word-by-Word Question Answering</title>
      <author><first>HyoJung</first><last>Han</last></author>
      <author><first>Marine</first><last>Carpuat</last></author>
      <author><first>Jordan</first><last>Boyd-Graber</last></author>
      <pages>5598-5616</pages>
      <abstract>Detractors of neural machine translation admit that while its translations are fluent, it sometimes gets key facts wrong. This is particularly important in simultaneous interpretation where translations have to be provided as fast as possible: before a sentence is complete. Yet, evaluations of simultaneous machine translation (SimulMT) fail to capture if systems correctly translate the most salient elements of a question: people, places, and dates. To address this problem, we introduce a downstream word-by-word question answering evaluation task (SimQA): given a source language question, translate the question word by word into the target language, and answer as soon as possible. SimQA jointly measures whether the SimulMT models translate the question quickly and accurately, and can reveal shortcomings in existing neural systems—hallucinating or omitting facts.</abstract>
      <url hash="99fcbabb">2022.emnlp-main.378</url>
      <bibkey>han-etal-2022-simqa</bibkey>
    </paper>
    <paper id="379">
      <title>Discovering Low-rank Subspaces for Language-agnostic Multilingual Representations</title>
      <author><first>Zhihui</first><last>Xie</last></author>
      <author><first>Handong</first><last>Zhao</last></author>
      <author><first>Tong</first><last>Yu</last></author>
      <author><first>Shuai</first><last>Li</last></author>
      <pages>5617-5633</pages>
      <abstract>Large pretrained multilingual language models (ML-LMs) have shown remarkable capabilities of zero-shot cross-lingual transfer, without direct cross-lingual supervision. While these results are promising, follow-up works found that, within the multilingual embedding spaces, there exists strong language identity information which hinders the expression of linguistic factors shared across languages. For semantic tasks like cross-lingual sentence retrieval, it is desired to remove such language identity signals to fully leverage semantic information. In this work, we provide a novel view of projecting away language-specific factors from a multilingual embedding space. Specifically, we discover that there exists a low-rank subspace that primarily encodes information irrelevant to semantics (e.g., syntactic information). To identify this subspace, we present a simple but effective unsupervised method based on singular value decomposition with multiple monolingual corpora as input. Once the subspace is found, we can directly project the original embeddings into the null space to boost language agnosticism without finetuning. We systematically evaluate our method on various tasks including the challenging language-agnostic QA retrieval task. Empirical results show that applying our method consistently leads to improvements over commonly used ML-LMs.</abstract>
      <url hash="c8a7e70e">2022.emnlp-main.379</url>
      <bibkey>xie-etal-2022-discovering</bibkey>
    </paper>
    <paper id="380">
      <title>Rethinking the Authorship Verification Experimental Setups</title>
      <author><first>Florin</first><last>Brad</last></author>
      <author><first>Andrei</first><last>Manolache</last></author>
      <author><first>Elena</first><last>Burceanu</last></author>
      <author><first>Antonio</first><last>Barbalau</last></author>
      <author><first>Radu Tudor</first><last>Ionescu</last></author>
      <author><first>Marius</first><last>Popescu</last></author>
      <pages>5634-5643</pages>
      <abstract>One of the main drivers of the recent advances in authorship verification is the PAN large-scale authorship dataset. Despite generating significant progress in the field, inconsistent performance differences between the closed and open test sets have been reported. To this end, we improve the experimental setup by proposing five new public splits over the PAN dataset, specifically designed to isolate and identify biases related to the text topic and to the author’s writing style. We evaluate several BERT-like baselines on these splits, showing that such models are competitive with authorship verification state-of-the-art methods. Furthermore, using explainable AI, we find that these baselines are biased towards named entities. We show that models trained without the named entities obtain better results and generalize better when tested on DarkReddit, our new dataset for authorship verification.</abstract>
      <url hash="68190024">2022.emnlp-main.380</url>
      <bibkey>brad-etal-2022-rethinking</bibkey>
    </paper>
    <paper id="381">
      <title>Borrowing Human Senses: Comment-Aware Self-Training for Social Media Multimodal Classification</title>
      <author><first>Chunpu</first><last>Xu</last></author>
      <author><first>Jing</first><last>Li</last></author>
      <pages>5644-5656</pages>
      <abstract>Social media is daily creating massive multimedia content with paired image and text, presenting the pressing need to automate the vision and language understanding for various multimodal classification tasks. Compared to the commonly researched visual-lingual data, social media posts tend to exhibit more implicit image-text relations. To better glue the cross-modal semantics therein, we capture hinting features from user comments, which are retrieved via jointly leveraging visual and lingual similarity. Afterwards, the classification tasks are explored via self-training in a teacher-student framework, motivated by the usually limited labeled data scales in existing benchmarks. Substantial experiments are conducted on four multimodal social media benchmarks for image-text relation classification, sarcasm detection, sentiment classification, and hate speech detection. The results show that our method further advances the performance of previous state-of-the-art models, which do not employ comment modeling or self-training.</abstract>
      <url hash="182ea152">2022.emnlp-main.381</url>
      <bibkey>xu-li-2022-borrowing</bibkey>
    </paper>
    <paper id="382">
      <title>Training Language Models with Memory Augmentation</title>
      <author><first>Zexuan</first><last>Zhong</last></author>
      <author><first>Tao</first><last>Lei</last></author>
      <author><first>Danqi</first><last>Chen</last></author>
      <pages>5657-5673</pages>
      <abstract>Recent work has improved language models (LMs) remarkably by equipping them with a non-parametric memory component. However, most existing approaches only introduce mem-ories at testing time or represent them using a separately trained encoder, resulting in suboptimal training of the language model. In this work, we present TRIME, a novel yet simple training approach designed for training LMs with memory augmentation. Our approach uses a training objective that directly takes in-batch examples as accessible memory. We also present new methods for memory construction and data batching, which are used for adapting to different sets of memories—local, long-term, and external memory—at testing time. We evaluate TRIME on multiple language modeling and machine translation benchmarks and show that it is able to achieve significant improvements across all the settings. Concretely, TRIME reduces the perplexity from 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory set from the training corpus. Compared to standard LM training, TRIME adds negligible computational overhead and is compatible with different neural architectures, making it a versatile solution for training memory-augmented LMs.</abstract>
      <url hash="16e073dd">2022.emnlp-main.382</url>
      <bibkey>zhong-etal-2022-training</bibkey>
    </paper>
    <paper id="383">
      <title>Data-Efficient Strategies for Expanding Hate Speech Detection into Under-Resourced Languages</title>
      <author><first>Paul</first><last>Röttger</last></author>
      <author><first>Debora</first><last>Nozza</last></author>
      <author><first>Federico</first><last>Bianchi</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <pages>5674-5691</pages>
      <abstract>Hate speech is a global phenomenon, but most hate speech datasets so far focus on English-language content. This hinders the development of more effective hate speech detection models in hundreds of languages spoken by billions across the world. More data is needed, but annotating hateful content is expensive, time-consuming and potentially harmful to annotators. To mitigate these issues, we explore data-efficient strategies for expanding hate speech detection into under-resourced languages. In a series of experiments with mono- and multilingual models across five non-English languages, we find that 1) a small amount of target-language fine-tuning data is needed to achieve strong performance, 2) the benefits of using more such data decrease exponentially, and 3) initial fine-tuning on readily-available English data can partially substitute target-language data and improve model generalisability. Based on these findings, we formulate actionable recommendations for hate speech detection in low-resource language settings.</abstract>
      <url hash="ba3e1c16">2022.emnlp-main.383</url>
      <bibkey>rottger-etal-2022-data</bibkey>
    </paper>
    <paper id="384">
      <title>Dimension Reduction for Efficient Dense Retrieval via Conditional Autoencoder</title>
      <author><first>Zhenghao</first><last>Liu</last></author>
      <author><first>Han</first><last>Zhang</last></author>
      <author><first>Chenyan</first><last>Xiong</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Yu</first><last>Gu</last></author>
      <author><first>Xiaohua</first><last>Li</last></author>
      <pages>5692-5698</pages>
      <abstract>Dense retrievers encode queries and documents and map them in an embedding space using pre-trained language models. These embeddings need to be high-dimensional to fit training signals and guarantee the retrieval effectiveness of dense retrievers. However, these high-dimensional embeddings lead to larger index storage and higher retrieval latency. To reduce the embedding dimensions of dense retrieval, this paper proposes a Conditional Autoencoder (ConAE) to compress the high-dimensional embeddings to maintain the same embedding distribution and better recover the ranking features. Our experiments show that ConAE is effective in compressing embeddings by achieving comparable ranking performance with its teacher model and making the retrieval system more efficient. Our further analyses show that ConAE can alleviate the redundancy of the embeddings of dense retrieval with only one linear layer. All codes of this work are available at https://github.com/NEUIR/ConAE.</abstract>
      <url hash="1b501bd8">2022.emnlp-main.384</url>
      <bibkey>liu-etal-2022-dimension</bibkey>
    </paper>
    <paper id="385">
      <title>Controlled Text Reduction</title>
      <author><first>Aviv</first><last>Slobodkin</last></author>
      <author><first>Paul</first><last>Roit</last></author>
      <author><first>Eran</first><last>Hirsch</last></author>
      <author><first>Ori</first><last>Ernst</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <pages>5699-5715</pages>
      <abstract>Producing a reduced version of a source text, as in generic or focused summarization, inherently involves two distinct subtasks: deciding on targeted content and generating a coherent text conveying it. While some popular approaches address summarization as a single end-to-end task, prominent works support decomposed modeling for individual subtasks. Further, semi-automated text reduction is also very appealing, where users may identify targeted content while models would generate a corresponding coherent summary.In this paper, we focus on the second subtask, of generating coherent text given pre-selected content. Concretely, we formalize <i>Controlled Text Reduction</i> as a standalone task, whose input is a source text with marked spans of targeted content (“highlighting”).A model then needs to generate a coherent text that includes all and only the target information.We advocate the potential of such models, both for modular fully-automatic summarization, as well as for semi-automated human-in-the-loop use cases.Facilitating proper research, we crowdsource high-quality dev and test datasets for the task. Further, we automatically generate a larger “silver” training dataset from available summarization benchmarks, leveraging a pretrained summary-source alignment model.Finally, employing these datasets, we present a supervised baseline model, showing promising results and insightful analyses.</abstract>
      <url hash="bf5bbeec">2022.emnlp-main.385</url>
      <bibkey>slobodkin-etal-2022-controlled</bibkey>
    </paper>
    <paper id="386">
      <title>Questioning the Validity of Summarization Datasets and Improving Their Factual Consistency</title>
      <author><first>Yanzhu</first><last>Guo</last></author>
      <author><first>Chloé</first><last>Clavel</last></author>
      <author><first>Moussa</first><last>Kamal Eddine</last></author>
      <author><first>Michalis</first><last>Vazirgiannis</last></author>
      <pages>5716-5727</pages>
      <abstract>The topic of summarization evaluation has recently attracted a surge of attention due to the rapid development of abstractive summarization systems. However, the formulation of the task is rather ambiguous, neither the linguistic nor the natural language processing communities have succeeded in giving a mutually agreed-upon definition. Due to this lack of well-defined formulation, a large number of popular abstractive summarization datasets are constructed in a manner that neither guarantees validity nor meets one of the most essential criteria of summarization: factual consistency. In this paper, we address this issue by combining state-of-the-art factual consistency models to identify the problematic instances present in popular summarization datasets. We release SummFC, a filtered summarization dataset with improved factual consistency, and demonstrate that models trained on this dataset achieve improved performance in nearly all quality aspects. We argue that our dataset should become a valid benchmark for developing and evaluating summarization systems.</abstract>
      <url hash="1d771c3e">2022.emnlp-main.386</url>
      <bibkey>guo-etal-2022-questioning</bibkey>
    </paper>
    <paper id="387">
      <title>Invariant Language Modeling</title>
      <author><first>Maxime</first><last>Peyrard</last></author>
      <author><first>Sarvjeet</first><last>Ghotra</last></author>
      <author><first>Martin</first><last>Josifoski</last></author>
      <author><first>Vidhan</first><last>Agarwal</last></author>
      <author><first>Barun</first><last>Patra</last></author>
      <author><first>Dean</first><last>Carignan</last></author>
      <author><first>Emre</first><last>Kiciman</last></author>
      <author><first>Saurabh</first><last>Tiwary</last></author>
      <author><first>Robert</first><last>West</last></author>
      <pages>5728-5743</pages>
      <abstract>Modern pretrained language models are critical components of NLP pipelines. Yet, they suffer from spurious correlations, poor out-of-domain generalization, and biases.Inspired by recent progress in causal machine learning, in particular the invariant risk minimization (IRM) paradigm, we propose invariant language modeling, a framework for learning invariant representations that generalize better across multiple environments. In particular, we adapt a game-theoretic implementation of IRM (IRM-games) to language models, where the invariance emerges from a specific training schedule in which all the environments compete to optimize their own environment-specific loss by updating subsets of the model in a round-robin fashion.We focused on controlled experiments to precisely demonstrate the ability of our method to (i) remove structured noise, (ii) ignore specific spurious correlations without affecting global performance, and (iii) achieve better out-of-domain generalization.These benefits come with a negligible computational overhead compared to standard training, do not require changing the local loss, and can be applied to any language model. We believe this framework is promising to help mitigate spurious correlations and biases in language models.</abstract>
      <url hash="5d0ddedf">2022.emnlp-main.387</url>
      <bibkey>peyrard-etal-2022-invariant</bibkey>
    </paper>
    <paper id="388">
      <title><fixed-case>A</fixed-case>da<fixed-case>M</fixed-case>ix: Mixture-of-Adaptations for Parameter-efficient Model Tuning</title>
      <author><first>Yaqing</first><last>Wang</last></author>
      <author><first>Sahaj</first><last>Agarwal</last></author>
      <author><first>Subhabrata</first><last>Mukherjee</last></author>
      <author><first>Xiaodong</first><last>Liu</last></author>
      <author><first>Jing</first><last>Gao</last></author>
      <author><first>Ahmed Hassan</first><last>Awadallah</last></author>
      <author><first>Jianfeng</first><last>Gao</last></author>
      <pages>5744-5760</pages>
      <abstract>Standard fine-tuning of large pre-trained language models (PLMs) for downstream tasks requires updating hundreds of millions to billions of parameters, and storing a large copy of the PLM weights for every task resulting in increased cost for storing, sharing and serving the models. To address this, parameter-efficient fine-tuning (PEFT) techniques were introduced where small trainable components are injected in the PLM and updated during fine-tuning. We propose AdaMix as a general PEFT method that tunes a mixture of adaptation modules – given the underlying PEFT method of choice – introduced in each Transformer layer while keeping most of the PLM weights frozen. For instance, AdaMix can leverage a mixture of adapters like Houlsby or a mixture of low rank decomposition matrices like LoRA to improve downstream task performance over the corresponding PEFT methods for fully supervised and few-shot NLU and NLG tasks. Further, we design AdaMix such that it matches the same computational cost and the number of tunable parameters as the underlying PEFT method. By only tuning 0.1-0.2% of PLM parameters, we show that AdaMix outperforms SOTA parameter-efficient fine-tuning and full model fine-tuning for both NLU and NLG tasks.</abstract>
      <url hash="72b57b08">2022.emnlp-main.388</url>
      <bibkey>wang-etal-2022-adamix</bibkey>
    </paper>
    <paper id="389">
      <title>How “Multi” is Multi-Document Summarization?</title>
      <author><first>Ruben</first><last>Wolhandler</last></author>
      <author><first>Arie</first><last>Cattan</last></author>
      <author><first>Ori</first><last>Ernst</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <pages>5761-5769</pages>
      <abstract>The task of multi-document summarization (MDS) aims at models that, given multiple documents as input, are able to generate a summary that combines disperse information, originally spread __across__ these documents. Accordingly, it is expected that both reference summaries in MDS datasets, as well as system summaries, would indeed be based on such dispersed information. In this paper, we argue for quantifying and assessing this expectation. To that end, we propose an automated measure for evaluating the degree to which a summary is “disperse”, in the sense of the number of source documents needed to cover its content. We apply our measure to empirically analyze several popular MDS datasets, with respect to their reference summaries, as well as the output of state-of-the-art systems. Our results show that certain MDS datasets barely require combining information from multiple documents, where a single document often covers the full summary content. Overall, we advocate using our metric for assessing and improving the degree to which summarization datasets require combining multi-document information, and similarly how summarization models actually meet this challenge.</abstract>
      <url hash="cb19ad2c">2022.emnlp-main.389</url>
      <bibkey>wolhandler-etal-2022-multi</bibkey>
    </paper>
    <paper id="390">
      <title><fixed-case>B</fixed-case>io<fixed-case>R</fixed-case>eader: a Retrieval-Enhanced Text-to-Text Transformer for Biomedical Literature</title>
      <author><first>Giacomo</first><last>Frisoni</last></author>
      <author><first>Miki</first><last>Mizutani</last></author>
      <author><first>Gianluca</first><last>Moro</last></author>
      <author><first>Lorenzo</first><last>Valgimigli</last></author>
      <pages>5770-5793</pages>
      <abstract>The latest batch of research has equipped language models with the ability to attend over relevant and factual information from non-parametric external sources, drawing a complementary path to architectural scaling. Besides mastering language, exploiting and contextualizing the latent world knowledge is crucial in complex domains like biomedicine. However, most works in the field rely on general-purpose models supported by databases like Wikipedia and Books. We introduce BioReader, the first retrieval-enhanced text-to-text model for biomedical natural language processing. Our domain-specific T5-based solution augments the input prompt by fetching and assembling relevant scientific literature chunks from a neural database with ≈60 million tokens centered on PubMed. We fine-tune and evaluate BioReader on a broad array of downstream tasks, significantly outperforming several state-of-the-art methods despite using up to 3x fewer parameters. In tandem with extensive ablation studies, we show that domain knowledge can be easily altered or supplemented to make the model generate correct predictions bypassing the retraining step and thus addressing the literature overload issue.</abstract>
      <url hash="055e8d03">2022.emnlp-main.390</url>
      <bibkey>frisoni-etal-2022-bioreader</bibkey>
    </paper>
    <paper id="391">
      <title><fixed-case>T</fixed-case>-Modules: Translation Modules for Zero-Shot Cross-Modal Machine Translation</title>
      <author><first>Paul-Ambroise</first><last>Duquenne</last></author>
      <author><first>Hongyu</first><last>Gong</last></author>
      <author><first>Benoît</first><last>Sagot</last></author>
      <author><first>Holger</first><last>Schwenk</last></author>
      <pages>5794-5806</pages>
      <abstract>We present a new approach to perform zero-shot cross-modal transfer between speech and text for translation tasks. Multilingual speech and text are encoded in a joint fixed-size representation space. Then, we compare different approaches to decode these multimodal and multilingual fixed-size representations, enabling zero-shot translation between languages and modalities. All our models are trained without the need of cross-modal labeled translation data.Despite a fixed-size representation, we achieve very competitive results on several text and speech translation tasks. In particular, we significantly improve the state-of-the-art for zero-shot speech translation on Must-C. Incorporating a speech decoder in our framework, we introduce the first results for zero-shot direct speech-to-speech and text-to-speech translation.</abstract>
      <url hash="098bfab1">2022.emnlp-main.391</url>
      <bibkey>duquenne-etal-2022-modules</bibkey>
    </paper>
    <paper id="392">
      <title><fixed-case>LILA</fixed-case>: A Unified Benchmark for Mathematical Reasoning</title>
      <author><first>Swaroop</first><last>Mishra</last></author>
      <author><first>Matthew</first><last>Finlayson</last></author>
      <author><first>Pan</first><last>Lu</last></author>
      <author><first>Leonard</first><last>Tang</last></author>
      <author><first>Sean</first><last>Welleck</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <author><first>Tanmay</first><last>Rajpurohit</last></author>
      <author><first>Oyvind</first><last>Tafjord</last></author>
      <author><first>Ashish</first><last>Sabharwal</last></author>
      <author><first>Peter</first><last>Clark</last></author>
      <author><first>Ashwin</first><last>Kalyan</last></author>
      <pages>5807-5832</pages>
      <abstract>Mathematical reasoning skills are essential for general-purpose intelligentsystems to perform tasks from grocery shopping to climate modeling.Towards evaluating and improving AI systems in this domain, we proposeLILA, a unified mathematical reasoning benchmark consisting of 23 diversetasks along four dimensions:(i) mathematical abilities e.g., arithmetic, calculus (ii) language format e.g., question-answering, fill-in-the-blanks (iii) language diversity e.g., no language, simple language (iv) external knowledge e.g., commonsense, physics. We construct our benchmark by extending 20 datasets benchmark by collecting task instructions and solutions in the form of Python programs,thereby obtaining explainable solutions in addition to the correct answer.We additionally introduce two evaluation datasets to measure out-of-distribution performance and robustness to language perturbation.Finally, we introduce BHASKARA,a general-purpose mathematical reasoning model trained on LILA. Importantly, we find that multi-tasking leads to significant improvements (average relative improvement of 21.83% F1 score vs. single-task models),while the best performing model only obtains 60.40%,indicating the room for improvement in general mathematical reasoning and understanding.</abstract>
      <url hash="7fd59a3a">2022.emnlp-main.392</url>
      <bibkey>mishra-etal-2022-lila</bibkey>
    </paper>
    <paper id="393">
      <title>Leveraging Affirmative Interpretations from Negation Improves Natural Language Understanding</title>
      <author><first>Md Mosharaf</first><last>Hossain</last></author>
      <author><first>Eduardo</first><last>Blanco</last></author>
      <pages>5833-5847</pages>
      <abstract>Negation poses a challenge in many natural language understanding tasks. Inspired by the fact that understanding a negated statement often requires humans to infer affirmative interpretations, in this paper we show that doing so benefits models for three natural language understanding tasks. We present an automated procedure to collect pairs of sentences with negation and their affirmative interpretations, resulting in over 150,000 pairs. Experimental results show that leveraging these pairs helps (a) T5 generate affirmative interpretations from negations in a previous benchmark, and (b) a RoBERTa-based classifier solve the task of natural language inference. We also leverage our pairs to build a plug-and-play neural generator that given a negated statement generates an affirmative interpretation. Then, we incorporate the pretrained generator into a RoBERTa-based classifier for sentiment analysis and show that doing so improves the results. Crucially, our proposal does not require any manual effort.</abstract>
      <url hash="80d82250">2022.emnlp-main.393</url>
      <bibkey>hossain-blanco-2022-leveraging</bibkey>
    </paper>
    <paper id="394">
      <title><fixed-case>G</fixed-case>raph<fixed-case>Q</fixed-case> <fixed-case>IR</fixed-case>: Unifying the Semantic Parsing of Graph Query Languages with One Intermediate Representation</title>
      <author><first>Lunyiu</first><last>Nie</last></author>
      <author><first>Shulin</first><last>Cao</last></author>
      <author><first>Jiaxin</first><last>Shi</last></author>
      <author><first>Jiuding</first><last>Sun</last></author>
      <author><first>Qi</first><last>Tian</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <author><first>Jidong</first><last>Zhai</last></author>
      <pages>5848-5865</pages>
      <abstract>Subject to the huge semantic gap between natural and formal languages, neural semantic parsing is typically bottlenecked by its complexity of dealing with both input semantics and output syntax. Recent works have proposed several forms of supplementary supervision but none is generalized across multiple formal languages. This paper proposes a unified intermediate representation for graph query languages, named GraphQ IR. It has a natural-language-like expression that bridges the semantic gap and formally defined syntax that maintains the graph structure. Therefore, a neural semantic parser can more precisely convert user queries into GraphQ IR, which can be later losslessly compiled into various downstream graph query languages. Extensive experiments on several benchmarks including KQA Pro, Overnight, GrailQA, and MetaQA-Cypher under the standard i.i.d., out-of-distribution, and low-resource settings validate GraphQ IR’s superiority over the previous state-of-the-arts with a maximum 11% accuracy improvement.</abstract>
      <url hash="53c871a1">2022.emnlp-main.394</url>
      <bibkey>nie-etal-2022-graphq</bibkey>
    </paper>
    <paper id="395">
      <title><fixed-case>I</fixed-case>nfor<fixed-case>M</fixed-case>ask: Unsupervised Informative Masking for Language Model Pretraining</title>
      <author><first>Nafis</first><last>Sadeq</last></author>
      <author><first>Canwen</first><last>Xu</last></author>
      <author><first>Julian</first><last>McAuley</last></author>
      <pages>5866-5878</pages>
      <abstract>Masked language modeling is widely used for pretraining large language models for natural language understanding (NLU). However, random masking is suboptimal, allocating an equal masking rate for all tokens. In this paper, we propose InforMask, a new unsupervised masking strategy for training masked language models. InforMask exploits Pointwise Mutual Information (PMI) to select the most informative tokens to mask. We further propose two optimizations for InforMask to improve its efficiency. With a one-off preprocessing step, InforMask outperforms random masking and previously proposed masking strategies on the factual recall benchmark LAMA and the question answering benchmark SQuAD v1 and v2.</abstract>
      <url hash="7dc3051f">2022.emnlp-main.395</url>
      <bibkey>sadeq-etal-2022-informask</bibkey>
    </paper>
    <paper id="396">
      <title><fixed-case>CTRL</fixed-case>sum: Towards Generic Controllable Text Summarization</title>
      <author><first>Junxian</first><last>He</last></author>
      <author><first>Wojciech</first><last>Kryscinski</last></author>
      <author><first>Bryan</first><last>McCann</last></author>
      <author><first>Nazneen</first><last>Rajani</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <pages>5879-5915</pages>
      <abstract>Current summarization systems yield generic summaries that are disconnected from users’ preferences and expectations. To address this limitation, we present CTRLsum, a generic framework to control generated summaries through a set of keywords. During training keywords are extracted automatically without requiring additional human annotations. At test time CTRLsum features a control function to map control signal to keywords; through engineering the control function, the same trained model is able to be applied to control summaries on various dimensions, while neither affecting the model training process nor the pretrained models. We additionally explore the combination of keywords and text prompts for more control tasks. Experiments demonstrate the effectiveness of CTRLsum on three domains of summarization datasets and five control tasks: (1) entity-centric and (2) length-controllable summarization, (3) contribution summarization on scientific papers, (4) invention purpose summarization on patent filings, and (5) question-guided summarization on news articles. Moreover, when used in a standard, unconstrained summarization setting, CTRLsum is comparable or better than strong pretrained systems.</abstract>
      <url hash="795ca680">2022.emnlp-main.396</url>
      <bibkey>he-etal-2022-ctrlsum</bibkey>
    </paper>
    <paper id="397">
      <title>Missing Counter-Evidence Renders <fixed-case>NLP</fixed-case> Fact-Checking Unrealistic for Misinformation</title>
      <author><first>Max</first><last>Glockner</last></author>
      <author><first>Yufang</first><last>Hou</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>5916-5936</pages>
      <abstract>Misinformation emerges in times of uncertainty when credible information is limited. This is challenging for NLP-based fact-checking as it relies on counter-evidence, which may not yet be available. Despite increasing interest in automatic fact-checking, it is still unclear if automated approaches can realistically refute harmful real-world misinformation. Here, we contrast and compare NLP fact-checking with how professional fact-checkers combat misinformation in the absence of counter-evidence. In our analysis, we show that, by design, existing NLP task definitions for fact-checking cannot refute misinformation as professional fact-checkers do for the majority of claims. We then define two requirements that the evidence in datasets must fulfill for realistic fact-checking: It must be (1) sufficient to refute the claim and (2) not leaked from existing fact-checking articles. We survey existing fact-checking datasets and find that all of them fail to satisfy both criteria. Finally, we perform experiments to demonstrate that models trained on a large-scale fact-checking dataset rely on leaked evidence, which makes them unsuitable in real-world scenarios. Taken together, we show that current NLP fact-checking cannot realistically combat real-world misinformation because it depends on unrealistic assumptions about counter-evidence in the data.</abstract>
      <url hash="e4b783ea">2022.emnlp-main.397</url>
      <bibkey>glockner-etal-2022-missing</bibkey>
    </paper>
    <paper id="398">
      <title>A Framework for Adapting Pre-Trained Language Models to Knowledge Graph Completion</title>
      <author><first>Justin</first><last>Lovelace</last></author>
      <author><first>Carolyn</first><last>Rosé</last></author>
      <pages>5937-5955</pages>
      <abstract>Recent work has demonstrated that entity representations can be extracted from pre-trained language models to develop knowledge graph completion models that are more robust to the naturally occurring sparsity found in knowledge graphs. In this work, we conduct a comprehensive exploration of how to best extract and incorporate those embeddings into knowledge graph completion models. We explore the suitability of the extracted embeddings for direct use in entity ranking and introduce both unsupervised and supervised processing methods that can lead to improved downstream performance. We then introduce supervised embedding extraction methods that can extract more informative representations. We then synthesize our findings and develop a knowledge graph completion model that significantly outperforms recent neural models.</abstract>
      <url hash="f85b4599">2022.emnlp-main.398</url>
      <bibkey>lovelace-rose-2022-framework</bibkey>
    </paper>
    <paper id="399">
      <title>Mutual Information Alleviates Hallucinations in Abstractive Summarization</title>
      <author><first>Liam</first><last>van der Poel</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Clara</first><last>Meister</last></author>
      <pages>5956-5965</pages>
      <abstract>Despite significant progress in the quality of language generated from abstractive summarization models, these models still exhibit the tendency to hallucinate, i.e., output content not supported by the source document. A number of works have tried to fix—or at least uncover the source of—the problem with limited success. In this paper, we identify a simple criterion under which models are significantly more likely to assign more probability to hallucinated content during generation: high model uncertainty. This finding offers a potential explanation for hallucinations: models default to favoring text with high marginal probability, i.e., high-frequency occurrences in the training set, when uncertain about a continuation. It also motivates possible routes for real-time intervention during decoding to prevent such hallucinations. We propose a decoding strategy that switches to optimizing for pointwise mutual information of the source and target token—rather than purely the probability of the target token—when the model exhibits uncertainty. Experiments on the dataset show that our method decreases the probability of hallucinated tokens while maintaining the Rouge and BERT-S scores of top-performing decoding strategies.</abstract>
      <url hash="5be46de8">2022.emnlp-main.399</url>
      <bibkey>van-der-poel-etal-2022-mutual</bibkey>
    </paper>
    <paper id="400">
      <title>Toward the Limitation of Code-Switching in Cross-Lingual Transfer</title>
      <author><first>Yukun</first><last>Feng</last></author>
      <author><first>Feng</first><last>Li</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>5966-5971</pages>
      <abstract>Multilingual pretrained models have shown strong cross-lingual transfer ability. Some works used code-switching sentences, which consist of tokens from multiple languages, to enhance the cross-lingual representation further, and have shown success in many zero-shot cross-lingual tasks. However, code-switched tokens are likely to cause grammatical incoherence in newly substituted sentences, and negatively affect the performance on token-sensitive tasks, such as Part-of-Speech (POS) tagging and Named-Entity-Recognition (NER). This paper mitigates the limitation of the code-switching method by not only making the token replacement but considering the similarity between the context and the switched tokens so that the newly substituted sentences are grammatically consistent during both training and inference. We conduct experiments on cross-lingual POS and NER over 30+ languages, and demonstrate the effectiveness of our method by outperforming the mBERT by 0.95 and original code-switching method by 1.67 on F1 scores.</abstract>
      <url hash="fa0cc6ae">2022.emnlp-main.400</url>
      <bibkey>feng-etal-2022-toward</bibkey>
    </paper>
    <paper id="401">
      <title>Syntactically Rich Discriminative Training: An Effective Method for Open Information Extraction</title>
      <author><first>Frank</first><last>Mtumbuka</last></author>
      <author><first>Thomas</first><last>Lukasiewicz</last></author>
      <pages>5972-5987</pages>
      <abstract>Open information extraction (OIE) is the task of extracting facts "(Subject, Relation, Object)” from natural language text. We propose several new methods for training neural OIE models in this paper. First, we propose a novel method for computing syntactically rich text embeddings using the structure of dependency trees. Second, we propose a new discriminative training approach to OIE in which tokens in the generated fact are classified as “real” or “fake”, i.e., those tokens that are in both the generated and gold tuples, and those that are only in the generated tuple but not in the gold tuple. We also address the issue of repetitive tokens in generated facts and improve the models’ ability to generate implicit facts. Our approach reduces repetitive tokens by a factor of 23%. Finally, we present paraphrased versions of the CaRB, OIE2016, and LSOIE datasets, and show that the models’ performance substantially improves when trained on augmented datasets. Our best model beats the SOTA of IMoJIE on the recent CaRB dataset, with an improvement of 39.63% in F1 score.</abstract>
      <url hash="492561a1">2022.emnlp-main.401</url>
      <bibkey>mtumbuka-lukasiewicz-2022-syntactically</bibkey>
    </paper>
    <paper id="402">
      <title>Transformer-based Entity Typing in Knowledge Graphs</title>
      <author><first>Zhiwei</first><last>Hu</last></author>
      <author><first>Victor</first><last>Gutierrez-Basulto</last></author>
      <author><first>Zhiliang</first><last>Xiang</last></author>
      <author><first>Ru</first><last>Li</last></author>
      <author><first>Jeff</first><last>Pan</last></author>
      <pages>5988-6001</pages>
      <abstract>We investigate the knowledge graph entity typing task which aims at inferring plausible entity types. In this paper, we propose a novel Transformer-based Entity Typing (TET) approach, effectively encoding the content of neighbours of an entity by means of a transformer mechanism. More precisely, TET is composed of three different mechanisms: a local transformer allowing to infer missing entity types by independently encoding the information provided by each of its neighbours; a global transformer aggregating the information of all neighbours of an entity into a single long sequence to reason about more complex entity types; and a context transformer integrating neighbours content in a differentiated way through information exchange between neighbour pairs, while preserving the graph structure. Furthermore, TET uses information about class membership of types to semantically strengthen the representation of an entity. Experiments on two real-world datasets demonstrate the superior performance of TET compared to the state-of-the-art.</abstract>
      <url hash="27bf416c">2022.emnlp-main.402</url>
      <bibkey>hu-etal-2022-transformer</bibkey>
    </paper>
    <paper id="403">
      <title><fixed-case>N</fixed-case>ews<fixed-case>C</fixed-case>laims: A New Benchmark for Claim Detection from News with Attribute Knowledge</title>
      <author><first>Revanth</first><last>Gangi Reddy</last></author>
      <author><first>Sai Chetan</first><last>Chinthakindi</last></author>
      <author><first>Zhenhailong</first><last>Wang</last></author>
      <author><first>Yi</first><last>Fung</last></author>
      <author><first>Kathryn</first><last>Conger</last></author>
      <author><first>Ahmed</first><last>ELsayed</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <author><first>Kevin</first><last>Small</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <pages>6002-6018</pages>
      <abstract>Claim detection and verification are crucial for news understanding and have emerged as promising technologies for mitigating misinformation and disinformation in the news. However, most existing work has focused on claim sentence analysis while overlooking additional crucial attributes (e.g., the claimer and the main object associated with the claim).In this work, we present NewsClaims, a new benchmark for attribute-aware claim detection in the news domain. We extend the claim detection problem to include extraction of additional attributes related to each claim and release 889 claims annotated over 143 news articles. NewsClaims aims to benchmark claim detection systems in emerging scenarios, comprising unseen topics with little or no training data. To this end, we see that zero-shot and prompt-based baselines show promising performance on this benchmark, while still considerably behind human performance.</abstract>
      <url hash="c3f1f102">2022.emnlp-main.403</url>
      <bibkey>gangi-reddy-etal-2022-newsclaims</bibkey>
    </paper>
    <paper id="404">
      <title><fixed-case>I</fixed-case>so<fixed-case>V</fixed-case>ec: Controlling the Relative Isomorphism of Word Embedding Spaces</title>
      <author><first>Kelly</first><last>Marchisio</last></author>
      <author><first>Neha</first><last>Verma</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <author><first>Philipp</first><last>Koehn</last></author>
      <pages>6019-6033</pages>
      <abstract>The ability to extract high-quality translation dictionaries from monolingual word embedding spaces depends critically on the geometric similarity of the spaces—their degree of “isomorphism.” We address the root-cause of faulty cross-lingual mapping: that word embedding training resulted in the underlying spaces being non-isomorphic. We incorporate global measures of isomorphism directly into the skipgram loss function, successfully increasing the relative isomorphism of trained word embedding spaces and improving their ability to be mapped to a shared cross-lingual space. The result is improved bilingual lexicon induction in general data conditions, under domain mismatch, and with training algorithm dissimilarities. We release IsoVec at https://github.com/kellymarchisio/isovec.</abstract>
      <url hash="47fc7b6a">2022.emnlp-main.404</url>
      <bibkey>marchisio-etal-2022-isovec</bibkey>
    </paper>
    <paper id="405">
      <title>Adversarial Concept Erasure in Kernel Space</title>
      <author><first>Shauli</first><last>Ravfogel</last></author>
      <author><first>Francisco</first><last>Vargas</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <pages>6034-6055</pages>
      <abstract>The representation space of neural models for textual data emerges in an unsupervised manner during training. Understanding how human-interpretable concepts, such as gender, are encoded in these representations would improve the ability of users to control the content of these representations and analyze the working of the models that rely on them. One prominent approach to the control problem is the identification and removal of linear concept subspaces – subspaces in the representation space that correspond to a given concept. While those are tractable and interpretable, neural network do not necessarily represent concepts in linear subspaces. We propose a kernelization of the recently-proposed linear concept-removal objective, and show that it is effective in guarding against the ability of certain nonlinear adversaries to recover the concept. Interestingly, our findings suggest that the division between linear and nonlinear models is overly simplistic: when considering the concept of binary gender and its neutralization, we do not find a single kernel space that exclusively contains all the concept-related information. It is therefore challenging to protect against all nonlinear adversaries at once.</abstract>
      <url hash="bc410e7f">2022.emnlp-main.405</url>
      <bibkey>ravfogel-etal-2022-adversarial</bibkey>
    </paper>
    <paper id="406">
      <title>The Authenticity Gap in Human Evaluation</title>
      <author><first>Kawin</first><last>Ethayarajh</last></author>
      <author><first>Dan</first><last>Jurafsky</last></author>
      <pages>6056-6070</pages>
      <abstract>Human ratings are the gold standard in NLG evaluation. The standard protocol is to collect ratings of generated text, average across annotators, and rank NLG systems by their average scores. However, little consideration has been given as to whether this approach faithfully captures human preferences. Analyzing this standard protocol through the lens of utility theory in economics, we identify the implicit assumptions it makes about annotators. These assumptions are often violated in practice, in which case annotator ratings cease to reflect their preferences. The most egregious violations come from using Likert scales, which provably reverse the direction of the true preference in certain cases. We suggest improvements to the standard protocol to make it more theoretically sound, but even in its improved form, it cannot be used to evaluate open-ended tasks like story generation. For the latter, we propose a new human evaluation protocol called system-level probabilistic assessment (SPA). When human evaluation of stories is done with SPA, we can recover the ordering of GPT-3 models by size, with statistically significant results. However, when human evaluation is done with the standard protocol, less than half of the expected preferences can be recovered (e.g., there is no significant difference between curie and davinci, despite using a highly powered test).</abstract>
      <url hash="7dab18f6">2022.emnlp-main.406</url>
      <bibkey>ethayarajh-jurafsky-2022-authenticity</bibkey>
    </paper>
    <paper id="407">
      <title><fixed-case>BERT</fixed-case> in Plutarch’s Shadows</title>
      <author><first>Ivan</first><last>Yamshchikov</last></author>
      <author><first>Alexey</first><last>Tikhonov</last></author>
      <author><first>Yorgos</first><last>Pantis</last></author>
      <author><first>Charlotte</first><last>Schubert</last></author>
      <author><first>Jürgen</first><last>Jost</last></author>
      <pages>6071-6080</pages>
      <abstract>The extensive surviving corpus of the ancient scholar Plutarch of Chaeronea (ca. 45-120 CE) also contains several texts which, according to current scholarly opinion, did not originate with him and are therefore attributed to an anonymous author Pseudo-Plutarch. These include, in particular, the work Placita Philosophorum (Quotations and Opinions of the Ancient Philosophers), which is extremely important for the history of ancient philosophy. Little is known about the identity of that anonymous author and its relation to other authors from the same period. This paper presents a BERT language model for Ancient Greek. The model discovers previously unknown statistical properties relevant to these literary, philosophical, and historical problems and can shed new light on this authorship question. In particular, the Placita Philosophorum, together with one of the other Pseudo-Plutarch texts, shows similarities with the texts written by authors from an Alexandrian context (2nd/3rd century CE).</abstract>
      <url hash="27da7256">2022.emnlp-main.407</url>
      <bibkey>yamshchikov-etal-2022-bert</bibkey>
    </paper>
    <paper id="408">
      <title>Leveraging Locality in Abstractive Text Summarization</title>
      <author><first>Yixin</first><last>Liu</last></author>
      <author><first>Ansong</first><last>Ni</last></author>
      <author><first>Linyong</first><last>Nan</last></author>
      <author><first>Budhaditya</first><last>Deb</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Ahmed Hassan</first><last>Awadallah</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <pages>6081-6093</pages>
      <abstract>Neural attention models have achieved significant improvements on many natural language processing tasks. However, the quadratic memory complexity of the self-attention module with respect to the input length hinders their applications in long text summarization. Instead of designing more efficient attention modules, we approach this problem by investigating if models with a restricted context can have competitive performance compared with the memory-efficient attention models that maintain a global context by treating the input as a single sequence. Our model is applied to individual pages, which contain parts of inputs grouped by the principle of locality, during both the encoding and decoding stages. We empirically investigated three kinds of locality in text summarization at different levels of granularity, ranging from sentences to documents. Our experimental results show that our model has a better performance compared with strong baseline models with efficient attention modules, and our analysis provides further insights into our locality-aware modeling strategy.</abstract>
      <url hash="2f43f058">2022.emnlp-main.408</url>
      <bibkey>liu-etal-2022-leveraging-locality</bibkey>
    </paper>
    <paper id="409">
      <title>Salience Allocation as Guidance for Abstractive Summarization</title>
      <author><first>Fei</first><last>Wang</last></author>
      <author><first>Kaiqiang</first><last>Song</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Lifeng</first><last>Jin</last></author>
      <author><first>Sangwoo</first><last>Cho</last></author>
      <author><first>Wenlin</first><last>Yao</last></author>
      <author><first>Xiaoyang</first><last>Wang</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <pages>6094-6106</pages>
      <abstract>Abstractive summarization models typically learn to capture the salient information from scratch implicitly.Recent literature adds extractive summaries as guidance for abstractive summarization models to provide hints of salient content and achieves better performance.However, extractive summaries as guidance could be over strict, leading to information loss or noisy signals.Furthermore, it cannot easily adapt to documents with various abstractiveness.As the number and allocation of salience content pieces varies, it is hard to find a fixed threshold deciding which content should be included in the guidance.In this paper, we propose a novel summarization approach with a flexible and reliable salience guidance, namely SEASON (SaliencE Allocation as Guidance for Abstractive SummarizatiON).SEASON utilizes the allocation of salience expectation to guide abstractive summarization and adapts well to articles in different abstractiveness.Automatic and human evaluations on two benchmark datasets show that the proposed method is effective and reliable.Empirical results on more than one million news articles demonstrate a natural fifteen-fifty salience split for news article sentences, providing a useful insight for composing news articles.</abstract>
      <url hash="93b78d60">2022.emnlp-main.409</url>
      <bibkey>wang-etal-2022-salience</bibkey>
    </paper>
    <paper id="410">
      <title>Fine-tuned Language Models are Continual Learners</title>
      <author><first>Thomas</first><last>Scialom</last></author>
      <author><first>Tuhin</first><last>Chakrabarty</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <pages>6107-6122</pages>
      <abstract>Recent work on large language models relies on the intuition that most natural language processing tasks can be described via natural language instructions and that models trained on these instructions show strong zero-shot performance on several standard datasets. However, these models even though impressive still perform poorly on a wide range of tasks outside of their respective training and evaluation sets.To address this limitation, we argue that a model should be able to keep extending its knowledge and abilities, without forgetting previous skills. In spite of the limited success of Continual Learning, we show that <i>Fine-tuned Language Models can be continual learners</i>.We empirically investigate the reason for this success and conclude that Continual Learning emerges from self-supervision pre-training. Our resulting model Continual-T0 (CT0) is able to learn 8 new diverse language generation tasks, while still maintaining good performance on previous tasks, spanning in total of 70 datasets. Finally, we show that CT0 is able to combine instructions in ways it was never trained for, demonstrating some level of instruction compositionality.</abstract>
      <url hash="11cc9cb2">2022.emnlp-main.410</url>
      <bibkey>scialom-etal-2022-fine</bibkey>
    </paper>
    <paper id="411">
      <title>Natural Logic-guided Autoregressive Multi-hop Document Retrieval for Fact Verification</title>
      <author><first>Rami</first><last>Aly</last></author>
      <author><first>Andreas</first><last>Vlachos</last></author>
      <pages>6123-6135</pages>
      <abstract>A key component of fact verification is the evidence retrieval, often from multiple documents. Recent approaches use dense representations and condition the retrieval of each document on the previously retrieved ones. The latter step is performed over all the documents in the collection, requiring storing their dense representations in an index, thus incurring a high memory footprint. An alternative paradigm is retrieve-and-rerank, where documents are retrieved using methods such as BM25, their sentences are reranked, and further documents are retrieved conditioned on these sentences, reducing the memory requirements. However, such approaches can be brittle as they rely on heuristics and assume hyperlinks between documents.We propose a novel retrieve-and-rerank method for multi-hop retrieval, that consists of a retriever that jointly scores documents in the knowledge source and sentences from previously retrieved documents using an autoregressive formulation and is guided by a proof system based on natural logic that dynamically terminates the retrieval process if the evidence is deemed sufficient.This method exceeds or is on par with the current state-of-the-art on FEVER, HoVer and FEVEROUS-S, while using 5 to 10 times less memory than competing systems. Evaluation on an adversarial dataset indicates improved stability of our approach compared to commonly deployed threshold-based methods. Finally, the proof system helps humans predict model decisions correctly more often than using the evidence alone.</abstract>
      <url hash="da1d32b5">2022.emnlp-main.411</url>
      <bibkey>aly-vlachos-2022-natural</bibkey>
    </paper>
    <paper id="412">
      <title><fixed-case>AX</fixed-case>-<fixed-case>MABSA</fixed-case>: A Framework for Extremely Weakly Supervised Multi-label Aspect Based Sentiment Analysis</title>
      <author><first>Sabyasachi</first><last>Kamila</last></author>
      <author><first>Walid</first><last>Magdy</last></author>
      <author><first>Sourav</first><last>Dutta</last></author>
      <author><first>MingXue</first><last>Wang</last></author>
      <pages>6136-6147</pages>
      <abstract>Aspect Based Sentiment Analysis is a dominant research area with potential applications in social media analytics, business, finance, and health. Prior works in this area are primarily based on supervised methods, with a few techniques using weak supervision limited to predicting a single aspect category per review sentence. In this paper, we present an extremely weakly supervised multi-label Aspect Category Sentiment Analysis framework which does not use any labelled data. We only rely on a single word per class as an initial indicative information. We further propose an automatic word selection technique to choose these seed categories and sentiment words. We explore unsupervised language model post-training to improve the overall performance, and propose a multi-label generator model to generate multiple aspect category-sentiment pairs per review sentence. Experiments conducted on four benchmark datasets showcase our method to outperform other weakly supervised baselines by a significant margin.</abstract>
      <url hash="9dca9981">2022.emnlp-main.412</url>
      <bibkey>kamila-etal-2022-ax</bibkey>
    </paper>
    <paper id="413">
      <title>Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning</title>
      <author><first>Roshanak</first><last>Mirzaee</last></author>
      <author><first>Parisa</first><last>Kordjamshidi</last></author>
      <pages>6148-6165</pages>
      <abstract>Recent research shows synthetic data as a source of supervision helps pretrained language models (PLM) transfer learning to new target tasks/domains. However, this idea is less explored for spatial language. We provide two new data resources on multiple spatial language processing tasks. The first dataset is synthesized for transfer learning on spatial question answering (SQA) and spatial role labeling (SpRL). Compared to previous SQA datasets, we include a larger variety of spatial relation types and spatial expressions. Our data generation process is easily extendable with new spatial expression lexicons. The second one is a real-world SQA dataset with human-generated questions built on an existing corpus with SPRL annotations. This dataset can be used to evaluate spatial language processing models in realistic situations. We show pretraining with automatically generated data significantly improves the SOTA results on several SQA and SPRL benchmarks, particularly when the training data in the target domain is small.</abstract>
      <url hash="2ec09419">2022.emnlp-main.413</url>
      <bibkey>mirzaee-kordjamshidi-2022-transfer</bibkey>
    </paper>
    <paper id="414">
      <title>A Survey of Active Learning for Natural Language Processing</title>
      <author><first>Zhisong</first><last>Zhang</last></author>
      <author><first>Emma</first><last>Strubell</last></author>
      <author><first>Eduard</first><last>Hovy</last></author>
      <pages>6166-6190</pages>
      <abstract>In this work, we provide a literature review of active learning (AL) for its applications in natural language processing (NLP). In addition to a fine-grained categorization of query strategies, we also investigate several other important aspects of applying AL to NLP problems. These include AL for structured prediction tasks, annotation cost, model learning (especially with deep neural models), and starting and stopping AL. Finally, we conclude with a discussion of related topics and future directions.</abstract>
      <url hash="9e2de8e3">2022.emnlp-main.414</url>
      <bibkey>zhang-etal-2022-survey</bibkey>
    </paper>
    <paper id="415">
      <title>Bernice: A Multilingual Pre-trained Encoder for <fixed-case>T</fixed-case>witter</title>
      <author><first>Alexandra</first><last>DeLucia</last></author>
      <author><first>Shijie</first><last>Wu</last></author>
      <author><first>Aaron</first><last>Mueller</last></author>
      <author><first>Carlos</first><last>Aguirre</last></author>
      <author><first>Philip</first><last>Resnik</last></author>
      <author><first>Mark</first><last>Dredze</last></author>
      <pages>6191-6205</pages>
      <abstract>The language of Twitter differs significantly from that of other domains commonly included in large language model training. While tweets are typically multilingual and contain informal language, including emoji and hashtags, most pre-trained language models for Twitter are either monolingual, adapted from other domains rather than trained exclusively on Twitter, or are trained on a limited amount of in-domain Twitter data.We introduce Bernice, the first multilingual RoBERTa language model trained from scratch on 2.5 billion tweets with a custom tweet-focused tokenizer. We evaluate on a variety of monolingual and multilingual Twitter benchmarks, finding that our model consistently exceeds or matches the performance of a variety of models adapted to social media data as well as strong multilingual baselines, despite being trained on less data overall.We posit that it is more efficient compute- and data-wise to train completely on in-domain data with a specialized domain-specific tokenizer.</abstract>
      <url hash="79cf2e60">2022.emnlp-main.415</url>
      <bibkey>delucia-etal-2022-bernice</bibkey>
    </paper>
    <paper id="416">
      <title><fixed-case>CEFR</fixed-case>-Based Sentence Difficulty Annotation and Assessment</title>
      <author><first>Yuki</first><last>Arase</last></author>
      <author><first>Satoru</first><last>Uchida</last></author>
      <author><first>Tomoyuki</first><last>Kajiwara</last></author>
      <pages>6206-6219</pages>
      <abstract>Controllable text simplification is a crucial assistive technique for language learning and teaching. One of the primary factors hindering its advancement is the lack of a corpus annotated with sentence difficulty levels based on language ability descriptions. To address this problem, we created the CEFR-based Sentence Profile (CEFR-SP) corpus, containing 17k English sentences annotated with the levels based on the Common European Framework of Reference for Languages assigned by English-education professionals. In addition, we propose a sentence-level assessment model to handle unbalanced level distribution because the most basic and highly proficient sentences are naturally scarce. In the experiments in this study, our method achieved a macro-F1 score of 84.5% in the level assessment, thus outperforming strong baselines employed in readability assessment.</abstract>
      <url hash="298ad869">2022.emnlp-main.416</url>
      <bibkey>arase-etal-2022-cefr</bibkey>
    </paper>
    <paper id="417">
      <title>Simple Questions Generate Named Entity Recognition Datasets</title>
      <author><first>Hyunjae</first><last>Kim</last></author>
      <author><first>Jaehyo</first><last>Yoo</last></author>
      <author><first>Seunghyun</first><last>Yoon</last></author>
      <author><first>Jinhyuk</first><last>Lee</last></author>
      <author><first>Jaewoo</first><last>Kang</last></author>
      <pages>6220-6236</pages>
      <abstract>Recent named entity recognition (NER) models often rely on human-annotated datasets requiring the vast engagement of professional knowledge on the target domain and entities. This work introduces an ask-to-generate approach, which automatically generates NER datasets by asking simple natural language questions to an open-domain question answering system (e.g., “Which disease?”). Despite using fewer training resources, our models solely trained on the generated datasets largely outperform strong low-resource models by 19.5 F1 score across six popular NER benchmarks. Our models also show competitive performance with rich-resource models that additionally leverage in-domain dictionaries provided by domain experts. In few-shot NER, we outperform the previous best model by 5.2 F1 score on three benchmarks and achieve new state-of-the-art performance.</abstract>
      <url hash="dbcbf63e">2022.emnlp-main.417</url>
      <bibkey>kim-etal-2022-simple</bibkey>
    </paper>
    <paper id="418">
      <title><fixed-case>T</fixed-case>emporal<fixed-case>W</fixed-case>iki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models</title>
      <author><first>Joel</first><last>Jang</last></author>
      <author><first>Seonghyeon</first><last>Ye</last></author>
      <author><first>Changho</first><last>Lee</last></author>
      <author><first>Sohee</first><last>Yang</last></author>
      <author><first>Joongbo</first><last>Shin</last></author>
      <author><first>Janghoon</first><last>Han</last></author>
      <author><first>Gyeonghun</first><last>Kim</last></author>
      <author><first>Minjoon</first><last>Seo</last></author>
      <pages>6237-6250</pages>
      <abstract>Language Models (LMs) become outdated as the world changes; they often fail to perform tasks requiring recent factual information which was absent or different during training, a phenomenon called temporal misalignment. This is especially a challenging problem because the research community still lacks a coherent dataset for assessing the adaptability of LMs to frequently-updated knowledge corpus such as Wikipedia. To this end, we introduce TemporalWiki, a lifelong benchmark for ever-evolving LMs that utilizes the difference between consecutive snapshots of English Wikipedia and English Wikidata for training and evaluation, respectively. The benchmark hence allows researchers to periodically track an LM’s ability to retain previous knowledge and acquire updated/new knowledge at each point in time. We also find that training an LM on the diff data through continual learning methods achieves similar or better perplexity than on the entire snapshot in our benchmark with 12 times less computational cost, which verifies that factual knowledge in LMs can be safely updated with minimal training data via continual learning.</abstract>
      <url hash="e7991c09">2022.emnlp-main.418</url>
      <bibkey>jang-etal-2022-temporalwiki</bibkey>
    </paper>
    <paper id="419">
      <title>Bi-Directional Iterative Prompt-Tuning for Event Argument Extraction</title>
      <author><first>Lu</first><last>Dai</last></author>
      <author><first>Bang</first><last>Wang</last></author>
      <author><first>Wei</first><last>Xiang</last></author>
      <author><first>Yijun</first><last>Mo</last></author>
      <pages>6251-6263</pages>
      <abstract>Recently, prompt-tuning has attracted growing interests in event argument extraction (EAE). However, the existing prompt-tuning methods have not achieved satisfactory performance due to the lack of consideration of entity information. In this paper, we propose a bi-directional iterative prompt-tuning method for EAE, where the EAE task is treated as a cloze-style task to take full advantage of entity information and pre-trained language models (PLMs). Furthermore, our method explores event argument interactions by introducing the argument roles of contextual entities into prompt construction. Since template and verbalizer are two crucial components in a cloze-style prompt, we propose to utilize the role label semantic knowledge to construct a semantic verbalizer and design three kind of templates for the EAE task. Experiments on the ACE 2005 English dataset with standard and low-resource settings show that the proposed method significantly outperforms the peer state-of-the-art methods.</abstract>
      <url hash="46bc57c7">2022.emnlp-main.419</url>
      <bibkey>dai-etal-2022-bi</bibkey>
    </paper>
    <paper id="420">
      <title>Learning Robust Representations for Continual Relation Extraction via Adversarial Class Augmentation</title>
      <author><first>Peiyi</first><last>Wang</last></author>
      <author><first>Yifan</first><last>Song</last></author>
      <author><first>Tianyu</first><last>Liu</last></author>
      <author><first>Binghuai</first><last>Lin</last></author>
      <author><first>Yunbo</first><last>Cao</last></author>
      <author><first>Sujian</first><last>Li</last></author>
      <author><first>Zhifang</first><last>Sui</last></author>
      <pages>6264-6278</pages>
      <abstract>Continual relation extraction (CRE) aims to continually learn new relations from a class-incremental data stream. CRE model usually suffers from catastrophic forgetting problem, i.e., the performance of old relations seriously degrades when the model learns new relations. Most previous work attributes catastrophic forgetting to the corruption of the learned representations as new relations come, with an implicit assumption that the CRE models have adequately learned the old relations. In this paper, through empirical studies we argue that this assumption may not hold, and an important reason for catastrophic forgetting is that the learned representations do not have good robustness against the appearance of analogous relations in the subsequent learning process. To address this issue, we encourage the model to learn more precise and robust representations through a simple yet effective adversarial class augmentation mechanism (ACA), which is easy to implement and model-agnostic.Experimental results show that ACA can consistently improve the performance of state-of-the-art CRE models on two popular benchmarks.</abstract>
      <url hash="f54a12bd">2022.emnlp-main.420</url>
      <bibkey>wang-etal-2022-learning-robust</bibkey>
    </paper>
    <paper id="421">
      <title><fixed-case>C</fixed-case>onv<fixed-case>F</fixed-case>in<fixed-case>QA</fixed-case>: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering</title>
      <author><first>Zhiyu</first><last>Chen</last></author>
      <author><first>Shiyang</first><last>Li</last></author>
      <author><first>Charese</first><last>Smiley</last></author>
      <author><first>Zhiqiang</first><last>Ma</last></author>
      <author><first>Sameena</first><last>Shah</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>6279-6292</pages>
      <abstract>With the recent advance in large pre-trained language models, researchers have achieved record performances in NLP tasks that mostly focus on language pattern matching. The community is experiencing the shift of the challenge from how to model language to the imitation of complex reasoning abilities like human beings. In this work, we investigate the application domain of finance that involves real-world, complex numerical reasoning. We propose a new large-scale dataset, ConvFinQA, aiming to study the chain of numerical reasoning in conversational question answering. Our dataset poses great challenge in modeling long-range, complex numerical reasoning paths in real-world conversations. We conduct comprehensive experiments and analyses with both the neural symbolic methods and the prompting-based methods, to provide insights into the reasoning mechanisms of these two divisions. We believe our new dataset should serve as a valuable resource to push forward the exploration of real-world, complex reasoning tasks as the next research focus. Our dataset and code is publicly available at https://github.com/czyssrs/ConvFinQA.</abstract>
      <url hash="94c77730">2022.emnlp-main.421</url>
      <bibkey>chen-etal-2022-convfinqa</bibkey>
    </paper>
    <paper id="422">
      <title>A Span-based Multimodal Variational Autoencoder for Semi-supervised Multimodal Named Entity Recognition</title>
      <author><first>Baohang</first><last>Zhou</last></author>
      <author><first>Ying</first><last>Zhang</last></author>
      <author><first>Kehui</first><last>Song</last></author>
      <author><first>Wenya</first><last>Guo</last></author>
      <author><first>Guoqing</first><last>Zhao</last></author>
      <author><first>Hongbin</first><last>Wang</last></author>
      <author><first>Xiaojie</first><last>Yuan</last></author>
      <pages>6293-6302</pages>
      <abstract>Multimodal named entity recognition (MNER) on social media is a challenging task which aims to extract named entities in free text and incorporate images to classify them into user-defined types. However, the annotation for named entities on social media demands a mount of human efforts. The existing semi-supervised named entity recognition methods focus on the text modal and are utilized to reduce labeling costs in traditional NER. However, the previous methods are not efficient for semi-supervised MNER. Because the MNER task is defined to combine the text information with image one and needs to consider the mismatch between the posted text and image. To fuse the text and image features for MNER effectively under semi-supervised setting, we propose a novel span-based multimodal variational autoencoder (SMVAE) model for semi-supervised MNER. The proposed method exploits modal-specific VAEs to model text and image latent features, and utilizes product-of-experts to acquire multimodal features. In our approach, the implicit relations between labels and multimodal features are modeled by multimodal VAE. Thus, the useful information of unlabeled data can be exploited in our method under semi-supervised setting. Experimental results on two benchmark datasets demonstrate that our approach not only outperforms baselines under supervised setting, but also improves MNER performance with less labeled data than existing semi-supervised methods.</abstract>
      <url hash="4ce1116b">2022.emnlp-main.422</url>
      <bibkey>zhou-etal-2022-span</bibkey>
    </paper>
    <paper id="423">
      <title><fixed-case>R</fixed-case>-<fixed-case>T</fixed-case>ea<fixed-case>F</fixed-case>or: Regularized Teacher-Forcing for Abstractive Summarization</title>
      <author><first>Guan-Yu</first><last>Lin</last></author>
      <author><first>Pu-Jen</first><last>Cheng</last></author>
      <pages>6303-6311</pages>
      <abstract>Teacher-forcing is widely used in training sequence generation models to improve sampling efficiency and to stabilize training. However, teacher-forcing is vulnerable to the exposure bias problem. Previous works have attempted to address exposure bias by modifying the training data to simulate model-generated results. Nevertheless, they do not consider the pairwise relationship between the original training data and the modified ones, which provides more information during training. Hence, we propose Regularized Teacher-Forcing (R-TeaFor) to utilize this relationship for better regularization. Empirically, our experiments show that R-TeaFor outperforms previous summarization state-of-the-art models, and the results can be generalized to different pre-trained models.</abstract>
      <url hash="4147e48d">2022.emnlp-main.423</url>
      <bibkey>lin-cheng-2022-r</bibkey>
    </paper>
    <paper id="424">
      <title>Modeling Consistency Preference via Lexical Chains for Document-level Neural Machine Translation</title>
      <author><first>Xinglin</first><last>Lyu</last></author>
      <author><first>Junhui</first><last>Li</last></author>
      <author><first>Shimin</first><last>Tao</last></author>
      <author><first>Hao</first><last>Yang</last></author>
      <author><first>Ying</first><last>Qin</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>6312-6326</pages>
      <abstract>In this paper we aim to relieve the issue of lexical translation inconsistency for document-level neural machine translation (NMT) by modeling consistency preference for lexical chains, which consist of repeated words in a source-side document and provide a representation of the lexical consistency structure of the document. Specifically, we first propose lexical-consistency attention to capture consistency context among words in the same lexical chains. Then for each lexical chain we define and learn a consistency-tailored latent variable, which will guide the translation of corresponding sentences to enhance lexical translation consistency. Experimental results on Chinese→English and French→English document-level translation tasks show that our approach not only significantly improves translation performance in BLEU, but also substantially alleviates the problem of the lexical translation inconsistency.</abstract>
      <url hash="f14f2665">2022.emnlp-main.424</url>
      <bibkey>lyu-etal-2022-modeling</bibkey>
    </paper>
    <paper id="425">
      <title>Just Fine-tune Twice: Selective Differential Privacy for Large Language Models</title>
      <author><first>Weiyan</first><last>Shi</last></author>
      <author><first>Ryan</first><last>Shea</last></author>
      <author><first>Si</first><last>Chen</last></author>
      <author><first>Chiyuan</first><last>Zhang</last></author>
      <author><first>Ruoxi</first><last>Jia</last></author>
      <author><first>Zhou</first><last>Yu</last></author>
      <pages>6327-6340</pages>
      <abstract>Protecting large language models from privacy leakage is becoming increasingly crucial with their wide adoption in real-world products. Yet applying *differential privacy* (DP), a canonical notion with provable privacy guarantees for machine learning models, to those models remains challenging due to the trade-off between model utility and privacy loss. Utilizing the fact that sensitive information in language data tends to be sparse, Shi et al. (2021) formalized a DP notion extension called *Selective Differential Privacy* (SDP) to protect only the sensitive tokens defined by a policy function. However, their algorithm only works for RNN-based models. In this paper, we develop a novel framework, *Just Fine-tune Twice* (JFT), that achieves SDP for state-of-the-art large transformer-based models. Our method is easy to implement: it first fine-tunes the model with *redacted* in-domain data, and then fine-tunes it again with the *original* in-domain data using a private training mechanism. Furthermore, we study the scenario of imperfect implementation of policy functions that misses sensitive tokens and develop systematic methods to handle it. Experiments show that our method achieves strong utility compared to previous baselines. We also analyze the SDP privacy guarantee empirically with the canary insertion attack.</abstract>
      <url hash="4ec21d80">2022.emnlp-main.425</url>
      <bibkey>shi-etal-2022-just</bibkey>
    </paper>
    <paper id="426">
      <title>Factorizing Content and Budget Decisions in Abstractive Summarization of Long Documents</title>
      <author><first>Marcio</first><last>Fonseca</last></author>
      <author><first>Yftah</first><last>Ziser</last></author>
      <author><first>Shay B.</first><last>Cohen</last></author>
      <pages>6341-6364</pages>
      <abstract>We argue that disentangling content selection from the budget used to cover salient content improves the performance and applicability of abstractive summarizers. Our method, FactorSum, does this disentanglement by factorizing summarization into two steps through an energy function: (1) generation of abstractive summary views covering salient information in subsets of the input document (document views); (2) combination of these views into a final summary, following a budget and content guidance. This guidance may come from different sources, including from an advisor model such as BART or BigBird, or in oracle mode – from the reference. This factorization achieves significantly higher ROUGE scores on multiple benchmarks for long document summarization, namely PubMed, arXiv, and GovReport. Most notably, our model is effective for domain adaptation. When trained only on PubMed samples, it achieves a 46.29 ROUGE-1 score on arXiv, outperforming PEGASUS trained in domain by a large margin. Our experimental results indicate that the performance gains are due to more flexible budget adaptation and processing of shorter contexts provided by partial document views.</abstract>
      <url hash="4b0492bb">2022.emnlp-main.426</url>
      <bibkey>fonseca-etal-2022-factorizing</bibkey>
    </paper>
    <paper id="427">
      <title>Open-Domain Sign Language Translation Learned from Online Video</title>
      <author><first>Bowen</first><last>Shi</last></author>
      <author><first>Diane</first><last>Brentari</last></author>
      <author><first>Gregory</first><last>Shakhnarovich</last></author>
      <author><first>Karen</first><last>Livescu</last></author>
      <pages>6365-6379</pages>
      <abstract>Existing work on sign language translation – that is, translation from sign language videos into sentences in a written language – has focused mainly on (1) data collected in a controlled environment or (2) data in a specific domain, which limits the applicability to real-world settings. In this paper, we introduce OpenASL, a large-scale American Sign Language (ASL) - English dataset collected from online video sites (e.g., YouTube).OpenASL contains 288 hours of ASL videos in multiple domains from over 200 signers and is the largest publicly available ASL translation dataset to date. To tackle the challenges of sign language translation in realistic settings and without glosses, we propose a set of techniques including sign search as a pretext task for pre-training and fusion of mouthing and handshape features. The proposed techniques produce consistent and large improvements in translation quality, over baseline models basedon prior work.</abstract>
      <url hash="4bf6d261">2022.emnlp-main.427</url>
      <bibkey>shi-etal-2022-open</bibkey>
    </paper>
    <paper id="428">
      <title>Improving Temporal Generalization of Pre-trained Language Models with Lexical Semantic Change</title>
      <author><first>Zhaochen</first><last>Su</last></author>
      <author><first>Zecheng</first><last>Tang</last></author>
      <author><first>Xinyan</first><last>Guan</last></author>
      <author><first>Lijun</first><last>Wu</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Juntao</first><last>Li</last></author>
      <pages>6380-6393</pages>
      <abstract>Recent research has revealed that neural language models at scale suffer from poor temporal generalization capability, i.e., language model pre-trained on static data from past years performs worse over time on emerging data. Existing methods mainly perform continual training to mitigate such a misalignment. While effective to some extent but is far from being addressed on both the language modeling and downstream tasks. In this paper, we empirically observe that temporal generalization is closely affiliated with lexical semantic change, which is one of the essential phenomena of natural languages. Based on this observation, we propose a simple yet effective lexical-level masking strategy to post-train a converged language model. Experiments on two pre-trained language models, two different classification tasks, and four benchmark datasets demonstrate the effectiveness of our proposed method over existing temporal adaptation methods, i.e., continual training with new data. Our code is available at https://github.com/zhaochen0110/LMLM.</abstract>
      <url hash="dc4eeabe">2022.emnlp-main.428</url>
      <bibkey>su-etal-2022-improving</bibkey>
    </paper>
    <paper id="429">
      <title><fixed-case>ULN</fixed-case>: Towards Underspecified Vision-and-Language Navigation</title>
      <author><first>Weixi</first><last>Feng</last></author>
      <author><first>Tsu-Jui</first><last>Fu</last></author>
      <author><first>Yujie</first><last>Lu</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>6394-6412</pages>
      <abstract>Vision-and-Language Navigation (VLN) is a task to guide an embodied agent moving to a target position using language instructions. Despite the significant performance improvement, the wide use of fine-grained instructions fails to characterize more practical linguistic variations in reality. To fill in this gap, we introduce a new setting, namely Underspecified vision-and-Language Navigation (ULN), and associated evaluation datasets. ULN evaluates agents using multi-level underspecified instructions instead of purely fine-grained or coarse-grained, which is a more realistic and general setting. As a primary step toward ULN, we propose a VLN framework that consists of a classification module, a navigation agent, and an Exploitation-to-Exploration (E2E) module. Specifically, we propose to learn Granularity Specific Sub-networks (GSS) for the agent to ground multi-level instructions with minimal additional parameters. Then, our E2E module estimates grounding uncertainty and conducts multi-step lookahead exploration to improve the success rate further. Experimental results show that existing VLN models are still brittle to multi-level language underspecification. Our framework is more robust and outperforms the baselines on ULN by ~10% relative success rate across all levels.</abstract>
      <url hash="4a0d69dd">2022.emnlp-main.429</url>
      <bibkey>feng-etal-2022-uln</bibkey>
    </paper>
    <paper id="430">
      <title>Federated Model Decomposition with Private Vocabulary for Text Classification</title>
      <author><first>Zhuo</first><last>Zhang</last></author>
      <author><first>Xiangjing</first><last>Hu</last></author>
      <author><first>Lizhen</first><last>Qu</last></author>
      <author><first>Qifan</first><last>Wang</last></author>
      <author><first>Zenglin</first><last>Xu</last></author>
      <pages>6413-6425</pages>
      <abstract>With the necessity of privacy protection, it becomes increasingly vital to train deep neural models in a federated learning manner for natural language processing (NLP) tasks. However, recent studies show eavesdroppers (i.e., dishonest servers) can still reconstruct the private input in federated learning (FL). Such a data reconstruction attack relies on the mappings between vocabulary and associated word embedding in NLP tasks, which are unfortunately less studied in current FL methods. In this paper, we propose a fedrated model decomposition method that protects the privacy of vocabularies, shorted as FEDEVOCAB. In FEDEVOCAB, each participant keeps the local embedding layer in the local device and detaches the local embedding parameters from federated aggregation. However, it is challenging to train an accurate NLP model when the private mappings are unknown and vary across participants in a cross-device FL setting. To address this problem, we further propose an adaptive updating technique to improve the performance of local models. Experimental results show that FEDEVOCAB maintains competitive performance and provides better privacy-preserving capacity compared to status quo methods.</abstract>
      <url hash="fad1f725">2022.emnlp-main.430</url>
      <bibkey>zhang-etal-2022-federated</bibkey>
    </paper>
    <paper id="431">
      <title><fixed-case>R</fixed-case>e<fixed-case>C</fixed-case>o: Reliable Causal Chain Reasoning via Structural Causal Recurrent Neural Networks</title>
      <author><first>Kai</first><last>Xiong</last></author>
      <author><first>Xiao</first><last>Ding</last></author>
      <author><first>Zhongyang</first><last>Li</last></author>
      <author><first>Li</first><last>Du</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <author><first>Yi</first><last>Zheng</last></author>
      <author><first>Baoxing</first><last>Huai</last></author>
      <pages>6426-6438</pages>
      <abstract>Causal chain reasoning (CCR) is an essential ability for many decision-making AI systems, which requires the model to build reliable causal chains by connecting causal pairs. However, CCR suffers from two main transitive problems: threshold effect and scene drift. In other words, the causal pairs to be spliced may have a conflicting threshold boundary or scenario.To address these issues, we propose a novel Reliable Causal chain reasoning framework (ReCo), which introduces exogenous variables to represent the threshold and scene factors of each causal pair within the causal chain, and estimates the threshold and scene contradictions across exogenous variables via structural causal recurrent neural networks (SRNN). Experiments show that ReCo outperforms a series of strong baselines on both Chinese and English CCR datasets. Moreover, by injecting reliable causal chain knowledge distilled by ReCo, BERT can achieve better performances on four downstream causal-related tasks than BERT models enhanced by other kinds of knowledge.</abstract>
      <url hash="8a9e692a">2022.emnlp-main.431</url>
      <bibkey>xiong-etal-2022-reco</bibkey>
    </paper>
    <paper id="432">
      <title>Video Question Answering: Datasets, Algorithms and Challenges</title>
      <author><first>Yaoyao</first><last>Zhong</last></author>
      <author><first>Wei</first><last>Ji</last></author>
      <author><first>Junbin</first><last>Xiao</last></author>
      <author><first>Yicong</first><last>Li</last></author>
      <author><first>Weihong</first><last>Deng</last></author>
      <author><first>Tat-Seng</first><last>Chua</last></author>
      <pages>6439-6455</pages>
      <abstract>This survey aims to sort out the recent advances in video question answering (VideoQA) and point towards future directions. We firstly categorize the datasets into 1) normal VideoQA, multi-modal VideoQA and knowledge-based VideoQA, according to the modalities invoked in the question-answer pairs, or 2) factoid VideoQA and inference VideoQA, according to the technical challenges in comprehending the questions and deriving the correct answers. We then summarize the VideoQA techniques, including those mainly designed for Factoid QA (e.g., the early spatio-temporal attention-based methods and the recently Transformer-based ones) and those targeted at explicit relation and logic inference (e.g., neural modular networks, neural symbolic methods, and graph-structured methods). Aside from the backbone techniques, we delve into the specific models and find out some common and useful insights either for video modeling, question answering, or for cross-modal correspondence learning. Finally, we point out the research trend of studying beyond factoid VideoQA to inference VideoQA, as well as towards the robustness and interpretability. Additionally, we maintain a repository, https://github.com/VRU-NExT/VideoQA, to keep trace of the latest VideoQA papers, datasets, and their open-source implementations if available. With these efforts, we strongly hope this survey could shed light on the follow-up VideoQA research.</abstract>
      <url hash="e1633b7a">2022.emnlp-main.432</url>
      <bibkey>zhong-etal-2022-video</bibkey>
    </paper>
    <paper id="433">
      <title>Retrofitting Multilingual Sentence Embeddings with <fixed-case>A</fixed-case>bstract <fixed-case>M</fixed-case>eaning <fixed-case>R</fixed-case>epresentation</title>
      <author><first>Deng</first><last>Cai</last></author>
      <author><first>Xin</first><last>Li</last></author>
      <author><first>Jackie Chun-Sing</first><last>Ho</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <pages>6456-6472</pages>
      <abstract>We introduce a new method to improve existing multilingual sentence embeddings with Abstract Meaning Representation (AMR). Compared with the original textual input, AMR is a structured semantic representation that presents the core concepts and relations in a sentence explicitly and unambiguously. It also helps reduce the surface variations across different expressions and languages. Unlike most prior work that only evaluates the ability to measure semantic similarity, we present a thorough evaluation of existing multilingual sentence embeddings and our improved versions, which include a collection of five transfer tasks in different downstream applications. Experiment results show that retrofitting multilingual sentence embeddings with AMR leads to better state-of-the-art performance on both semantic textual similarity and transfer tasks.</abstract>
      <url hash="708e9461">2022.emnlp-main.433</url>
      <bibkey>cai-etal-2022-retrofitting</bibkey>
    </paper>
    <paper id="434">
      <title>Breaking the Representation Bottleneck of <fixed-case>C</fixed-case>hinese Characters: Neural Machine Translation with Stroke Sequence Modeling</title>
      <author><first>Zhijun</first><last>Wang</last></author>
      <author><first>Xuebo</first><last>Liu</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>6473-6484</pages>
      <abstract>Existing research generally treats Chinese character as a minimum unit for representation. However, such Chinese character representation will suffer two bottlenecks: 1) Learning bottleneck, the learning cannot benefit from its rich internal features (e.g., radicals and strokes); and 2) Parameter bottleneck, each individual character has to be represented by a unique vector. In this paper, we introduce a novel representation method for Chinese characters to break the bottlenecks, namely StrokeNet, which represents a Chinese character by a Latinized stroke sequence (e.g., “凹 (concave)” to “ajaie” and “凸 (convex)” to “aeaqe”). Specifically, StrokeNet maps each stroke to a specific Latin character, thus allowing similar Chinese characters to have similar Latin representations. With the introduction of StrokeNet to neural machine translation (NMT), many powerful but not applicable techniques to non-Latin languages (e.g., shared subword vocabulary learning and ciphertext-based data augmentation) can now be perfectly implemented. Experiments on the widely-used NIST Chinese-English, WMT17 Chinese-English and IWSLT17 Japanese-English NMT tasks show that StrokeNet can provide a significant performance boost over the strong baselines with fewer model parameters, achieving 26.5 BLEU on the WMT17 Chinese-English task which is better than any previously reported results without using monolingual data. Code and scripts are freely available at https://github.com/zjwang21/StrokeNet.</abstract>
      <url hash="24d39d53">2022.emnlp-main.434</url>
      <bibkey>wang-etal-2022-breaking</bibkey>
    </paper>
    <paper id="435">
      <title>Boundary-Driven Table-Filling for Aspect Sentiment Triplet Extraction</title>
      <author><first>Yice</first><last>Zhang</last></author>
      <author><first>Yifan</first><last>Yang</last></author>
      <author><first>Yihui</first><last>Li</last></author>
      <author><first>Bin</first><last>Liang</last></author>
      <author><first>Shiwei</first><last>Chen</last></author>
      <author><first>Yixue</first><last>Dang</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <pages>6485-6498</pages>
      <abstract>Aspect Sentiment Triplet Extraction (ASTE) aims to extract the aspect terms along with the corresponding opinion terms and the expressed sentiments in the review, which is an important task in sentiment analysis. Previous research efforts generally address the ASTE task in an end-to-end fashion through the table-filling formalization, in which the triplets are represented by a two-dimensional (2D) table of word-pair relations. Under this formalization, a term-level relation is decomposed into multiple independent word-level relations, which leads to relation inconsistency and boundary insensitivity in the face of multi-word aspect terms and opinion terms. To overcome these issues, we propose Boundary-Driven Table-Filling (BDTF), which represents each triplet as a relation region in the 2D table and transforms the ASTE task into detection and classification of relation regions. We also notice that the quality of the table representation greatly affects the performance of BDTF. Therefore, we develop an effective relation representation learning approach to learn the table representation, which can fully exploit both word-to-word interactions and relation-to-relation interactions. Experiments on several public benchmarks show that the proposed approach achieves state-of-the-art performances.</abstract>
      <url hash="41d0dcdb">2022.emnlp-main.435</url>
      <bibkey>zhang-etal-2022-boundary</bibkey>
    </paper>
    <paper id="436">
      <title>Attention and Edge-Label Guided Graph Convolutional Networks for Named Entity Recognition</title>
      <author><first>Renjie</first><last>Zhou</last></author>
      <author><first>Zhongyi</first><last>Xie</last></author>
      <author><first>Jian</first><last>Wan</last></author>
      <author><first>Jilin</first><last>Zhang</last></author>
      <author><first>Yong</first><last>Liao</last></author>
      <author><first>Qiang</first><last>Liu</last></author>
      <pages>6499-6510</pages>
      <abstract>It has been shown that named entity recognition (NER) could benefit from incorporating the long-distance structured information captured by dependency trees. However, dependency trees built by tools usually have a certain percentage of errors. Under such circumstances, how to better use relevant structured information while ignoring irrelevant or wrong structured information from the dependency trees to improve NER performance is still a challenging research problem. In this paper, we propose the Attention and Edge-Label guided Graph Convolution Network (AELGCN) model. Then, we integrate it into BiLSTM-CRF to form BiLSTM-AELGCN-CRF model. We design an edge-aware node joint update module and introduce a node-aware edge update module to explore hidden in structured information entirely and solve the wrong dependency label information to some extent. After two modules, we apply attention-guided GCN, which automatically learns how to attend to the relevant structured information selectively. We conduct extensive experiments on several standard datasets across four languages and achieve better results than previous approaches. Through experimental analysis, it is found that our proposed model can better exploit the structured information on the dependency tree to improve the recognition of long entities.</abstract>
      <url hash="e458a803">2022.emnlp-main.436</url>
      <bibkey>zhou-etal-2022-attention</bibkey>
    </paper>
    <paper id="437">
      <title><fixed-case>T</fixed-case>itle2<fixed-case>E</fixed-case>vent: Benchmarking Open Event Extraction with a Large-scale <fixed-case>C</fixed-case>hinese Title Dataset</title>
      <author><first>Haolin</first><last>Deng</last></author>
      <author><first>Yanan</first><last>Zhang</last></author>
      <author><first>Yangfan</first><last>Zhang</last></author>
      <author><first>Wangyang</first><last>Ying</last></author>
      <author><first>Changlong</first><last>Yu</last></author>
      <author><first>Jun</first><last>Gao</last></author>
      <author><first>Wei</first><last>Wang</last></author>
      <author><first>Xiaoling</first><last>Bai</last></author>
      <author><first>Nan</first><last>Yang</last></author>
      <author><first>Jin</first><last>Ma</last></author>
      <author><first>Xiang</first><last>Chen</last></author>
      <author><first>Tianhua</first><last>Zhou</last></author>
      <pages>6511-6524</pages>
      <abstract>Event extraction (EE) is crucial to downstream tasks such as new aggregation and event knowledge graph construction. Most existing EE datasets manually define fixed event types and design specific schema for each of them, failing to cover diverse events emerging from the online text. Moreover, news titles, an important source of event mentions, have not gained enough attention in current EE research. In this paper, we present Title2Event, a large-scale sentence-level dataset benchmarking Open Event Extraction without restricting event types. Title2Event contains more than 42,000 news titles in 34 topics collected from Chinese web pages. To the best of our knowledge, it is currently the largest manually annotated Chinese dataset for open event extraction. We further conduct experiments on Title2Event with different models and show that the characteristics of titles make it challenging for event extraction, addressing the significance of advanced study on this problem. The dataset and baseline codes are available at https://open-event-hub.github.io/title2event.</abstract>
      <url hash="2dc655c9">2022.emnlp-main.437</url>
      <bibkey>deng-etal-2022-title2event</bibkey>
    </paper>
    <paper id="438">
      <title>Cascading Biases: Investigating the Effect of Heuristic Annotation Strategies on Data and Models</title>
      <author><first>Chaitanya</first><last>Malaviya</last></author>
      <author><first>Sudeep</first><last>Bhatia</last></author>
      <author><first>Mark</first><last>Yatskar</last></author>
      <pages>6525-6540</pages>
      <abstract>Cognitive psychologists have documented that humans use cognitive heuristics, or mental shortcuts, to make quick decisions while expending less effort. While performing annotation work on crowdsourcing platforms, we hypothesize that such heuristic use among annotators cascades on to data quality and model robustness. In this work, we study cognitive heuristic use in the context of annotating multiple-choice reading comprehension datasets. We propose tracking annotator heuristic traces, where we tangibly measure low-effort annotation strategies that could indicate usage of various cognitive heuristics. We find evidence that annotators might be using multiple such heuristics, based on correlations with a battery of psychological tests. Importantly, heuristic use among annotators determines data quality along several dimensions: (1) known biased models, such as partial input models, more easily solve examples authoredby annotators that rate highly on heuristic use, (2) models trained on annotators scoring highly on heuristic use don’t generalize as well, and (3) heuristic-seeking annotators tend to create qualitatively less challenging examples. Our findings suggest that tracking heuristic usage among annotators can potentially help with collecting challenging datasets and diagnosing model biases.</abstract>
      <url hash="30546cad">2022.emnlp-main.438</url>
      <bibkey>malaviya-etal-2022-cascading</bibkey>
    </paper>
    <paper id="439">
      <title>Teaching Broad Reasoning Skills for Multi-Step <fixed-case>QA</fixed-case> by Generating Hard Contexts</title>
      <author><first>Harsh</first><last>Trivedi</last></author>
      <author><first>Niranjan</first><last>Balasubramanian</last></author>
      <author><first>Tushar</first><last>Khot</last></author>
      <author><first>Ashish</first><last>Sabharwal</last></author>
      <pages>6541-6566</pages>
      <abstract>Question-answering datasets require a broad set of reasoning skills. We show how to use question decompositions to teach language models these broad reasoning skills in a robust fashion. Specifically, we use widely available QDMR representations to programmatically create hard-to-cheat synthetic contexts for real questions in six multi-step reasoning datasets. These contexts are carefully designed to avoid common reasoning shortcuts prevalent in real contexts that prevent models from learning the right skills. This results in a pretraining dataset, named TeaBReaC, containing 525K multi-step questions (with associated formal programs) covering about 900 reasoning patterns. We show that pretraining standard language models (LMs) on TeaBReaC before fine-tuning them on target datasets improves their performance by up to 13 F1 points across 4 multi-step QA datasets, with up to 21 point gain on more complex questions. The resulting models also demonstrate higher robustness, with a 5-8 F1 point improvement on two contrast sets. Furthermore, TeaBReaC pretraining substantially improves model performance and robustness even when starting with numerate LMs pretrained using recent methods (e.g., PReasM, POET). Our work thus shows how to effectively use decomposition-guided contexts to robustly teach multi-step reasoning.</abstract>
      <url hash="d8a60d7e">2022.emnlp-main.439</url>
      <bibkey>trivedi-etal-2022-teaching</bibkey>
    </paper>
    <paper id="440">
      <title><fixed-case>ADDMU</fixed-case>: Detection of Far-Boundary Adversarial Examples with Data and Model Uncertainty Estimation</title>
      <author><first>Fan</first><last>Yin</last></author>
      <author><first>Yao</first><last>Li</last></author>
      <author><first>Cho-Jui</first><last>Hsieh</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <pages>6567-6584</pages>
      <abstract>Adversarial Examples Detection (AED) is a crucial defense technique against adversarial attacks and has drawn increasing attention from the Natural Language Processing (NLP) community. Despite the surge of new AED methods, our studies show that existing methods heavily rely on a shortcut to achieve good performance. In other words, current search-based adversarial attacks in NLP stop once model predictions change, and thus most adversarial examples generated by those attacks are located near model decision boundaries. To surpass this shortcut and fairly evaluate AED methods, we propose to test AED methods with <b>F</b>ar <b>B</b>oundary (<b>FB</b>) adversarial examples. Existing methods show worse than random guess performance under this scenario. To overcome this limitation, we propose a new technique, <b>ADDMU</b>, <b>a</b>dversary <b>d</b>etection with <b>d</b>ata and <b>m</b>odel <b>u</b>ncertainty, which combines two types of uncertainty estimation for both regular and FB adversarial example detection. Our new method outperforms previous methods by 3.6 and 6.0 <i>AUC</i> points under each scenario. Finally, our analysis shows that the two types of uncertainty provided by <b>ADDMU</b> can be leveraged to characterize adversarialexamples and identify the ones that contribute most to model’s robustness in adversarial training.</abstract>
      <url hash="07019b6b">2022.emnlp-main.440</url>
      <bibkey>yin-etal-2022-addmu</bibkey>
    </paper>
    <paper id="441">
      <title><fixed-case>G</fixed-case>-<fixed-case>MAP</fixed-case>: General Memory-Augmented Pre-trained Language Model for Domain Tasks</title>
      <author><first>Zhongwei</first><last>Wan</last></author>
      <author><first>Yichun</first><last>Yin</last></author>
      <author><first>Wei</first><last>Zhang</last></author>
      <author><first>Jiaxin</first><last>Shi</last></author>
      <author><first>Lifeng</first><last>Shang</last></author>
      <author><first>Guangyong</first><last>Chen</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>6585-6597</pages>
      <abstract>General pre-trained language models (PLMs), such as BERT, have achieved remarkable performance on various NLP tasks. Recently, domain-specific PLMs have been proposed to boost the task performance of specific domains (e.g., biomedical and computer science) by continuing to pre-train general PLMs with domain-specific corpora. However, this domain-adaptive pre-training (DAPT (CITATION)) tends to forget the previous general knowledge acquired by general PLMs, which leads to a <i>catastrophic forgetting</i> phenomenon and sub-optimal performance. To alleviate this problem, we propose a new framework of <b>M</b>emory-<b>A</b>ugmented <b>P</b>re-trained Language Model (<b>MAP</b>), which augments the domain-specific PLM by a memory built from the frozen general PLM without losing the general knowledge. Specifically, we propose a new memory-augmented layer, and based on it, different augmentation strategies are explored to build memory and fusion memory into domain-specific PLM. We demonstrate the effectiveness of MAP on different domains (biomedical and computer science publications, news, and reviews) and different kinds (text classification, QA, NER) of tasks, and the extensive results show that the proposed MAP can achieve SOTA results on these tasks.</abstract>
      <url hash="f44fdfca">2022.emnlp-main.441</url>
      <bibkey>wan-etal-2022-g</bibkey>
    </paper>
    <paper id="442">
      <title>Towards Unifying Reference Expression Generation and Comprehension</title>
      <author><first>Duo</first><last>Zheng</last></author>
      <author><first>Tao</first><last>Kong</last></author>
      <author><first>Ya</first><last>Jing</last></author>
      <author><first>Jiaan</first><last>Wang</last></author>
      <author><first>Xiaojie</first><last>Wang</last></author>
      <pages>6598-6611</pages>
      <abstract>Reference Expression Generation (REG) and Comprehension (REC) are two highly correlated tasks. Modeling REG and REC simultaneously for utilizing the relation between them is a promising way to improve both. However, the problem of distinct inputs, as well as building connections between them in a single model, brings challenges to the design and training of the joint model. To address the problems, we propose a unified model for REG and REC, named UniRef. It unifies these two tasks with the carefully-designed Image-Region-Text Fusion layer (IRTF), which fuses the image, region and text via the image cross-attention and region cross-attention. Additionally, IRTF could generate pseudo input regions for the REC task to enable a uniform way for sharing the identical representation space across the REC and REG. We further propose Vision-conditioned Masked Language Modeling (VMLM) and Text-Conditioned Region Prediction (TRP) to pre-train UniRef model on multi-granular corpora. The VMLM and TRP are directly related to REG and REC, respectively, but could help each other. We conduct extensive experiments on three benchmark datasets, RefCOCO, RefCOCO+ and RefCOCOg. Experimental results show that our model outperforms previous state-of-the-art methods on both REG and REC.</abstract>
      <url hash="c4f1506a">2022.emnlp-main.442</url>
      <bibkey>zheng-etal-2022-towards-unifying</bibkey>
    </paper>
    <paper id="443">
      <title>Textual Manifold-based Defense Against Natural Language Adversarial Examples</title>
      <author><first>Dang</first><last>Nguyen Minh</last></author>
      <author><first>Anh Tuan</first><last>Luu</last></author>
      <pages>6612-6625</pages>
      <abstract>Despite the recent success of large pretrained language models in NLP, they are susceptible to adversarial examples. Concurrently, several studies on adversarial images have observed an intriguing property: the adversarial images tend to leave the low-dimensional natural data manifold. In this study, we find a similar phenomenon occurs in the contextualized embedding space of natural sentences induced by pretrained language models in which textual adversarial examples tend to have their embeddings diverge off the manifold of natural sentence embeddings. Based on this finding, we propose Textual Manifold-based Defense (TMD), a defense mechanism that learns the embedding space manifold of the underlying language model and projects novel inputs back to the approximated structure before classification. Through extensive experiments, we find that our method consistently and significantly outperforms previous defenses under various attack settings while remaining unaffected to the clean accuracy. To the best of our knowledge, this is the first kind of manifold-based defense adapted to the NLP domain.</abstract>
      <url hash="2e9cd763">2022.emnlp-main.443</url>
      <bibkey>nguyen-minh-luu-2022-textual</bibkey>
    </paper>
    <paper id="444">
      <title>Tiny-Attention Adapter: Contexts Are More Important Than the Number of Parameters</title>
      <author><first>Hongyu</first><last>Zhao</last></author>
      <author><first>Hao</first><last>Tan</last></author>
      <author><first>Hongyuan</first><last>Mei</last></author>
      <pages>6626-6638</pages>
      <abstract>Adapter-tuning is a paradigm that transfers a pretrained language model to downstream tasks by adding and tuning a small number of new parameters. Previously proposed adapter architectures are all feed-forward neural networks. In this paper, we investigate the effectiveness of using tiny-attention—i.e., attention with extremely small per-head dimensionality—as adapters. Our tiny-attention adapter learns to modify the hidden states at each position directly conditioned on the hidden states at all the other positions, which is missed by the previously proposed adapters. Moreover, we view its multiple attention heads as a mixture of experts and propose to average their weights during deployment, which further reduces its inference computation cost. On the GLUE benchmark, our tiny-attention adapter outperforms the other parameter-efficient transfer learning methods as well as full fine-tuning while only updating 0.05% of the parameters. On the FewGLUE benchmark, its performance is comparable to that of GPT-3 and PET.</abstract>
      <url hash="18613286">2022.emnlp-main.444</url>
      <bibkey>zhao-etal-2022-tiny</bibkey>
    </paper>
    <paper id="445">
      <title>Reduce Catastrophic Forgetting of Dense Retrieval Training with Teleportation Negatives</title>
      <author><first>Si</first><last>Sun</last></author>
      <author><first>Chenyan</first><last>Xiong</last></author>
      <author><first>Yue</first><last>Yu</last></author>
      <author><first>Arnold</first><last>Overwijk</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Jie</first><last>Bao</last></author>
      <pages>6639-6654</pages>
      <abstract>In this paper, we investigate the instability in the standard dense retrieval training, which iterates between model training and hard negative selection using the being-trained model. We show the catastrophic forgetting phenomena behind the training instability, where models learn and forget different negative groups during training iterations. We then propose ANCE-Tele, which accumulates momentum negatives from past iterations and approximates future iterations using lookahead negatives, as “teleportations” along the time axis to smooth the learning process. On web search and OpenQA, ANCE-Tele outperforms previous state-of-the-art systems of similar size, eliminates the dependency on sparse retrieval negatives, and is competitive among systems using significantly more (50x) parameters. Our analysis demonstrates that teleportation negatives reduce catastrophic forgetting and improve convergence speed for dense retrieval training. The source code of this paper is available at https://github.com/OpenMatch/ANCE-Tele.</abstract>
      <url hash="fb783d86">2022.emnlp-main.445</url>
      <bibkey>sun-etal-2022-reduce</bibkey>
    </paper>
    <paper id="446">
      <title><fixed-case>ATTEMPT</fixed-case>: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts</title>
      <author><first>Akari</first><last>Asai</last></author>
      <author><first>Mohammadreza</first><last>Salehi</last></author>
      <author><first>Matthew</first><last>Peters</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <pages>6655-6672</pages>
      <abstract>This work introduces a new multi-task, parameter-efficient language model (LM) tuning method that learns to transfer knowledge across different tasks via a mixture of soft prompts—small prefix embedding vectors pre-trained for different tasks. Our method, called ATTEMPT (ATTEntional Mixtures of Prompt Tuning), obtains source prompts as encodings of large-scale source tasks into a small number of parameters and trains an attention module to interpolate the source prompts and a newly initialized target prompt for every instance in the target task. During training, only the target task prompt and the attention weights, which are shared between tasks in multi-task training, are updated, while the original LM and source prompts are intact. ATTEMPT is highly parameter-efficient (e.g., updates 2,300 times fewer parameters than full fine-tuning), while it overcomes instability of prompt tuning and achieves high task performance using learned knowledge from high-resource tasks. Moreover, it is modular using pre-trained soft prompts, and can flexibly add or remove source prompts for effective knowledge transfer. Our experimental results across 21 diverse NLP datasets show that ATTEMPT significantly outperforms prompt tuning and outperforms or matches fully fine-tuned or other parameter-efficient tuning approaches that use 10 times more parameters. Finally, ATTEMPT outperforms previous work in few-shot learning settings.</abstract>
      <url hash="da4805ba">2022.emnlp-main.446</url>
      <bibkey>asai-etal-2022-attempt</bibkey>
    </paper>
    <paper id="447">
      <title>Exploration of the Usage of Color Terms by Color-blind Participants in Online Discussion Platforms</title>
      <author><first>Ella</first><last>Rabinovich</last></author>
      <author><first>Boaz</first><last>Carmeli</last></author>
      <pages>6673-6685</pages>
      <abstract>Prominent questions about the role of sensory vs. linguistic input in the way we acquire and use language have been extensively studied in the psycholinguistic literature. However, the relative effect of various factors in a person’s overall experience on their linguistic system remains unclear. We study this question by making a step forward towards a better understanding of the conceptual perception of colors by color-blind individuals, as reflected in their spontaneous linguistic productions. Using a novel and carefully curated dataset, we show that red-green color-blind speakers use the “red” and “green” color terms in less predictable contexts, and in linguistic environments evoking mental image to a lower extent, when compared to their normal-sighted counterparts. These findings shed some new and interesting light on the role of sensory experience on our linguistic system.</abstract>
      <url hash="55546afa">2022.emnlp-main.447</url>
      <bibkey>rabinovich-carmeli-2022-exploration</bibkey>
    </paper>
    <paper id="448">
      <title><fixed-case>DEER</fixed-case>: Descriptive Knowledge Graph for Explaining Entity Relationships</title>
      <author><first>Jie</first><last>Huang</last></author>
      <author><first>Kerui</first><last>Zhu</last></author>
      <author><first>Kevin Chen-Chuan</first><last>Chang</last></author>
      <author><first>Jinjun</first><last>Xiong</last></author>
      <author><first>Wen-mei</first><last>Hwu</last></author>
      <pages>6686-6698</pages>
      <abstract>We propose DEER (Descriptive Knowledge Graph for Explaining Entity Relationships) - an open and informative form of modeling entity relationships. In DEER, relationships between entities are represented by free-text relation descriptions. For instance, the relationship between entities of machine learning and algorithm can be represented as “Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.” To construct DEER, we propose a self-supervised learning method to extract relation descriptions with the analysis of dependency patterns and generate relation descriptions with a transformer-based relation description synthesizing model, where no human labeling is required. Experiments demonstrate that our system can extract and generate high-quality relation descriptions for explaining entity relationships. The results suggest that we can build an open and informative knowledge graph without human annotation.</abstract>
      <url hash="28ea4003">2022.emnlp-main.448</url>
      <bibkey>huang-etal-2022-deer</bibkey>
    </paper>
    <paper id="449">
      <title><fixed-case>META</fixed-case>-<fixed-case>GUI</fixed-case>: Towards Multi-modal Conversational Agents on Mobile <fixed-case>GUI</fixed-case></title>
      <author><first>Liangtai</first><last>Sun</last></author>
      <author><first>Xingyu</first><last>Chen</last></author>
      <author><first>Lu</first><last>Chen</last></author>
      <author><first>Tianle</first><last>Dai</last></author>
      <author><first>Zichen</first><last>Zhu</last></author>
      <author><first>Kai</first><last>Yu</last></author>
      <pages>6699-6712</pages>
      <abstract>Task-oriented dialogue (TOD) systems have been widely used by mobile phone intelligent assistants to accomplish tasks such as calendar scheduling or hotel reservation. Current TOD systems usually focus on multi-turn text/speech interaction, then they would call back-end APIs designed for TODs to perform the task. However, this API-based architecture greatly limits the information-searching capability of intelligent assistants and may even lead to task failure if TOD-specific APIs are not available or the task is too complicated to be executed by the provided APIs. In this paper, we propose a new TOD architecture: GUI-based task-oriented dialogue system (GUI-TOD). A GUI-TOD system can directly perform GUI operations on real APPs and execute tasks without invoking TOD-specific backend APIs. Furthermore, we release META-GUI, a dataset for training a Multi-modal convErsaTional Agent on mobile GUI. We also propose a multi-model action prediction and response model, which show promising results on META-GUI. The dataset, codes and leaderboard are publicly available.</abstract>
      <url hash="2aa9d078">2022.emnlp-main.449</url>
      <bibkey>sun-etal-2022-meta</bibkey>
    </paper>
    <paper id="450">
      <title>Understanding and Improving Knowledge Distillation for Quantization Aware Training of Large Transformer Encoders</title>
      <author><first>Minsoo</first><last>Kim</last></author>
      <author><first>Sihwa</first><last>Lee</last></author>
      <author><first>Suk-Jin</first><last>Hong</last></author>
      <author><first>Du-Seong</first><last>Chang</last></author>
      <author><first>Jungwook</first><last>Choi</last></author>
      <pages>6713-6725</pages>
      <abstract>Knowledge distillation (KD) has been a ubiquitous method for model compression to strengthen the capability of a lightweight model with the transferred knowledge from the teacher. In particular, KD has been employed in quantization-aware training (QAT) of Transformer encoders like BERT to improve the accuracy of the student model with the reduced-precision weight parameters. However, little is understood about which of the various KD approaches best fits the QAT of Transformers. In this work, we provide an in-depth analysis of the mechanism of KD on attention recovery of quantized large Transformers. In particular, we reveal that the previously adopted MSE loss on the attention score is insufficient for recovering the self-attention information. Therefore, we propose two KD methods; attention-map and attention-output losses. Furthermore, we explore the unification of both losses to address task-dependent preference between attention-map and output losses. The experimental results on various Transformer encoder models demonstrate that the proposed KD methods achieve state-of-the-art accuracy for QAT with sub-2-bit weight quantization.</abstract>
      <url hash="846ea1c5">2022.emnlp-main.450</url>
      <bibkey>kim-etal-2022-understanding</bibkey>
    </paper>
    <paper id="451">
      <title>Exploring Mode Connectivity for Pre-trained Language Models</title>
      <author><first>Yujia</first><last>Qin</last></author>
      <author><first>Cheng</first><last>Qian</last></author>
      <author><first>Jing</first><last>Yi</last></author>
      <author><first>Weize</first><last>Chen</last></author>
      <author><first>Yankai</first><last>Lin</last></author>
      <author><first>Xu</first><last>Han</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>6726-6746</pages>
      <abstract>Recent years have witnessed the prevalent application of pre-trained language models (PLMs) in NLP. From the perspective of parameter space, PLMs provide generic initialization, starting from which high-performance minima could be found. Although plenty of works have studied how to effectively and efficiently adapt PLMs to high-performance minima, little is known about the connection of various minima reached under different adaptation configurations. In this paper, we investigate the geometric connections of different minima through the lens of mode connectivity, which measures whether two minima can be connected with a low-loss path. We conduct empirical analyses to investigate three questions: (1) how could hyperparameters, specific tuning methods, and training data affect PLM’s mode connectivity? (2) How does mode connectivity change during pre-training? (3) How does the PLM’s task knowledge change along the path connecting two minima? In general, exploring the mode connectivity of PLMs conduces to understanding the geometric connection of different minima, which may help us fathom the inner workings of PLM downstream adaptation. The codes are publicly available at https://github.com/thunlp/Mode-Connectivity-PLM.</abstract>
      <url hash="8ef82abc">2022.emnlp-main.451</url>
      <bibkey>qin-etal-2022-exploring</bibkey>
    </paper>
    <paper id="452">
      <title>Synergy with Translation Artifacts for Training and Inference in Multilingual Tasks</title>
      <author><first>Jaehoon</first><last>Oh</last></author>
      <author><first>Jongwoo</first><last>Ko</last></author>
      <author><first>Se-Young</first><last>Yun</last></author>
      <pages>6747-6754</pages>
      <abstract>Translation has played a crucial role in improving the performance on multilingual tasks: (1) to generate the target language data from the source language data for training and (2) to generate the source language data from the target language data for inference. However, prior works have not considered the use of both translations simultaneously. This paper shows that combining them can synergize the results on various multilingual sentence classification tasks. We empirically find that translation artifacts stylized by translators are the main factor of the performance gain. Based on this analysis, we adopt two training methods, SupCon and MixUp, considering translation artifacts. Furthermore, we propose a cross-lingual fine-tuning algorithm called MUSC, which uses SupCon and MixUp jointly and improves the performance. Our code is available at https://github.com/jongwooko/MUSC.</abstract>
      <url hash="ac380ec6">2022.emnlp-main.452</url>
      <bibkey>oh-etal-2022-synergy</bibkey>
    </paper>
    <paper id="453">
      <title>Increasing Visual Awareness in Multimodal Neural Machine Translation from an Information Theoretic Perspective</title>
      <author><first>Baijun</first><last>Ji</last></author>
      <author><first>Tong</first><last>Zhang</last></author>
      <author><first>Yicheng</first><last>Zou</last></author>
      <author><first>Bojie</first><last>Hu</last></author>
      <author><first>Si</first><last>Shen</last></author>
      <pages>6755-6764</pages>
      <abstract>Multimodal machine translation (MMT) aims to improve translation quality by equipping the source sentence with its corresponding image. Despite the promising performance, MMT models still suffer the problem of input degradation: models focus more on textual information while visual information is generally overlooked. In this paper, we endeavor to improve MMT performance by increasing visual awareness from an information theoretic perspective. In detail, we decompose the informative visual signals into two parts: source-specific information and target-specific information. We use mutual information to quantify them and propose two methods for objective optimization to better leverage visual signals. Experiments on two datasets demonstrate that our approach can effectively enhance the visual awareness of MMT model and achieve superior results against strong baselines.</abstract>
      <url hash="369b92cb">2022.emnlp-main.453</url>
      <bibkey>ji-etal-2022-increasing</bibkey>
    </paper>
    <paper id="454">
      <title>Improving Event Coreference Resolution Using Document-level and Topic-level Information</title>
      <author><first>Sheng</first><last>Xu</last></author>
      <author><first>Peifeng</first><last>Li</last></author>
      <author><first>Qiaoming</first><last>Zhu</last></author>
      <pages>6765-6775</pages>
      <abstract>Event coreference resolution (ECR) aims to cluster event mentions that refer to the same real-world events. Deep learning methods have achieved SOTA results on the ECR task. However, due to the encoding length limitation, previous methods either adopt classical pairwise models based on sentence-level context or split each document into multiple chunks and encode them separately. They failed to capture the interactions and contextual cues among those long-distance event mentions. Besides, high-level information, such as event topics, is rarely considered to enhance representation learning for ECR. To address the above two issues, we first apply a Longformer-based encoder to obtain the document-level embeddings and an encoder with a trigger-mask mechanism to learn sentence-level embeddings based on local context. In addition, we propose an event topic generator to infer the latent topic-level representations. Finally, using the above event embeddings, we employ a multiple tensor matching method to capture their interactions at the document, sentence, and topic levels. Experimental results on the KBP 2017 dataset show that our model outperforms the SOTA baselines.</abstract>
      <url hash="55e9a583">2022.emnlp-main.454</url>
      <bibkey>xu-etal-2022-improving</bibkey>
    </paper>
    <paper id="455">
      <title>Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding</title>
      <author><first>Rishabh</first><last>Bhardwaj</last></author>
      <author><first>Amrita</first><last>Saha</last></author>
      <author><first>Steven C.H.</first><last>Hoi</last></author>
      <author><first>Soujanya</first><last>Poria</last></author>
      <pages>6776-6791</pages>
      <abstract>Prompt Tuning has been largely successful as a parameter-efficient method of conditioning large-scale pre-trained language models to perform downstream tasks. Thus far, soft prompt tuning learns a fixed set of task-specific continuous vectors, i.e., soft tokens that remain static across the task samples. A fixed prompt, however, may not generalize well to the diverse kinds of inputs the task comprises. In order to address this, we propose Vector-quantized Input-contextualized Prompts (VIP) as an extension to the soft prompt tuning framework. VIP particularly focuses on two aspects—contextual prompts that learns input-specific contextualization of the soft prompt tokens through a small-scale sentence encoder and quantized prompts that maps the contextualized prompts to a set of learnable codebook vectors through a Vector quantization network. On various language understanding tasks like SuperGLUE, QA, Relation classification, NER and NLI, VIP outperforms the soft prompt tuning (PT) baseline by an average margin of 1.19%. Further, our generalization studies show that VIP learns more robust prompt representations, surpassing PT by a margin of 0.6% - 5.3% on Out-of-domain QA and NLI tasks respectively, and by 0.75% on Multi-Task setup over 4 tasks spanning across 12 domains.</abstract>
      <url hash="20af1200">2022.emnlp-main.455</url>
      <bibkey>bhardwaj-etal-2022-vector</bibkey>
    </paper>
    <paper id="456">
      <title>Boosting Natural Language Generation from Instructions with Meta-Learning</title>
      <author><first>Budhaditya</first><last>Deb</last></author>
      <author><first>Ahmed Hassan</first><last>Awadallah</last></author>
      <author><first>Guoqing</first><last>Zheng</last></author>
      <pages>6792-6808</pages>
      <abstract>Recent work has shown that language models (LMs) trained with multi-task <i>instructional learning</i> (MTIL) can solve diverse NLP tasks in zero- and few-shot settings with improved performance compared to prompt tuning. MTIL illustrates that LMs can extract and use information about the task from instructions beyond the surface patterns of the inputs and outputs. This suggests that meta-learning may further enhance the utilization of instructions for effective task transfer. In this paper we investigate whether meta-learning applied to MTIL can further improve generalization to unseen tasks in a zero-shot setting. Specifically, we propose to adapt meta-learning to MTIL in three directions: 1) Model Agnostic Meta Learning (MAML), 2) Hyper-Network (HNet) based adaptation to generate task specific parameters conditioned on instructions, and 3) an approach combining HNet and MAML. Through extensive experiments on the large scale Natural Instructions V2 dataset, we show that our proposed approaches significantly improve over strong baselines in zero-shot settings. In particular, meta-learning improves the effectiveness of instructions and is most impactful when the test tasks are strictly zero-shot (i.e. no similar tasks in the training set) and are “hard” for LMs, illustrating the potential of meta-learning for MTIL for out-of-distribution tasks.</abstract>
      <url hash="d378d9aa">2022.emnlp-main.456</url>
      <bibkey>deb-etal-2022-boosting</bibkey>
    </paper>
    <paper id="457">
      <title>Topical Segmentation of Spoken Narratives: A Test Case on Holocaust Survivor Testimonies</title>
      <author><first>Eitan</first><last>Wagner</last></author>
      <author><first>Renana</first><last>Keydar</last></author>
      <author><first>Amit</first><last>Pinchevski</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <pages>6809-6821</pages>
      <abstract>The task of topical segmentation is well studied, but previous work has mostly addressed it in the context of structured, well-defined segments, such as segmentation into paragraphs, chapters, or segmenting text that originated from multiple sources. We tackle the task of segmenting running (spoken) narratives, which poses hitherto unaddressed challenges. As a test case, we address Holocaust survivor testimonies, given in English. Other than the importance of studying these testimonies for Holocaust research, we argue that they provide an interesting test case for topical segmentation, due to their unstructured surface level, relative abundance (tens of thousands of such testimonies were collected), and the relatively confined domain that they cover. We hypothesize that boundary points between segments correspond to low mutual information between the sentences proceeding and following the boundary. Based on this hypothesis, we explore a range of algorithmic approaches to the task, building on previous work on segmentation that uses generative Bayesian modeling and state-of-the-art neural machinery. Compared to manually annotated references, we find that the developed approaches show considerable improvements over previous work.</abstract>
      <url hash="235e15e4">2022.emnlp-main.457</url>
      <bibkey>wagner-etal-2022-topical</bibkey>
    </paper>
    <paper id="458">
      <title>Unifying the Convergences in Multilingual Neural Machine Translation</title>
      <author><first>Yichong</first><last>Huang</last></author>
      <author><first>Xiaocheng</first><last>Feng</last></author>
      <author><first>Xinwei</first><last>Geng</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <pages>6822-6835</pages>
      <abstract>Although all-in-one-model multilingual neural machine translation (MNMT) has achieved remarkable progress, the convergence inconsistency in the joint training is ignored, i.e., different language pairs reaching convergence in different epochs. This leads to the trained MNMT model over-fitting low-resource language translations while under-fitting high-resource ones. In this paper, we propose a novel training strategy named LSSD (LanguageSpecific Self-Distillation), which can alleviate the convergence inconsistency and help MNMT models achieve the best performance on each language pair simultaneously. Specifically, LSSD picks up language-specific best checkpoints for each language pair to teach the current model on the fly. Furthermore, we systematically explore three sample-level manipulations of knowledge transferring. Experimental results on three datasets show that LSSD obtains consistent improvements towards all language pairs and achieves the state-of-the-art.</abstract>
      <url hash="4f741794">2022.emnlp-main.458</url>
      <bibkey>huang-etal-2022-unifying</bibkey>
    </paper>
    <paper id="459">
      <title>Modeling Label Correlations for Ultra-Fine Entity Typing with Neural Pairwise Conditional Random Field</title>
      <author><first>Chengyue</first><last>Jiang</last></author>
      <author><first>Yong</first><last>Jiang</last></author>
      <author><first>Weiqi</first><last>Wu</last></author>
      <author><first>Pengjun</first><last>Xie</last></author>
      <author><first>Kewei</first><last>Tu</last></author>
      <pages>6836-6847</pages>
      <abstract>Ultra-fine entity typing (UFET) aims to predict a wide range of type phrases that correctly describe the categories of a given entity mention in a sentence. Most recent works infer each entity type independently, ignoring the correlations between types, e.g., when an entity is inferred as a <i>president</i>, it should also be a <i>politician</i> and a <i>leader</i>. To this end, we use an undirected graphical model called pairwise conditional random field (PCRF) to formulate the UFET problem, in which the type variables are not only unarily influenced by the input but also pairwisely relate to all the other type variables. We use various modern backbones for entity typing to compute unary potentials, and derive pairwise potentials from type phrase representations that both capture prior semantic information and facilitate accelerated inference. We use mean-field variational inference for efficient type inference on very large type sets and unfold it as a neural network module to enable end-to-end training. Experiments on UFET show that the Neural-PCRF consistently outperforms its backbones with little cost and results in a competitive performance against cross-encoder based SOTA while being <i>thousands of times</i> faster. We also find Neural-PCRF effective on a widely used fine-grained entity typing dataset with a smaller type set. We pack Neural-PCRF as a network module that can be plugged onto multi-label type classifiers with ease and release it in .</abstract>
      <url hash="da073084">2022.emnlp-main.459</url>
      <bibkey>jiang-etal-2022-modeling</bibkey>
    </paper>
    <paper id="460">
      <title>Help me write a Poem - Instruction Tuning as a Vehicle for Collaborative Poetry Writing</title>
      <author><first>Tuhin</first><last>Chakrabarty</last></author>
      <author><first>Vishakh</first><last>Padmakumar</last></author>
      <author><first>He</first><last>He</last></author>
      <pages>6848-6863</pages>
      <abstract>Recent work in training large language models (LLMs) to follow natural language instructions has opened up exciting opportunities for natural language interface design. Building on the prior success of large language models in the realm of computer assisted creativity, in this work, we present <i>CoPoet</i>, a collaborative poetry writing system, with the goal of to study if LLM’s actually improve the quality of the generated content. In contrast to auto-completing a user’s text, CoPoet is controlled by user instructions that specify the attributes of the desired text, such as <i>Write a sentence about ‘love’</i> or <i>Write a sentence ending in ‘fly’</i>. The core component of our system is a language model fine-tuned on a diverse collection of instructions for poetry writing. Our model is not only competitive to publicly available LLMs trained on instructions (InstructGPT), but also capable of satisfying unseen compositional instructions. A study with 15 qualified crowdworkers shows that users successfully write poems with CoPoet on diverse topics ranging from <i>Monarchy</i> to <i>Climate change</i>, which are preferred by third-party evaluators over poems written without the system.</abstract>
      <url hash="3d20e414">2022.emnlp-main.460</url>
      <bibkey>chakrabarty-etal-2022-help</bibkey>
    </paper>
    <paper id="461">
      <title>Open Relation and Event Type Discovery with Type Abstraction</title>
      <author><first>Sha</first><last>Li</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>6864-6877</pages>
      <abstract>Conventional “closed-world” information extraction (IE) approaches rely on human ontologies to define the scope for extraction. As a result, such approaches fall short when applied to new domains. This calls for systems that can automatically infer new types from given corpora, a task which we refer to as type discovery.To tackle this problem, we introduce the idea of type abstraction, where the model is prompted to generalize and name the type. Then we use the similarity between inferred names to induce clusters. Observing that this abstraction-based representation is often complementary to the entity/trigger token representation, we set up these two representations as two views and design our model as a co-training framework. Our experiments on multiple relation extraction and event extraction datasets consistently show the advantage of our type abstraction approach.</abstract>
      <url hash="59f6ba1c">2022.emnlp-main.461</url>
      <bibkey>li-etal-2022-open</bibkey>
    </paper>
    <paper id="462">
      <title>Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples</title>
      <author><first>Linlin</first><last>Liu</last></author>
      <author><first>Xin</first><last>Li</last></author>
      <author><first>Ruidan</first><last>He</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Shafiq</first><last>Joty</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <pages>6878-6890</pages>
      <abstract>Knowledge-enhanced language representation learning has shown promising results across various knowledge-intensive NLP tasks. However, prior methods are limited in efficient utilization of multilingual knowledge graph (KG) data for language model (LM) pretraining. They often train LMs with KGs in indirect ways, relying on extra entity/relation embeddings to facilitate knowledge injection. In this work, we explore methods to make better use of the multilingual annotation and language agnostic property of KG triples, and present novel knowledge based multilingual language models (KMLMs) trained directly on the knowledge triples. We first generate a large amount of multilingual synthetic sentences using the Wikidata KG triples. Then based on the intra- and inter-sentence structures of the generated data, we design pretraining tasks to enable the LMs to not only memorize the factual knowledge but also learn useful logical patterns. Our pretrained KMLMs demonstrate significant performance improvements on a wide range of knowledge-intensive cross-lingual tasks, including named entity recognition (NER), factual knowledge retrieval, relation classification, and a newly designed logical reasoning task.</abstract>
      <url hash="2e7bf95f">2022.emnlp-main.462</url>
      <bibkey>liu-etal-2022-enhancing-multilingual</bibkey>
    </paper>
    <paper id="463">
      <title>Revisiting Grammatical Error Correction Evaluation and Beyond</title>
      <author><first>Peiyuan</first><last>Gong</last></author>
      <author><first>Xuebo</first><last>Liu</last></author>
      <author><first>Heyan</first><last>Huang</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>6891-6902</pages>
      <abstract>Pretraining-based (PT-based) automatic evaluation metrics (e.g., BERTScore and BARTScore) have been widely used in several sentence generation tasks (e.g., machine translation and text summarization) due to their better correlation with human judgments over traditional overlap-based methods. Although PT-based methods have become the de facto standard for training grammatical error correction (GEC) systems, GEC evaluation still does not benefit from pretrained knowledge. This paper takes the first step towards understanding and improving GEC evaluation with pretraining. We first find that arbitrarily applying PT-based metrics to GEC evaluation brings unsatisfactory correlation results because of the excessive attention to inessential systems outputs (e.g., unchanged parts). To alleviate the limitation, we propose a novel GEC evaluation metric to achieve the best of both worlds, namely PT-M2 which only uses PT-based metrics to score those corrected parts. Experimental results on the CoNLL14 evaluation task show that PT-M2 significantly outperforms existing methods, achieving a new state-of-the-art result of 0.949 Pearson correlation. Further analysis reveals that PT-M2 is robust to evaluate competitive GEC systems. Source code and scripts are freely available at https://github.com/pygongnlp/PT-M2.</abstract>
      <url hash="5e03ef79">2022.emnlp-main.463</url>
      <bibkey>gong-etal-2022-revisiting</bibkey>
    </paper>
    <paper id="464">
      <title><fixed-case>R</fixed-case>2<fixed-case>D</fixed-case>2: Robust Data-to-Text with Replacement Detection</title>
      <author><first>Linyong</first><last>Nan</last></author>
      <author><first>Lorenzo Jaime</first><last>Flores</last></author>
      <author><first>Yilun</first><last>Zhao</last></author>
      <author><first>Yixin</first><last>Liu</last></author>
      <author><first>Luke</first><last>Benson</last></author>
      <author><first>Weijin</first><last>Zou</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <pages>6903-6917</pages>
      <abstract>Unfaithful text generation is a common problem for text generation systems. In the case of Data-to-Text (D2T) systems, the factuality of the generated text is particularly crucial for any real-world applications. We introduce R2D2, a training framework that addresses unfaithful Data-to-Text generation by training a system both as a generator and a faithfulness discriminator with additional replacement detection and unlikelihood learning tasks. To facilitate such training, we propose two methods for sampling unfaithful sentences. We argue that the poor entity retrieval capability of D2T systems is one of the primary sources of unfaithfulness, so in addition to the existing metrics, we further propose named entity based metrics to evaluate the fidelity of D2T generations. Our experimental results show that R2D2 systems could effectively mitigate the unfaithful text generation, and they achieve new state-of-theart results on FeTaQA, LogicNLG, and ToTTo, all with significant improvements.</abstract>
      <url hash="0fea5629">2022.emnlp-main.464</url>
      <bibkey>nan-etal-2022-r2d2</bibkey>
    </paper>
    <paper id="465">
      <title><fixed-case>IDK</fixed-case>-<fixed-case>MRC</fixed-case>: Unanswerable Questions for <fixed-case>I</fixed-case>ndonesian Machine Reading Comprehension</title>
      <author><first>Rifki Afina</first><last>Putri</last></author>
      <author><first>Alice</first><last>Oh</last></author>
      <pages>6918-6933</pages>
      <abstract>Machine Reading Comprehension (MRC) has become one of the essential tasks in Natural Language Understanding (NLU) as it is often included in several NLU benchmarks (Liang et al., 2020; Wilie et al., 2020). However, most MRC datasets only have answerable question type, overlooking the importance of unanswerable questions. MRC models trained only on answerable questions will select the span that is most likely to be the answer, even when the answer does not actually exist in the given passage (Rajpurkar et al., 2018). This problem especially remains in medium- to low-resource languages like Indonesian. Existing Indonesian MRC datasets (Purwarianti et al., 2007; Clark et al., 2020) are still inadequate because of the small size and limited question types, i.e., they only cover answerable questions. To fill this gap, we build a new Indonesian MRC dataset called I(n)don’tKnow- MRC (IDK-MRC) by combining the automatic and manual unanswerable question generation to minimize the cost of manual dataset construction while maintaining the dataset quality. Combined with the existing answerable questions, IDK-MRC consists of more than 10K questions in total. Our analysis shows that our dataset significantly improves the performance of Indonesian MRC models, showing a large improvement for unanswerable questions.</abstract>
      <url hash="dcd2fd5e">2022.emnlp-main.465</url>
      <bibkey>putri-oh-2022-idk</bibkey>
    </paper>
    <paper id="466">
      <title><fixed-case>XLM</fixed-case>-<fixed-case>D</fixed-case>: Decorate Cross-lingual Pre-training Model as Non-Autoregressive Neural Machine Translation</title>
      <author><first>Yong</first><last>Wang</last></author>
      <author><first>Shilin</first><last>He</last></author>
      <author><first>Guanhua</first><last>Chen</last></author>
      <author><first>Yun</first><last>Chen</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <pages>6934-6946</pages>
      <abstract>Pre-training language models have achieved thriving success in numerous natural language understanding and autoregressive generation tasks, but non-autoregressive generation in applications such as machine translation has not sufficiently benefited from the pre-training paradigm. In this work, we establish the connection between a pre-trained masked language model (MLM) and non-autoregressive generation on machine translation. From this perspective, we present XLM-D, which seamlessly transforms an off-the-shelf cross-lingual pre-training model into a non-autoregressive translation (NAT) model with a lightweight yet effective decorator. Specifically, the decorator ensures the representation consistency of the pre-trained model and brings only one additional trainable parameter. Extensive experiments on typical translation datasets show that our models obtain state-of-the-art performance while realizing the inference speed-up by 19.9x. One striking result is that on WMT14 En-De, our XLM-D obtains 29.80 BLEU points with multiple iterations, which outperforms the previous mask-predict model by 2.77 points.</abstract>
      <url hash="cb12cafb">2022.emnlp-main.466</url>
      <bibkey>wang-etal-2022-xlm</bibkey>
    </paper>
    <paper id="467">
      <title>Cross-stitching Text and Knowledge Graph Encoders for Distantly Supervised Relation Extraction</title>
      <author><first>Qin</first><last>Dai</last></author>
      <author><first>Benjamin</first><last>Heinzerling</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>6947-6958</pages>
      <abstract>Bi-encoder architectures for distantly-supervised relation extraction are designed to make use of the complementary information found in text and knowledge graphs (KG).However, current architectures suffer from two drawbacks. They either do not allow any sharing between the text encoder and the KG encoder at all, or, in case of models with KG-to-text attention, only share information in one direction. Here, we introduce cross-stitch bi-encoders, which allow full interaction between the text encoder and the KG encoder via a cross-stitch mechanism. The cross-stitch mechanism allows sharing and updating representations between the two encoders at any layer, with the amount of sharing being dynamically controlled via cross-attention-based gates. Experimental results on two relation extraction benchmarks from two different domains show that enabling full interaction between the two encoders yields strong improvements.</abstract>
      <url hash="e3f82512">2022.emnlp-main.467</url>
      <bibkey>dai-etal-2022-cross</bibkey>
    </paper>
    <paper id="468">
      <title>Assist Non-native Viewers: Multimodal Cross-Lingual Summarization for How2 Videos</title>
      <author><first>Nayu</first><last>Liu</last></author>
      <author><first>Kaiwen</first><last>Wei</last></author>
      <author><first>Xian</first><last>Sun</last></author>
      <author><first>Hongfeng</first><last>Yu</last></author>
      <author><first>Fanglong</first><last>Yao</last></author>
      <author><first>Li</first><last>Jin</last></author>
      <author><first>Guo</first><last>Zhi</last></author>
      <author><first>Guangluan</first><last>Xu</last></author>
      <pages>6959-6969</pages>
      <abstract>Multimodal summarization for videos aims to generate summaries from multi-source information (videos, audio transcripts), which has achieved promising progress. However, existing works are restricted to monolingual video scenarios, ignoring the demands of non-native video viewers to understand the cross-language videos in practical applications. It stimulates us to propose a new task, named Multimodal Cross-Lingual Summarization for videos (MCLS), which aims to generate cross-lingual summaries from multimodal inputs of videos. First, to make it applicable to MCLS scenarios, we conduct a Video-guided Dual Fusion network (VDF) that integrates multimodal and cross-lingual information via diverse fusion strategies at both encoder and decoder. Moreover, to alleviate the problem of high annotation costs and limited resources in MCLS, we propose a triple-stage training framework to assist MCLS by transferring the knowledge from monolingual multimodal summarization data, which includes: 1) multimodal summarization on sufficient prevalent language videos with a VDF model; 2) knowledge distillation (KD) guided adjustment on bilingual transcripts; 3) multimodal summarization for cross-lingual videos with a KD induced VDF model. Experiment results on the reorganized How2 dataset show that the VDF model alone outperforms previous methods for multimodal summarization, and the performance further improves by a large margin via the proposed triple-stage training framework.</abstract>
      <url hash="82e4d0cd">2022.emnlp-main.468</url>
      <bibkey>liu-etal-2022-assist</bibkey>
    </paper>
    <paper id="469">
      <title><fixed-case>PACIFIC</fixed-case>: Towards Proactive Conversational Question Answering over Tabular and Textual Data in Finance</title>
      <author><first>Yang</first><last>Deng</last></author>
      <author><first>Wenqiang</first><last>Lei</last></author>
      <author><first>Wenxuan</first><last>Zhang</last></author>
      <author><first>Wai</first><last>Lam</last></author>
      <author><first>Tat-Seng</first><last>Chua</last></author>
      <pages>6970-6984</pages>
      <abstract>To facilitate conversational question answering (CQA) over hybrid contexts in finance, we present a new dataset, named PACIFIC. Compared with existing CQA datasets, PACIFIC exhibits three key features: (i) proactivity, (ii) numerical reasoning, and (iii) hybrid context of tables and text. A new task is defined accordingly to study Proactive Conversational Question Answering (PCQA), which combines clarification question generation and CQA. In addition, we propose a novel method, namely UniPCQA, to adapt a hybrid format of input and output content in PCQA into the Seq2Seq problem, including the reformulation of the numerical reasoning process as code generation. UniPCQA performs multi-task learning over all sub-tasks in PCQA and incorporates a simple ensemble strategy to alleviate the error propagation issue in the multi-task learning by cross-validating top-<tex-math>k</tex-math> sampled Seq2Seq outputs. We benchmark the PACIFIC dataset with extensive baselines and provide comprehensive evaluations on each sub-task of PCQA.</abstract>
      <url hash="c2d65476">2022.emnlp-main.469</url>
      <bibkey>deng-etal-2022-pacific</bibkey>
    </paper>
    <paper id="470">
      <title>Generative Data Augmentation with Contrastive Learning for Zero-Shot Stance Detection</title>
      <author><first>Yang</first><last>Li</last></author>
      <author><first>Jiawei</first><last>Yuan</last></author>
      <pages>6985-6995</pages>
      <abstract>Stance detection aims to identify whether the author of an opinionated text is in favor of, against, or neutral towards a given target. Remarkable success has been achieved when sufficient labeled training data is available. However, it is labor-intensive to annotate sufficient data and train the model for every new target.Therefore, zero-shot stance detection, aiming at identifying stances of unseen targets with seen targets, has gradually attracted attention. Among them, one of the important challenges is to reduce the domain transfer between seen and unseen targets. To tackle this problem, we propose a generative data augmentation approach to generate training samples containing targets and stances for testing data, and map the real samples and generated synthetic samples into the same embedding space with contrastive learning, then perform the final classification based on the augmented data. We evaluate our proposed model on two benchmark datasets. Experimental results show that our approach achieves state-of-the-art performance on most topics in the task of zero-shot stance detection.</abstract>
      <url hash="f3ac580f">2022.emnlp-main.470</url>
      <bibkey>li-yuan-2022-generative</bibkey>
    </paper>
    <paper id="471">
      <title>Better Few-Shot Relation Extraction with Label Prompt Dropout</title>
      <author><first>Peiyuan</first><last>Zhang</last></author>
      <author><first>Wei</first><last>Lu</last></author>
      <pages>6996-7006</pages>
      <abstract>Few-shot relation extraction aims to learn to identify the relation between two entities based on very limited training examples. Recent efforts found that textual labels (i.e., relation names and relation descriptions) could be extremely useful for learning class representations, which will benefit the few-shot learning task. However, what is the best way to leverage such label information in the learning process is an important research question. Existing works largely assume such textual labels are always present during both learning and prediction. In this work, we argue that such approaches may not always lead to optimal results. Instead, we present a novel approach called label prompt dropout, which randomly removes label descriptions in the learning process. Our experiments show that our approach is able to lead to improved class representations, yielding significantly better results on the few-shot relation extraction task.</abstract>
      <url hash="ba4f15a2">2022.emnlp-main.471</url>
      <bibkey>zhang-lu-2022-better</bibkey>
    </paper>
    <paper id="472">
      <title>Break it Down into <fixed-case>BTS</fixed-case>: Basic, Tiniest Subword Units for <fixed-case>K</fixed-case>orean</title>
      <author><first>Nayeon</first><last>Kim</last></author>
      <author><first>Jun-Hyung</first><last>Park</last></author>
      <author><first>Joon-Young</first><last>Choi</last></author>
      <author><first>Eojin</first><last>Jeon</last></author>
      <author><first>Youjin</first><last>Kang</last></author>
      <author><first>SangKeun</first><last>Lee</last></author>
      <pages>7007-7024</pages>
      <abstract>We introduce Basic, Tiniest Subword (BTS) units for the Korean language, which are inspired by the invention principle of Hangeul, the Korean writing system. Instead of relying on 51 Korean consonant and vowel letters, we form the letters from BTS units by adding strokes or combining them. To examine the impact of BTS units on Korean language processing, we develop a novel BTS-based word embedding framework that is readily applicable to various models. Our experiments reveal that BTS units significantly improve the performance of Korean word embedding on all intrinsic and extrinsic tasks in our evaluation. In particular, BTS-based word embedding outperforms the state-of-theart Korean word embedding by 11.8% in word analogy. We further investigate the unique advantages provided by BTS units through indepth analysis.</abstract>
      <url hash="c7bc883d">2022.emnlp-main.472</url>
      <bibkey>kim-etal-2022-break</bibkey>
    </paper>
    <paper id="473">
      <title>The Devil in Linear Transformer</title>
      <author><first>Zhen</first><last>Qin</last></author>
      <author><first>Xiaodong</first><last>Han</last></author>
      <author><first>Weixuan</first><last>Sun</last></author>
      <author><first>Dongxu</first><last>Li</last></author>
      <author><first>Lingpeng</first><last>Kong</last></author>
      <author><first>Nick</first><last>Barnes</last></author>
      <author><first>Yiran</first><last>Zhong</last></author>
      <pages>7025-7041</pages>
      <abstract>Linear transformers aim to reduce the quadratic space-time complexity of vanilla transformers. However, they usually suffer from degraded performances on various tasks and corpus. In this paper, we examine existing kernel-based linear transformers and identify two key issues that lead to such performance gaps: 1) unbounded gradients in the attention computation adversely impact the convergence of linear transformer models; 2) attention dilution which trivially distributes attention scores over long sequences while neglecting neighbouring structures. To address these issues, we first identify that the scaling of attention matrices is the devil in unbounded gradients, which turns out unnecessary in linear attention as we show theoretically and empirically. To this end, we propose a new linear attention that replaces the scaling operation with a normalization to stabilize gradients. For the issue of attention dilution, we leverage a diagonal attention to confine attention to only neighbouring tokens in early layers. Benefiting from the stable gradients and improved attention, our new linear transformer model, transNormer, demonstrates superior performance on text classification and language modeling tasks, as well as on the challenging Long-Range Arena benchmark, surpassing vanilla transformer and existing linear variants by a clear margin while being significantly more space-time efficient. The code is available at https://github.com/OpenNLPLab/Transnormer .</abstract>
      <url hash="aa67d158">2022.emnlp-main.473</url>
      <bibkey>qin-etal-2022-devil</bibkey>
    </paper>
    <paper id="474">
      <title>Zero-Shot Learners for Natural Language Understanding via a Unified Multiple Choice Perspective</title>
      <author><first>Ping</first><last>Yang</last></author>
      <author><first>Junjie</first><last>Wang</last></author>
      <author><first>Ruyi</first><last>Gan</last></author>
      <author><first>Xinyu</first><last>Zhu</last></author>
      <author><first>Lin</first><last>Zhang</last></author>
      <author><first>Ziwei</first><last>Wu</last></author>
      <author><first>Xinyu</first><last>Gao</last></author>
      <author><first>Jiaxing</first><last>Zhang</last></author>
      <author><first>Tetsuya</first><last>Sakai</last></author>
      <pages>7042-7055</pages>
      <abstract>We propose a new paradigm for zero-shot learners that is format agnostic, i.e., it is compatible with any format and applicable to a list of language tasks, such as text classification, commonsense reasoning, coreference resolution, and sentiment analysis. Zero-shot learning aims to train a model on a given task such that it can address new learning tasks without any additional training. Our approach converts zero-shot learning into multiple-choice tasks, avoiding problems in commonly used large-scale generative models such as FLAN. It not only adds generalization ability to models but also significantly reduces the number of parameters. Our method shares the merits of efficient training and deployment. Our approach shows state-of-the-art performance on several benchmarks and produces satisfactory results on tasks such as natural language inference and text classification. Our model achieves this success with only 235M parameters, which is substantially smaller than state-of-the-art models with billions of parameters. The code and pre-trained models are available at https://github.com/IDEA-CCNL/Fengshenbang-LM/tree/main/fengshen/examples/unimc .</abstract>
      <url hash="e72eed5a">2022.emnlp-main.474</url>
      <bibkey>yang-etal-2022-zero</bibkey>
    </paper>
    <paper id="475">
      <title>Hypoformer: Hybrid Decomposition Transformer for Edge-friendly Neural Machine Translation</title>
      <author><first>Sunzhu</first><last>Li</last></author>
      <author><first>Peng</first><last>Zhang</last></author>
      <author><first>Guobing</first><last>Gan</last></author>
      <author><first>Xiuqing</first><last>Lv</last></author>
      <author><first>Benyou</first><last>Wang</last></author>
      <author><first>Junqiu</first><last>Wei</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <pages>7056-7068</pages>
      <abstract>Transformer has been demonstrated effective in Neural Machine Translation (NMT). However, it is memory-consuming and time-consuming in edge devices, resulting in some difficulties for real-time feedback. To compress and accelerate Transformer, we propose a Hybrid Tensor-Train (HTT) decomposition, which retains full rank and meanwhile reduces operations and parameters. A Transformer using HTT, named Hypoformer, consistently and notably outperforms the recent light-weight SOTA methods on three standard translation tasks under different parameter and speed scales. In extreme low resource scenarios, Hypoformer has 7.1 points absolute improvement in BLEU and 1.27 X speedup than vanilla Transformer on IWSLT’14 De-En task.</abstract>
      <url hash="04cf4c01">2022.emnlp-main.475</url>
      <bibkey>li-etal-2022-hypoformer</bibkey>
    </paper>
    <paper id="476">
      <title><fixed-case>F</fixed-case>ig<fixed-case>M</fixed-case>emes: A Dataset for Figurative Language Identification in Politically-Opinionated Memes</title>
      <author><first>Chen</first><last>Liu</last></author>
      <author><first>Gregor</first><last>Geigle</last></author>
      <author><first>Robin</first><last>Krebs</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>7069-7086</pages>
      <abstract>Real-world politically-opinionated memes often rely on figurative language to cloak propaganda and radical ideas to help them spread. It is not only a scientific challenge to develop machine learning models to recognize them in memes, but also sociologically beneficial to understand hidden meanings at scale and raise awareness. These memes are fast-evolving (in both topics and visuals) and it remains unclear whether current multimodal machine learning models are robust to such distribution shifts. To enable future research into this area, we first present FigMemes, a dataset for figurative language classification in politically-opinionated memes. We evaluate the performance of state-of-the-art unimodal and multimodal models and provide comprehensive benchmark results. The key contributions of this proposed dataset include annotations of six commonly used types of figurative language in politically-opinionated memes, and a wide range of topics and visual styles.We also provide analyses on the ability of multimodal models to generalize across distribution shifts in memes. Our dataset poses unique machine learning challenges and our results show that current models have significant room for improvement in both performance and robustness to distribution shifts.</abstract>
      <url hash="89b25820">2022.emnlp-main.476</url>
      <bibkey>liu-etal-2022-figmemes</bibkey>
    </paper>
    <paper id="477">
      <title><fixed-case>U</fixed-case>ni<fixed-case>R</fixed-case>el: Unified Representation and Interaction for Joint Relational Triple Extraction</title>
      <author><first>Wei</first><last>Tang</last></author>
      <author><first>Benfeng</first><last>Xu</last></author>
      <author><first>Yuyue</first><last>Zhao</last></author>
      <author><first>Zhendong</first><last>Mao</last></author>
      <author><first>Yifeng</first><last>Liu</last></author>
      <author><first>Yong</first><last>Liao</last></author>
      <author><first>Haiyong</first><last>Xie</last></author>
      <pages>7087-7099</pages>
      <abstract>Relational triple extraction is challenging for its difficulty in capturing rich correlations between entities and relations. Existing works suffer from 1) heterogeneous representations of entities and relations, and 2) heterogeneous modeling of entity-entity interactions and entity-relation interactions. Therefore, the rich correlations are not fully exploited by existing works. In this paper, we propose UniRel to address these challenges. Specifically, we unify the representations of entities and relations by jointly encoding them within a concatenated natural language sequence, and unify the modeling of interactions with a proposed Interaction Map, which is built upon the off-the-shelf self-attention mechanism within any Transformer block. With comprehensive experiments on two popular relational triple extraction datasets, we demonstrate that UniRel is more effective and computationally efficient. The source code is available at https://github.com/wtangdev/UniRel.</abstract>
      <url hash="03464264">2022.emnlp-main.477</url>
      <bibkey>tang-etal-2022-unirel</bibkey>
    </paper>
    <paper id="478">
      <title><fixed-case>X</fixed-case>-<fixed-case>FACTOR</fixed-case>: A Cross-metric Evaluation of Factual Correctness in Abstractive Summarization</title>
      <author><first>Subhajit</first><last>Chaudhury</last></author>
      <author><first>Sarathkrishna</first><last>Swaminathan</last></author>
      <author><first>Chulaka</first><last>Gunasekara</last></author>
      <author><first>Maxwell</first><last>Crouse</last></author>
      <author><first>Srinivas</first><last>Ravishankar</last></author>
      <author><first>Daiki</first><last>Kimura</last></author>
      <author><first>Keerthiram</first><last>Murugesan</last></author>
      <author><first>Ramón</first><last>Fernandez Astudillo</last></author>
      <author><first>Tahira</first><last>Naseem</last></author>
      <author><first>Pavan</first><last>Kapanipathi</last></author>
      <author><first>Alexander</first><last>Gray</last></author>
      <pages>7100-7110</pages>
      <abstract>Abstractive summarization models often produce factually inconsistent summaries that are not supported by the original article. Recently, a number of fact-consistent evaluation techniques have been proposed to address this issue; however, a detailed analysis of how these metrics agree with one another has yet to be conducted. In this paper, we present X-FACTOR, a cross-evaluation of three high-performing fact-aware abstractive summarization methods. First, we show that summarization models are often fine-tuned on datasets that contain factually inconsistent summaries and propose a fact-aware filtering mechanism that improves the quality of training data and, consequently, the factuality of these models. Second, we propose a corrector module that can be used to improve the factual consistency of generated summaries. Third, we present a re-ranking technique that samples summary instances from the output distribution of a summarization model and re-ranks the sampled instances based on their factuality. Finally, we provide a detailed cross-metric agreement analysis that shows how tuning a model to output summaries based on a particular factuality metric influences factuality as determined by the other metrics. Our goal in this work is to facilitate research that improves the factuality and faithfulness of abstractive summarization models.</abstract>
      <url hash="5ce9cfdc">2022.emnlp-main.478</url>
      <bibkey>chaudhury-etal-2022-x</bibkey>
    </paper>
    <paper id="479">
      <title><fixed-case>P</fixed-case>ara<fixed-case>T</fixed-case>ag: A Dataset of Paraphrase Tagging for Fine-Grained Labels, <fixed-case>NLG</fixed-case> Evaluation, and Data Augmentation</title>
      <author><first>Shuohang</first><last>Wang</last></author>
      <author><first>Ruochen</first><last>Xu</last></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Michael</first><last>Zeng</last></author>
      <pages>7111-7122</pages>
      <abstract>Paraphrase identification has been formulated as a binary classification task to decide whether two sentences hold a paraphrase relationship. Existing paraphrase datasets only annotate a binary label for each sentence pair. However, after a systematical analysis of existing paraphrase datasets, we found that the degree of paraphrase cannot be well characterized by a single binary label. And the criteria of paraphrase are not even consistent within the same dataset. We hypothesize that such issues would limit the effectiveness of paraphrase models trained on these data. To this end, we propose a novel fine-grained paraphrase annotation schema that labels the minimum spans of tokens in a sentence that don’t have the corresponding paraphrases in the other sentence. Under this setting, we frame paraphrasing as a sequence tagging task. We collect 30k sentence pairs in English with the new annotation schema, resulting in the ParaTag dataset. In addition to reporting baseline results on ParaTag using state-of-art language models, we show that ParaTag is especially useful for training an automatic scorer for language generation evaluation. Finally, we train a paraphrase generation model from ParaTag and achieve better data augmentation performance on the GLUE benchmark than other public paraphrasing datasets.</abstract>
      <url hash="e1be07cc">2022.emnlp-main.479</url>
      <bibkey>wang-etal-2022-paratag</bibkey>
    </paper>
    <paper id="480">
      <title>Factual Accuracy is not Enough: Planning Consistent Description Order for Radiology Report Generation</title>
      <author><first>Toru</first><last>Nishino</last></author>
      <author><first>Yasuhide</first><last>Miura</last></author>
      <author><first>Tomoki</first><last>Taniguchi</last></author>
      <author><first>Tomoko</first><last>Ohkuma</last></author>
      <author><first>Yuki</first><last>Suzuki</last></author>
      <author><first>Shoji</first><last>Kido</last></author>
      <author><first>Noriyuki</first><last>Tomiyama</last></author>
      <pages>7123-7138</pages>
      <abstract>Radiology report generation systems have the potential to reduce the workload of radiologists by automatically describing the findings in medical images.To broaden the application of the report generation system, the system should generate reports that are not only factually accurate but also chronologically consistent, describing images that are presented in time order, that is, the correct order.We employ a planning-based radiology report generation system that generates the overall structure of reports as “plans’” prior to generating reports that are accurate and consistent in order.Additionally, we propose a novel reinforcement learning and inference method, Coordinated Planning (CoPlan), that includes a content planner and a text generator to train and infer in a coordinated manner to alleviate the cascading of errors that are often inherent in planning-based models.We conducted experiments with single-phase diagnostic reports in which the factual accuracy is critical and multi-phase diagnostic reports in which the description order is critical.Our proposed CoPlan improves the content order score by 5.1 pt in time series critical scenarios and the clinical factual accuracy F-score by 9.1 pt in time series irrelevant scenarios, compared those of the baseline models without CoPlan.</abstract>
      <url hash="25a4a144">2022.emnlp-main.480</url>
      <bibkey>nishino-etal-2022-factual</bibkey>
    </paper>
    <paper id="481">
      <title><fixed-case>FLUTE</fixed-case>: Figurative Language Understanding through Textual Explanations</title>
      <author><first>Tuhin</first><last>Chakrabarty</last></author>
      <author><first>Arkadiy</first><last>Saakyan</last></author>
      <author><first>Debanjan</first><last>Ghosh</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <pages>7139-7159</pages>
      <abstract>Figurative language understanding has been recently framed as a recognizing textual entailment (RTE) task (a.k.a. natural language inference (NLI)). However, similar to classical RTE/NLI datasets they suffer from spurious correlations and annotation artifacts. To tackle this problem, work on NLI has built explanation-based datasets such as eSNLI, allowing us to probe whether language models are right for the right reasons. Yet no such data exists for figurative language, making it harder to assess genuine understanding of such expressions. To address this issue, we release FLUTE, a dataset of 9,000 figurative NLI instances with explanations, spanning four categories: Sarcasm, Simile, Metaphor, and Idioms. We collect the data through a Human-AI collaboration framework based on GPT-3, crowd workers, and expert annotators. We show how utilizing GPT-3 in conjunction with human annotators (novices and experts) can aid in scaling up the creation of datasets even for such complex linguistic phenomena as figurative language. The baseline performance of the T5 model fine-tuned on FLUTE shows that our dataset can bring us a step closer to developing models that understand figurative language through textual explanations.</abstract>
      <url hash="45152d3b">2022.emnlp-main.481</url>
      <bibkey>chakrabarty-etal-2022-flute</bibkey>
    </paper>
    <paper id="482">
      <title>Precisely the Point: Adversarial Augmentations for Faithful and Informative Text Generation</title>
      <author><first>Wenhao</first><last>Wu</last></author>
      <author><first>Wei</first><last>Li</last></author>
      <author><first>Jiachen</first><last>Liu</last></author>
      <author><first>Xinyan</first><last>Xiao</last></author>
      <author><first>Sujian</first><last>Li</last></author>
      <author><first>Yajuan</first><last>Lyu</last></author>
      <pages>7160-7176</pages>
      <abstract>Though model robustness has been extensively studied in language understanding, the robustness of Seq2Seq generation remains understudied.In this paper, we conduct the first quantitative analysis on the robustness of pre-trained Seq2Seq models. We find that even current SOTA pre-trained Seq2Seq model (BART) is still vulnerable, which leads to significant degeneration in faithfulness and informativeness for text generation tasks.This motivated us to further propose a novel adversarial augmentation framework, namely AdvSeq, for generally improving faithfulness and informativeness of Seq2Seq models via enhancing their robustness. AdvSeq automatically constructs two types of adversarial augmentations during training, including implicit adversarial samples by perturbing word representations and explicit adversarial samples by word swapping, both of which effectively improve Seq2Seq robustness.Extensive experiments on three popular text generation tasks demonstrate that AdvSeq significantly improves both the faithfulness and informativeness of Seq2Seq generation under both automatic and human evaluation settings.</abstract>
      <url hash="e0758d84">2022.emnlp-main.482</url>
      <bibkey>wu-etal-2022-precisely</bibkey>
    </paper>
    <paper id="483">
      <title><fixed-case>RLET</fixed-case>: A Reinforcement Learning Based Approach for Explainable <fixed-case>QA</fixed-case> with Entailment Trees</title>
      <author><first>Tengxiao</first><last>Liu</last></author>
      <author><first>Qipeng</first><last>Guo</last></author>
      <author><first>Xiangkun</first><last>Hu</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <author><first>Xipeng</first><last>Qiu</last></author>
      <author><first>Zheng</first><last>Zhang</last></author>
      <pages>7177-7189</pages>
      <abstract>Interpreting the reasoning process from questions to answers poses a challenge in approaching explainable QA. A recently proposed structured reasoning format, entailment tree, manages to offer explicit logical deductions with entailment steps in a tree structure. To generate entailment trees, prior single pass sequence-to-sequence models lack visible internal decision probability, while stepwise approaches are supervised with extracted single step data and cannot model the tree as a whole. In this work, we propose RLET, a Reinforcement Learning based Entailment Tree generation framework, which is trained utilising the cumulative signals across the whole tree. RLET iteratively performs single step reasoning with sentence selection and deduction generation modules, from which the training signal is accumulated across the tree with elaborately designed aligned reward function that is consistent with the evaluation. To the best of our knowledge, we are the first to introduce RL into the entailment tree generation task. Experiments on three settings of the EntailmentBank dataset demonstrate the strength of using RL framework.</abstract>
      <url hash="13b12a2a">2022.emnlp-main.483</url>
      <bibkey>liu-etal-2022-rlet</bibkey>
    </paper>
    <paper id="484">
      <title>Let the <fixed-case>CAT</fixed-case> out of the bag: Contrastive Attributed explanations for Text</title>
      <author><first>Saneem</first><last>Chemmengath</last></author>
      <author><first>Amar Prakash</first><last>Azad</last></author>
      <author><first>Ronny</first><last>Luss</last></author>
      <author><first>Amit</first><last>Dhurandhar</last></author>
      <pages>7190-7206</pages>
      <abstract>Contrastive explanations for understanding the behavior of black box models has gained a lot of attention recently as they provide potential for recourse. In this paper, we propose a method Contrastive Attributed explanations for Text (CAT) which provides contrastive explanations for natural language text data with a novel twist as we build and exploit attribute classifiers leading to more semantically meaningful explanations. To ensure that our contrastive generated text has the fewest possible edits with respect to the original text, while also being fluent and close to a human generated contrastive, we resort to a minimal perturbation approach regularized using a BERT language model and attribute classifiers trained on available attributes. We show through qualitative examples and a user study that our method not only conveys more insight because of these attributes, but also leads to better quality (contrastive) text. Quantitatively, we show that our method outperforms other state-of-the-art methods across four data sets on four benchmark metrics.</abstract>
      <url hash="224c22db">2022.emnlp-main.484</url>
      <bibkey>chemmengath-etal-2022-cat</bibkey>
    </paper>
    <paper id="485">
      <title>mono<fixed-case>QA</fixed-case>: Multi-Task Learning of Reranking and Answer Extraction for Open-Retrieval Conversational Question Answering</title>
      <author><first>Sarawoot</first><last>Kongyoung</last></author>
      <author><first>Craig</first><last>Macdonald</last></author>
      <author><first>Iadh</first><last>Ounis</last></author>
      <pages>7207-7218</pages>
      <abstract>To address the Conversational Question Answering (ORConvQA) task, previous work has considered an effective three-stage architecture, consisting of a retriever, a reranker, and a reader to extract the answers. In order to effectively answer the users’ questions, a number of existing approaches have applied multi-task learning, such that the same model is shared between the reranker and the reader. Such approaches also typically tackle reranking and reading as classification tasks. On the other hand, recent text generation models, such as monoT5 and UnifiedQA, have been shown to respectively yield impressive performances in passage reranking and reading. However, no prior work has combined monoT5 and UnifiedQA to share a single text generation model that directly extracts the answers for the users instead of predicting the start/end positions in a retrieved passage. In this paper, we investigate the use of Multi-Task Learning (MTL) to improve performance on the ORConvQA task by sharing the reranker and reader’s learned structure in a generative model. In particular, we propose monoQA, which uses a text generation model with multi-task learning for both the reranker and reader. Our model, which is based on the T5 text generation model, is fine-tuned simultaneously for both reranking (in order to improve the precision of the top retrieved passages) and extracting the answer. Our results on the OR-QuAC and OR-CoQA datasets demonstrate the effectiveness of our proposed model, which significantly outperforms existing strong baselines with improvements ranging from +12.31% to +19.51% in MAP and from +5.70% to +23.34% in F1 on all used test sets.</abstract>
      <url hash="0ca6a708">2022.emnlp-main.485</url>
      <bibkey>kongyoung-etal-2022-monoqa</bibkey>
    </paper>
    <paper id="486">
      <title>Composing Ci with Reinforced Non-autoregressive Text Generation</title>
      <author><first>Yan</first><last>Song</last></author>
      <pages>7219-7229</pages>
      <abstract>Composing Ci (also widely known as Song Ci), a special type of classical Chinese poetry, requires to follow particular format once their tune patterns are given. To automatically generate a well-formed Ci, text generation systems should strictly take into account pre-defined rigid formats (e.g., length and rhyme). Yet, most existing approaches regard Ci generation as a conventional sequence-to-sequence task and use autoregressive models, while it is challenging for such models to properly handle the constraints (according to tune patterns) of Ci during the generation process. Moreover, consider that with the format prepared, Ci generation can be operated by an efficient synchronous process, where autoregressive models are limited in doing so since they follow the character-by-character generation protocol. Therefore, in this paper, we propose to compose Ci through a non-autoregressive approach, which not only ensure that the generation process accommodates tune patterns by controlling the rhythm and essential meaning of each sentence, but also allow the model to perform synchronous generation. In addition, we further improve our approach by applying reinforcement learning to the generation process with the rigid constraints of Ci as well as the diversity in content serving as rewards, so as to further maintain the format and content requirement. Experiments on a collected Ci dataset confirm that our proposed approach outperforms strong baselines and previous studies in terms of both automatic evaluation metrics and human judgements.</abstract>
      <url hash="e640551c">2022.emnlp-main.486</url>
      <bibkey>song-2022-composing</bibkey>
    </paper>
    <paper id="487">
      <title><fixed-case>M</fixed-case>eta<fixed-case>TKG</fixed-case>: Learning Evolutionary Meta-Knowledge for Temporal Knowledge Graph Reasoning</title>
      <author><first>Yuwei</first><last>Xia</last></author>
      <author><first>Mengqi</first><last>Zhang</last></author>
      <author><first>Qiang</first><last>Liu</last></author>
      <author><first>Shu</first><last>Wu</last></author>
      <author><first>Xiao-Yu</first><last>Zhang</last></author>
      <pages>7230-7240</pages>
      <abstract>Reasoning over Temporal Knowledge Graphs (TKGs) aims to predict future facts based on given history. One of the key challenges for prediction is to learn the evolution of facts. Most existing works focus on exploring evolutionary information in history to obtain effective temporal embeddings for entities and relations, but they ignore the variation in evolution patterns of facts, which makes them struggle to adapt to future data with different evolution patterns. Moreover, new entities continue to emerge along with the evolution of facts over time. Since existing models highly rely on historical information to learn embeddings for entities, they perform poorly on such entities with little historical information. To tackle these issues, we propose a novel Temporal Meta-learning framework for TKG reasoning, MetaTKG for brevity. Specifically, our method regards TKG prediction as many temporal meta-tasks, and utilizes the designed Temporal Meta-learner to learn evolutionary meta-knowledge from these meta-tasks. The proposed method aims to guide the backbones to learn to adapt quickly to future data and deal with entities with little historical information by the learned meta-knowledge. Specially, in temporal meta-learner, we design a Gating Integration module to adaptively establish temporal correlations between meta-tasks. Extensive experiments on four widely-used datasets and three backbones demonstrate that our method can greatly improve the performance.</abstract>
      <url hash="e83e715f">2022.emnlp-main.487</url>
      <bibkey>xia-etal-2022-metatkg</bibkey>
    </paper>
    <paper id="488">
      <title>m<fixed-case>PLUG</fixed-case>: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections</title>
      <author><first>Chenliang</first><last>Li</last></author>
      <author><first>Haiyang</first><last>Xu</last></author>
      <author><first>Junfeng</first><last>Tian</last></author>
      <author><first>Wei</first><last>Wang</last></author>
      <author><first>Ming</first><last>Yan</last></author>
      <author><first>Bin</first><last>Bi</last></author>
      <author><first>Jiabo</first><last>Ye</last></author>
      <author><first>He</first><last>Chen</last></author>
      <author><first>Guohai</first><last>Xu</last></author>
      <author><first>Zheng</first><last>Cao</last></author>
      <author><first>Ji</first><last>Zhang</last></author>
      <author><first>Songfang</first><last>Huang</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <author><first>Jingren</first><last>Zhou</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <pages>7241-7259</pages>
      <abstract>Large-scale pre-trained foundation models have been an emerging paradigm for building artificial intelligence (AI) systems, which can be quickly adapted to a wide range of downstream tasks. This paper presents mPLUG, a new vision-language foundation model for both cross-modal understanding and generation. Most existing pre-trained models suffer from inefficiency and linguistic signal overwhelmed by long visual sequences in cross-modal alignment. To address both problems, mPLUG introduces an effective and efficient vision-language architecture with novel cross-modal skip-connections.mPLUG is pre-trained end-to-end on large-scale image-text pairs with both discriminative and generative objectives. It achieves state-of-the-art results on a wide range of vision-language downstream tasks, including image captioning, image-text retrieval, visual grounding and visual question answering. mPLUG also demonstrates strong zero-shot transferability on vision-language and video-language tasks. The code and pre-trained models are available at https://github.com/alibaba/AliceMind</abstract>
      <url hash="1268471f">2022.emnlp-main.488</url>
      <bibkey>li-etal-2022-mplug</bibkey>
    </paper>
    <paper id="489">
      <title><fixed-case>Q</fixed-case>-<fixed-case>TOD</fixed-case>: A Query-driven Task-oriented Dialogue System</title>
      <author><first>Xin</first><last>Tian</last></author>
      <author><first>Yingzhan</first><last>Lin</last></author>
      <author><first>Mengfei</first><last>Song</last></author>
      <author><first>Siqi</first><last>Bao</last></author>
      <author><first>Fan</first><last>Wang</last></author>
      <author><first>Huang</first><last>He</last></author>
      <author><first>Shuqi</first><last>Sun</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <pages>7260-7271</pages>
      <abstract>Existing pipelined task-oriented dialogue systems usually have difficulties adapting to unseen domains, whereas end-to-end systems are plagued by large-scale knowledge bases in practice. In this paper, we introduce a novel query-driven task-oriented dialogue system, namely Q-TOD. The essential information from the dialogue context is extracted into a query, which is further employed to retrieve relevant knowledge records for response generation. Firstly, as the query is in the form of natural language and not confined to the schema of the knowledge base, the issue of domain adaption is alleviated remarkably in Q-TOD. Secondly, as the query enables the decoupling of knowledge retrieval from the generation, Q-TOD gets rid of the issue of knowledge base scalability. To evaluate the effectiveness of the proposed Q-TOD, we collect query annotations for three publicly available task-oriented dialogue datasets. Comprehensive experiments verify that Q-TOD outperforms strong baselines and establishes a new state-of-the-art performance on these datasets.</abstract>
      <url hash="ed4a342f">2022.emnlp-main.489</url>
      <bibkey>tian-etal-2022-q</bibkey>
    </paper>
    <paper id="490">
      <title>Dial2vec: Self-Guided Contrastive Learning of Unsupervised Dialogue Embeddings</title>
      <author><first>Che</first><last>Liu</last></author>
      <author><first>Rui</first><last>Wang</last></author>
      <author><first>Junfeng</first><last>Jiang</last></author>
      <author><first>Yongbin</first><last>Li</last></author>
      <author><first>Fei</first><last>Huang</last></author>
      <pages>7272-7282</pages>
      <abstract>In this paper, we introduce the task of learning unsupervised dialogue embeddings.Trivial approaches such as combining pre-trained word or sentence embeddings and encoding through pre-trained language models (PLMs) have been shown to be feasible for this task.However, these approaches typically ignore the conversational interactions between interlocutors, resulting in poor performance.To address this issue, we proposed a self-guided contrastive learning approach named dial2vec.Dial2vec considers a dialogue as an information exchange process.It captures the interaction patterns between interlocutors and leverages them to guide the learning of the embeddings corresponding to each interlocutor.Then the dialogue embedding is obtained by an aggregation of the embeddings from all interlocutors.To verify our approach, we establish a comprehensive benchmark consisting of six widely-used dialogue datasets.We consider three evaluation tasks: domain categorization, semantic relatedness, and dialogue retrieval.Dial2vec achieves on average 8.7, 9.0, and 13.8 points absolute improvements in terms of purity, Spearman’s correlation, and mean average precision (MAP) over the strongest baseline on the three tasks respectively.Further analysis shows that dial2vec obtains informative and discriminative embeddings for both interlocutors under the guidance of the conversational interactions and achieves the best performance when aggregating them through the interlocutor-level pooling strategy.All codes and data are publicly available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/dial2vec.</abstract>
      <url hash="362360e1">2022.emnlp-main.490</url>
      <bibkey>liu-etal-2022-dial2vec</bibkey>
    </paper>
    <paper id="491">
      <title><fixed-case>WR</fixed-case>-<fixed-case>O</fixed-case>ne2<fixed-case>S</fixed-case>et: Towards Well-Calibrated Keyphrase Generation</title>
      <author><first>Binbin</first><last>Xie</last></author>
      <author><first>Xiangpeng</first><last>Wei</last></author>
      <author><first>Baosong</first><last>Yang</last></author>
      <author><first>Huan</first><last>Lin</last></author>
      <author><first>Jun</first><last>Xie</last></author>
      <author><first>Xiaoli</first><last>Wang</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <author><first>Jinsong</first><last>Su</last></author>
      <pages>7283-7293</pages>
      <abstract>Keyphrase generation aims to automatically generate short phrases summarizing an input document. The recently emerged ONE2SET paradigm (Ye et al., 2021) generates keyphrases as a set and has achieved competitive performance. Nevertheless, we observe serious calibration errors outputted by ONE2SET, especially in the over-estimation of ∅ token (means “no corresponding keyphrase”). In this paper, we deeply analyze this limitation and identify two main reasons behind: 1) the parallel generation has to introduce excessive ∅ as padding tokens into training instances; and 2) the training mechanism assigning target to each slot is unstable and further aggravates the ∅ token over-estimation. To make the model well-calibrated, we propose WR-ONE2SET which extends ONE2SET with an adaptive instance-level cost Weighting strategy and a target Re-assignment mechanism. The former dynamically penalizes the over-estimated slots for different instances thus smoothing the uneven training distribution. The latter refines the original inappropriate assignment and reduces the supervisory signals of over-estimated slots. Experimental results on commonly-used datasets demonstrate the effectiveness and generality of our proposed paradigm.</abstract>
      <url hash="493a8771">2022.emnlp-main.491</url>
      <bibkey>xie-etal-2022-wr</bibkey>
    </paper>
    <paper id="492">
      <title>Eeny, meeny, miny, moe. How to choose data for morphological inflection.</title>
      <author><first>Saliha</first><last>Muradoglu</last></author>
      <author><first>Mans</first><last>Hulden</last></author>
      <pages>7294-7303</pages>
      <abstract>Data scarcity is a widespread problem for numerous natural language processing (NLP) tasks within low-resource languages. Within morphology, the labour-intensive task of tagging/glossing data is a serious bottleneck for both NLP and fieldwork. Active learning (AL) aims to reduce the cost of data annotation by selecting data that is most informative for the model. In this paper, we explore four sampling strategies for the task of morphological inflection using a Transformer model: a pair of oracle experiments where data is chosen based on correct/incorrect predictions by the model, model confidence, entropy, and random selection. We investigate the robustness of each sampling strategy across 30 typologically diverse languages, as well as a 10-cycle iteration using Natügu as a case study. Our results show a clear benefit to selecting data based on model confidence. Unsurprisingly, the oracle experiment, which is presented as a proxy for linguist/language informer feedback, shows the most improvement. This is followed closely by low-confidence and high-entropy forms. We also show that despite the conventional wisdom of larger data sets yielding better accuracy, introducing more instances of high-confidence, low-entropy, or forms that the model can already inflect correctly, can reduce model performance.</abstract>
      <url hash="3717eca2">2022.emnlp-main.492</url>
      <bibkey>muradoglu-hulden-2022-eeny</bibkey>
    </paper>
    <paper id="493">
      <title>An Adaptive Logical Rule Embedding Model for Inductive Reasoning over Temporal Knowledge Graphs</title>
      <author><first>Xin</first><last>Mei</last></author>
      <author><first>Libin</first><last>Yang</last></author>
      <author><first>Xiaoyan</first><last>Cai</last></author>
      <author><first>Zuowei</first><last>Jiang</last></author>
      <pages>7304-7316</pages>
      <abstract>Temporal knowledge graphs (TKGs) extrapolation reasoning predicts future events based on historical information, which has great research significance and broad application value. Existing methods can be divided into embedding-based methods and logical rule-based methods. Embedding-based methods rely on learned entity and relation embeddings to make predictions and thus lack interpretability. Logical rule-based methods bring scalability problems due to being limited by the learned logical rules. We combine the two methods to capture deep causal logic by learning rule embeddings, and propose an interpretable model for temporal knowledge graph reasoning called adaptive logical rule embedding model for inductive reasoning (ALRE-IR). ALRE-IR can adaptively extract and assess reasons contained in historical events, and make predictions based on causal logic. Furthermore, we propose a one-class augmented matching loss for optimization. When evaluated on the ICEWS14, ICEWS0515 and ICEWS18 datasets, the performance of ALRE-IR outperforms other state-of-the-art baselines. The results also demonstrate that ALRE-IR still shows outstanding performance when transferred to related dataset with common relation vocabulary, indicating our proposed model has good zero-shot reasoning ability.</abstract>
      <url hash="ac0b47c1">2022.emnlp-main.493</url>
      <bibkey>mei-etal-2022-adaptive</bibkey>
    </paper>
    <paper id="494">
      <title><fixed-case>U</fixed-case>ni<fixed-case>NL</fixed-case>: Aligning Representation Learning with Scoring Function for <fixed-case>OOD</fixed-case> Detection via Unified Neighborhood Learning</title>
      <author><first>Yutao</first><last>Mou</last></author>
      <author><first>Pei</first><last>Wang</last></author>
      <author><first>Keqing</first><last>He</last></author>
      <author><first>Yanan</first><last>Wu</last></author>
      <author><first>Jingang</first><last>Wang</last></author>
      <author><first>Wei</first><last>Wu</last></author>
      <author><first>Weiran</first><last>Xu</last></author>
      <pages>7317-7325</pages>
      <abstract>Detecting out-of-domain (OOD) intents from user queries is essential for avoiding wrong operations in task-oriented dialogue systems. The key challenge is how to distinguish in-domain (IND) and OOD intents. Previous methods ignore the alignment between representation learning and scoring function, limiting the OOD detection performance. In this paper, we propose a unified neighborhood learning framework (UniNL) to detect OOD intents. Specifically, we design a KNCL objective for representation learning, and introduce a KNN-based scoring function for OOD detection. We aim to align representation learning with scoring function. Experiments and analysis on two benchmark datasets show the effectiveness of our method.</abstract>
      <url hash="09c8459b">2022.emnlp-main.494</url>
      <bibkey>mou-etal-2022-uninl</bibkey>
    </paper>
    <paper id="495">
      <title>Open-domain Video Commentary Generation</title>
      <author><first>Edison</first><last>Marrese-Taylor</last></author>
      <author><first>Yumi</first><last>Hamazono</last></author>
      <author><first>Tatsuya</first><last>Ishigaki</last></author>
      <author><first>Goran</first><last>Topić</last></author>
      <author><first>Yusuke</first><last>Miyao</last></author>
      <author><first>Ichiro</first><last>Kobayashi</last></author>
      <author><first>Hiroya</first><last>Takamura</last></author>
      <pages>7326-7339</pages>
      <abstract>Live commentary plays an important role in sports broadcasts and video games, making spectators more excited and immersed. In this context, though approaches for automatically generating such commentary have been proposed in the past, they have been generally concerned with specific fields, where it is possible to leverage domain-specific information. In light of this, we propose the task of generating video commentary in an open-domain fashion. We detail the construction of a new large-scale dataset of transcribed commentary aligned with videos containing various human actions in a variety of domains, and propose approaches based on well-known neural architectures to tackle the task. To understand the strengths and limitations of current approaches, we present an in-depth empirical study based on our data. Our results suggest clear trade-offs between textual and visual inputs for the models and highlight the importance of relying on external knowledge in this open-domain setting, resulting in a set of robust baselines for our task.</abstract>
      <url hash="27ed7566">2022.emnlp-main.495</url>
      <bibkey>marrese-taylor-etal-2022-open</bibkey>
    </paper>
    <paper id="496">
      <title>One size does not fit all: Investigating strategies for differentially-private learning across <fixed-case>NLP</fixed-case> tasks</title>
      <author><first>Manuel</first><last>Senge</last></author>
      <author><first>Timour</first><last>Igamberdiev</last></author>
      <author><first>Ivan</first><last>Habernal</last></author>
      <pages>7340-7353</pages>
      <abstract>Preserving privacy in contemporary NLP models allows us to work with sensitive data, but unfortunately comes at a price. We know that stricter privacy guarantees in differentially-private stochastic gradient descent (DP-SGD) generally degrade model performance. However, previous research on the efficiency of DP-SGD in NLP is inconclusive or even counter-intuitive. In this short paper, we provide an extensive analysis of different privacy preserving strategies on seven downstream datasets in five different ‘typical’ NLP tasks with varying complexity using modern neural models based on BERT and XtremeDistil architectures. We show that unlike standard non-private approaches to solving NLP tasks, where bigger is usually better, privacy-preserving strategies do not exhibit a winning pattern, and each task and privacy regime requires a special treatment to achieve adequate performance.</abstract>
      <url hash="93cdafa7">2022.emnlp-main.496</url>
      <bibkey>senge-etal-2022-one</bibkey>
    </paper>
    <paper id="497">
      <title>Counterfactual Recipe Generation: Exploring Compositional Generalization in a Realistic Scenario</title>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Yansong</first><last>Feng</last></author>
      <author><first>Jizhi</first><last>Tang</last></author>
      <author><first>Chengang</first><last>Hu</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <pages>7354-7370</pages>
      <abstract>People can acquire knowledge in an unsupervised manner by reading, and compose the knowledge to make novel combinations. In this paper, we investigate whether pretrained language models can perform compositional generalization in a realistic setting: recipe generation. We design the counterfactual recipe generation task, which asks models to modify a base recipe according to the change of an ingredient. This task requires compositional generalization at two levels: the surface level of incorporating the new ingredient into the base recipe, and the deeper level of adjusting actions related to the changing ingredient. We collect a large-scale recipe dataset in Chinese for models to learn culinary knowledge, and a subset of action-level fine-grained annotations for evaluation.We finetune pretrained language models on the recipe corpus, and use unsupervised counterfactual generation methods to generate modified recipes.Results show that existing models have difficulties in modifying the ingredients while preserving the original text style, and often miss actions that need to be adjusted. Although pretrained language models can generate fluent recipe texts, they fail to truly learn and use the culinary knowledge in a compositional way. Code and data are available at https://github.com/xxxiaol/counterfactual-recipe-generation.</abstract>
      <url hash="cb5d444a">2022.emnlp-main.497</url>
      <bibkey>liu-etal-2022-counterfactual</bibkey>
    </paper>
    <paper id="498">
      <title>Tutoring Helps Students Learn Better: Improving Knowledge Distillation for <fixed-case>BERT</fixed-case> with Tutor Network</title>
      <author><first>Junho</first><last>Kim</last></author>
      <author><first>Jun-Hyung</first><last>Park</last></author>
      <author><first>Mingyu</first><last>Lee</last></author>
      <author><first>Wing-Lam</first><last>Mok</last></author>
      <author><first>Joon-Young</first><last>Choi</last></author>
      <author><first>SangKeun</first><last>Lee</last></author>
      <pages>7371-7382</pages>
      <abstract>Pre-trained language models have achieved remarkable successes in natural language processing tasks, coming at the cost of increasing model size. To address this issue, knowledge distillation (KD) has been widely applied to compress language models. However, typical KD approaches for language models have overlooked the difficulty of training examples, suffering from incorrect teacher prediction transfer and sub-efficient training. In this paper, we propose a novel KD framework, Tutor-KD, which improves the distillation effectiveness by controlling the difficulty of training examples during pre-training. We introduce a tutor network that generates samples that are easy for the teacher but difficult for the student, with training on a carefully designed policy gradient method. Experimental results show that Tutor-KD significantly and consistently outperforms the state-of-the-art KD methods with variously sized student models on the GLUE benchmark, demonstrating that the tutor can effectively generate training examples for the student.</abstract>
      <url hash="8efffac1">2022.emnlp-main.498</url>
      <bibkey>kim-etal-2022-tutoring</bibkey>
    </paper>
    <paper id="499">
      <title>Does Corpus Quality Really Matter for Low-Resource Languages?</title>
      <author><first>Mikel</first><last>Artetxe</last></author>
      <author><first>Itziar</first><last>Aldabe</last></author>
      <author><first>Rodrigo</first><last>Agerri</last></author>
      <author><first>Olatz</first><last>Perez-de-Viñaspre</last></author>
      <author><first>Aitor</first><last>Soroa</last></author>
      <pages>7383-7390</pages>
      <abstract>The vast majority of non-English corpora are derived from automatically filtered versions of CommonCrawl. While prior work has identified major issues on the quality of these datasets (Kreutzer et al., 2021), it is not clear how this impacts downstream performance. Taking representation learning in Basque as a case study, we explore tailored crawling (manually identifying and scraping websites with high-quality content) as an alternative to filtering CommonCrawl. Our new corpus, called EusCrawl, is similar in size to the Basque portion of popular multilingual corpora like CC100 and mC4, yet it has a much higher quality according to native annotators. For instance, 66% of documents are rated as high-quality for EusCrawl, in contrast with &lt;33% for both mC4 and CC100. Nevertheless, we obtain similar results on downstream NLU tasks regardless of the corpus used for pre-training. Our work suggests that NLU performance in low-resource languages is not primarily constrained by the quality of the data, and other factors like corpus size and domain coverage can play a more important role.</abstract>
      <url hash="74057de7">2022.emnlp-main.499</url>
      <bibkey>artetxe-etal-2022-corpus</bibkey>
    </paper>
    <paper id="500">
      <title>Unifying Data Perspectivism and Personalization: An Application to Social Norms</title>
      <author><first>Joan</first><last>Plepi</last></author>
      <author><first>Béla</first><last>Neuendorf</last></author>
      <author><first>Lucie</first><last>Flek</last></author>
      <author><first>Charles</first><last>Welch</last></author>
      <pages>7391-7402</pages>
      <abstract>Instead of using a single ground truth for language processing tasks, several recent studies have examined how to represent and predict the labels of the set of annotators. However, often little or no information about annotators is known, or the set of annotators is small. In this work, we examine a corpus of social media posts about conflict from a set of 13k annotators and 210k judgements of social norms. We provide a novel experimental setup that applies personalization methods to the modeling of annotators and compare their effectiveness for predicting the perception of social norms. We further provide an analysis of performance across subsets of social situations that vary by the closeness of the relationship between parties in conflict, and assess where personalization helps the most.</abstract>
      <url hash="0fca185c">2022.emnlp-main.500</url>
      <bibkey>plepi-etal-2022-unifying</bibkey>
    </paper>
    <paper id="501">
      <title>Does Self-Rationalization Improve Robustness to Spurious Correlations?</title>
      <author><first>Alexis</first><last>Ross</last></author>
      <author><first>Matthew</first><last>Peters</last></author>
      <author><first>Ana</first><last>Marasovic</last></author>
      <pages>7403-7416</pages>
      <abstract>Rationalization is fundamental to human reasoning and learning. NLP models trained to produce rationales along with predictions, called self-rationalization models, have been investigated for their interpretability and utility to end-users. However, the extent to which training with human-written rationales facilitates learning remains an under-explored question. We ask whether training models to self-rationalize can aid in their learning to solve tasks for the right reasons. Specifically, we evaluate how training self-rationalization models with free-text rationales affects robustness to spurious correlations in fine-tuned encoder-decoder and decoder-only models of six different sizes. We evaluate robustness to spurious correlations by measuring performance on 1) manually annotated challenge datasets and 2) subsets of original test sets where reliance on spurious correlations would fail to produce correct answers. We find that while self-rationalization can improve robustness to spurious correlations in low-resource settings, it tends to hurt robustness in higher-resource settings. Furthermore, these effects depend on model family and size, as well as on rationale content. Together, our results suggest that explainability can come at the cost of robustness; thus, appropriate care should be taken when training self-rationalizing models with the goal of creating more trustworthy models.</abstract>
      <url hash="0772b1cc">2022.emnlp-main.501</url>
      <bibkey>ross-etal-2022-self</bibkey>
    </paper>
    <paper id="502">
      <title>Efficient Pre-training of Masked Language Model via Concept-based Curriculum Masking</title>
      <author><first>Mingyu</first><last>Lee</last></author>
      <author><first>Jun-Hyung</first><last>Park</last></author>
      <author><first>Junho</first><last>Kim</last></author>
      <author><first>Kang-Min</first><last>Kim</last></author>
      <author><first>SangKeun</first><last>Lee</last></author>
      <pages>7417-7427</pages>
      <abstract>Self-supervised pre-training has achieved remarkable success in extensive natural language processing tasks. Masked language modeling (MLM) has been widely used for pre-training effective bidirectional representations but comes at a substantial training cost. In this paper, we propose a novel concept-based curriculum masking (CCM) method to efficiently pre-train a language model. CCM has two key differences from existing curriculum learning approaches to effectively reflect the nature of MLM. First, we introduce a novel curriculum that evaluates the MLM difficulty of each token based on a carefully-designed linguistic difficulty criterion. Second, we construct a curriculum that masks easy words and phrases first and gradually masks related ones to the previously masked ones based on a knowledge graph. Experimental results show that CCM significantly improves pre-training efficiency. Specifically, the model trained with CCM shows comparative performance with the original BERT on the General Language Understanding Evaluation benchmark at half of the training cost.</abstract>
      <url hash="b8fb6f73">2022.emnlp-main.502</url>
      <bibkey>lee-etal-2022-efficient-pre</bibkey>
    </paper>
    <paper id="503">
      <title>Subword Evenness (<fixed-case>S</fixed-case>u<fixed-case>E</fixed-case>) as a Predictor of Cross-lingual Transfer to Low-resource Languages</title>
      <author><first>Olga</first><last>Pelloni</last></author>
      <author><first>Anastassia</first><last>Shaitarova</last></author>
      <author><first>Tanja</first><last>Samardzic</last></author>
      <pages>7428-7445</pages>
      <abstract>Pre-trained multilingual models, such as mBERT, XLM-R and mT5, are used to improve the performance on various tasks in low-resource languages via cross-lingual transfer. In this framework, English is usually seen as the most natural choice for a transfer language (for fine-tuning or continued training of a multilingual pre-trained model), but it has been revealed recently that this is often not the best choice. The success of cross-lingual transfer seems to depend on some properties of languages, which are currently hard to explain. Successful transfer often happens between unrelated languages and it often cannot be explained by data-dependent factors.In this study, we show that languages written in non-Latin and non-alphabetic scripts (mostly Asian languages) are the best choices for improving performance on the task of Masked Language Modelling (MLM) in a diverse set of 30 low-resource languages and that the success of the transfer is well predicted by our novel measure of Subword Evenness (SuE). Transferring language models over the languages that score low on our measure results in the lowest average perplexity over target low-resource languages. Our correlation coefficients obtained with three different pre-trained multilingual models are consistently higher than all the other predictors, including text-based measures (type-token ratio, entropy) and linguistically motivated choice (genealogical and typological proximity).</abstract>
      <url hash="e0444b45">2022.emnlp-main.503</url>
      <bibkey>pelloni-etal-2022-subword</bibkey>
    </paper>
    <paper id="504">
      <title>A Unified Neural Network Model for Readability Assessment with Feature Projection and Length-Balanced Loss</title>
      <author><first>Wenbiao</first><last>Li</last></author>
      <author><first>Wang</first><last>Ziyang</last></author>
      <author><first>Yunfang</first><last>Wu</last></author>
      <pages>7446-7457</pages>
      <abstract>Readability assessment is a basic research task in the field of education. Traditional methods mainly employ machine learning classifiers with hundreds of linguistic features. Although the deep learning model has become the prominent approach for almost all NLP tasks, it is less explored for readability assessment. In this paper, we propose a BERT-based model with feature projection and length-balanced loss (BERT-FP-LBL) to determine the difficulty level of a given text. First, we introduce topic features guided by difficulty knowledge to complement the traditional linguistic features. From the linguistic features, we extract really useful orthogonal features to supplement BERT representations by means of projection filtering. Furthermore, we design a length-balanced loss to handle the greatly varying length distribution of the readability data. We conduct experiments on three English benchmark datasets and one Chinese dataset, and the experimental results show that our proposed model achieves significant improvements over baseline models. Interestingly, our proposed model achieves comparable results with human experts in consistency test.</abstract>
      <url hash="4a65333e">2022.emnlp-main.504</url>
      <bibkey>li-etal-2022-unified</bibkey>
    </paper>
    <paper id="505">
      <title>Speaker Overlap-aware Neural Diarization for Multi-party Meeting Analysis</title>
      <author><first>Zhihao</first><last>Du</last></author>
      <author><first>ShiLiang</first><last>Zhang</last></author>
      <author><first>Siqi</first><last>Zheng</last></author>
      <author><first>Zhi-Jie</first><last>Yan</last></author>
      <pages>7458-7469</pages>
      <abstract>Recently, hybrid systems of clustering and neural diarization models have been successfully applied in multi-party meeting analysis. However, current models always treat overlapped speaker diarization as a multi-label classification problem, where speaker dependency and overlaps are not well considered. To overcome the disadvantages, we reformulate overlapped speaker diarization task as a single-label prediction problem via the proposed power set encoding (PSE). Through this formulation, speaker dependency and overlaps can be explicitly modeled. To fully leverage this formulation, we further propose the speaker overlap-aware neural diarization (SOND) model, which consists of a context-independent (CI) scorer to model global speaker discriminability, a context-dependent scorer (CD) to model local discriminability, and a speaker combining network (SCN) to combine and reassign speaker activities. Experimental results show that using the proposed formulation can outperform the state-of-the-art methods based on target speaker voice activity detection, and the performance can be further improved with SOND, resulting in a 6.30% relative diarization error reduction.</abstract>
      <url hash="d5766fa4">2022.emnlp-main.505</url>
      <bibkey>du-etal-2022-speaker</bibkey>
    </paper>
    <paper id="506">
      <title><fixed-case>GREENER</fixed-case>: Graph Neural Networks for News Media Profiling</title>
      <author><first>Panayot</first><last>Panayotov</last></author>
      <author><first>Utsav</first><last>Shukla</last></author>
      <author><first>Husrev Taha</first><last>Sencar</last></author>
      <author><first>Mohamed</first><last>Nabeel</last></author>
      <author><first>Preslav</first><last>Nakov</last></author>
      <pages>7470-7480</pages>
      <abstract>We study the problem of profiling news media on the Web with respect to their factuality of reporting and bias. This is an important but under-studied problem related to disinformation and “fake news” detection, but it addresses the issue at a coarser granularity compared to looking at an individual article or an individual claim. This is useful as it allows to profile entire media outlets in advance. Unlike previous work, which has focused primarily on text (e.g., on the text of the articles published by the target website, or on the textual description in their social media profiles or in Wikipedia), here our main focus is on modeling the similarity between media outlets based on the overlap of their audience. This is motivated by homophily considerations, i.e., the tendency of people to have connections to people with similar interests, which we extend to media, hypothesizing that similar types of media would be read by similar kinds of users. In particular, we propose GREENER (GRaph nEural nEtwork for News mEdia pRofiling), a model that builds a graph of inter-media connections based on their audience overlap, and then uses graph neural networks to represent each medium. We find that such representations are quite useful for predicting the factuality and the bias of news media outlets, yielding improvements over state-of-the-art results reported on two datasets. When augmented with conventionally used representations obtained from news articles, Twitter, YouTube, Facebook, and Wikipedia, prediction accuracy is found to improve by 2.5-27 macro-F1 points for the two tasks.</abstract>
      <url hash="60b48468">2022.emnlp-main.506</url>
      <bibkey>panayotov-etal-2022-greener</bibkey>
    </paper>
    <paper id="507">
      <title>Graph <fixed-case>H</fixed-case>awkes Transformer for Extrapolated Reasoning on Temporal Knowledge Graphs</title>
      <author><first>Haohai</first><last>Sun</last></author>
      <author><first>Shangyi</first><last>Geng</last></author>
      <author><first>Jialun</first><last>Zhong</last></author>
      <author><first>Han</first><last>Hu</last></author>
      <author><first>Kun</first><last>He</last></author>
      <pages>7481-7493</pages>
      <abstract>Temporal Knowledge Graph (TKG) reasoning has attracted increasing attention due to its enormous potential value, and the critical issue is how to model the complex temporal structure information effectively. Recent studies use the method of encoding graph snapshots into hidden vector space and then performing heuristic deductions, which perform well on the task of entity prediction. However, these approaches cannot predict when an event will occur and have the following limitations: 1) there are many facts not related to the query that can confuse the model; 2) there exists information forgetting caused by long-term evolutionary processes. To this end, we propose a Graph Hawkes Transformer (GHT) for both TKG entity prediction and time prediction tasks in the future time. In GHT, there are two variants of Transformer, which capture the instantaneous structural information and temporal evolution information, respectively, and a new relational continuous-time encoding function to facilitate feature evolution with the Hawkes process. Extensive experiments on four public datasets demonstrate its superior performance, especially on long-term evolutionary tasks.</abstract>
      <url hash="f54124ae">2022.emnlp-main.507</url>
      <bibkey>sun-etal-2022-graph</bibkey>
    </paper>
    <paper id="508">
      <title><fixed-case>U</fixed-case>ni<fixed-case>RPG</fixed-case>: Unified Discrete Reasoning over Table and Text as Program Generation</title>
      <author><first>Yongwei</first><last>Zhou</last></author>
      <author><first>Junwei</first><last>Bao</last></author>
      <author><first>Chaoqun</first><last>Duan</last></author>
      <author><first>Youzheng</first><last>Wu</last></author>
      <author><first>Xiaodong</first><last>He</last></author>
      <author><first>Tiejun</first><last>Zhao</last></author>
      <pages>7494-7507</pages>
      <abstract>Question answering requiring discrete reasoning, e.g., arithmetic computing, comparison, and counting, over knowledge is a challenging task.In this paper, we propose UniRPG, a semantic-parsing-based approach advanced in interpretability and scalability, to perform Unified discrete Reasoning over heterogeneous knowledge resources, i.e., table and text, as Program Generation. Concretely, UniRPG consists of a neural programmer and a symbolic program executor,where a program is the composition of a set of pre-defined general atomic and higher-order operations and arguments extracted from table and text.First, the programmer parses a question into a program by generating operations and copying arguments, and then, the executor derives answers from table and text based on the program.To alleviate the costly program annotation issue, we design a distant supervision approach for programmer learning, where pseudo programs are automatically constructed without annotated derivations.Extensive experiments on the TAT-QA dataset show that UniRPG achieves tremendous improvements and enhances interpretability and scalability compared with previous state-of-the-art methods, even without derivation annotation.Moreover, it achieves promising performance on the textual dataset DROP without derivation annotation.</abstract>
      <url hash="3a1c572b">2022.emnlp-main.508</url>
      <bibkey>zhou-etal-2022-unirpg</bibkey>
    </paper>
    <paper id="509">
      <title>Don’t Prompt, Search! Mining-based Zero-Shot Learning with Language Models</title>
      <author><first>Mozes</first><last>van de Kar</last></author>
      <author><first>Mengzhou</first><last>Xia</last></author>
      <author><first>Danqi</first><last>Chen</last></author>
      <author><first>Mikel</first><last>Artetxe</last></author>
      <pages>7508-7520</pages>
      <abstract>Masked language models like BERT can perform text classification in a zero-shot fashion by reformulating downstream tasks as text infilling. However, this approach is highly sensitive to the template used to prompt the model, yet practitioners are blind when designing them in strict zero-shot settings. In this paper, we propose an alternative mining-based approach for zero-shot learning. Instead of prompting language models, we use regular expressions to mine labeled examples from unlabeled corpora, which can optionally be filtered through prompting, and used to finetune a pretrained model. Our method is more flexible and interpretable than prompting, and outperforms it on a wide range of tasks when using comparable templates. Our results suggest that the success of prompting can partly be explained by the model being exposed to similar examples during pretraining, which can be directly retrieved through regular expressions.</abstract>
      <url hash="58752f4b">2022.emnlp-main.509</url>
      <bibkey>van-de-kar-etal-2022-dont</bibkey>
    </paper>
    <paper id="510">
      <title><fixed-case>SEMG</fixed-case>raph: Incorporating Sentiment Knowledge and Eye Movement into Graph Model for Sentiment Analysis</title>
      <author><first>Bingbing</first><last>Wang</last></author>
      <author><first>Bin</first><last>Liang</last></author>
      <author><first>Jiachen</first><last>Du</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <pages>7521-7531</pages>
      <abstract>This paper investigates the sentiment analysis task from a novel perspective by incorporating sentiment knowledge and eye movement into a graph architecture, aiming to draw the eye movement-based sentiment relationships for learning the sentiment expression of the context. To be specific, we first explore a linguistic probing eye movement paradigm to extract eye movement features based on the close relationship between linguistic features and the early and late processes of human reading behavior. Furthermore, to derive eye movement features with sentiment concepts, we devise a novel weighting strategy to integrate sentiment scores extracted from affective commonsense knowledge into eye movement features, called sentiment-eye movement weights. Then, the sentiment-eye movement weights are exploited to build the sentiment-eye movement guided graph (SEMGraph) model, so as to model the intricate sentiment relationships in the context. Experimental results on two sentiment analysis datasets with eye movement signals and three sentiment analysis datasets without eye movement signals show that the proposed SEMGraph achieves state-of-the-art performance, and can also be directly generalized to those sentiment analysis datasets without eye movement signals.</abstract>
      <url hash="78493b1c">2022.emnlp-main.510</url>
      <bibkey>wang-etal-2022-semgraph</bibkey>
    </paper>
    <paper id="511">
      <title>Cross-lingual neural fuzzy matching for exploiting target-language monolingual corpora in computer-aided translation</title>
      <author><first>Miquel</first><last>Esplà-Gomis</last></author>
      <author><first>Víctor M.</first><last>Sánchez-Cartagena</last></author>
      <author><first>Juan Antonio</first><last>Pérez-Ortiz</last></author>
      <author><first>Felipe</first><last>Sánchez-Martínez</last></author>
      <pages>7532-7543</pages>
      <abstract>Computer-aided translation (CAT) tools based on translation memories (MT) play a prominent role in the translation workflow of professional translators. However, the reduced availability of in-domain TMs, as compared to in-domain monolingual corpora, limits its adoption for a number of translation tasks. In this paper, we introduce a novel neural approach aimed at overcoming this limitation by exploiting not only TMs, but also in-domain target-language (TL) monolingual corpora, and still enabling a similar functionality to that offered by conventional TM-based CAT tools. Our approach relies on cross-lingual sentence embeddings to retrieve translation proposals from TL monolingual corpora, and on a neural model to estimate their post-editing effort. The paper presents an automatic evaluation of these techniques on four language pairs that shows that our approach can successfully exploit monolingual texts in a TM-based CAT environment, increasing the amount of useful translation proposals, and that our neural model for estimating the post-editing effort enables the combination of translation proposals obtained from monolingual corpora and from TMs in the usual way. A human evaluation performed on a single language pair confirms the results of the automatic evaluation and seems to indicate that the translation proposals retrieved with our approach are more useful than what the automatic evaluation shows.</abstract>
      <url hash="be383d2d">2022.emnlp-main.511</url>
      <bibkey>espla-gomis-etal-2022-cross</bibkey>
    </paper>
    <paper id="512">
      <title>Multi-Label Intent Detection via Contrastive Task Specialization of Sentence Encoders</title>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Iñigo</first><last>Casanueva</last></author>
      <author><first>Georgios</first><last>Spithourakis</last></author>
      <author><first>Avishek</first><last>Mondal</last></author>
      <author><first>Tsung-Hsien</first><last>Wen</last></author>
      <author><first>Paweł</first><last>Budzianowski</last></author>
      <pages>7544-7559</pages>
      <abstract>Deploying task-oriented dialog ToD systems for new domains and tasks requires natural language understanding models that are 1) resource-efficient and work under low-data regimes; 2) adaptable, efficient, and quick-to-train; 3) expressive and can handle complex ToD scenarios with multiple user intents in a single utterance. Motivated by these requirements, we introduce a novel framework for multi-label intent detection (mID): MultI-ConvFiT (Multi-Label Intent Detection via Contrastive Conversational Fine-Tuning). While previous work on efficient single-label intent detection learns a classifier on top of a fixed sentence encoder (SE), we propose to 1) transform general-purpose SEs into task-specialized SEs via contrastive fine-tuning on annotated multi-label data, 2) where task specialization knowledge can be stored into lightweight adapter modules without updating the original parameters of the input SE, and then 3) we build improved mID classifiers stacked on top of fixed specialized SEs. Our main results indicate that MultI-ConvFiT yields effective mID models, with large gains over non-specialized SEs reported across a spectrum of different mID datasets, both in low-data and high-data regimes.</abstract>
      <url hash="1642e25e">2022.emnlp-main.512</url>
      <bibkey>vulic-etal-2022-multi</bibkey>
    </paper>
    <paper id="513">
      <title>Discovering Language-neutral Sub-networks in Multilingual Language Models</title>
      <author><first>Negar</first><last>Foroutan</last></author>
      <author><first>Mohammadreza</first><last>Banaei</last></author>
      <author><first>Rémi</first><last>Lebret</last></author>
      <author><first>Antoine</first><last>Bosselut</last></author>
      <author><first>Karl</first><last>Aberer</last></author>
      <pages>7560-7575</pages>
      <abstract>Multilingual pre-trained language models transfer remarkably well on cross-lingual downstream tasks. However, the extent to which they learn language-neutral representations (i.e., shared representations that encode similar phenomena across languages), and the effect of such representations on cross-lingual transfer performance, remain open questions.In this work, we conceptualize language neutrality of multilingual models as a function of the overlap between language-encoding sub-networks of these models. We employ the lottery ticket hypothesis to discover sub-networks that are individually optimized for various languages and tasks. Our evaluation across three distinct tasks and eleven typologically-diverse languages demonstrates that sub-networks for different languages are topologically similar (i.e., language-neutral), making them effective initializations for cross-lingual transfer with limited performance degradation.</abstract>
      <url hash="bcfe0dbc">2022.emnlp-main.513</url>
      <bibkey>foroutan-etal-2022-discovering</bibkey>
    </paper>
    <paper id="514">
      <title>Parameter-Efficient Tuning Makes a Good Classification Head</title>
      <author><first>Zhuoyi</first><last>Yang</last></author>
      <author><first>Ming</first><last>Ding</last></author>
      <author><first>Yanhui</first><last>Guo</last></author>
      <author><first>Qingsong</first><last>Lv</last></author>
      <author><first>Jie</first><last>Tang</last></author>
      <pages>7576-7586</pages>
      <abstract>In recent years, pretrained models revolutionized the paradigm of natural language understanding (NLU), where we append a randomly initialized classification head after the pretrained backbone, e.g. BERT, and finetune the whole model. As the pretrained backbone makes a major contribution to the improvement, we naturally expect a good pretrained classification head can also benefit the training. However, the final-layer output of the backbone, i.e. the input of the classification head, will change greatly during finetuning, making the usual head-only pretraining ineffective. In this paper, we find that parameter-efficient tuning makes a good classification head, with which we can simply replace the randomly initialized heads for a stable performance gain. Our experiments demonstrate that the classification head jointly pretrained with parameter-efficient tuning consistently improves the performance on 9 tasks in GLUE and SuperGLUE.</abstract>
      <url hash="dc32f902">2022.emnlp-main.514</url>
      <bibkey>yang-etal-2022-parameter</bibkey>
    </paper>
    <paper id="515">
      <title><fixed-case>STGN</fixed-case>: an Implicit Regularization Method for Learning with Noisy Labels in Natural Language Processing</title>
      <author><first>Tingting</first><last>Wu</last></author>
      <author><first>Xiao</first><last>Ding</last></author>
      <author><first>Minji</first><last>Tang</last></author>
      <author><first>Hao</first><last>Zhang</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <author><first>Ting</first><last>Liu</last></author>
      <pages>7587-7598</pages>
      <abstract>Noisy labels are ubiquitous in natural language processing (NLP) tasks. Existing work, namely learning with noisy labels in NLP, is often limited to dedicated tasks or specific training procedures, making it hard to be widely used. To address this issue, SGD noise has been explored to provide a more general way to alleviate the effect of noisy labels by involving benign noise in the process of stochastic gradient descent. However, previous studies exert identical perturbation for all samples, which may cause overfitting on incorrect ones or optimizing correct ones inadequately. To facilitate this, we propose a novel stochastic tailor-made gradient noise (STGN), mitigating the effect of inherent label noise by introducing tailor-made benign noise for each sample. Specifically, we investigate multiple principles to precisely and stably discriminate correct samples from incorrect ones and thus apply different intensities of perturbation to them. A detailed theoretical analysis shows that STGN has good properties, beneficial for model generalization. Experiments on three different NLP tasks demonstrate the effectiveness and versatility of STGN. Also, STGN can boost existing robust training methods.</abstract>
      <url hash="87aa402d">2022.emnlp-main.515</url>
      <bibkey>wu-etal-2022-stgn</bibkey>
    </paper>
    <paper id="516">
      <title>Cross-Modal Similarity-Based Curriculum Learning for Image Captioning</title>
      <author><first>Hongkuan</first><last>Zhang</last></author>
      <author><first>Saku</first><last>Sugawara</last></author>
      <author><first>Akiko</first><last>Aizawa</last></author>
      <author><first>Lei</first><last>Zhou</last></author>
      <author><first>Ryohei</first><last>Sasano</last></author>
      <author><first>Koichi</first><last>Takeda</last></author>
      <pages>7599-7606</pages>
      <abstract>Image captioning models require the high-level generalization ability to describe the contents of various images in words. Most existing approaches treat the image–caption pairs equally in their training without considering the differences in their learning difficulties. Several image captioning approaches introduce curriculum learning methods that present training data with increasing levels of difficulty. However, their difficulty measurements are either based on domain-specific features or prior model training. In this paper, we propose a simple yet efficient difficulty measurement for image captioning using cross-modal similarity calculated by a pretrained vision–language model. Experiments on the COCO and Flickr30k datasets show that our proposed approach achieves superior performance and competitive convergence speed to baselines without requiring heuristics or incurring additional training costs. Moreover, the higher model performance on difficult examples and unseen data also demonstrates the generalization ability.</abstract>
      <url hash="625281a6">2022.emnlp-main.516</url>
      <bibkey>zhang-etal-2022-cross</bibkey>
    </paper>
    <paper id="517">
      <title>Debiasing Masks: A New Framework for Shortcut Mitigation in <fixed-case>NLU</fixed-case></title>
      <author><first>Johannes Mario</first><last>Meissner</last></author>
      <author><first>Saku</first><last>Sugawara</last></author>
      <author><first>Akiko</first><last>Aizawa</last></author>
      <pages>7607-7613</pages>
      <abstract>Debiasing language models from unwanted behaviors in Natural Language Understanding (NLU) tasks is a topic with rapidly increasing interest in the NLP community. Spurious statistical correlations in the data allow models to perform shortcuts and avoid uncovering more advanced and desirable linguistic features.A multitude of effective debiasing approaches has been proposed, but flexibility remains a major issue. For the most part, models must be retrained to find a new set of weights with debiased behavior.We propose a new debiasing method in which we identify debiased pruning masks that can be applied to a finetuned model. This enables the selective and conditional application of debiasing behaviors.We assume that bias is caused by a certain subset of weights in the network; our method is, in essence, a mask search to identify and remove biased weights.Our masks show equivalent or superior performance to the standard counterparts, while offering important benefits.Pruning masks can be stored with high efficiency in memory, and it becomes possible to switch among several debiasing behaviors (or revert back to the original biased model) at inference time. Finally, it opens the doors to further research on how biases are acquired by studying the generated masks. For example, we observed that the early layers and attention heads were pruned more aggressively, possibly hinting towards the location in which biases may be encoded.</abstract>
      <url hash="68135a38">2022.emnlp-main.517</url>
      <bibkey>meissner-etal-2022-debiasing</bibkey>
    </paper>
    <paper id="518">
      <title>Extending Phrase Grounding with Pronouns in Visual Dialogues</title>
      <author><first>Panzhong</first><last>Lu</last></author>
      <author><first>Xin</first><last>Zhang</last></author>
      <author><first>Meishan</first><last>Zhang</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>7614-7625</pages>
      <abstract>Conventional phrase grounding aims to localize noun phrases mentioned in a given caption to their corresponding image regions, which has achieved great success recently. Apparently, sole noun phrase grounding is not enough for cross-modal visual language understanding. Here we extend the task by considering pronouns as well. First, we construct a dataset of phrase grounding with both noun phrases and pronouns to image regions. Based on the dataset, we test the performance of phrase grounding by using a state-of-the-art literature model of this line. Then, we enhance the baseline grounding model with coreference information which should help our task potentially, modeling the coreference structures with graph convolutional networks. Experiments on our dataset, interestingly, show that pronouns are easier to ground than noun phrases, where the possible reason might be that these pronouns are much less ambiguous. Additionally, our final model with coreference information can significantly boost the grounding performance of both noun phrases and pronouns.</abstract>
      <url hash="ebaf9d9d">2022.emnlp-main.518</url>
      <bibkey>lu-etal-2022-extending</bibkey>
    </paper>
    <paper id="519">
      <title><fixed-case>EUR</fixed-case>-Lex-Sum: A Multi- and Cross-lingual Dataset for Long-form Summarization in the Legal Domain</title>
      <author><first>Dennis</first><last>Aumiller</last></author>
      <author><first>Ashish</first><last>Chouhan</last></author>
      <author><first>Michael</first><last>Gertz</last></author>
      <pages>7626-7639</pages>
      <abstract>Existing summarization datasets come with two main drawbacks: (1) They tend to focus on overly exposed domains, such as news articles or wiki-like texts, and (2) are primarily monolingual, with few multilingual datasets.In this work, we propose a novel dataset, called EUR-Lex-Sum, based on manually curated document summaries of legal acts from the European Union law platform (EUR-Lex). Documents and their respective summaries exist as cross-lingual paragraph-aligned data in several of the 24 official European languages, enabling access to various cross-lingual and lower-resourced summarization setups. We obtain up to 1,500 document/summary pairs per language, including a subset of 375 cross-lingually aligned legal acts with texts available in *all* 24 languages. In this work, the data acquisition process is detailed and key characteristics of the resource are compared to existing summarization resources. In particular, we illustrate challenging sub-problems and open questions on the dataset that could help the facilitation of future research in the direction of domain-specific cross-lingual summarization.Limited by the extreme length and language diversity of samples, we further conduct experiments with suitable extractive monolingual and cross-lingual baselines for future work. Code for the extraction as well as access to our data and baselines is available online at: [https://github.com/achouhan93/eur-lex-sum](https://github.com/achouhan93/eur-lex-sum).</abstract>
      <url hash="b48ff711">2022.emnlp-main.519</url>
      <bibkey>aumiller-etal-2022-eur</bibkey>
    </paper>
    <paper id="520">
      <title>Differentiable Data Augmentation for Contrastive Sentence Representation Learning</title>
      <author><first>Tianduo</first><last>Wang</last></author>
      <author><first>Wei</first><last>Lu</last></author>
      <pages>7640-7653</pages>
      <abstract>Fine-tuning a pre-trained language model via the contrastive learning framework with a large amount of unlabeled sentences or labeled sentence pairs is a common way to obtain high-quality sentence representations. Although the contrastive learning framework has shown its superiority on sentence representation learning over previous methods, the potential of such a framework is under-explored so far due to the simple method it used to construct positive pairs. Motivated by this, we propose a method that makes hard positives from the original training examples. A pivotal ingredient of our approach is the use of prefix that attached to a pre-trained language model, which allows for differentiable data augmentation during contrastive learning. Our method can be summarized in two steps: supervised prefix-tuning followed by joint contrastive fine-tuning with unlabeled or labeled examples. Our experiments confirm the effectiveness of our data augmentation approach. The proposed method yields significant improvements over existing methods under both semi-supervised and supervised settings. Our experiments under a low labeled data setting also show that our method is more label-efficient than the state-of-the-art contrastive learning methods.</abstract>
      <url hash="46b024d1">2022.emnlp-main.520</url>
      <bibkey>wang-lu-2022-differentiable</bibkey>
    </paper>
    <paper id="521">
      <title>Text Style Transferring via Adversarial Masking and Styled Filling</title>
      <author><first>Jiarui</first><last>Wang</last></author>
      <author><first>Richong</first><last>Zhang</last></author>
      <author><first>Junfan</first><last>Chen</last></author>
      <author><first>Jaein</first><last>Kim</last></author>
      <author><first>Yongyi</first><last>Mao</last></author>
      <pages>7654-7663</pages>
      <abstract>Text style transfer is an important task in natural language processing with broad applications. Existing models following the masking and filling scheme suffer two challenges: the word masking procedure may mistakenly remove unexpected words and the selected words in the word filling procedure may lack diversity and semantic consistency. To tackle both challenges, in this study, we propose a style transfer model, with an adversarial masking approach and a styled filling technique (AMSF). Specifically, AMSF first trains a mask predictor by adversarial training without manual configuration. Then two additional losses, i.e. an entropy maximization loss and a consistency regularization loss, are introduced in training the word filling module to guarantee the diversity and semantic consistency of the transferred texts. Experimental results and analysis on two benchmark text style transfer data sets demonstrate the effectiveness of the proposed approaches.</abstract>
      <url hash="7b845ace">2022.emnlp-main.521</url>
      <bibkey>wang-etal-2022-text</bibkey>
    </paper>
    <paper id="522">
      <title>Character-level White-Box Adversarial Attacks against Transformers via Attachable Subwords Substitution</title>
      <author><first>Aiwei</first><last>Liu</last></author>
      <author><first>Honghai</first><last>Yu</last></author>
      <author><first>Xuming</first><last>Hu</last></author>
      <author><first>Shu’ang</first><last>Li</last></author>
      <author><first>Li</first><last>Lin</last></author>
      <author><first>Fukun</first><last>Ma</last></author>
      <author><first>Yawen</first><last>Yang</last></author>
      <author><first>Lijie</first><last>Wen</last></author>
      <pages>7664-7676</pages>
      <abstract>We propose the first character-level white-box adversarial attack method against transformer models. The intuition of our method comes from the observation that words are split into subtokens before being fed into the transformer models and the substitution between two close subtokens has a similar effect with the character modification. Our method mainly contains three steps. First, a gradient-based method is adopted to find the most vulnerable words in the sentence. Then we split the selected words into subtokens to replace the origin tokenization result from the transformer tokenizer. Finally, we utilize an adversarial loss to guide the substitution of attachable subtokens in which the Gumbel-softmax trick is introduced to ensure gradient propagation.Meanwhile, we introduce the visual and length constraint in the optimization process to achieve minimum character modifications.Extensive experiments on both sentence-level and token-level tasks demonstrate that our method could outperform the previous attack methods in terms of success rate and edit distance. Furthermore, human evaluation verifies our adversarial examples could preserve their origin labels.</abstract>
      <url hash="ca5d658a">2022.emnlp-main.522</url>
      <bibkey>liu-etal-2022-character</bibkey>
    </paper>
    <paper id="523">
      <title>Query-based Instance Discrimination Network for Relational Triple Extraction</title>
      <author><first>Zeqi</first><last>Tan</last></author>
      <author><first>Yongliang</first><last>Shen</last></author>
      <author><first>Xuming</first><last>Hu</last></author>
      <author><first>Wenqi</first><last>Zhang</last></author>
      <author><first>Xiaoxia</first><last>Cheng</last></author>
      <author><first>Weiming</first><last>Lu</last></author>
      <author><first>Yueting</first><last>Zhuang</last></author>
      <pages>7677-7690</pages>
      <abstract>Joint entity and relation extraction has been a core task in the field of information extraction. Recent approaches usually consider the extraction of relational triples from a stereoscopic perspective, either learning a relation-specific tagger or separate classifiers for each relation type. However, they still suffer from error propagation, relation redundancy and lack of high-level connections between triples. To address these issues, we propose a novel query-based approach to construct instance-level representations for relational triples. By metric-based comparison between query embeddings and token embeddings, we can extract all types of triples in one step, thus eliminating the error propagation problem. In addition, we learn the instance-level representation of relational triples via contrastive learning. In this way, relational triples can not only enclose rich class-level semantics but also access to high-order global connections. Experimental results show that our proposed method achieves the state of the art on five widely used benchmarks.</abstract>
      <url hash="15563a74">2022.emnlp-main.523</url>
      <bibkey>tan-etal-2022-query</bibkey>
    </paper>
    <paper id="524">
      <title>Learning Inter-Entity-Interaction for Few-Shot Knowledge Graph Completion</title>
      <author><first>Yuling</first><last>Li</last></author>
      <author><first>Kui</first><last>Yu</last></author>
      <author><first>Xiaoling</first><last>Huang</last></author>
      <author><first>Yuhong</first><last>Zhang</last></author>
      <pages>7691-7700</pages>
      <abstract>Few-shot knowledge graph completion (FKGC) aims to infer unknown fact triples of a relation using its few-shot reference entity pairs. Recent FKGC studies focus on learning semantic representations of entity pairs by separately encoding the neighborhoods of head and tail entities. Such practice, however, ignores the inter-entity interaction, resulting in low-discrimination representations for entity pairs, especially when these entity pairs are associated with 1-to-N, N-to-1, and N-to-N relations. To address this issue, this paper proposes a novel FKGC model, named Cross-Interaction Attention Network (CIAN) to investigate the inter-entity interaction between head and tail entities. Specifically, we first explore the interactions within entities by computing the attention between the task relation and each entity neighbor, and then model the interactions between head and tail entities by letting an entity to attend to the neighborhood of its paired entity. In this way, CIAN can figure out the relevant semantics between head and tail entities, thereby generating more discriminative representations for entity pairs. Extensive experiments on two public datasets show that CIAN outperforms several state-of-the-art methods. The source code is available at <url>https://github.com/cjlyl/FKGC-CIAN</url>.</abstract>
      <url hash="b43a461f">2022.emnlp-main.524</url>
      <bibkey>li-etal-2022-learning-inter</bibkey>
    </paper>
    <paper id="525">
      <title>Empowering the Fact-checkers! Automatic Identification of Claim Spans on <fixed-case>T</fixed-case>witter</title>
      <author><first>Megha</first><last>Sundriyal</last></author>
      <author><first>Atharva</first><last>Kulkarni</last></author>
      <author><first>Vaibhav</first><last>Pulastya</last></author>
      <author><first>Md. Shad</first><last>Akhtar</last></author>
      <author><first>Tanmoy</first><last>Chakraborty</last></author>
      <pages>7701-7715</pages>
      <abstract>The widespread diffusion of medical and political claims in the wake of COVID-19 has led to a voluminous rise in misinformation and fake news. The current vogue is to employ manual fact-checkers to efficiently classify and verify such data to combat this avalanche of claim-ridden misinformation. However, the rate of information dissemination is such that it vastly outpaces the fact-checkers’ strength. Therefore, to aid manual fact-checkers in eliminating the superfluous content, it becomes imperative to automatically identify and extract the snippets of claim-worthy (mis)information present in a post. In this work, we introduce the novel task of Claim Span Identification (CSI). We propose CURT, a large-scale Twitter corpus with token-level claim spans on more than 7.5k tweets. Furthermore, along with the standard token classification baselines, we benchmark our dataset with DABERTa, an adapter-based variation of RoBERTa. The experimental results attest that DABERTa outperforms the baseline systems across several evaluation metrics, improving by about 1.5 points. We also report detailed error analysis to validate the model’s performance along with the ablation studies. Lastly, we release our comprehensive span annotation guidelines for public use.</abstract>
      <url hash="c3a116be">2022.emnlp-main.525</url>
      <bibkey>sundriyal-etal-2022-empowering</bibkey>
    </paper>
    <paper id="526">
      <title><fixed-case>C</fixed-case>lid<fixed-case>S</fixed-case>um: A Benchmark Dataset for Cross-Lingual Dialogue Summarization</title>
      <author><first>Jiaan</first><last>Wang</last></author>
      <author><first>Fandong</first><last>Meng</last></author>
      <author><first>Ziyao</first><last>Lu</last></author>
      <author><first>Duo</first><last>Zheng</last></author>
      <author><first>Zhixu</first><last>Li</last></author>
      <author><first>Jianfeng</first><last>Qu</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>7716-7729</pages>
      <abstract>We present ClidSum, a benchmark dataset towards building cross-lingual summarization systems on dialogue documents. It consists of 67k+ dialogue documents and 112k+ annotated summaries in different target languages. Based on the proposed ClidSum, we introduce two benchmark settings for supervised and semi-supervised scenarios, respectively. We then build various baseline systems in different paradigms (pipeline and end-to-end) and conduct extensive experiments on ClidSum to provide deeper analyses. Furthermore, we propose mDialBART which extends mBART via further pre-training, where the multiple objectives help the pre-trained model capture the structural characteristics as well as key content in dialogues and the transformation from source to the target language. Experimental results show the superiority of mDialBART, as an end-to-end model, outperforms strong pipeline models on ClidSum. Finally, we discuss specific challenges that current approaches faced with this task and give multiple promising directions for future research. We have released the dataset and code at https://github.com/krystalan/ClidSum.</abstract>
      <url hash="506e075d">2022.emnlp-main.526</url>
      <bibkey>wang-etal-2022-clidsum</bibkey>
    </paper>
    <paper id="527">
      <title>Spectral Probing</title>
      <author><first>Max</first><last>Müller-Eberstein</last></author>
      <author><first>Rob</first><last>van der Goot</last></author>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>7730-7741</pages>
      <abstract>Linguistic information is encoded at varying timescales (subwords, phrases, etc.) and communicative levels, such as syntax and semantics. Contextualized embeddings have analogously been found to capture these phenomena at distinctive layers and frequencies. Leveraging these findings, we develop a fully learnable frequency filter to identify spectral profiles for any given task. It enables vastly more granular analyses than prior handcrafted filters, and improves on efficiency. After demonstrating the informativeness of spectral probing over manual filters in a monolingual setting, we investigate its multilingual characteristics across seven diverse NLP tasks in six languages. Our analyses identify distinctive spectral profiles which quantify cross-task similarity in a linguistically intuitive manner, while remaining consistent across languages—highlighting their potential as robust, lightweight task descriptors.</abstract>
      <url hash="7c730014">2022.emnlp-main.527</url>
      <bibkey>muller-eberstein-etal-2022-spectral</bibkey>
    </paper>
    <paper id="528">
      <title><fixed-case>QAS</fixed-case>em Parsing: Text-to-text Modeling of <fixed-case>QA</fixed-case>-based Semantics</title>
      <author><first>Ayal</first><last>Klein</last></author>
      <author><first>Eran</first><last>Hirsch</last></author>
      <author><first>Ron</first><last>Eliav</last></author>
      <author><first>Valentina</first><last>Pyatkin</last></author>
      <author><first>Avi</first><last>Caciularu</last></author>
      <author><first>Ido</first><last>Dagan</last></author>
      <pages>7742-7756</pages>
      <abstract>Various works suggest the appeals of incorporating explicit semantic representations when addressing challenging realistic NLP scenarios. Common approaches offer either comprehensive linguistically-based formalisms, like AMR, or alternatively Open-IE, which provides a shallow and partial representation. More recently, an appealing trend introduces semi-structured natural-language structures as an intermediate meaning-capturing representation, often in the form of questions and answers.In this work, we further promote this line of research by considering three prior QA-based semantic representations. These cover verbal, nominalized and discourse-based predications, regarded as jointly providing a comprehensive representation of textual information — termed QASem. To facilitate this perspective, we investigate how to best utilize pre-trained sequence-to-sequence language models, which seem particularly promising for generating representations that consist of natural language expressions (questions and answers). In particular, we examine and analyze input and output linearization strategies, as well as data augmentation and multitask learning for a scarce training data setup. Consequently, we release the first unified QASem parsing tool, easily applicable for downstream tasks that can benefit from an explicit semi-structured account of information units in text.</abstract>
      <url hash="cac12bf3">2022.emnlp-main.528</url>
      <bibkey>klein-etal-2022-qasem</bibkey>
    </paper>
    <paper id="529">
      <title>Keyphrase Generation via Soft and Hard Semantic Corrections</title>
      <author><first>Guangzhen</first><last>Zhao</last></author>
      <author><first>Guoshun</first><last>Yin</last></author>
      <author><first>Peng</first><last>Yang</last></author>
      <author><first>Yu</first><last>Yao</last></author>
      <pages>7757-7768</pages>
      <abstract>Keyphrase generation aims to generate a set of condensed phrases given a source document. Although maximum likelihood estimation (MLE) based keyphrase generation methods have shown impressive performance, they suffer from the bias on the source-prediction sequence pair and the bias on the prediction-target pair. To tackle the above biases, we propose a novel correction model CorrKG on top of the MLE pipeline, where the biases are corrected via the optimal transport (OT) and a frequency-based filtering-and-sorting (FreqFS) strategy. Specifically, OT is introduced as soft correction to facilitate the alignment of salient information and rectify the semantic bias in the source document and predicted keyphrases pair. An adaptive semantic mass learning scheme is conducted on the vanilla OT to achieve a proper pair-wise optimal transport procedure, which promotes the OT learning brought by rectifying semantic masses dynamically. Besides, the FreqFS strategy is designed as hard correction to reduce the bias of predicted and ground truth keyphrases, and thus to generate accurate and sufficient keyphrases. Extensive experiments over multiple benchmark datasets show that our model achieves superior keyphrase generation as compared with the state-of-the-arts.</abstract>
      <url hash="c19e4fcf">2022.emnlp-main.529</url>
      <bibkey>zhao-etal-2022-keyphrase</bibkey>
    </paper>
    <paper id="530">
      <title>Modal-specific Pseudo Query Generation for Video Corpus Moment Retrieval</title>
      <author><first>Minjoon</first><last>Jung</last></author>
      <author><first>SeongHo</first><last>Choi</last></author>
      <author><first>JooChan</first><last>Kim</last></author>
      <author><first>Jin-Hwa</first><last>Kim</last></author>
      <author><first>Byoung-Tak</first><last>Zhang</last></author>
      <pages>7769-7781</pages>
      <abstract>Video corpus moment retrieval (VCMR) is the task to retrieve the most relevant video moment from a large video corpus using a natural language query.For narrative videos, e.g., drama or movies, the holistic understanding of temporal dynamics and multimodal reasoning are crucial.Previous works have shown promising results; however, they relied on the expensive query annotations for the VCMR, i.e., the corresponding moment intervals.To overcome this problem, we propose a self-supervised learning framework: Modal-specific Pseudo Query Generation Network (MPGN).First, MPGN selects candidate temporal moments via subtitle-based moment sampling.Then, it generates pseudo queries exploiting both visualand textual information from the selected temporal moments.Through the multimodal information in the pseudo queries, we show that MPGN successfully learns to localize the video corpus moment without any explicit annotation.We validate the effectiveness of MPGN on TVR dataset, showing the competitive results compared with both supervised models and unsupervised setting models.</abstract>
      <url hash="bdd5bd7d">2022.emnlp-main.530</url>
      <bibkey>jung-etal-2022-modal</bibkey>
    </paper>
    <paper id="531">
      <title><fixed-case>D</fixed-case>u<fixed-case>QM</fixed-case>: A <fixed-case>C</fixed-case>hinese Dataset of Linguistically Perturbed Natural Questions for Evaluating the Robustness of Question Matching Models</title>
      <author><first>Hongyu</first><last>Zhu</last></author>
      <author><first>Yan</first><last>Chen</last></author>
      <author><first>Jing</first><last>Yan</last></author>
      <author><first>Jing</first><last>Liu</last></author>
      <author><first>Yu</first><last>Hong</last></author>
      <author><first>Ying</first><last>Chen</last></author>
      <author><first>Hua</first><last>Wu</last></author>
      <author><first>Haifeng</first><last>Wang</last></author>
      <pages>7782-7794</pages>
      <abstract>In this paper, we focus on the robustness evaluation of Chinese Question Matching (QM) models. Most of the previous work on analyzing robustness issues focus on just one or a few types of artificial adversarial examples. Instead, we argue that a comprehensive evaluation should be conducted on natural texts, which takes into account the fine-grained linguistic capabilities of QM models. For this purpose, we create a Chinese dataset namely DuQM which contains natural questions with linguistic perturbations to evaluate the robustness of QM models. DuQM contains 3 categories and 13 subcategories with 32 linguistic perturbations. The extensive experiments demonstrate that DuQM has a better ability to distinguish different models. Importantly, the detailed breakdown of evaluation by the linguistic phenomena in DuQM helps us easily diagnose the strength and weakness of different models. Additionally, our experiment results show that the effect of artificial adversarial examples does not work on natural texts. Our baseline codes and a leaderboard are now publicly available.</abstract>
      <url hash="3bc6682e">2022.emnlp-main.531</url>
      <bibkey>zhu-etal-2022-duqm</bibkey>
    </paper>
    <paper id="532">
      <title><fixed-case>D</fixed-case>iv<fixed-case>EMT</fixed-case>: Neural Machine Translation Post-Editing Effort Across Typologically Diverse Languages</title>
      <author><first>Gabriele</first><last>Sarti</last></author>
      <author><first>Arianna</first><last>Bisazza</last></author>
      <author><first>Ana</first><last>Guerberof-Arenas</last></author>
      <author><first>Antonio</first><last>Toral</last></author>
      <pages>7795-7816</pages>
      <abstract>We introduce DivEMT, the first publicly available post-editing study of Neural Machine Translation (NMT) over a typologically diverse set of target languages. Using a strictly controlled setup, 18 professional translators were instructed to translate or post-edit the same set of English documents into Arabic, Dutch, Italian, Turkish, Ukrainian, and Vietnamese. During the process, their edits, keystrokes, editing times and pauses were recorded, enabling an in-depth, cross-lingual evaluation of NMT quality and post-editing effectiveness. Using this new dataset, we assess the impact of two state-of-the-art NMT systems, Google Translate and the multilingual mBART-50 model, on translation productivity. We find that post-editing is consistently faster than translation from scratch. However, the magnitude of productivity gains varies widely across systems and languages, highlighting major disparities in post-editing effectiveness for languages at different degrees of typological relatedness to English, even when controlling for system architecture and training data size. We publicly release the complete dataset including all collected behavioral data, to foster new research on the translation capabilities of NMT systems for typologically diverse languages.</abstract>
      <url hash="38e5957a">2022.emnlp-main.532</url>
      <bibkey>sarti-etal-2022-divemt</bibkey>
    </paper>
    <paper id="533">
      <title>Bridging Fairness and Environmental Sustainability in Natural Language Processing</title>
      <author><first>Marius</first><last>Hessenthaler</last></author>
      <author><first>Emma</first><last>Strubell</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <author><first>Anne</first><last>Lauscher</last></author>
      <pages>7817-7836</pages>
      <abstract>Fairness and environmental impact are important research directions for the sustainable development of artificial intelligence. However, while each topic is an active research area in natural language processing (NLP), there is a surprising lack of research on the interplay between the two fields. This lacuna is highly problematic, since there is increasing evidence that an exclusive focus on fairness can actually hinder environmental sustainability, and vice versa. In this work, we shed light on this crucial intersection in NLP by (1) investigating the efficiency of current fairness approaches through surveying example methods for reducing unfair stereotypical bias from the literature, and (2) evaluating a common technique to reduce energy consumption (and thus environmental impact) of English NLP models, knowledge distillation (KD), for its impact on fairness. In this case study, we evaluate the effect of important KD factors, including layer and dimensionality reduction, with respect to: (a) performance on the distillation task (natural language inference and semantic similarity prediction), and (b) multiple measures and dimensions of stereotypical bias (e.g., gender bias measured via the Word Embedding Association Test). Our results lead us to clarify current assumptions regarding the effect of KD on unfair bias: contrary to other findings, we show that KD can actually decrease model fairness.</abstract>
      <url hash="af463954">2022.emnlp-main.533</url>
      <bibkey>hessenthaler-etal-2022-bridging</bibkey>
    </paper>
    <paper id="534">
      <title><fixed-case>U</fixed-case>ni<fixed-case>MSE</fixed-case>: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition</title>
      <author><first>Guimin</first><last>Hu</last></author>
      <author><first>Ting-En</first><last>Lin</last></author>
      <author><first>Yi</first><last>Zhao</last></author>
      <author><first>Guangming</first><last>Lu</last></author>
      <author><first>Yuchuan</first><last>Wu</last></author>
      <author><first>Yongbin</first><last>Li</last></author>
      <pages>7837-7851</pages>
      <abstract>Multimodal sentiment analysis (MSA) and emotion recognition in conversation (ERC) are key research topics for computers to understand human behaviors. From a psychological perspective, emotions are the expression of affect or feelings during a short period, while sentiments are formed and held for a longer period. However, most existing works study sentiment and emotion separately and do not fully exploit the complementary knowledge behind the two. In this paper, we propose a multimodal sentiment knowledge-sharing framework (UniMSE) that unifies MSA and ERC tasks from features, labels, and models. We perform modality fusion at the syntactic and semantic levels and introduce contrastive learning between modalities and samples to better capture the difference and consistency between sentiments and emotions. Experiments on four public benchmark datasets, MOSI, MOSEI, MELD, and IEMOCAP, demonstrate the effectiveness of the proposed method and achieve consistent improvements compared with state-of-the-art methods.</abstract>
      <url hash="2b3ba3f1">2022.emnlp-main.534</url>
      <bibkey>hu-etal-2022-unimse</bibkey>
    </paper>
    <paper id="535">
      <title>Is the Brain Mechanism for Hierarchical Structure Building Universal Across Languages? An f<fixed-case>MRI</fixed-case> Study of <fixed-case>C</fixed-case>hinese and <fixed-case>E</fixed-case>nglish</title>
      <author><first>Xiaohan</first><last>Zhang</last></author>
      <author><first>Shaonan</first><last>Wang</last></author>
      <author><first>Nan</first><last>Lin</last></author>
      <author><first>Chengqing</first><last>Zong</last></author>
      <pages>7852-7861</pages>
      <abstract>Evidence from psycholinguistic studies suggests that the human brain builds a hierarchical syntactic structure during language comprehension. However, it is still unknown whether the neural basis of such structures is universal across languages. In this paper, we first analyze the differences in language structure between two diverse languages: Chinese and English. By computing the working memory requirements when applying parsing strategies to different language structures, we find that top-down parsing generates less memory load for the right-branching English and bottom-up parsing is less memory-demanding for Chinese.Then we use functional magnetic resonance imaging (fMRI) to investigate whether the brain has different syntactic adaptation strategies in processing Chinese and English. Specifically, for both Chinese and English, we extract predictors from the implementations of different parsing strategies, i.e., bottom-up and top-down. Then, these predictors are separately associated with fMRI signals. Results show that for Chinese and English, the brain utilizes bottom-up and top-down parsing strategies separately. These results reveal that the brain adopts parsing strategies with less memory processing load according to different language structures.</abstract>
      <url hash="e070165a">2022.emnlp-main.535</url>
      <bibkey>zhang-etal-2022-brain</bibkey>
    </paper>
    <paper id="536">
      <title><fixed-case>H</fixed-case>ash<fixed-case>F</fixed-case>ormers: Towards Vocabulary-independent Pre-trained Transformers</title>
      <author><first>Huiyin</first><last>Xue</last></author>
      <author><first>Nikolaos</first><last>Aletras</last></author>
      <pages>7862-7874</pages>
      <abstract>Transformer-based pre-trained language models are vocabulary-dependent, mapping by default each token to its corresponding embedding. This one-to-one mapping results into embedding matrices that occupy a lot of memory (i.e. millions of parameters) and grow linearly with the size of the vocabulary. Previous work on on-device transformers dynamically generate token embeddings on-the-fly without embedding matrices using locality-sensitive hashing over morphological information. These embeddings are subsequently fed into transformer layers for text classification. However, these methods are not pre-trained. Inspired by this line of work, we propose HashFormers, a new family of vocabulary-independent pre-trained transformers that support an unlimited vocabulary (i.e. all possible tokens in a corpus) given a substantially smaller fixed-sized embedding matrix. We achieve this by first introducing computationally cheap hashing functions that bucket together individual tokens to embeddings. We also propose three variants that do not require an embedding matrix at all, further reducing the memory requirements. We empirically demonstrate that HashFormers are more memory efficient compared to standard pre-trained transformers while achieving comparable predictive performance when fine-tuned on multiple text classification tasks. For example, our most efficient HashFormer variant has a negligible performance degradation (0.4% on GLUE) using only 99.1K parameters for representing the embeddings compared to 12.3-38M parameters of state-of-the-art models.</abstract>
      <url hash="94da5570">2022.emnlp-main.536</url>
      <bibkey>xue-aletras-2022-hashformers</bibkey>
    </paper>
    <paper id="537">
      <title><fixed-case>M</fixed-case>atch<fixed-case>P</fixed-case>rompt: Prompt-based Open Relation Extraction with Semantic Consistency Guided Clustering</title>
      <author><first>Jiaxin</first><last>Wang</last></author>
      <author><first>Lingling</first><last>Zhang</last></author>
      <author><first>Jun</first><last>Liu</last></author>
      <author><first>Xi</first><last>Liang</last></author>
      <author><first>Yujie</first><last>Zhong</last></author>
      <author><first>Yaqiang</first><last>Wu</last></author>
      <pages>7875-7888</pages>
      <abstract>Relation clustering is a general approach for open relation extraction (OpenRE). Current methods have two major problems. One is that their good performance relies on large amounts of labeled and pre-defined relational instances for pre-training, which are costly to acquire in reality. The other is that they only focus on learning a high-dimensional metric space to measure the similarity of novel relations and ignore the specific relational representations of clusters. In this work, we propose a new prompt-based framework named MatchPrompt, which can realize OpenRE with efficient knowledge transfer from only a few pre-defined relational instances as well as mine the specific meanings for cluster interpretability. To our best knowledge, we are the first to introduce a prompt-based framework for unlabeled clustering. Experimental results on different datasets show that MatchPrompt achieves the new SOTA results for OpenRE.</abstract>
      <url hash="5c66a0eb">2022.emnlp-main.537</url>
      <bibkey>wang-etal-2022-matchprompt</bibkey>
    </paper>
    <paper id="538">
      <title>Improving Aspect Sentiment Quad Prediction via Template-Order Data Augmentation</title>
      <author><first>Mengting</first><last>Hu</last></author>
      <author><first>Yike</first><last>Wu</last></author>
      <author><first>Hang</first><last>Gao</last></author>
      <author><first>Yinhao</first><last>Bai</last></author>
      <author><first>Shiwan</first><last>Zhao</last></author>
      <pages>7889-7900</pages>
      <abstract>Recently, aspect sentiment quad prediction (ASQP) has become a popular task in the field of aspect-level sentiment analysis. Previous work utilizes a predefined template to paraphrase the original sentence into a structure target sequence, which can be easily decoded as quadruplets of the form (aspect category, aspect term, opinion term, sentiment polarity). The template involves the four elements in a fixed order. However, we observe that this solution contradicts with the order-free property of the ASQP task, since there is no need to fix the template order as long as the quadruplet is extracted correctly. Inspired by the observation, we study the effects of template orders and find that some orders help the generative model achieve better performance. It is hypothesized that different orders provide various views of the quadruplet. Therefore, we propose a simple but effective method to identify the most proper orders, and further combine multiple proper templates as data augmentation to improve the ASQP task. Specifically, we use the pre-trained language model to select the orders with minimal entropy. By fine-tuning the pre-trained language model with these template orders, our approach improves the performance of quad prediction, and outperforms state-of-the-art methods significantly in low-resource settings.</abstract>
      <url hash="c3d73f30">2022.emnlp-main.538</url>
      <bibkey>hu-etal-2022-improving-aspect</bibkey>
    </paper>
    <paper id="539">
      <title><fixed-case>S</fixed-case>ocio<fixed-case>P</fixed-case>robe: What, When, and Where Language Models Learn about Sociodemographics</title>
      <author><first>Anne</first><last>Lauscher</last></author>
      <author><first>Federico</first><last>Bianchi</last></author>
      <author><first>Samuel R.</first><last>Bowman</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <pages>7901-7918</pages>
      <abstract>Pre-trained language models (PLMs) have outperformed other NLP models on a wide range of tasks. Opting for a more thorough understanding of their capabilities and inner workings, researchers have established the extend to which they capture lower-level knowledge like grammaticality, and mid-level semantic knowledge like factual understanding. However, there is still little understanding of their knowledge of higher-level aspects of language. In particular, despite the importance of sociodemographic aspects in shaping our language, the questions of whether, where, and how PLMs encode these aspects, e.g., gender or age, is still unexplored. We address this research gap by probing the sociodemographic knowledge of different single-GPU PLMs on multiple English data sets via traditional classifier probing and information-theoretic minimum description length probing. Our results show that PLMs do encode these sociodemographics, and that this knowledge is sometimes spread across the layers of some of the tested PLMs. We further conduct a multilingual analysis and investigate the effect of supplementary training to further explore to what extent, where, and with what amount of pre-training data the knowledge is encoded. Our overall results indicate that sociodemographic knowledge is still a major challenge for NLP. PLMs require large amounts of pre-training data to acquire the knowledge and models that excel in general language understanding do not seem to own more knowledge about these aspects.</abstract>
      <url hash="1a241e6d">2022.emnlp-main.539</url>
      <bibkey>lauscher-etal-2022-socioprobe</bibkey>
    </paper>
    <paper id="540">
      <title>When does Parameter-Efficient Transfer Learning Work for Machine Translation?</title>
      <author><first>Ahmet</first><last>Üstün</last></author>
      <author><first>Asa</first><last>Cooper Stickland</last></author>
      <pages>7919-7933</pages>
      <abstract>Parameter-efficient fine-tuning methods (PEFTs) offer the promise of adapting large pre-trained models while only tuning a small number of parameters. They have been shown to be competitive with full model fine-tuning for many downstream tasks. However, prior work indicates that PEFTs may not work as well for machine translation (MT), and there is no comprehensive study showing when PEFTs work for MT. We conduct a comprehensive empirical study of PEFTs for MT, considering (1) various parameter budgets, (2) a diverse set of language-pairs, and (3) different pre-trained models. We find that ‘adapters’, in which small feed-forward networks are added after every layer, are indeed on par with full model fine-tuning when the parameter budget corresponds to 10% of total model parameters. Nevertheless, as the number of tuned parameters decreases, the performance of PEFTs decreases. The magnitude of this decrease depends on the language pair, with PEFTs particularly struggling for distantly related language-pairs. We find that using PEFTs with a larger pre-trained model outperforms full fine-tuning with a smaller model, and for smaller training data sizes, PEFTs outperform full fine-tuning for the same pre-trained model.</abstract>
      <url hash="4895c816">2022.emnlp-main.540</url>
      <bibkey>ustun-cooper-stickland-2022-parameter</bibkey>
    </paper>
    <paper id="541">
      <title>Hyper-<fixed-case>X</fixed-case>: A Unified Hypernetwork for Multi-Task Multilingual Transfer</title>
      <author><first>Ahmet</first><last>Üstün</last></author>
      <author><first>Arianna</first><last>Bisazza</last></author>
      <author><first>Gosse</first><last>Bouma</last></author>
      <author><first>Gertjan</first><last>van Noord</last></author>
      <author><first>Sebastian</first><last>Ruder</last></author>
      <pages>7934-7949</pages>
      <abstract>Massively multilingual models are promising for transfer learning across tasks and languages. However, existing methods are unable to fully leverage training data when it is available in different task-language combinations. To exploit such heterogeneous supervision, we propose Hyper-X, a single hypernetwork that unifies multi-task and multilingual learning with efficient adaptation. It generates weights for adapter modules conditioned on both tasks and language embeddings. By learning to combine task and language-specific knowledge, our model enables zero-shot transfer for unseen languages and task-language combinations. Our experiments on a diverse set of languages demonstrate that Hyper-X achieves the best or competitive gain when a mixture of multiple resources is available, while on par with strong baseline in the standard scenario. Hyper-X is also considerably more efficient in terms of parameters and resources compared to methods that train separate adapters. Finally, Hyper-X consistently produces strong results in few-shot scenarios for new languages, showing the versatility of our approach beyond zero-shot transfer.</abstract>
      <url hash="de360359">2022.emnlp-main.541</url>
      <bibkey>ustun-etal-2022-hyper</bibkey>
    </paper>
    <paper id="542">
      <title>Towards Robust Numerical Question Answering: Diagnosing Numerical Capabilities of <fixed-case>NLP</fixed-case> Systems</title>
      <author><first>Jialiang</first><last>Xu</last></author>
      <author><first>Mengyu</first><last>Zhou</last></author>
      <author><first>Xinyi</first><last>He</last></author>
      <author><first>Shi</first><last>Han</last></author>
      <author><first>Dongmei</first><last>Zhang</last></author>
      <pages>7950-7966</pages>
      <abstract>Numerical Question Answering is the task of answering questions that require numerical capabilities. Previous works introduce general adversarial attacks to Numerical Question Answering, while not systematically exploring numerical capabilities specific to the topic. In this paper, we propose to conduct numerical capability diagnosis on a series of Numerical Question Answering systems and datasets. A series of numerical capabilities are highlighted, and corresponding dataset perturbations are designed. Empirical results indicate that existing systems are severely challenged by these perturbations. E.g., Graph2Tree experienced a 53.83% absolute accuracy drop against the “Extra” perturbation on ASDiv-a, and BART experienced 13.80% accuracy drop against the “Language” perturbation on the numerical subset of DROP. As a counteracting approach, we also investigate the effectiveness of applying perturbations as data augmentation to relieve systems’ lack of robust numerical capabilities. With experiment analysis and empirical studies, it is demonstrated that Numerical Question Answering with robust numerical capabilities is still to a large extent an open question. We discuss future directions of Numerical Question Answering and summarize guidelines on future dataset collection and system design.</abstract>
      <url hash="dacd24b0">2022.emnlp-main.542</url>
      <bibkey>xu-etal-2022-towards-robust</bibkey>
    </paper>
    <paper id="543">
      <title>Enhancing Joint Multiple Intent Detection and Slot Filling with Global Intent-Slot Co-occurrence</title>
      <author><first>Mengxiao</first><last>Song</last></author>
      <author><first>Bowen</first><last>Yu</last></author>
      <author><first>Li</first><last>Quangang</last></author>
      <author><first>Wang</first><last>Yubin</last></author>
      <author><first>Tingwen</first><last>Liu</last></author>
      <author><first>Hongbo</first><last>Xu</last></author>
      <pages>7967-7977</pages>
      <abstract>Multi-intent detection and slot filling joint model attracts more and more attention since it can handle multi-intent utterances, which is closer to complex real-world scenarios. Most existing joint models rely entirely on the training procedure to obtain the implicit correlation between intents and slots. However, they ignore the fact that leveraging the rich global knowledge in the corpus can determine the intuitive and explicit correlation between intents and slots. In this paper, we aim to make full use of the statistical co-occurrence frequency between intents and slots as prior knowledge to enhance joint multiple intent detection and slot filling. To be specific, an intent-slot co-occurrence graph is constructed based on the entire training corpus to globally discover correlation between intents and slots. Based on the global intent-slot co-occurrence, we propose a novel graph neural network to model the interaction between the two subtasks. Experimental results on two public multi-intent datasets demonstrate that our approach outperforms the state-of-the-art models.</abstract>
      <url hash="2d192c4d">2022.emnlp-main.543</url>
      <bibkey>song-etal-2022-enhancing</bibkey>
    </paper>
    <paper id="544">
      <title>Towards Pragmatic Production Strategies for Natural Language Generation Tasks</title>
      <author><first>Mario</first><last>Giulianelli</last></author>
      <pages>7978-7984</pages>
      <abstract>This position paper proposes a conceptual framework for the design of Natural Language Generation (NLG) systems that follow efficient and effective production strategies in order to achieve complex communicative goals. In this general framework, efficiency is characterised as the parsimonious regulation of production and comprehension costs while effectiveness is measured with respect to task-oriented and contextually grounded communicative goals. We provide concrete suggestions for the estimation of goals, costs, and utility via modern statistical methods, demonstrating applications of our framework to the classic pragmatic task of visually grounded referential games and to abstractive text summarisation, two popular generation tasks with real-world applications. In sum, we advocate for the development of NLG systems that learn to make pragmatic production decisions from experience, by reasoning about goals, costs, and utility in a human-like way.</abstract>
      <url hash="e11a7e68">2022.emnlp-main.544</url>
      <bibkey>giulianelli-2022-towards</bibkey>
    </paper>
    <paper id="545">
      <title><fixed-case>L</fixed-case>ite<fixed-case>VL</fixed-case>: Efficient Video-Language Learning with Enhanced Spatial-Temporal Modeling</title>
      <author><first>Dongsheng</first><last>Chen</last></author>
      <author><first>Chaofan</first><last>Tao</last></author>
      <author><first>Lu</first><last>Hou</last></author>
      <author><first>Lifeng</first><last>Shang</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>7985-7997</pages>
      <abstract>Recent large-scale video-language pre-trained models have shown appealing performance on various downstream tasks. However, the pre-training process is computationally expensive due to the requirement of millions of video-text pairs and the redundant data structure of each video. To mitigate these problems, we propose LiteVL, which adapts a pre-trained image-language model BLIP into a video-text model directly on downstream tasks, without heavy pre-training. To enhance the temporal modeling lacking in the image-language model, we propose to add temporal attention modules in the image encoder of BLIP with dynamic temporal scaling. Besides the model-wise adaptation, we also propose a non-parametric pooling mechanism to adaptively reweight the fine-grained video embedding conditioned on the text. Experimental results on text-video retrieval and video question answering show that the proposed LiteVL even outperforms previous video-language pre-trained models by a clear margin, though without any video-language pre-training.</abstract>
      <url hash="1a48fce5">2022.emnlp-main.545</url>
      <bibkey>chen-etal-2022-litevl</bibkey>
    </paper>
    <paper id="546">
      <title>Communication breakdown: On the low mutual intelligibility between human and neural captioning</title>
      <author><first>Roberto</first><last>Dessì</last></author>
      <author><first>Eleonora</first><last>Gualdoni</last></author>
      <author><first>Francesca</first><last>Franzon</last></author>
      <author><first>Gemma</first><last>Boleda</last></author>
      <author><first>Marco</first><last>Baroni</last></author>
      <pages>7998-8007</pages>
      <abstract>We compare the 0-shot performance of a neural caption-based image retriever when given as input either human-produced captions or captions generated by a neural captioner. We conduct this comparison on the recently introduced ImageCoDe data-set (Krojer et al. 2022), which contains hard distractors nearly identical to the images to be retrieved. We find that the neural retriever has much higher performance when fed neural rather than human captions, despite the fact that the former, unlike the latter, were generated without awareness of the distractors that make the task hard. Even more remarkably, when the same neural captions are given to human subjects, their retrieval performance is almost at chance level. Our results thus add to the growing body of evidence that, even when the “language” of neural models resembles English, this superficial resemblance might be deeply misleading.</abstract>
      <url hash="4ef876ba">2022.emnlp-main.546</url>
      <bibkey>dessi-etal-2022-communication</bibkey>
    </paper>
    <paper id="547">
      <title>Normalizing Mutual Information for Robust Adaptive Training for Translation</title>
      <author><first>Youngwon</first><last>Lee</last></author>
      <author><first>Changmin</first><last>Lee</last></author>
      <author><first>Hojin</first><last>Lee</last></author>
      <author><first>Seung-won</first><last>Hwang</last></author>
      <pages>8008-8015</pages>
      <abstract>Despite the success of neural machine translation models, tensions between fluency of optimizing target language modeling and source-faithfulness remain as challenges. Previously, Conditional Bilingual Mutual Information (CBMI), a scoring metric for the importance of target sentences and tokens, was proposed to encourage fluent and faithful translations. The score is obtained by combining the probability from the translation model and the target language model, which is then used to assign different weights to losses from sentences and tokens. Meanwhile, we argue this metric is not properly normalized, for which we propose Normalized Pointwise Mutual Information (NPMI). NPMI utilizes an additional language model on source language to approximate the joint likelihood of source-target pair and the likelihood of the source, which is then used for normalizing the score. We showed that NPMI better captures the dependence between source-target and that NPMI-based token-level adaptive training brings improvements over baselines with empirical results from En-De, De-En, and En-Ro translation tasks.</abstract>
      <url hash="54cedfd3">2022.emnlp-main.547</url>
      <bibkey>lee-etal-2022-normalizing</bibkey>
    </paper>
    <paper id="548">
      <title>Bilingual Synchronization: Restoring Translational Relationships with Editing Operations</title>
      <author><first>Jitao</first><last>Xu</last></author>
      <author><first>Josep</first><last>Crego</last></author>
      <author><first>François</first><last>Yvon</last></author>
      <pages>8016-8030</pages>
      <abstract>Machine Translation (MT) is usually viewed as a one-shot process that generates the target language equivalent of some source text from scratch. We consider here a more general setting which assumes an initial target sequence, that must be transformed into a valid translation of the source, thereby restoring parallelism between source and target. For this bilingual synchronization task, we consider several architectures (both autoregressive and non-autoregressive) and training regimes, and experiment with multiple practical settings such as simulated interactive MT, translating with Translation Memory (TM) and TM cleaning. Our results suggest that one single generic edit-based system, once fine-tuned, can compare with, or even outperform, dedicated systems specifically trained for these tasks.</abstract>
      <url hash="76380ec7">2022.emnlp-main.548</url>
      <bibkey>xu-etal-2022-bilingual</bibkey>
    </paper>
    <paper id="549">
      <title>Human-Machine Collaboration Approaches to Build a Dialogue Dataset for Hate Speech Countering</title>
      <author><first>Helena</first><last>Bonaldi</last></author>
      <author><first>Sara</first><last>Dellantonio</last></author>
      <author><first>Serra Sinem</first><last>Tekiroğlu</last></author>
      <author><first>Marco</first><last>Guerini</last></author>
      <pages>8031-8049</pages>
      <abstract>Fighting online hate speech is a challenge that is usually addressed using Natural Language Processing via automatic detection and removal of hate content. Besides this approach, counter narratives have emerged as an effective tool employed by NGOs to respond to online hate on social media platforms. For this reason, Natural Language Generation is currently being studied as a way to automatize counter narrative writing. However, the existing resources necessary to train NLG models are limited to 2-turn interactions (a hate speech and a counter narrative as response), while in real life, interactions can consist of multiple turns. In this paper, we present a hybrid approach for dialogical data collection, which combines the intervention of human expert annotators over machine generated dialogues obtained using 19 different configurations. The result of this work is DIALOCONAN, the first dataset comprising over 3000 fictitious multi-turn dialogues between a hater and an NGO operator, covering 6 targets of hate.</abstract>
      <url hash="9f42d517">2022.emnlp-main.549</url>
      <bibkey>bonaldi-etal-2022-human</bibkey>
    </paper>
    <paper id="550">
      <title><fixed-case>JANUS</fixed-case>: Joint Autoregressive and Non-autoregressive Training with Auxiliary Loss for Sequence Generation</title>
      <author><first>Xiaobo</first><last>Liang</last></author>
      <author><first>Lijun</first><last>Wu</last></author>
      <author><first>Juntao</first><last>Li</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>8050-8060</pages>
      <abstract>Transformer-based autoregressive and non-autoregressive models have played an essential role in sequence generation tasks. The autoregressive model can obtain excellent performance, while the non-autoregressive model brings fast decoding speed for inference. In this paper, we propose <b>JANUS</b>, a <b>J</b>oint <b>A</b>utoregressive and <b>N</b>on-autoregressive training method using a<b>U</b>xiliary los<b>S</b> to enhance the model performance in both AR and NAR manner simultaneously and effectively alleviate the problem of distribution discrepancy.Further, we pre-train BART with JANUS on a large corpus with minimal cost (16 GPU days) and make the BART-JANUS capable of non-autoregressive generation, demonstrating that our approach can transfer the AR knowledge to NAR. Empirically, we show our approach and BART-JANUS can achieve significant improvement on multiple generation tasks, including machine translation and GLGE benchmarks. Our code is available at Github.</abstract>
      <url hash="8d6abdb9">2022.emnlp-main.550</url>
      <bibkey>liang-etal-2022-janus</bibkey>
    </paper>
    <paper id="551">
      <title>Entity-Focused Dense Passage Retrieval for Outside-Knowledge Visual Question Answering</title>
      <author><first>Jialin</first><last>Wu</last></author>
      <author><first>Raymond</first><last>Mooney</last></author>
      <pages>8061-8072</pages>
      <abstract>Most Outside-Knowledge Visual Question Answering (OK-VQA) systems employ a two-stage framework that first retrieves external knowledge given the visual question and then predicts the answer based on the retrieved content. However, the retrieved knowledge is often inadequate. Retrievals are frequently too general and fail to cover specific knowledge needed to answer the question. Also, the naturally available supervision (whether the passage contains the correct answer) is weak and does not guarantee question relevancy. To address these issues, we propose an Entity-Focused Retrieval (EnFoRe) model that provides stronger supervision during training and recognizes question-relevant entities to help retrieve more specific knowledge. Experiments show that our EnFoRe model achieves superior retrieval performance on OK-VQA, the currently largest outside-knowledge VQA dataset. We also combine the retrieved knowledge with state-of-the-art VQA models, and achieve a new state-of-the-art performance on OK-VQA.</abstract>
      <url hash="9baf92d5">2022.emnlp-main.551</url>
      <bibkey>wu-mooney-2022-entity</bibkey>
    </paper>
    <paper id="552">
      <title>Cross-Linguistic Syntactic Difference in Multilingual <fixed-case>BERT</fixed-case>: How Good is It and How Does It Affect Transfer?</title>
      <author><first>Ningyu</first><last>Xu</last></author>
      <author><first>Tao</first><last>Gui</last></author>
      <author><first>Ruotian</first><last>Ma</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Jingting</first><last>Ye</last></author>
      <author><first>Menghan</first><last>Zhang</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <pages>8073-8092</pages>
      <abstract>Multilingual BERT (mBERT) has demonstrated considerable cross-lingual syntactic ability, whereby it enables effective zero-shot cross-lingual transfer of syntactic knowledge. The transfer is more successful between some languages, but it is not well understood what leads to this variation and whether it fairly reflects difference between languages. In this work, we investigate the distributions of grammatical relations induced from mBERT in the context of 24 typologically different languages. We demonstrate that the distance between the distributions of different languages is highly consistent with the syntactic difference in terms of linguistic formalisms. Such difference learnt via self-supervision plays a crucial role in the zero-shot transfer performance and can be predicted by variation in morphosyntactic properties between languages. These results suggest that mBERT properly encodes languages in a way consistent with linguistic diversity and provide insights into the mechanism of cross-lingual transfer.</abstract>
      <url hash="a897b77a">2022.emnlp-main.552</url>
      <bibkey>xu-etal-2022-cross</bibkey>
    </paper>
    <paper id="553">
      <title>“It’s Not Just Hate”: A Multi-Dimensional Perspective on Detecting Harmful Speech Online</title>
      <author><first>Federico</first><last>Bianchi</last></author>
      <author><first>Stefanie</first><last>HIlls</last></author>
      <author><first>Patricia</first><last>Rossini</last></author>
      <author><first>Dirk</first><last>Hovy</last></author>
      <author><first>Rebekah</first><last>Tromble</last></author>
      <author><first>Nava</first><last>Tintarev</last></author>
      <pages>8093-8099</pages>
      <abstract>Well-annotated data is a prerequisite for good Natural Language Processing models. Too often, though, annotation decisions are governed by optimizing time or annotator agreement. We make a case for nuanced efforts in an interdisciplinary setting for annotating offensive online speech. Detecting offensive content is rapidly becoming one of the most important real-world NLP tasks. However, most datasets use a single binary label, e.g., for hate or incivility, even though each concept is multi-faceted. This modeling choice severely limits nuanced insights, but also performance.We show that a more fine-grained multi-label approach to predicting incivility and hateful or intolerant content addresses both conceptual and performance issues.We release a novel dataset of over 40,000 tweets about immigration from the US and UK, annotated with six labels for different aspects of incivility and intolerance.Our dataset not only allows for a more nuanced understanding of harmful speech online, models trained on it also outperform or match performance on benchmark datasets</abstract>
      <url hash="88319f71">2022.emnlp-main.553</url>
      <bibkey>bianchi-etal-2022-just</bibkey>
    </paper>
    <paper id="554">
      <title>Long Text Generation with Topic-aware Discrete Latent Variable Model</title>
      <author><first>Erguang</first><last>Yang</last></author>
      <author><first>Mingtong</first><last>Liu</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <author><first>Yujie</first><last>Zhang</last></author>
      <author><first>Yufeng</first><last>Chen</last></author>
      <author><first>Jinan</first><last>Xu</last></author>
      <pages>8100-8107</pages>
      <abstract>Generating coherent long texts is an important yet challenging task, particularly forthe open-ended generation. Prior work based on discrete latent codes focuses on the modeling of discourse relation, resulting in discrete codes only learning shallow semantics (Ji and Huang, 2021). A natural text always revolves around several related topics and the transition across them is natural and smooth.In this work, we investigate whether discrete latent codes can learn information of topics. To this end, we build a topic-aware latent code-guided text generation model. To encourage discrete codes to model information about topics, we propose a span-level bag-of-words training objective for the model. Automatic and manual evaluation experiments show that our method can generate more topic-relevant and coherent texts.</abstract>
      <url hash="70027e65">2022.emnlp-main.554</url>
      <bibkey>yang-etal-2022-long</bibkey>
    </paper>
    <paper id="555">
      <title><fixed-case>TIARA</fixed-case>: Multi-grained Retrieval for Robust Question Answering over Large Knowledge Base</title>
      <author><first>Yiheng</first><last>Shu</last></author>
      <author><first>Zhiwei</first><last>Yu</last></author>
      <author><first>Yuhan</first><last>Li</last></author>
      <author><first>Börje</first><last>Karlsson</last></author>
      <author><first>Tingting</first><last>Ma</last></author>
      <author><first>Yuzhong</first><last>Qu</last></author>
      <author><first>Chin-Yew</first><last>Lin</last></author>
      <pages>8108-8121</pages>
      <abstract>Pre-trained language models (PLMs) have shown their effectiveness in multiple scenarios. However, KBQA remains challenging, especially regarding coverage and generalization settings. This is due to two main factors: i) understanding the semantics of both questions and relevant knowledge from the KB; ii) generating executable logical forms with both semantic and syntactic correctness. In this paper, we present a new KBQA model, TIARA, which addresses those issues by applying multi-grained retrieval to help the PLM focus on the most relevant KB context, viz., entities, exemplary logical forms, and schema items. Moreover, constrained decoding is used to control the output space and reduce generation errors. Experiments over important benchmarks demonstrate the effectiveness of our approach. TIARA outperforms previous SOTA, including those using PLMs or oracle entity annotations, by at least 4.1 and 1.1 F1 points on GrailQA and WebQuestionsSP, respectively. Specifically on GrailQA, TIARA outperforms previous models in all categories, with an improvement of 4.7 F1 points in zero-shot generalization.</abstract>
      <url hash="eec8c102">2022.emnlp-main.555</url>
      <bibkey>shu-etal-2022-tiara</bibkey>
    </paper>
    <paper id="556">
      <title>Structure-Unified <fixed-case>M</fixed-case>-Tree Coding Solver for Math Word Problem</title>
      <author><first>Bin</first><last>Wang</last></author>
      <author><first>Jiangzhou</first><last>Ju</last></author>
      <author><first>Yang</first><last>Fan</last></author>
      <author><first>Xinyu</first><last>Dai</last></author>
      <author><first>Shujian</first><last>Huang</last></author>
      <author><first>Jiajun</first><last>Chen</last></author>
      <pages>8122-8132</pages>
      <abstract>As one of the challenging NLP tasks, designing math word problem (MWP) solvers has attracted increasing research attention for the past few years. In previous work, models designed by taking into account the properties of the binary tree structure of mathematical expressions at the output side have achieved better performance. However, the expressions corresponding to a MWP are often diverse (e.g., <tex-math>n_1+n_2 \times n_3-n_4</tex-math>, <tex-math>n_3\times n_2-n_4+n_1</tex-math>, etc.), and so are the corresponding binary trees, which creates difficulties in model learning due to the non-deterministic output space. In this paper, we propose the Structure-Unified M-Tree Coding Solver (SUMC-Solver), which applies a tree with any M branches (M-tree) to unify the output structures. To learn the M-tree, we use a mapping to convert the M-tree into the M-tree codes, where codes store the information of the paths from tree root to leaf nodes and the information of leaf nodes themselves, and then devise a Sequence-to-Code (seq2code) model to generate the codes. Experimental results on the widely used MAWPS and Math23K datasets have demonstrated that SUMC-Solver not only outperforms several state-of-the-art models under similar experimental settings but also performs much better under low-resource conditions.</abstract>
      <url hash="8c8feb15">2022.emnlp-main.556</url>
      <bibkey>wang-etal-2022-structure</bibkey>
    </paper>
    <paper id="557">
      <title><fixed-case>F</fixed-case>orm<fixed-case>LM</fixed-case>: Recommending Creation Ideas for Online Forms by Modelling Semantic and Structural Information</title>
      <author><first>Yijia</first><last>Shao</last></author>
      <author><first>Mengyu</first><last>Zhou</last></author>
      <author><first>Yifan</first><last>Zhong</last></author>
      <author><first>Tao</first><last>Wu</last></author>
      <author><first>Hongwei</first><last>Han</last></author>
      <author><first>Shi</first><last>Han</last></author>
      <author><first>Gideon</first><last>Huang</last></author>
      <author><first>Dongmei</first><last>Zhang</last></author>
      <pages>8133-8149</pages>
      <abstract>Online forms are widely used to collect data from human and have a multi-billion market. Many software products provide online services for creating semi-structured forms where questions and descriptions are organized by predefined structures. However, the design and creation process of forms is still tedious and requires expert knowledge. To assist form designers, in this work we present FormLM to model online forms (by enhancing pre-trained language model with form structural information) and recommend form creation ideas (including question / options recommendations and block type suggestion). For model training and evaluation, we collect the first public online form dataset with 62K online forms. Experiment results show that FormLM significantly outperforms general-purpose language models on all tasks, with an improvement by 4.71 on Question Recommendation and 10.6 on Block Type Suggestion in terms of ROUGE-1 and Macro-F1, respectively.</abstract>
      <url hash="16633486">2022.emnlp-main.557</url>
      <bibkey>shao-etal-2022-formlm</bibkey>
    </paper>
    <paper id="558">
      <title>Generate, Discriminate and Contrast: A Semi-Supervised Sentence Representation Learning Framework</title>
      <author><first>Yiming</first><last>Chen</last></author>
      <author><first>Yan</first><last>Zhang</last></author>
      <author><first>Bin</first><last>Wang</last></author>
      <author><first>Zuozhu</first><last>Liu</last></author>
      <author><first>Haizhou</first><last>Li</last></author>
      <pages>8150-8161</pages>
      <abstract>Most sentence embedding techniques heavily rely on expensive human-annotated sentence pairs as the supervised signals. Despite the use of large-scale unlabeled data, the performance of unsupervised methods typically lags far behind that of the supervised counterparts in most downstream tasks. In this work, we propose a semi-supervised sentence embedding framework, GenSE, that effectively leverages large-scale unlabeled data. Our method include three parts: 1) Generate: A generator/discriminator model is jointly trained to synthesize sentence pairs from open-domain unlabeled corpus; 2) Discriminate: Noisy sentence pairs are filtered out by the discriminator to acquire high-quality positive and negative sentence pairs; 3) Contrast: A prompt-based contrastive approach is presented for sentence representation learning with both annotated and synthesized data. Comprehensive experiments show that GenSE achieves an average correlation score of 85.19 on the STS datasets and consistent performance improvement on four domain adaptation tasks, significantly surpassing the state-of-the-art methods and convincingly corroborating its effectiveness and generalization ability.</abstract>
      <url hash="4392fad8">2022.emnlp-main.558</url>
      <bibkey>chen-etal-2022-generate</bibkey>
    </paper>
    <paper id="559">
      <title><fixed-case>GPS</fixed-case>: Genetic Prompt Search for Efficient Few-Shot Learning</title>
      <author><first>Hanwei</first><last>Xu</last></author>
      <author><first>Yujun</first><last>Chen</last></author>
      <author><first>Yulun</first><last>Du</last></author>
      <author><first>Nan</first><last>Shao</last></author>
      <author><first>Wang</first><last>Yanggang</last></author>
      <author><first>Haiyu</first><last>Li</last></author>
      <author><first>Zhilin</first><last>Yang</last></author>
      <pages>8162-8171</pages>
      <abstract>Prompt-based techniques have demostrated great potential for improving the few-shot generalization of pretrained language models. However, their performance heavily relies on the manual design of prompts and thus requiring a lot of human efforts. In this paper, we introduce Genetic Prompt Search (GPS) to improve few-shot learning with prompts, which utilizes a genetic algorithm to automatically search for the best prompt.GPS is gradient-free and requires no update of model parameters but only a small validation set. Experiments on diverse datasets proved the effectiveness of GPS, which outperforms manual prompts by a large margin of 2.6 points. Our method is also better than other parameter-efficient tuning methods such as prompt tuning.</abstract>
      <url hash="f95cdd87">2022.emnlp-main.559</url>
      <bibkey>xu-etal-2022-gps</bibkey>
    </paper>
    <paper id="560">
      <title>Multitask Instruction-based Prompting for Fallacy Recognition</title>
      <author><first>Tariq</first><last>Alhindi</last></author>
      <author><first>Tuhin</first><last>Chakrabarty</last></author>
      <author><first>Elena</first><last>Musi</last></author>
      <author><first>Smaranda</first><last>Muresan</last></author>
      <pages>8172-8187</pages>
      <abstract>Fallacies are used as seemingly valid arguments to support a position and persuade the audience about its validity. Recognizing fallacies is an intrinsically difficult task both for humans and machines. Moreover, a big challenge for computational models lies in the fact that fallacies are formulated differently across the datasets with differences in the input format (e.g., question-answer pair, sentence with fallacy fragment), genre (e.g., social media, dialogue, news), as well as types and number of fallacies (from 5 to 18 types per dataset). To move towards solving the fallacy recognition task, we approach these differences across datasets as multiple tasks and show how instruction-based prompting in a multitask setup based on the T5 model improves the results against approaches built for a specific dataset such as T5, BERT or GPT-3. We show the ability of this multitask prompting approach to recognize 28 unique fallacies across domains and genres and study the effect of model size and prompt choice by analyzing the per-class (i.e., fallacy type) results. Finally, we analyze the effect of annotation quality on model performance, and the feasibility of complementing this approach with external knowledge.</abstract>
      <url hash="27485143">2022.emnlp-main.560</url>
      <bibkey>alhindi-etal-2022-multitask</bibkey>
    </paper>
    <paper id="561">
      <title>Rethinking Multi-Modal Alignment in Multi-Choice <fixed-case>V</fixed-case>ideo<fixed-case>QA</fixed-case> from Feature and Sample Perspectives</title>
      <author><first>Shaoning</first><last>Xiao</last></author>
      <author><first>Long</first><last>Chen</last></author>
      <author><first>Kaifeng</first><last>Gao</last></author>
      <author><first>Zhao</first><last>Wang</last></author>
      <author><first>Yi</first><last>Yang</last></author>
      <author><first>Zhimeng</first><last>Zhang</last></author>
      <author><first>Jun</first><last>Xiao</last></author>
      <pages>8188-8198</pages>
      <abstract>Reasoning about causal and temporal event relations in videos is a new destination of Video Question Answering (VideoQA). The major stumbling block to achieve this purpose is the semantic gap between language and video since they are at different levels of abstraction. Existing efforts mainly focus on designing sophisticated architectures while utilizing frame- or object-level visual representations. In this paper, we reconsider the multi-modal alignment problem in VideoQA from feature and sample perspectives to achieve better performance. From the view of feature, we break down the video into trajectories and first leverage trajectory feature in VideoQA to enhance the alignment between two modalities. Moreover, we adopt a heterogeneous graph architecture and design a hierarchical framework to align both trajectory-level and frame-level visual feature with language feature. In addition, we found that VideoQA models are largely dependent on languagepriors and always neglect visual-language interactions. Thus, two effective yet portable training augmentation strategies are designed to strengthen the cross-modal correspondence ability of our model from the view of sample. Extensive results show that our method outperforms all the state-of the-art models on the challenging NExT-QA benchmark.</abstract>
      <url hash="b5b00e10">2022.emnlp-main.561</url>
      <bibkey>xiao-etal-2022-rethinking</bibkey>
    </paper>
    <paper id="562">
      <title>Towards Table-to-Text Generation with Pretrained Language Model: A Table Structure Understanding and Text Deliberating Approach</title>
      <author><first>Miao</first><last>Chen</last></author>
      <author><first>Xinjiang</first><last>Lu</last></author>
      <author><first>Tong</first><last>Xu</last></author>
      <author><first>Yanyan</first><last>Li</last></author>
      <author><first>Zhou</first><last>Jingbo</last></author>
      <author><first>Dejing</first><last>Dou</last></author>
      <author><first>Hui</first><last>Xiong</last></author>
      <pages>8199-8210</pages>
      <abstract>Although remarkable progress on the neural table-to-text methods has been made, the generalization issues hinder the applicability of these models due to the limited source tables. Large-scale pretrained language models sound like a promising solution to tackle such issues. However, how to effectively bridge the gap between the structured table and the text input by fully leveraging table information to fuel the pretrained model is still not well explored. Besides, another challenge of integrating the deliberation mechanism into the text-to-text pretrained model for solving the table-to-text task remains seldom studied. In this paper, to implement the table-to-text generation with pretrained language model, we propose a table structure understanding and text deliberating approach, namely TASD. To be specific, we devise a three-layered multi-head attention network to realize the table-structureaware text generation model with the help of the pretrained language model. Furthermore, a multi-pass decoder framework is adopted to enhance the capability of polishing generated text for table descriptions. The empirical studies, as well as human evaluation, on two public datasets, validate that our approach can generate faithful and fluent descriptive texts for different types of tables.</abstract>
      <url hash="5913d13d">2022.emnlp-main.562</url>
      <bibkey>chen-etal-2022-towards-table</bibkey>
    </paper>
    <paper id="563">
      <title>Hierarchical Phrase-Based Sequence-to-Sequence Learning</title>
      <author><first>Bailin</first><last>Wang</last></author>
      <author><first>Ivan</first><last>Titov</last></author>
      <author><first>Jacob</first><last>Andreas</last></author>
      <author><first>Yoon</first><last>Kim</last></author>
      <pages>8211-8229</pages>
      <abstract>This paper describes a neural transducer that maintains the flexibility of standard sequence-to-sequence (seq2seq) models while incorporating hierarchical phrases as a source of inductive bias during training and as explicit constraints during inference. Our approach trains two models: a discriminative parser based on a bracketing transduction grammar whose derivation tree hierarchically aligns source and target phrases, and a neural seq2seq model that learns to translate the aligned phrases one-by-one. We use the same seq2seq model to translate at all phrase scales, which results in two inference modes: one mode in which the parser is discarded and only the seq2seq component is used at the sequence-level, and another in which the parser is combined with the seq2seq model. Decoding in the latter mode is done with the cube-pruned CKY algorithm, which is more involved but can make use of new translation rules during inference. We formalize our model as a source-conditioned synchronous grammar and develop an efficient variational inference algorithm for training. When applied on top of both randomly initialized and pretrained seq2seq models, we find that it performs well compared to baselines on small scale machine translation benchmarks.</abstract>
      <url hash="7be34feb">2022.emnlp-main.563</url>
      <bibkey>wang-etal-2022-hierarchical-phrase</bibkey>
    </paper>
    <paper id="564">
      <title>Natural Language Deduction with Incomplete Information</title>
      <author><first>Zayne</first><last>Sprague</last></author>
      <author><first>Kaj</first><last>Bostrom</last></author>
      <author><first>Swarat</first><last>Chaudhuri</last></author>
      <author><first>Greg</first><last>Durrett</last></author>
      <pages>8230-8258</pages>
      <abstract>A growing body of work studies how to answer a question or verify a claim by generating a natural language “proof:” a chain of deductive inferences yielding the answer based on a set of premises. However, these methods can only make sound deductions when they follow from evidence that is given. We propose a new system that can handle the underspecified setting where not all premises are stated at the outset; that is, additional assumptions need to be materialized to prove a claim. By using a natural language generation model to abductively infer a premise given another premise and a conclusion, we can impute missing pieces of evidence needed for the conclusion to be true. Our system searches over two fringes in a bidirectional fashion, interleaving deductive (forward-chaining) and abductive (backward-chaining) generation steps. We sample multiple possible outputs for each step to achieve coverage of the search space, at the same time ensuring correctness by filtering low-quality generations with a round-trip validation procedure. Results on a modified version of the EntailmentBank dataset and a new dataset called Everyday Norms: Why Not? Show that abductive generation with validation can recover premises across in- and out-of-domain settings.</abstract>
      <url hash="f4d5d26e">2022.emnlp-main.564</url>
      <bibkey>sprague-etal-2022-natural</bibkey>
    </paper>
    <paper id="565">
      <title>Character-centric Story Visualization via Visual Planning and Token Alignment</title>
      <author><first>Hong</first><last>Chen</last></author>
      <author><first>Rujun</first><last>Han</last></author>
      <author><first>Te-Lin</first><last>Wu</last></author>
      <author><first>Hideki</first><last>Nakayama</last></author>
      <author><first>Nanyun</first><last>Peng</last></author>
      <pages>8259-8272</pages>
      <abstract>Story visualization advances the traditional text-to-image generation by enabling multiple image generation based on a complete story. This task requires machines to 1) understand long text inputs, and 2) produce a globally consistent image sequence that illustrates the contents of the story. A key challenge of consistent story visualization is to preserve characters that are essential in stories. To tackle the challenge, we propose to adapt a recent work that augments VQ-VAE with a text-to-visual-token (transformer) architecture. Specifically, we modify the text-to-visual-token module with a two-stage framework: 1) character token planning model that predicts the visual tokens for characters only; 2) visual token completion model that generates the remaining visual token sequence, which is sent to VQ-VAE for finalizing image generations. To encourage characters to appear in the images, we further train the two-stage framework with a character-token alignment objective. Extensive experiments and evaluations demonstrate that the proposed method excels at preserving characters and can produce higher quality image sequences compared with the strong baselines.</abstract>
      <url hash="c6a9ff20">2022.emnlp-main.565</url>
      <bibkey>chen-etal-2022-character</bibkey>
    </paper>
    <paper id="566">
      <title><fixed-case>ASQA</fixed-case>: Factoid Questions Meet Long-Form Answers</title>
      <author><first>Ivan</first><last>Stelmakh</last></author>
      <author><first>Yi</first><last>Luan</last></author>
      <author><first>Bhuwan</first><last>Dhingra</last></author>
      <author><first>Ming-Wei</first><last>Chang</last></author>
      <pages>8273-8288</pages>
      <abstract>Recent progress on open domain factoid question answering (QA) does not easily transfer to the task of long-form QA, where the goal is to answer questions that require in-depth explanations. The hurdles include a lack of high-quality data and the absence of a well-defined notion of an answer’s quality. In this work, we address these problems by releasing a novel dataset and a task that we call ASQA (Answer Summaries for Questions which are Ambiguous); and proposing a reliable metric for measuring performance on ASQA. Our task focuses on ambiguous factoid questions which have different correct answers depending on the interpretation. Answers to ambiguous questions should combine factual information from multiple sources into a coherent long-form summary that resolves the ambiguity. In contrast to existing long-form QA tasks (such as ELI5), ASQA admits a clear notion of correctness: a user faced with a good summary should be able to answer different interpretations of the original ambiguous question. Our analysis demonstrates an agreement between this metric and human judgments, and reveals a considerable gap between human performance and strong baselines.</abstract>
      <url hash="02767665">2022.emnlp-main.566</url>
      <bibkey>stelmakh-etal-2022-asqa</bibkey>
    </paper>
    <paper id="567">
      <title>Algorithms for Acyclic Weighted Finite-State Automata with Failure Arcs</title>
      <author><first>Anej</first><last>Svete</last></author>
      <author><first>Benjamin</first><last>Dayan</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>Tim</first><last>Vieira</last></author>
      <author><first>Jason</first><last>Eisner</last></author>
      <pages>8289-8305</pages>
      <abstract>Weighted finite-state automata (WSFAs) arecommonly used in NLP. Failure transitions area useful extension for compactly representingbackoffs or interpolation in n-gram modelsand CRFs, which are special cases of WFSAs.Unfortunately, applying standard algorithmsfor computing the pathsum requires expand-ing these compact failure transitions. As aresult, na ̈ıve computation of the pathsum inacyclic WFSAs with failure transitions runs inO(|Q|2|Σ|) (O(|Q||Σ|) for deterministic WF-SAs) while the equivalent algorithm in normalWFSAs runs in O(|E|), where E representsthe set of transitions, Q the set of states, andΣ the alphabet. In this work, we present moreefficient algorithms for computing the pathsumin sparse acyclic WFSAs, i.e., WFSAs with av-erage out symbol fraction s ≪ 1. In those,backward runs in O(s|Q||Σ|). We proposean algorithm for semiring-weighted automatawhich runs in O(|E| + s|Σ||Q||Tmax| log |Σ|),where |Tmax| is the size of the largest con-nected component of failure transitions. Ad-ditionally, we propose faster algorithms fortwo specific cases. For ring-weighted WF-SAs we propose an algorithm with complex-ity O(|E| + s|Σ||Q||πmax|), where |πmax| de-notes the longest path length of failure transi-tions stemming from q and Σ(q) the set of sym-bols on the outgoing transitions from q. Forsemiring-weighted WFSAs whose failure tran-sition topology satisfies a condition exemplifiedby CRFs, we propose an algorithm with com-plexity O(|E| + s|Σ||Q| log |Σ|).</abstract>
      <url hash="0a9c8c6c">2022.emnlp-main.567</url>
      <bibkey>svete-etal-2022-algorithms</bibkey>
    </paper>
    <paper id="568">
      <title>Towards Better Document-level Relation Extraction via Iterative Inference</title>
      <author><first>Liang</first><last>Zhang</last></author>
      <author><first>Jinsong</first><last>Su</last></author>
      <author><first>Yidong</first><last>Chen</last></author>
      <author><first>Zhongjian</first><last>Miao</last></author>
      <author><first>Min</first><last>Zijun</last></author>
      <author><first>Qingguo</first><last>Hu</last></author>
      <author><first>Xiaodong</first><last>Shi</last></author>
      <pages>8306-8317</pages>
      <abstract>Document-level relation extraction (RE) aims to extract the relations between entities from the input document that usually containing many difficultly-predicted entity pairs whose relations can only be predicted through relational inference. Existing methods usually directly predict the relations of all entity pairs of input document in a one-pass manner, ignoring the fact that predictions of some entity pairs heavily depend on the predicted results of other pairs. To deal with this issue, in this paper, we propose a novel document-level RE model with iterative inference. Our model is mainly composed of two modules: 1) a base module expected to provide preliminary relation predictions on entity pairs; 2) an inference module introduced to refine these preliminary predictions by iteratively dealing with difficultly-predicted entity pairs depending on other pairs in an easy-to-hard manner. Unlike previous methods which only consider feature information of entity pairs, our inference module is equipped with two Extended Cross Attention units, allowing it to exploit both feature information and previous predictions of entity pairs during relational inference. Furthermore, we adopt a two-stage strategy to train our model. At the first stage, we only train our base module. During the second stage, we train the whole model, where contrastive learning is introduced to enhance the training of inference module. Experimental results on three commonly-used datasets show that our model consistently outperforms other competitive baselines.</abstract>
      <url hash="a664ca47">2022.emnlp-main.568</url>
      <bibkey>zhang-etal-2022-towards-better</bibkey>
    </paper>
    <paper id="569">
      <title>Efficient Adversarial Training with Robust Early-Bird Tickets</title>
      <author><first>Zhiheng</first><last>Xi</last></author>
      <author><first>Rui</first><last>Zheng</last></author>
      <author><first>Tao</first><last>Gui</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <pages>8318-8331</pages>
      <abstract>Adversarial training is one of the most powerful methods to improve the robustness of pre-trained language models (PLMs). However, this approach is typically more expensive than traditional fine-tuning because of the necessity to generate adversarial examples via gradient descent. Delving into the optimization process of adversarial training, we find that robust connectivity patterns emerge in the early training phase (typically 0.15~0.3 epochs), far before parameters converge. Inspired by this finding, we dig out robust early-bird tickets (i.e., subnetworks) to develop an efficient adversarial training method: (1) searching for robust tickets with structured sparsity in the early stage; (2) fine-tuning robust tickets in the remaining time. To extract the robust tickets as early as possible, we design a ticket convergence metric to automatically terminate the searching process. Experiments show that the proposed efficient adversarial training method can achieve up to <tex-math>7\times \sim 13 \times</tex-math> training speedups while maintaining comparable or even better robustness compared to the most competitive state-of-the-art adversarial training methods.</abstract>
      <url hash="e36d224f">2022.emnlp-main.569</url>
      <bibkey>xi-etal-2022-efficient</bibkey>
    </paper>
    <paper id="570">
      <title>Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks</title>
      <author><first>Fatemehsadat</first><last>Mireshghallah</last></author>
      <author><first>Kartik</first><last>Goyal</last></author>
      <author><first>Archit</first><last>Uniyal</last></author>
      <author><first>Taylor</first><last>Berg-Kirkpatrick</last></author>
      <author><first>Reza</first><last>Shokri</last></author>
      <pages>8332-8347</pages>
      <abstract>The wide adoption and application of Masked language models (MLMs) on sensitive data (from legal to medical) necessitates a thorough quantitative investigation into their privacy vulnerabilities. Prior attempts at measuring leakage of MLMs via membership inference attacks have been inconclusive, implying potential robustness of MLMs to privacy attacks.In this work, we posit that prior attempts were inconclusive because they based their attack solely on the MLM’s model score. We devise a stronger membership inference attack based on likelihood ratio hypothesis testing that involves an additional reference MLM to more accurately quantify the privacy risks of memorization in MLMs. We show that masked language models are indeed susceptible to likelihood ratio membership inference attacks: Our empirical results, on models trained on medical notes, show that our attack improves the AUC of prior membership inference attacks from 0.66 to an alarmingly high 0.90 level.</abstract>
      <url hash="5e3da49c">2022.emnlp-main.570</url>
      <bibkey>mireshghallah-etal-2022-quantifying</bibkey>
    </paper>
    <paper id="571">
      <title><fixed-case>SM</fixed-case>a<fixed-case>LL</fixed-case>-100: Introducing Shallow Multilingual Machine Translation Model for Low-Resource Languages</title>
      <author><first>Alireza</first><last>Mohammadshahi</last></author>
      <author><first>Vassilina</first><last>Nikoulina</last></author>
      <author><first>Alexandre</first><last>Berard</last></author>
      <author><first>Caroline</first><last>Brun</last></author>
      <author><first>James</first><last>Henderson</last></author>
      <author><first>Laurent</first><last>Besacier</last></author>
      <pages>8348-8359</pages>
      <abstract>In recent years, multilingual machine translation models have achieved promising performance on low-resource language pairs by sharing information between similar languages, thus enabling zero-shot translation. To overcome the “curse of multilinguality”, these models often opt for scaling up the number of parameters, which makes their use in resource-constrained environments challenging. We introduce SMaLL-100, a distilled version of the M2M-100(12B) model, a massively multilingual machine translation model covering 100 languages. We train SMaLL-100 with uniform sampling across all language pairs and therefore focus on preserving the performance of low-resource languages. We evaluate SMaLL-100 on different low-resource benchmarks: FLORES-101, Tatoeba, and TICO-19 and demonstrate that it outperforms previous massively multilingual models of comparable sizes (200-600M) while improving inference latency and memory usage. Additionally, our model achieves comparable results to M2M-100 (1.2B), while being 3.6x smaller and 4.3x faster at inference.</abstract>
      <url hash="a5b4fc9d">2022.emnlp-main.571</url>
      <bibkey>mohammadshahi-etal-2022-small</bibkey>
    </paper>
    <paper id="572">
      <title><fixed-case>T</fixed-case>ext<fixed-case>F</fixed-case>usion: Privacy-Preserving Pre-trained Model Inference via Token Fusion</title>
      <author><first>Xin</first><last>Zhou</last></author>
      <author><first>Jinzhu</first><last>Lu</last></author>
      <author><first>Tao</first><last>Gui</last></author>
      <author><first>Ruotian</first><last>Ma</last></author>
      <author><first>Zichu</first><last>Fei</last></author>
      <author><first>Yuran</first><last>Wang</last></author>
      <author><first>Yong</first><last>Ding</last></author>
      <author><first>Yibo</first><last>Cheung</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <pages>8360-8371</pages>
      <abstract>Recently, more and more pre-trained language models are released as a cloud service. It allows users who lack computing resources to perform inference with a powerful model by uploading data to the cloud. The plain text may contain private information, as the result, users prefer to do partial computations locally and upload intermediate representations to the cloud for subsequent inference.However, recent studies have shown that intermediate representations can also be recovered to plain text with reasonable accuracy, thus the risk of privacy leakage still exists. To address this issue, we propose TextFusion, a novel method for preserving inference privacy.Specifically, we train a Fusion Predictor to dynamically fuse token representations, which hides multiple private token representations behind an unrecognizable one.Furthermore, an adversarial training regime is employed to privatize these representations. In this way, the cloud only receives incomplete and perturbed representations, making it difficult to accurately recover the complete plain text.The experimental results on diverse classification tasks show that our approach can effectively preserve inference privacy without significantly sacrificing performance in different scenarios.</abstract>
      <url hash="a39ed32f">2022.emnlp-main.572</url>
      <bibkey>zhou-etal-2022-textfusion</bibkey>
    </paper>
    <paper id="573">
      <title>Learning to Explain Selectively: A Case Study on Question Answering</title>
      <author><first>Shi</first><last>Feng</last></author>
      <author><first>Jordan</first><last>Boyd-Graber</last></author>
      <pages>8372-8382</pages>
      <abstract>Explanations promise to bridge the gap between humans and AI, yet it remains difficult to achieve consistent improvement in AI-augmented human decision making. The usefulness of AI explanations depends on many factors, and always showing the same type of explanation in all cases is suboptimal—so is relying on heuristics to adapt explanations for each scenario. We propose learning to explain”selectively”: for each decision that the user makes, we use a model to choose the best explanation from a set of candidates and update this model with feedback to optimize human performance. We experiment on a question answering task, Quizbowl, and show that selective explanations improve human performance for both experts and crowdworkers.</abstract>
      <url hash="895bd01b">2022.emnlp-main.573</url>
      <bibkey>feng-boyd-graber-2022-learning</bibkey>
    </paper>
    <paper id="574">
      <title><fixed-case>C</fixed-case>onsist<fixed-case>TL</fixed-case>: Modeling Consistency in Transfer Learning for Low-Resource Neural Machine Translation</title>
      <author><first>Zhaocong</first><last>Li</last></author>
      <author><first>Xuebo</first><last>Liu</last></author>
      <author><first>Derek F.</first><last>Wong</last></author>
      <author><first>Lidia S.</first><last>Chao</last></author>
      <author><first>Min</first><last>Zhang</last></author>
      <pages>8383-8394</pages>
      <abstract>Transfer learning is a simple and powerful method that can be used to boost model performance of low-resource neural machine translation (NMT). Existing transfer learning methods for NMT are static, which simply transfer knowledge from a parent model to a child model once via parameter initialization. In this paper, we propose a novel transfer learning method for NMT, namely ConsistTL, which can continuously transfer knowledge from the parent model during the training of the child model. Specifically, for each training instance of the child model, ConsistTL constructs the semantically-equivalent instance for the parent model and encourages prediction consistency between the parent and child for this instance, which is equivalent to the child model learning each instance under the guidance of the parent model. Experimental results on five low-resource NMT tasks demonstrate that ConsistTL results in significant improvements over strong transfer learning baselines, with a gain up to 1.7 BLEU over the existing back-translation model on the widely-used WMT17 Turkish-English benchmark. Further analysis reveals that ConsistTL can improve the inference calibration of the child model. Code and scripts are freely available at https://github.com/NLP2CT/ConsistTL.</abstract>
      <url hash="8a1512bf">2022.emnlp-main.574</url>
      <bibkey>li-etal-2022-consisttl</bibkey>
    </paper>
    <paper id="575">
      <title>Better Hit the Nail on the Head than Beat around the Bush: Removing Protected Attributes with a Single Projection</title>
      <author><first>Pantea</first><last>Haghighatkhah</last></author>
      <author><first>Antske</first><last>Fokkens</last></author>
      <author><first>Pia</first><last>Sommerauer</last></author>
      <author><first>Bettina</first><last>Speckmann</last></author>
      <author><first>Kevin</first><last>Verbeek</last></author>
      <pages>8395-8416</pages>
      <abstract>Bias elimination and recent probing studies attempt to remove specific information from embedding spaces. Here it is important to remove as much of the target information as possible, while preserving any other information present. INLP is a popular recent method which removes specific information through iterative nullspace projections.Multiple iterations, however, increase the risk that information other than the target is negatively affected.We introduce two methods that find a single targeted projection: Mean Projection (MP, more efficient) and Tukey Median Projection (TMP, with theoretical guarantees). Our comparison between MP and INLP shows that (1) one MP projection removes linear separability based on the target and (2) MP has less impact on the overall space.Further analysis shows that applying random projections after MP leads to the same overall effects on the embedding space as the multiple projections of INLP. Applying one targeted (MP) projection hence is methodologically cleaner than applying multiple (INLP) projections that introduce random effects.</abstract>
      <url hash="945fb747">2022.emnlp-main.575</url>
      <bibkey>haghighatkhah-etal-2022-better</bibkey>
    </paper>
    <paper id="576">
      <title><fixed-case>IELM</fixed-case>: An Open Information Extraction Benchmark for Pre-Trained Language Models</title>
      <author><first>Chenguang</first><last>Wang</last></author>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Dawn</first><last>Song</last></author>
      <pages>8417-8437</pages>
      <abstract>We introduce a new open information extraction (OIE) benchmark for pre-trained language models (LM). Recent studies have demonstrated that pre-trained LMs, such as BERT and GPT, may store linguistic and relational knowledge. In particular, LMs are able to answer “fill-in-the-blank” questions when given a pre-defined relation category. Instead of focusing on pre-defined relations, we create an OIE benchmark aiming to fully examine the open relational information present in the pre-trained LMs. We accomplish this by turning pre-trained LMs into zero-shot OIE systems. Surprisingly, pre-trained LMs are able to obtain competitive performance on both standard OIE datasets (CaRB and Re-OIE2016) and two new large-scale factual OIE datasets (TAC KBP-OIE and Wikidata-OIE) that we establish via distant supervision. For instance, the zero-shot pre-trained LMs outperform the F1 score of the state-of-the-art supervised OIE methods on our factual OIE datasets without needing to use any training sets.</abstract>
      <url hash="d5892c94">2022.emnlp-main.576</url>
      <bibkey>wang-etal-2022-ielm</bibkey>
    </paper>
    <paper id="577">
      <title><fixed-case>C</fixed-case>on<fixed-case>NER</fixed-case>: Consistency Training for Cross-lingual Named Entity Recognition</title>
      <author><first>Ran</first><last>Zhou</last></author>
      <author><first>Xin</first><last>Li</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Erik</first><last>Cambria</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <author><first>Chunyan</first><last>Miao</last></author>
      <pages>8438-8449</pages>
      <abstract>Cross-lingual named entity recognition (NER) suffers from data scarcity in the target languages, especially under zero-shot settings. Existing translate-train or knowledge distillation methods attempt to bridge the language gap, but often introduce a high level of noise. To solve this problem, consistency training methods regularize the model to be robust towards perturbations on data or hidden states.However, such methods are likely to violate the consistency hypothesis, or mainly focus on coarse-grain consistency.We propose ConNER as a novel consistency training framework for cross-lingual NER, which comprises of: (1) translation-based consistency training on unlabeled target-language data, and (2) dropout-based consistency training on labeled source-language data. ConNER effectively leverages unlabeled target-language data and alleviates overfitting on the source language to enhance the cross-lingual adaptability. Experimental results show our ConNER achieves consistent improvement over various baseline methods.</abstract>
      <url hash="3c0ecdba">2022.emnlp-main.577</url>
      <bibkey>zhou-etal-2022-conner</bibkey>
    </paper>
    <paper id="578">
      <title>A Sequential Flow Control Framework for Multi-hop Knowledge Base Question Answering</title>
      <author><first>Minghui</first><last>Xie</last></author>
      <author><first>Chuzhan</first><last>Hao</last></author>
      <author><first>Peng</first><last>Zhang</last></author>
      <pages>8450-8460</pages>
      <abstract>One of the key challenges of knowledge base question answering (KBQA) is the multi-hop reasoning. Since in different hops, one attends to different parts of question, it is important to dynamically represent the question semantics for each hop. Existing methods, however, (i) infer the dynamic question representation only through coarse-grained attention mechanisms, which may bring information loss, (ii) and have not effectively modeled the sequential logic, which is crucial for the multi-hop reasoning process in KBQA.To address these issues, we propose a sequential reasoning self-attention mechanism to capture the crucial reasoning information of each single hop in a more fine-grained way. Based on Gated Recurrent Unit (GRU) which is good at modeling sequential process, we propose a simple but effective GRU-inspired Flow Control (GFC) framework to model sequential logic in the whole multi-hop process.Extensive experiments on three popular benchmark datasets have demonstrated the superior effectiveness of our model. In particular, GFC achieves new state-of-the-art Hits@1 of 76.8% on WebQSP and is also effective when KB is incomplete. Our code and data are available at https://github.com/Xie-Minghui/GFC.</abstract>
      <url hash="0f9b136f">2022.emnlp-main.578</url>
      <bibkey>xie-etal-2022-sequential</bibkey>
    </paper>
    <paper id="579">
      <title><fixed-case>ACEN</fixed-case>et: Attention Guided Commonsense Reasoning on Hybrid Knowledge Graph</title>
      <author><first>Chuzhan</first><last>Hao</last></author>
      <author><first>Minghui</first><last>Xie</last></author>
      <author><first>Peng</first><last>Zhang</last></author>
      <pages>8461-8471</pages>
      <abstract>Augmenting pre-trained language models (PLMs) with knowledge graphs (KGs) has demonstrated superior performance on commonsense reasoning. Given a commonsense based QA context (question and multiple choices), existing approaches usually estimate the plausibility of candidate choices separately based on their respective retrieved KGs, without considering the interference among different choices. In this paper, we propose an Attention guided Commonsense rEasoning Network (ACENet) to endow the neural network with the capability of integrating hybrid knowledge. Specifically, our model applies the multi-layer interaction of answer choices to continually strengthen correct choice information and guide the message passing of GNN. In addition, we also design a mix attention mechanism of nodes and edges to iteratively select supporting evidence on hybrid knowledge graph. Experimental results demonstrate the effectiveness of our proposed model through considerable performance gains across CommonsenseQA and OpenbookQA datasets.</abstract>
      <url hash="99e7bbfa">2022.emnlp-main.579</url>
      <bibkey>hao-etal-2022-acenet</bibkey>
    </paper>
    <paper id="580">
      <title>Revisiting <fixed-case>D</fixed-case>oc<fixed-case>RED</fixed-case> - Addressing the False Negative Problem in Relation Extraction</title>
      <author><first>Qingyu</first><last>Tan</last></author>
      <author><first>Lu</first><last>Xu</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Hwee Tou</first><last>Ng</last></author>
      <author><first>Sharifah Mahani</first><last>Aljunied</last></author>
      <pages>8472-8487</pages>
      <abstract>The DocRED dataset is one of the most popular and widely used benchmarks for document-level relation extraction (RE). It adopts a recommend-revise annotation scheme so as to have a large-scale annotated dataset. However, we find that the annotation of DocRED is incomplete, i.e., false negative samples are prevalent. We analyze the causes and effects of the overwhelming false negative problem in the DocRED dataset. To address the shortcoming, we re-annotate 4,053 documents in the DocRED dataset by adding the missed relation triples back to the original DocRED. We name our revised DocRED dataset Re-DocRED. We conduct extensive experiments with state-of-the-art neural models on both datasets, and the experimental results show that the models trained and evaluated on our Re-DocRED achieve performance improvements of around 13 F1 points. Moreover, we conduct a comprehensive analysis to identify the potential areas for further improvement.</abstract>
      <url hash="e161d566">2022.emnlp-main.580</url>
      <bibkey>tan-etal-2022-revisiting</bibkey>
    </paper>
    <paper id="581">
      <title>Towards Summary Candidates Fusion</title>
      <author><first>Mathieu</first><last>Ravaut</last></author>
      <author><first>Shafiq</first><last>Joty</last></author>
      <author><first>Nancy</first><last>Chen</last></author>
      <pages>8488-8504</pages>
      <abstract>Sequence-to-sequence deep neural models fine-tuned for abstractive summarization can achieve great performance on datasets with enough human annotations. Yet, it has been shown that they have not reached their full potential, with a wide gap between the top beam search output and the oracle beam. Recently, re-ranking methods have been proposed, to learn to select a better summary candidate. However, such methods are limited by the summary quality aspects captured by the first-stage candidates. To bypass this limitation, we propose a new paradigm in second-stage abstractive summarization called SummaFusion that fuses several summary candidates to produce a novel abstractive second-stage summary. Our method works well on several summarization datasets, improving both the ROUGE scores and qualitative properties of fused summaries. It is especially good when the candidates to fuse are worse, such as in the few-shot setup where we set a new state-of-the art. We will make our code and checkpoints available at https://github.com/ntunlp/SummaFusion/.</abstract>
      <url hash="0752b686">2022.emnlp-main.581</url>
      <bibkey>ravaut-etal-2022-towards</bibkey>
    </paper>
    <paper id="582">
      <title>Multimodal Robustness for Neural Machine Translation</title>
      <author><first>Yuting</first><last>Zhao</last></author>
      <author><first>Ioan</first><last>Calapodescu</last></author>
      <pages>8505-8516</pages>
      <abstract>In this paper, we look at the case of a Generic text-to-text NMT model that has to deal with data coming from various modalities, like speech, images, or noisy text extracted from the web. We propose a two-step method, based on composable adapters, to deal with this problem of Multimodal Robustness. In a first step, we separately learn domain adapters and modality specific adapters, to deal with noisy input coming from various sources: ASR, OCR, or noisy text (UGC). In a second step, we combine these components at runtime via dynamic routing or, when the source of noise is unknown, via two new transfer learning mechanisms (Fast Fusion and Multi Fusion). We show that our method provides a flexible, state-of-the-art, architecture able to deal with noisy multimodal inputs.</abstract>
      <url hash="92702980">2022.emnlp-main.582</url>
      <bibkey>zhao-calapodescu-2022-multimodal</bibkey>
    </paper>
    <paper id="583">
      <title><fixed-case>T</fixed-case>ran<fixed-case>SHER</fixed-case>: Translating Knowledge Graph Embedding with Hyper-Ellipsoidal Restriction</title>
      <author><first>Yizhi</first><last>Li</last></author>
      <author><first>Wei</first><last>Fan</last></author>
      <author><first>Chao</first><last>Liu</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <author><first>Jiang</first><last>Qian</last></author>
      <pages>8517-8528</pages>
      <abstract>Knowledge graph embedding methods are important for the knowledge graph completion (or link prediction) task.One state-of-the-art method, PairRE, leverages two separate vectors to model complex relations (i.e., 1-to-N, N-to-1, and N-to-N) in knowledge graphs. However, such a method strictly restricts entities on the hyper-ellipsoid surfaces which limits the optimization of entity distribution, leading to suboptimal performance of knowledge graph completion. To address this issue, we propose a novel score function TranSHER, which leverages relation-specific translations between head and tail entities to relax the constraint of hyper-ellipsoid restrictions. By introducing an intuitive and simple relation-specific translation, TranSHER can provide more direct guidance on optimization and capture more semantic characteristics of entities with complex relations. Experimental results show that TranSHER achieves state-of-the-art performance on link prediction and generalizes well to datasets in different domains and scales. Our codes are public available athttps://github.com/yizhilll/TranSHER.</abstract>
      <url hash="7a471d7b">2022.emnlp-main.583</url>
      <bibkey>li-etal-2022-transher</bibkey>
    </paper>
    <paper id="584">
      <title><fixed-case>IRRGN</fixed-case>: An Implicit Relational Reasoning Graph Network for Multi-turn Response Selection</title>
      <author><first>Jingcheng</first><last>Deng</last></author>
      <author><first>Hengwei</first><last>Dai</last></author>
      <author><first>Xuewei</first><last>Guo</last></author>
      <author><first>Yuanchen</first><last>Ju</last></author>
      <author><first>Wei</first><last>Peng</last></author>
      <pages>8529-8541</pages>
      <abstract>The task of response selection in multi-turn dialogue is to find the best option from all candidates. In order to improve the reasoning ability of the model, previous studies pay more attention to using explicit algorithms to model the dependencies between utterances, which are deterministic, limited and inflexible. In addition, few studies consider differences between the options before and after reasoning. In this paper, we propose an Implicit Relational Reasoning Graph Network to address these issues, which consists of the Utterance Relational Reasoner (URR) and the Option Dual Comparator (ODC). URR aims to implicitly extract dependencies between utterances, as well as utterances and options, and make reasoning with relational graph convolutional networks. ODC focuses on perceiving the difference between the options through dual comparison, which can eliminate the interference of the noise options. Experimental results on two multi-turn dialogue reasoning benchmark datasets MuTual and MuTualplus show that our method significantly improves the baseline of four pre-trained language models and achieves state-of-the-art performance. The model surpasses human performance for the first time on the MuTual dataset.</abstract>
      <url hash="edff70ca">2022.emnlp-main.584</url>
      <bibkey>deng-etal-2022-irrgn</bibkey>
    </paper>
    <paper id="585">
      <title>Predicting Prerequisite Relations for Unseen Concepts</title>
      <author><first>Yaxin</first><last>Zhu</last></author>
      <author><first>Hamed</first><last>Zamani</last></author>
      <pages>8542-8548</pages>
      <abstract>Concept prerequisite learning (CPL) plays a key role in developing technologies that assist people to learn a new complex topic or concept. Previous work commonly assumes that all concepts are given at training time and solely focuses on predicting the unseen prerequisite relationships between them. However, many real-world scenarios deal with concepts that are left undiscovered at training time, which is relatively unexplored. This paper studies this problem and proposes a novel alternating knowledge distillation approach to take advantage of both content- and graph-based models for this task. Extensive experiments on three public benchmarks demonstrate up to 10% improvements in terms of F1 score.</abstract>
      <url hash="1fee2286">2022.emnlp-main.585</url>
      <bibkey>zhu-zamani-2022-predicting</bibkey>
    </paper>
    <paper id="586">
      <title>Contrastive Learning with Expectation-Maximization for Weakly Supervised Phrase Grounding</title>
      <author><first>Keqin</first><last>Chen</last></author>
      <author><first>Richong</first><last>Zhang</last></author>
      <author><first>Samuel</first><last>Mensah</last></author>
      <author><first>Yongyi</first><last>Mao</last></author>
      <pages>8549-8559</pages>
      <abstract>Weakly supervised phrase grounding aims to learn an alignment between phrases in a caption and objects in a corresponding image using only caption-image annotations, i.e., without phrase-object annotations. Previous methods typically use a caption-image contrastive loss to indirectly supervise the alignment between phrases and objects, which hinders the maximum use of the intrinsic structure of the multimodal data and leads to unsatisfactory performance. In this work, we directly use the phrase-object contrastive loss in the condition that no positive annotation is available in the first place. Specifically, we propose a novel contrastive learning framework based on the expectation-maximization algorithm that adaptively refines the target prediction. Experiments on two widely used benchmarks, Flickr30K Entities and RefCOCO+, demonstrate the effectiveness of our framework. We obtain 63.05% top-1 accuracy on Flickr30K Entities and 59.51%/43.46% on RefCOCO+ TestA/TestB, outperforming the previous methods by a large margin, even surpassing a previous SoTA that uses a pre-trained vision-language model. Furthermore, we deliver a theoretical analysis of the effectiveness of our method from the perspective of the maximum likelihood estimate with latent variables.</abstract>
      <url hash="19b2c632">2022.emnlp-main.586</url>
      <bibkey>chen-etal-2022-contrastive</bibkey>
    </paper>
    <paper id="587">
      <title>Beyond prompting: Making Pre-trained Language Models Better Zero-shot Learners by Clustering Representations</title>
      <author><first>Yu</first><last>Fei</last></author>
      <author><first>Zhao</first><last>Meng</last></author>
      <author><first>Ping</first><last>Nie</last></author>
      <author><first>Roger</first><last>Wattenhofer</last></author>
      <author><first>Mrinmaya</first><last>Sachan</last></author>
      <pages>8560-8579</pages>
      <abstract>Recent work has demonstrated that pre-trained language models (PLMs) are zero-shot learners. However, most existing zero-shot methods involve heavy human engineering or complicated self-training pipelines, hindering their application to new situations. In this work, we show that zero-shot text classification can be improved simply by clustering texts in the embedding spaces of PLMs. Specifically, we fit the unlabeled texts with a Bayesian Gaussian Mixture Model after initializing cluster positions and shapes using class names. Despite its simplicity, this approach achieves superior or comparable performance on both topic and sentiment classification datasets and outperforms prior works significantly on unbalanced datasets. We further explore the applicability of our clustering approach by evaluating it on 14 datasets with more diverse topics, text lengths, and numbers of classes. Our approach achieves an average of 20% absolute improvement over prompt-based zero-shot learning. Finally, we compare different PLM embedding spaces and find that texts are well-clustered by topics even if the PLM is not explicitly pre-trained to generate meaningful sentence embeddings. This work indicates that PLM embeddings can categorize texts without task-specific fine-tuning, thus providing a new way to analyze and utilize their knowledge and zero-shot learning ability.</abstract>
      <url hash="dc261349">2022.emnlp-main.587</url>
      <bibkey>fei-etal-2022-beyond</bibkey>
    </paper>
    <paper id="588">
      <title>Generalizing over Long Tail Concepts for Medical Term Normalization</title>
      <author><first>Beatrice</first><last>Portelli</last></author>
      <author><first>Simone</first><last>Scaboro</last></author>
      <author><first>Enrico</first><last>Santus</last></author>
      <author><first>Hooman</first><last>Sedghamiz</last></author>
      <author><first>Emmanuele</first><last>Chersoni</last></author>
      <author><first>Giuseppe</first><last>Serra</last></author>
      <pages>8580-8591</pages>
      <abstract>Medical term normalization consists in mapping a piece of text to a large number of output classes.Given the small size of the annotated datasets and the extremely long tail distribution of the concepts, it is of utmost importance to develop models that are capable to generalize to scarce or unseen concepts.An important attribute of most target ontologies is their hierarchical structure. In this paper we introduce a simple and effective learning strategy that leverages such information to enhance the generalizability of both discriminative and generative models.The evaluation shows that the proposed strategy produces state-of-the-art performance on seen concepts and consistent improvements on unseen ones, allowing also for efficient zero-shot knowledge transfer across text typologies and datasets.</abstract>
      <url hash="71f30f87">2022.emnlp-main.588</url>
      <bibkey>portelli-etal-2022-generalizing</bibkey>
    </paper>
    <paper id="589">
      <title>Unsupervised Opinion Summarisation in the <fixed-case>W</fixed-case>asserstein Space</title>
      <author><first>Jiayu</first><last>Song</last></author>
      <author><first>Iman Munire</first><last>Bilal</last></author>
      <author><first>Adam</first><last>Tsakalidis</last></author>
      <author><first>Rob</first><last>Procter</last></author>
      <author><first>Maria</first><last>Liakata</last></author>
      <pages>8592-8607</pages>
      <abstract>Opinion summarisation synthesises opinions expressed in a group of documents discussingthe same topic to produce a single summary. Recent work has looked at opinion summarisation of clusters of social media posts. Such posts are noisy and have unpredictable structure, posing additional challenges for the construction of the summary distribution and the preservation of meaning compared to online reviews, which has been so far the focus on opinion summarisation. To address these challenges we present WassOS, an unsupervised abstractive summarization model which makesuse of the Wasserstein distance. A Variational Autoencoder is first used to obtain the distribution of documents/posts, and the summary distribution is obtained as the Wasserstein barycenter. We create separate disentangled latent semantic and syntactic representations of the summary, which are fed into a GRU decoder with a transformer layer to produce the final summary. Our experiments onmultiple datasets including reviews, Twitter clusters and Reddit threads show that WassOSalmost always outperforms the state-of-the-art on ROUGE metrics and consistently producesthe best summaries with respect to meaning preservation according to human evaluations.</abstract>
      <url hash="24617aa5">2022.emnlp-main.589</url>
      <bibkey>song-etal-2022-unsupervised</bibkey>
    </paper>
    <paper id="590">
      <title>Bloom Library: Multimodal Datasets in 300+ Languages for a Variety of Downstream Tasks</title>
      <author><first>Colin</first><last>Leong</last></author>
      <author><first>Joshua</first><last>Nemecek</last></author>
      <author><first>Jacob</first><last>Mansdorfer</last></author>
      <author><first>Anna</first><last>Filighera</last></author>
      <author><first>Abraham</first><last>Owodunni</last></author>
      <author><first>Daniel</first><last>Whitenack</last></author>
      <pages>8608-8621</pages>
      <abstract>We present Bloom Library, a linguistically diverse set of multimodal and multilingual datasets for language modeling, image captioning, visual storytelling, and speech synthesis/recognition. These datasets represent either the most, or among the most, multilingual datasets for each of the included downstream tasks. In total, the initial release of the Bloom Library datasets covers 363 languages across 32 language families. We train downstream task models for various languages represented in the data, showing the viability of the data for future work in low-resource, multimodal NLP and establishing the first known baselines for these downstream tasks in certain languages (e.g., Bisu [bzi], with an estimated population of 700 users). Some of these first-of-their-kind baselines are comparable to state-of-the-art performance for higher-resourced languages. The Bloom Library datasets are released under Creative Commons licenses on the Hugging Face datasets hub to catalyze more linguistically diverse research in the included downstream tasks.</abstract>
      <url hash="8003d83f">2022.emnlp-main.590</url>
      <bibkey>leong-etal-2022-bloom</bibkey>
    </paper>
    <paper id="591">
      <title>Disentangling Uncertainty in Machine Translation Evaluation</title>
      <author><first>Chrysoula</first><last>Zerva</last></author>
      <author><first>Taisiya</first><last>Glushkova</last></author>
      <author><first>Ricardo</first><last>Rei</last></author>
      <author><first>André F. T.</first><last>Martins</last></author>
      <pages>8622-8641</pages>
      <abstract>Trainable evaluation metrics for machine translation (MT) exhibit strong correlation with human judgements, but they are often hard to interpret and might produce unreliable scores under noisy or out-of-domain data. Recent work has attempted to mitigate this with simple uncertainty quantification techniques (Monte Carlo dropout and deep ensembles), however these techniques (as we show) are limited in several ways – for example, they are unable to distinguish between different kinds of uncertainty, and they are time and memory consuming. In this paper, we propose more powerful and efficient uncertainty predictors for MT evaluation, and we assess their ability to target different sources of aleatoric and epistemic uncertainty. To this end, we develop and compare training objectives for the COMET metric to enhance it with an uncertainty prediction output, including heteroscedastic regression, divergence minimization, and direct uncertainty prediction.Our experiments show improved results on uncertainty prediction for the WMT metrics task datasets, with a substantial reduction in computational costs. Moreover, they demonstrate the ability of these predictors to address specific uncertainty causes in MT evaluation, such as low quality references and out-of-domain data.</abstract>
      <url hash="b86c0bd3">2022.emnlp-main.591</url>
      <bibkey>zerva-etal-2022-disentangling</bibkey>
    </paper>
    <paper id="592">
      <title>Does Your Model Classify Entities Reasonably? Diagnosing and Mitigating Spurious Correlations in Entity Typing</title>
      <author><first>Nan</first><last>Xu</last></author>
      <author><first>Fei</first><last>Wang</last></author>
      <author><first>Bangzheng</first><last>Li</last></author>
      <author><first>Mingtao</first><last>Dong</last></author>
      <author><first>Muhao</first><last>Chen</last></author>
      <pages>8642-8658</pages>
      <abstract>Entity typing aims at predicting one or more words that describe the type(s) of a specific mention in a sentence. Due to shortcuts from surface patterns to annotated entity labels and biased training, existing entity typing models are subject to the problem of spurious correlations. To comprehensively investigate the faithfulness and reliability of entity typing methods, we first systematically define distinct kinds of model biases that are reflected mainly from spurious correlations. Particularly, we identify six types of existing model biases, including mention-context bias, lexical overlapping bias, named entity bias, pronoun bias, dependency bias, and overgeneralization bias. To mitigate model biases, we then introduce a counterfactual data augmentation method. By augmenting the original training set with their debiasedcounterparts, models are forced to fully comprehend sentences and discover the fundamental cues for entity typing, rather than relying on spurious correlations for shortcuts. Experimental results on the UFET dataset show our counterfactual data augmentation approach helps improve generalization of different entity typing models with consistently better performance on both the original and debiased test sets.</abstract>
      <url hash="13bfe2bc">2022.emnlp-main.592</url>
      <bibkey>xu-etal-2022-model</bibkey>
    </paper>
    <paper id="593">
      <title><fixed-case>EDIN</fixed-case>: An End-to-end Benchmark and Pipeline for Unknown Entity Discovery and Indexing</title>
      <author><first>Nora</first><last>Kassner</last></author>
      <author><first>Fabio</first><last>Petroni</last></author>
      <author><first>Mikhail</first><last>Plekhanov</last></author>
      <author><first>Sebastian</first><last>Riedel</last></author>
      <author><first>Nicola</first><last>Cancedda</last></author>
      <pages>8659-8673</pages>
      <abstract>Existing work on Entity Linking mostly assumes that the reference knowledge base is complete, and therefore all mentions can be linked. In practice this is hardly ever the case, as knowledge bases are incomplete and because novel concepts arise constantly. We introduce the temporally segmented Unknown Entity Discovery and Indexing (EDIN)-benchmark where unknown entities, that is entities not part of the knowledge base and without descriptions and labeled mentions, have to be integrated into an existing entity linking system. By contrasting EDIN with zero-shot entity linking, we provide insight on the additional challenges it poses. Building on dense-retrieval based entity linking, we introduce the end-to-end EDIN-pipeline that detects, clusters, and indexes mentions of unknown entities in context. Experiments show that indexing a single embedding per entity unifying the information of multiple mentions works better than indexing mentions independently.</abstract>
      <url hash="d07273c5">2022.emnlp-main.593</url>
      <bibkey>kassner-etal-2022-edin</bibkey>
    </paper>
    <paper id="594">
      <title><fixed-case>POQ</fixed-case>ue: Asking Participant-specific Outcome Questions for a Deeper Understanding of Complex Events</title>
      <author><first>Sai</first><last>Vallurupalli</last></author>
      <author><first>Sayontan</first><last>Ghosh</last></author>
      <author><first>Katrin</first><last>Erk</last></author>
      <author><first>Niranjan</first><last>Balasubramanian</last></author>
      <author><first>Francis</first><last>Ferraro</last></author>
      <pages>8674-8697</pages>
      <abstract>Knowledge about outcomes is critical for complex event understanding but is hard to acquire.We show that by pre-identifying a participant in a complex event, crowdworkers are ableto (1) infer the collective impact of salient events that make up the situation, (2) annotate the volitional engagement of participants in causing the situation, and (3) ground theoutcome of the situation in state changes of the participants. By creating a multi-step interface and a careful quality control strategy, we collect a high quality annotated dataset of8K short newswire narratives and ROCStories with high inter-annotator agreement (0.74-0.96weighted Fleiss Kappa). Our dataset, POQUe (Participant Outcome Questions), enables theexploration and development of models that address multiple aspects of semantic understanding. Experimentally, we show that current language models lag behind human performance in subtle ways through our task formulations that target abstract and specific comprehension of a complex event, its outcome, and a participant’s influence over the event culmination.</abstract>
      <url hash="d52357d2">2022.emnlp-main.594</url>
      <bibkey>vallurupalli-etal-2022-poque</bibkey>
    </paper>
    <paper id="595">
      <title>Measuring the Mixing of Contextual Information in the Transformer</title>
      <author><first>Javier</first><last>Ferrando</last></author>
      <author><first>Gerard I.</first><last>Gállego</last></author>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <pages>8698-8714</pages>
      <abstract>The Transformer architecture aggregates input information through the self-attention mechanism, but there is no clear understanding of how this information is mixed across the entire model. Additionally, recent works have demonstrated that attention weights alone are not enough to describe the flow of information. In this paper, we consider the whole attention block –multi-head attention, residual connection, and layer normalization– and define a metric to measure token-to-token interactions within each layer. Then, we aggregate layer-wise interpretations to provide input attribution scores for model predictions. Experimentally, we show that our method, ALTI (Aggregation of Layer-wise Token-to-token Interactions), provides more faithful explanations and increased robustness than gradient-based methods.</abstract>
      <url hash="ad3d0471">2022.emnlp-main.595</url>
      <bibkey>ferrando-etal-2022-measuring</bibkey>
    </paper>
    <paper id="596">
      <title>Dealing with Abbreviations in the <fixed-case>S</fixed-case>lovenian Biographical Lexicon</title>
      <author><first>Angel</first><last>Daza</last></author>
      <author><first>Antske</first><last>Fokkens</last></author>
      <author><first>Tomaž</first><last>Erjavec</last></author>
      <pages>8715-8720</pages>
      <abstract>Abbreviations present a significant challenge for NLP systems because they cause tokenization and out-of-vocabulary errors. They can also make the text less readable, especially in reference printed books, where they are extensively used. Abbreviations are especially problematic in low-resource settings, where systems are less robust to begin with. In this paper, we propose a new method for addressing the problems caused by a high density of domain-specific abbreviations in a text. We apply this method to the case of a Slovenian biographical lexicon and evaluate it on a newly developed gold-standard dataset of 51 Slovenian biographies. Our abbreviation identification method performs significantly better than commonly used ad-hoc solutions, especially at identifying unseen abbreviations. We also propose and present the results of a method for expanding the identified abbreviations in context.</abstract>
      <url hash="13b196b0">2022.emnlp-main.596</url>
      <bibkey>daza-etal-2022-dealing</bibkey>
    </paper>
    <paper id="597">
      <title><fixed-case>A</fixed-case>fri<fixed-case>CLIRM</fixed-case>atrix: Enabling Cross-Lingual Information Retrieval for <fixed-case>A</fixed-case>frican Languages</title>
      <author><first>Odunayo</first><last>Ogundepo</last></author>
      <author><first>Xinyu</first><last>Zhang</last></author>
      <author><first>Shuo</first><last>Sun</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <author><first>Jimmy</first><last>Lin</last></author>
      <pages>8721-8728</pages>
      <abstract>Language diversity in NLP is critical in enabling the development of tools for a wide range of users.However, there are limited resources for building such tools for many languages, particularly those spoken in Africa.For search, most existing datasets feature few or no African languages, directly impacting researchers’ ability to build and improve information access capabilities in those languages.Motivated by this, we created AfriCLIRMatrix, a test collection for cross-lingual information retrieval research in 15 diverse African languages.In total, our dataset contains 6 million queries in English and 23 million relevance judgments automatically mined from Wikipedia inter-language links, covering many more African languages than any existing information retrieval test collection.In addition, we release BM25, dense retrieval, and sparse–dense hybrid baselines to provide a starting point for the development of future systems.We hope that these efforts can spur additional work in search for African languages.AfriCLIRMatrix can be downloaded at https://github.com/castorini/africlirmatrix.</abstract>
      <url hash="56cf3e86">2022.emnlp-main.597</url>
      <bibkey>ogundepo-etal-2022-africlirmatrix</bibkey>
    </paper>
    <paper id="598">
      <title><fixed-case>CONDAQA</fixed-case>: A Contrastive Reading Comprehension Dataset for Reasoning about Negation</title>
      <author><first>Abhilasha</first><last>Ravichander</last></author>
      <author><first>Matt</first><last>Gardner</last></author>
      <author><first>Ana</first><last>Marasovic</last></author>
      <pages>8729-8755</pages>
      <abstract>The full power of human language-based communication cannot be realized without negation. All human languages have some form of negation. Despite this, negation remains a challenging phenomenon for current natural language understanding systems. To facilitate the future development of models that can process negation effectively, we present CONDAQA, the first English reading comprehension dataset which requires reasoning about the implications of negated statements in paragraphs. We collect paragraphs with diverse negation cues, then have crowdworkers ask questions about the implications of the negated statement in the passage. We also have workers make three kinds of edits to the passage—paraphrasing the negated statement, changing the scope of the negation, and reversing the negation—resulting in clusters of question-answer pairs that are difficult for models to answer with spurious shortcuts. CONDAQA features 14,182 question-answer pairs with over 200 unique negation cues and is challenging for current state-of-the-art models. The best performing model on CONDAQA (UnifiedQA-v2-3b) achieves only 42% on our consistency metric, well below human performance which is 81%. We release our dataset, along with fully-finetuned, few-shot, and zero-shot evaluations, to facilitate the development of future NLP methods that work on negated language.</abstract>
      <url hash="117c2ee2">2022.emnlp-main.598</url>
      <bibkey>ravichander-etal-2022-condaqa</bibkey>
    </paper>
    <paper id="599">
      <title>Towards Opening the Black Box of Neural Machine Translation: Source and Target Interpretations of the Transformer</title>
      <author><first>Javier</first><last>Ferrando</last></author>
      <author><first>Gerard I.</first><last>Gállego</last></author>
      <author><first>Belen</first><last>Alastruey</last></author>
      <author><first>Carlos</first><last>Escolano</last></author>
      <author><first>Marta R.</first><last>Costa-jussà</last></author>
      <pages>8756-8769</pages>
      <abstract>In Neural Machine Translation (NMT), each token prediction is conditioned on the source sentence and the target prefix (what has been previously translated at a decoding step). However, previous work on interpretability in NMT has mainly focused solely on source sentence tokens’ attributions. Therefore, we lack a full understanding of the influences of every input token (source sentence and target prefix) in the model predictions. In this work, we propose an interpretability method that tracks input tokens’ attributions for both contexts. Our method, which can be extended to any encoder-decoder Transformer-based model, allows us to better comprehend the inner workings of current NMT models. We apply the proposed method to both bilingual and multilingual Transformers and present insights into their behaviour.</abstract>
      <url hash="aac45d2f">2022.emnlp-main.599</url>
      <bibkey>ferrando-etal-2022-towards</bibkey>
    </paper>
    <paper id="600">
      <title><fixed-case>A</fixed-case>rt<fixed-case>EL</fixed-case>ingo: A Million Emotion Annotations of <fixed-case>W</fixed-case>iki<fixed-case>A</fixed-case>rt with Emphasis on Diversity over Language and Culture</title>
      <author><first>Youssef</first><last>Mohamed</last></author>
      <author><first>Mohamed</first><last>Abdelfattah</last></author>
      <author><first>Shyma</first><last>Alhuwaider</last></author>
      <author><first>Feifan</first><last>Li</last></author>
      <author><first>Xiangliang</first><last>Zhang</last></author>
      <author><first>Kenneth</first><last>Church</last></author>
      <author><first>Mohamed</first><last>Elhoseiny</last></author>
      <pages>8770-8785</pages>
      <abstract>This paper introduces ArtELingo, a new benchmark and dataset, designed to encourage work on diversity across languages and cultures. Following ArtEmis, a collection of 80k artworks from WikiArt with 0.45M emotion labels and English-only captions, ArtELingo adds another 0.79M annotations in Arabic and Chinese, plus 4.8K in Spanish to evaluate “cultural-transfer” performance. 51K artworks have 5 annotations or more in 3 languages. This diversity makes it possible to study similarities and differences across languages and cultures. Further, we investigate captioning tasks, and find diversity improves the performance of baseline models. ArtELingo is publicly available at ‘www.artelingo.org‘ with standard splits and baseline models. We hope our work will help ease future research on multilinguality and culturally-aware AI.</abstract>
      <url hash="55336d9a">2022.emnlp-main.600</url>
      <bibkey>mohamed-etal-2022-artelingo</bibkey>
    </paper>
    <paper id="601">
      <title>Decoding a Neural Retriever’s Latent Space for Query Suggestion</title>
      <author><first>Leonard</first><last>Adolphs</last></author>
      <author><first>Michelle</first><last>Chen Huebscher</last></author>
      <author><first>Christian</first><last>Buck</last></author>
      <author><first>Sertan</first><last>Girgin</last></author>
      <author><first>Olivier</first><last>Bachem</last></author>
      <author><first>Massimiliano</first><last>Ciaramita</last></author>
      <author><first>Thomas</first><last>Hofmann</last></author>
      <pages>8786-8804</pages>
      <abstract>Neural retrieval models have superseded classic bag-of-words methods such as BM25 as the retrieval framework of choice. However, neural systems lack the interpretability of bag-of-words models; it is not trivial to connect a query change to a change in the latent space that ultimately determines the retrieval results. To shed light on this embedding space, we learn a “query decoder” that, given a latent representation of a neural search engine, generates the corresponding query. We show that it is possible to decode a meaningful query from its latent representation and, when moving in the right direction in latent space, to decode a query that retrieves the relevant paragraph. In particular, the query decoder can be useful to understand “what should have been asked” to retrieve a particular paragraph from the collection. We employ the query decoder to generate a large synthetic dataset of query reformulations for MSMarco, leading to improved retrieval performance. On this data, we train a pseudo-relevance feedback (PRF) T5 model for the application of query suggestion that outperforms both query reformulation and PRF information retrieval baselines.</abstract>
      <url hash="cc1b4357">2022.emnlp-main.601</url>
      <bibkey>adolphs-etal-2022-decoding</bibkey>
    </paper>
    <paper id="602">
      <title><fixed-case>T</fixed-case>-<fixed-case>STAR</fixed-case>: Truthful Style Transfer using <fixed-case>AMR</fixed-case> Graph as Intermediate Representation</title>
      <author><first>Anubhav</first><last>Jangra</last></author>
      <author><first>Preksha</first><last>Nema</last></author>
      <author><first>Aravindan</first><last>Raghuveer</last></author>
      <pages>8805-8825</pages>
      <abstract>Unavailability of parallel corpora for training text style transfer (TST) models is a very challenging yet common scenario. Also, TST models implicitly need to preserve the content while transforming a source sentence into the target style. To tackle these problems, an intermediate representation is often constructed that is devoid of style while still preserving the meaning of the source sentence. In this work, we study the usefulness of Abstract Meaning Representation (AMR) graph as the intermediate style agnostic representation. We posit that semantic notations like AMR are a natural choice for an intermediate representation. Hence, we propose T-STAR: a model comprising of two components, text-to-AMR encoder and a AMR-to-text decoder. We propose several modeling improvements to enhance the style agnosticity of the generated AMR. To the best of our knowledge, T-STAR is the first work that uses AMR as an intermediate representation for TST. With thorough experimental evaluation we show T-STAR significantly outperforms state of the art techniques by achieving on an average 15.2% higher content preservation with negligible loss (~3%) in style accuracy. Through detailed human evaluation with 90,000 ratings, we also show that T-STAR has upto 50% lesser hallucinations compared to state of the art TST models.</abstract>
      <url hash="d8bb6c86">2022.emnlp-main.602</url>
      <bibkey>jangra-etal-2022-star</bibkey>
    </paper>
    <paper id="603">
      <title><fixed-case>P</fixed-case>rompt<fixed-case>BERT</fixed-case>: Improving <fixed-case>BERT</fixed-case> Sentence Embeddings with Prompts</title>
      <author><first>Ting</first><last>Jiang</last></author>
      <author><first>Jian</first><last>Jiao</last></author>
      <author><first>Shaohan</first><last>Huang</last></author>
      <author><first>Zihan</first><last>Zhang</last></author>
      <author><first>Deqing</first><last>Wang</last></author>
      <author><first>Fuzhen</first><last>Zhuang</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <author><first>Haizhen</first><last>Huang</last></author>
      <author><first>Denvy</first><last>Deng</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <pages>8826-8837</pages>
      <abstract>We propose PromptBERT, a novel contrastive learning method for learning better sentence representation. We firstly analysis the drawback of current sentence embedding from original BERT and find that it is mainly due to the static token embedding bias and ineffective BERT layers. Then we propose the first prompt-based sentence embeddings method and discuss two prompt representing methods and three prompt searching methods to make BERT achieve better sentence embeddings .Moreover, we propose a novel unsupervised training objective by the technology of template denoising, which substantially shortens the performance gap between the supervised and unsupervised settings. Extensive experiments show the effectiveness of our method. Compared to SimCSE, PromptBert achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.</abstract>
      <url hash="0ea18155">2022.emnlp-main.603</url>
      <bibkey>jiang-etal-2022-promptbert</bibkey>
    </paper>
    <paper id="604">
      <title>Extending Logic Explained Networks to Text Classification</title>
      <author><first>Rishabh</first><last>Jain</last></author>
      <author><first>Gabriele</first><last>Ciravegna</last></author>
      <author><first>Pietro</first><last>Barbiero</last></author>
      <author><first>Francesco</first><last>Giannini</last></author>
      <author><first>Davide</first><last>Buffelli</last></author>
      <author><first>Pietro</first><last>Lio</last></author>
      <pages>8838-8857</pages>
      <abstract>Recently, Logic Explained Networks (LENs) have been proposed as explainable-by-design neural models providing logic explanations for their predictions.However, these models have only been applied to vision and tabular data, and they mostly favour the generation of global explanations, while local ones tend to be noisy and verbose.For these reasons, we propose LEN&lt;sup&gt;p&lt;/sup&gt;, improving local explanations by perturbing input words, and we test it on text classification. Our results show that (i) LEN&lt;sup&gt;p&lt;/sup&gt; provides better local explanations than LIME in terms of sensitivity and faithfulness, and (ii) its logic explanations are more useful and user-friendly than the feature scoring provided by LIME as attested by a human survey.</abstract>
      <url hash="6f655769">2022.emnlp-main.604</url>
      <bibkey>jain-etal-2022-extending</bibkey>
    </paper>
    <paper id="605">
      <title>Uni-Parser: Unified Semantic Parser for Question Answering on Knowledge Base and Database</title>
      <author><first>Ye</first><last>Liu</last></author>
      <author><first>Semih</first><last>Yavuz</last></author>
      <author><first>Rui</first><last>Meng</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <author><first>Yingbo</first><last>Zhou</last></author>
      <pages>8858-8869</pages>
      <abstract>Parsing natural language questions into executable logical forms is a useful and interpretable way to perform question answering on structured data such as knowledge bases (KB) or databases (DB). However, existing approaches on semantic parsing cannot adapt to both modalities, as they suffer from the exponential growth of the logical form candidates and can hardly generalize to unseen data.In this work, we propose Uni-Parser, a unified semantic parser for question answering (QA) on both KB and DB. We define the primitive (relation and entity in KB, and table name, column name and cell value in DB) as the essential element in our framework. The number of primitives grows only at a linear rate to the number of retrieved relations in KB and DB, preventing us from exponential logic form candidates. We leverage the generator to predict final logical forms by altering and composing top-ranked primitives with different operations (e.g. select, where, count). With sufficiently pruned search space by a contrastive primitive ranker, the generator is empowered to capture the composition of primitives enhancing its generalization ability. We achieve competitive results on multiple KB and DB QA benchmarks with more efficiency, especially in the compositional and zero-shot settings.</abstract>
      <url hash="fa0b1d2f">2022.emnlp-main.605</url>
      <bibkey>liu-etal-2022-uni</bibkey>
    </paper>
    <paper id="606">
      <title><fixed-case>RAPO</fixed-case>: An Adaptive Ranking Paradigm for Bilingual Lexicon Induction</title>
      <author><first>Zhoujin</first><last>Tian</last></author>
      <author><first>Chaozhuo</first><last>Li</last></author>
      <author><first>Shuo</first><last>Ren</last></author>
      <author><first>Zhiqiang</first><last>Zuo</last></author>
      <author><first>Zengxuan</first><last>Wen</last></author>
      <author><first>Xinyue</first><last>Hu</last></author>
      <author><first>Xiao</first><last>Han</last></author>
      <author><first>Haizhen</first><last>Huang</last></author>
      <author><first>Denvy</first><last>Deng</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Xing</first><last>Xie</last></author>
      <pages>8870-8883</pages>
      <abstract>Bilingual lexicon induction induces the word translations by aligning independently trained word embeddings in two languages. Existing approaches generally focus on minimizing the distances between words in the aligned pairs, while suffering from low discriminative capability to distinguish the relative orders between positive and negative candidates. In addition, the mapping function is globally shared by all words, whose performance might be hindered by the deviations in the distributions of different languages. In this work, we propose a novel ranking-oriented induction model RAPO to learn personalized mapping function for each word. RAPO is capable of enjoying the merits from the unique characteristics of a single word and the cross-language isomorphism simultaneously. Extensive experimental results on public datasets including both rich-resource and low-resource languages demonstrate the superiority of our proposal. Our code is publicly available in <url>https://github.com/Jlfj345wf/RAPO</url>.</abstract>
      <url hash="a0b85367">2022.emnlp-main.606</url>
      <bibkey>tian-etal-2022-rapo</bibkey>
    </paper>
    <paper id="607">
      <title>On Parsing as Tagging</title>
      <author><first>Afra</first><last>Amini</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <pages>8884-8900</pages>
      <abstract>There are many proposals to reduce constituency parsing to tagging. To figure out what these approaches have in common, we offer a unifying pipeline, which consists of three steps: linearization, learning, and decoding. We prove that classic shift–reduce parsing can be reduced to tetratagging—the state-of-the-art constituency tagger—under two assumptions: right-corner transformation in the linearization step and factored scoring in the learning step. We ask what is the most critical factor that makes parsing-as-tagging methods accurate while being efficient. To answer this question, we empirically evaluate a taxonomy of tagging pipelines with different choices of linearizers, learners, and decoders. Based on the results in English as well as a set of 8 typologically diverse languages, we conclude that the linearization of the derivation tree and its alignment with the input sequence is the most critical factor in achieving accurate parsers as taggers.</abstract>
      <url hash="38524d30">2022.emnlp-main.607</url>
      <bibkey>amini-cotterell-2022-parsing</bibkey>
    </paper>
    <paper id="608">
      <title>Distilled Dual-Encoder Model for Vision-Language Understanding</title>
      <author><first>Zekun</first><last>Wang</last></author>
      <author><first>Wenhui</first><last>Wang</last></author>
      <author><first>Haichao</first><last>Zhu</last></author>
      <author><first>Ming</first><last>Liu</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <pages>8901-8913</pages>
      <abstract>On vision-language understanding (VLU) tasks, fusion-encoder vision-language models achieve superior results but sacrifice efficiency because of the simultaneous encoding of images and text. On the contrary, the dual encoder model that separately encodes images and text has the advantage in efficiency, while failing on VLU tasks due to the lack of deep cross-modal interactions. To get the best of both worlds, we propose DiDE, a framework that distills the knowledge of the fusion-encoder teacher model into the dual-encoder student model. Since the cross-modal interaction is the key to the superior performance of teacher model but is absent in the student model, we encourage the student not only to mimic the predictions of teacher, but also to calculate the cross-modal attention distributions and align with the teacher. Experimental results demonstrate that DiDE is competitive with the fusion-encoder teacher model in performance (only a 1% drop) while enjoying 4 times faster inference. Further analyses reveal that the proposed cross-modal attention distillation is crucial to the success of our framework.</abstract>
      <url hash="93b40e8e">2022.emnlp-main.608</url>
      <bibkey>wang-etal-2022-distilled</bibkey>
    </paper>
    <paper id="609">
      <title>Argument Mining for Review Helpfulness Prediction</title>
      <author><first>Zaiqian</first><last>Chen</last></author>
      <author><first>Daniel</first><last>Verdi do Amarante</last></author>
      <author><first>Jenna</first><last>Donaldson</last></author>
      <author><first>Yohan</first><last>Jo</last></author>
      <author><first>Joonsuk</first><last>Park</last></author>
      <pages>8914-8922</pages>
      <abstract>The importance of reliably determining the helpfulness of product reviews is rising as both helpful and unhelpful reviews continue to accumulate on e-commerce websites. And argumentational features—such as the structure of arguments and the types of underlying elementary units—have shown to be promising indicators of product review helpfulness. However, their adoption has been limited due to the lack of sufficient resources and large-scale experiments investigating their utility. To this end, we present the AMazon Argument Mining (AM<tex-math>^2</tex-math>) corpus—a corpus of 878 Amazon reviews on headphones annotated according to a theoretical argumentation model designed to evaluate argument quality.Experiments show that employing argumentational features leads to statistically significant improvements over the state-of-the-art review helpfulness predictors under both text-only and text-and-image settings.</abstract>
      <url hash="1b1b70d8">2022.emnlp-main.609</url>
      <bibkey>chen-etal-2022-argument</bibkey>
    </paper>
    <paper id="610">
      <title>Hierarchical Multi-Label Classification of Scientific Documents</title>
      <author><first>Mobashir</first><last>Sadat</last></author>
      <author><first>Cornelia</first><last>Caragea</last></author>
      <pages>8923-8937</pages>
      <abstract>Automatic topic classification has been studied extensively to assist managing and indexing scientific documents in a digital collection. With the large number of topics being available in recent years, it has become necessary to arrange them in a hierarchy. Therefore, the automatic classification systems need to be able to classify the documents hierarchically. In addition, each paper is often assigned to more than one relevant topic. For example, a paper can be assigned to several topics in a hierarchy tree. In this paper, we introduce a new dataset for hierarchical multi-label text classification (HMLTC) of scientific papers called SciHTC, which contains 186,160 papers and 1,234 categories from the ACM CCS tree. We establish strong baselines for HMLTC and propose a multi-task learning approach for topic classification with keyword labeling as an auxiliary task. Our best model achieves a Macro-F1 score of 34.57% which shows that this dataset provides significant research opportunities on hierarchical scientific topic classification. We make our dataset and code for all experiments publicly available.</abstract>
      <url hash="9ed65ebb">2022.emnlp-main.610</url>
      <bibkey>sadat-caragea-2022-hierarchical</bibkey>
    </paper>
    <paper id="611">
      <title>Rainier: Reinforced Knowledge Introspector for Commonsense Question Answering</title>
      <author><first>Jiacheng</first><last>Liu</last></author>
      <author><first>Skyler</first><last>Hallinan</last></author>
      <author><first>Ximing</first><last>Lu</last></author>
      <author><first>Pengfei</first><last>He</last></author>
      <author><first>Sean</first><last>Welleck</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>8938-8958</pages>
      <abstract>Knowledge underpins reasoning. Recent research demonstrates that when relevant knowledge is provided as additional context to commonsense question answering (QA), it can substantially enhance the performance even on top of state-of-the-art. The fundamental challenge is where and how to find such knowledge that is high quality and on point with respect to the question; knowledge retrieved from knowledge bases are incomplete and knowledge generated from language models are inconsistent.We present Rainier, or Reinforced Knowledge Introspector, that learns to generate contextually relevant knowledge in response to given questions. Our approach starts by imitating knowledge generated by GPT-3, then learns to generate its own knowledge via reinforcement learning where rewards are shaped based on the increased performance on the resulting question answering. Rainier demonstrates substantial and consistent performance gains when tested over 9 different commonsense benchmarks: including 5 datasets that are seen during model training, as well as 4 datasets that are kept unseen. Our work is the first to report that knowledge generated by models that are orders of magnitude smaller than GPT-3, even without direct supervision on the knowledge itself, can exceed the quality of commonsense knowledge elicited from GPT-3.</abstract>
      <url hash="90bf5739">2022.emnlp-main.611</url>
      <bibkey>liu-etal-2022-rainier</bibkey>
    </paper>
    <paper id="612">
      <title>A Major Obstacle for <fixed-case>NLP</fixed-case> Research: Let’s Talk about Time Allocation!</title>
      <author><first>Katharina</first><last>Kann</last></author>
      <author><first>Shiran</first><last>Dudy</last></author>
      <author><first>Arya D.</first><last>McCarthy</last></author>
      <pages>8959-8969</pages>
      <abstract>The field of natural language processing (NLP) has grown over the last few years: conferences have become larger, we have published an incredible amount of papers, and state-of-the-art research has been implemented in a large variety of customer-facing products. However, this paper argues that we have been less successful than we *should* have been and reflects on where and how the field fails to tap its full potential. Specifically, we demonstrate that, in recent years, **subpar time allocation has been a major obstacle for NLP research**. We outline multiple concrete problems together with their negative consequences and, importantly, suggest remedies to improve the status quo. We hope that this paper will be a starting point for discussions around which common practices are – or are *not* – beneficial for NLP research.</abstract>
      <url hash="1e7678c4">2022.emnlp-main.612</url>
      <bibkey>kann-etal-2022-major</bibkey>
    </paper>
    <paper id="613">
      <title>Towards Inter-character Relationship-driven Story Generation</title>
      <author><first>Anvesh Rao</first><last>Vijjini</last></author>
      <author><first>Faeze</first><last>Brahman</last></author>
      <author><first>Snigdha</first><last>Chaturvedi</last></author>
      <pages>8970-8987</pages>
      <abstract>In this paper, we introduce the task of modeling interpersonal relationships for story generation. For addressing this task, we propose Relationships as Latent Variables for Story Generation, (ReLiSt). ReLiSt generates stories sentence by sentence and has two major components - a relationship selector and a story continuer. The relationship selector specifies a latent variable to pick the relationship to exhibit in the next sentence and the story continuer generates the next sentence while expressing the selected relationship in a coherent way. Our automatic and human evaluations demonstrate that ReLiSt is able to generate stories with relationships that are more faithful to desired relationships while maintaining the content quality. The relationship assignments to sentences during inference brings interpretability to ReLiSt.</abstract>
      <url hash="2611a6ec">2022.emnlp-main.613</url>
      <bibkey>vijjini-etal-2022-towards</bibkey>
    </paper>
    <paper id="614">
      <title>Incorporating Relevance Feedback for Information-Seeking Retrieval using Few-Shot Document Re-Ranking</title>
      <author><first>Tim</first><last>Baumgärtner</last></author>
      <author><first>Leonardo F. R.</first><last>Ribeiro</last></author>
      <author><first>Nils</first><last>Reimers</last></author>
      <author><first>Iryna</first><last>Gurevych</last></author>
      <pages>8988-9005</pages>
      <abstract>Pairing a lexical retriever with a neural re-ranking model has set state-of-the-art performance on large-scale information retrieval datasets. This pipeline covers scenarios like question answering or navigational queries, however, for information-seeking scenarios, users often provide information on whether a document is relevant to their query in form of clicks or explicit feedback. Therefore, in this work, we explore how relevance feedback can be directly integrated into neural re-ranking models by adopting few-shot and parameter-efficient learning techniques. Specifically, we introduce a kNN approach that re-ranks documents based on their similarity with the query and the documents the user considers relevant. Further, we explore Cross-Encoder models that we pre-train using meta-learning and subsequently fine-tune for each query, training only on the feedback documents. To evaluate our different integration strategies, we transform four existing information retrieval datasets into the relevance feedback scenario. Extensive experiments demonstrate that integrating relevance feedback directly in neural re-ranking models improves their performance, and fusing lexical ranking with our best performing neural re-ranker outperforms all other methods by 5.2% nDCG@20.</abstract>
      <url hash="61d99425">2022.emnlp-main.614</url>
      <bibkey>baumgartner-etal-2022-incorporating</bibkey>
    </paper>
    <paper id="615">
      <title><fixed-case>R</fixed-case>eas<fixed-case>TAP</fixed-case>: Injecting Table Reasoning Skills During Pre-training via Synthetic Reasoning Examples</title>
      <author><first>Yilun</first><last>Zhao</last></author>
      <author><first>Linyong</first><last>Nan</last></author>
      <author><first>Zhenting</first><last>Qi</last></author>
      <author><first>Rui</first><last>Zhang</last></author>
      <author><first>Dragomir</first><last>Radev</last></author>
      <pages>9006-9018</pages>
      <abstract>Reasoning over tabular data requires both table structure understanding and a broad set of table reasoning skills. Current models with table-specific architectures and pre-training methods perform well on understanding table structures, but they still struggle with tasks that require various table reasoning skills. In this work, we develop ReasTAP to show that high-level table reasoning skills can be injected into models during pre-training without a complex table-specific architecture design. We define 7 table reasoning skills, such as numerical operation, temporal comparison, and conjunction. Each reasoning skill is associated with one example generator, which synthesizes questions over semi-structured tables according to the sampled templates. We model the table pre-training task as a sequence generation task and pre-train ReasTAP to generate precise answers of the synthetic examples. ReasTAP is evaluated on four benchmarks covering three downstream tasks including 1) WikiSQL-Weak and WikiTQ for Table Question Answering, 2) TabFact for Table Fact Verification, and 3) LogicNLG for Faithful Table-to-Text Generation. Experimental results demonstrate that ReasTAP achieves new state-of-the-art results on all of them and delivers a significant improvement under low-resource setting. Our code is publicly available at https://github.com/Yale-LILY/ReasTAP.</abstract>
      <url hash="8239a012">2022.emnlp-main.615</url>
      <bibkey>zhao-etal-2022-reastap</bibkey>
    </paper>
    <paper id="616">
      <title>Few-shot Learning with Multilingual Generative Language Models</title>
      <author><first>Xi Victoria</first><last>Lin</last></author>
      <author><first>Todor</first><last>Mihaylov</last></author>
      <author><first>Mikel</first><last>Artetxe</last></author>
      <author><first>Tianlu</first><last>Wang</last></author>
      <author><first>Shuohui</first><last>Chen</last></author>
      <author><first>Daniel</first><last>Simig</last></author>
      <author><first>Myle</first><last>Ott</last></author>
      <author><first>Naman</first><last>Goyal</last></author>
      <author><first>Shruti</first><last>Bhosale</last></author>
      <author><first>Jingfei</first><last>Du</last></author>
      <author><first>Ramakanth</first><last>Pasunuru</last></author>
      <author><first>Sam</first><last>Shleifer</last></author>
      <author><first>Punit Singh</first><last>Koura</last></author>
      <author><first>Vishrav</first><last>Chaudhary</last></author>
      <author><first>Brian</first><last>O’Horo</last></author>
      <author><first>Jeff</first><last>Wang</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <author><first>Zornitsa</first><last>Kozareva</last></author>
      <author><first>Mona</first><last>Diab</last></author>
      <author><first>Veselin</first><last>Stoyanov</last></author>
      <author><first>Xian</first><last>Li</last></author>
      <pages>9019-9052</pages>
      <abstract>Large-scale generative language models such as GPT-3 are competitive few-shot learners. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual generative language models on a corpus covering a diverse set of languages, and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We conduct an in-depth analysis of different multilingual prompting approaches, showing in particular that strong few-shot learning performance across languages can be achieved via cross-lingual transfer through both templates and demonstration examples.</abstract>
      <url hash="b00cf81a">2022.emnlp-main.616</url>
      <bibkey>lin-etal-2022-shot</bibkey>
    </paper>
    <paper id="617">
      <title>Are representations built from the ground up? An empirical examination of local composition in language models</title>
      <author><first>Emmy</first><last>Liu</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>9053-9073</pages>
      <abstract>Compositionality, the phenomenon where the meaning of a phrase can be derived from its constituent parts, is a hallmark of human language. At the same time, many phrases are non-compositional, carrying a meaning beyond that of each part in isolation. Representing both of these types of phrases is critical for language understanding, but it is an open question whether modern language models (LMs) learn to do so; in this work we examine this question. We first formulate a problem of predicting the LM-internal representations of longer phrases given those of their constituents. We find that the representation of a parent phrase can be predicted with some accuracy given an affine transformation of its children. While we would expect the predictive accuracy to correlate with human judgments of semantic compositionality, we find this is largely not the case, indicating that LMs may not accurately distinguish between compositional and non-compositional phrases. We perform a variety of analyses, shedding light on when different varieties of LMs do and do not generate compositional representations, and discuss implications for future modeling work.</abstract>
      <url hash="3f8d96fd">2022.emnlp-main.617</url>
      <bibkey>liu-neubig-2022-representations</bibkey>
    </paper>
    <paper id="618">
      <title>Detecting Label Errors by Using Pre-Trained Language Models</title>
      <author><first>Derek</first><last>Chong</last></author>
      <author><first>Jenny</first><last>Hong</last></author>
      <author><first>Christopher</first><last>Manning</last></author>
      <pages>9074-9091</pages>
      <abstract>We show that large pre-trained language models are inherently highly capable of identifying label errors in natural language datasets: simply examining out-of-sample data points in descending order of fine-tuned task loss significantly outperforms more complex error-detection mechanisms proposed in previous work. To this end, we contribute a novel method for introducing realistic, human-originated label noise into existing crowdsourced datasets such as SNLI and TweetNLP. We show that this noise has similar properties to real, hand-verified label errors, and is harder to detect than existing synthetic noise, creating challenges for model robustness.We argue that human-originated noise is a better standard for evaluation than synthetic noise. Finally, we use crowdsourced verification to evaluate the detection of real errors on IMDB, Amazon Reviews, and Recon, and confirm that pre-trained models perform at a 9–36% higher absolute Area Under the Precision-Recall Curve than existing models.</abstract>
      <url hash="0f6dd42f">2022.emnlp-main.618</url>
      <bibkey>chong-etal-2022-detecting</bibkey>
    </paper>
    <paper id="619">
      <title>Intriguing Properties of Compression on Multilingual Models</title>
      <author><first>Kelechi</first><last>Ogueji</last></author>
      <author><first>Orevaoghene</first><last>Ahia</last></author>
      <author><first>Gbemileke</first><last>Onilude</last></author>
      <author><first>Sebastian</first><last>Gehrmann</last></author>
      <author><first>Sara</first><last>Hooker</last></author>
      <author><first>Julia</first><last>Kreutzer</last></author>
      <pages>9092-9110</pages>
      <abstract>Multilingual models are often particularly dependent on scaling to generalize to a growing number of languages. Compression techniques are widely relied upon to reconcile the growth in model size with real world resource constraints, but compression can have a disparate effect on model performance for low-resource languages. It is thus crucial to understand the trade-offs between scale, multilingualism, and compression. In this work, we propose an experimental framework to characterize the impact of sparsifying multilingual pre-trained language models during fine-tuning.Applying this framework to mBERT named entity recognition models across 40 languages, we find that compression confers several intriguing and previously unknown generalization properties. In contrast to prior findings, we find that compression may improve model robustness over dense models. We additionally observe that under certain sparsification regimes compression may aid, rather than disproportionately impact the performance of low-resource languages.</abstract>
      <url hash="23c5bf59">2022.emnlp-main.619</url>
      <bibkey>ogueji-etal-2022-intriguing</bibkey>
    </paper>
    <paper id="620">
      <title>Sequence Models for Document Structure Identification in an Undeciphered Script</title>
      <author><first>Logan</first><last>Born</last></author>
      <author><first>M.</first><last>Monroe</last></author>
      <author><first>Kathryn</first><last>Kelley</last></author>
      <author><first>Anoop</first><last>Sarkar</last></author>
      <pages>9111-9121</pages>
      <abstract>This work describes the first thorough analysis of “header” signs in proto-Elamite, an undeciphered script from 3100-2900 BCE. Headers are a category of signs which have been provisionally identified through painstaking manual analysis of this script by domain experts. We use unsupervised neural and statistical sequence modeling techniques to provide new and independent evidence for the existence of headers, without supervision from domain experts. Having affirmed the existence of headers as a legitimate structural feature, we next arrive at a richer understanding of their possible meaning and purpose by (i) examining which features predict their presence; (ii) identifying correlations between these features and other document properties; and (iii) examining cases where these features predict the presence of a header in texts where domain experts do not expect one (or vice versa). We provide more concrete processes for labeling headers in this corpus and a clearer justification for existing intuitions about document structure in proto-Elamite.</abstract>
      <url hash="778fd20a">2022.emnlp-main.620</url>
      <bibkey>born-etal-2022-sequence</bibkey>
    </paper>
    <paper id="621">
      <title><fixed-case>E</fixed-case>nglish Contrastive Learning Can Learn Universal Cross-lingual Sentence Embeddings</title>
      <author><first>Yaushian</first><last>Wang</last></author>
      <author><first>Ashley</first><last>Wu</last></author>
      <author><first>Graham</first><last>Neubig</last></author>
      <pages>9122-9133</pages>
      <abstract>Universal cross-lingual sentence embeddings map semantically similar cross-lingual sentences into a shared embedding space. Aligning cross-lingual sentence embeddings usually requires supervised cross-lingual parallel sentences. In this work, we propose mSimCSE, which extends SimCSE to multilingual settings and reveal that contrastive learning on English data can surprisingly learn high-quality universal cross-lingual sentence embeddings without any parallel data.In unsupervised and weakly supervised settings, mSimCSE significantly improves previous sentence embedding methods on cross-lingual retrieval and multilingual STS tasks. The performance of unsupervised mSimCSE is comparable to fully supervised methods in retrieving low-resource languages and multilingual STS.The performance can be further enhanced when cross-lingual NLI data is available.</abstract>
      <url hash="c74d55ed">2022.emnlp-main.621</url>
      <bibkey>wang-etal-2022-english</bibkey>
    </paper>
    <paper id="622">
      <title>Active Example Selection for In-Context Learning</title>
      <author><first>Yiming</first><last>Zhang</last></author>
      <author><first>Shi</first><last>Feng</last></author>
      <author><first>Chenhao</first><last>Tan</last></author>
      <pages>9134-9148</pages>
      <abstract>With a handful of demonstration examples, large-scale language models demonstrate strong capability to perform various tasks by in-context learning from these examples, without any fine-tuning. We demonstrate that in-context learning performance can be highly unstable across samples of examples, indicating the idiosyncrasies of how language models acquire information. We formulate example selection for in-context learning as a sequential decision problem, and propose a reinforcement learning algorithm for identifying generalizable policies to select demonstration examples. For GPT-2, our learned policies demonstrate strong abilities of generalizing to unseen tasks in training, with a 5.8% improvement on average. Examples selected from our learned policies can even achieve a small improvement on GPT-3 Ada. However, the improvement diminishes on larger GPT-3 models, suggesting emerging capabilities of large language models.</abstract>
      <url hash="b20d0cac">2022.emnlp-main.622</url>
      <bibkey>zhang-etal-2022-active</bibkey>
    </paper>
    <paper id="623">
      <title>Improving Factual Consistency in Summarization with Compression-Based Post-Editing</title>
      <author><first>Alex</first><last>Fabbri</last></author>
      <author><first>Prafulla Kumar</first><last>Choubey</last></author>
      <author><first>Jesse</first><last>Vig</last></author>
      <author><first>Chien-Sheng</first><last>Wu</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <pages>9149-9156</pages>
      <abstract>State-of-the-art summarization models still struggle to be factually consistent with the input text. A model-agnostic way to address this problem is post-editing the generated summaries. However, existing approaches typically fail to remove entity errors if a suitable input entity replacement is not available or may insert erroneous content. In our work, we focus on removing extrinsic entity errors, or entities not in the source, to improve consistency while retaining the summary’s essential information and form. We propose to use sentence-compression data to train the post-editing model to take a summary with extrinsic entity errors marked with special tokens and output a compressed, well-formed summary with those errors removed. We show that this model improves factual consistency while maintaining ROUGE, improving entity precision by up to 30% on XSum, and that this model can be applied on top of another post-editor, improving entity precision by up to a total of 38%. We perform an extensive comparison of post-editing approaches that demonstrate trade-offs between factual consistency, informativeness, and grammaticality, and we analyze settings where post-editors show the largest improvements.</abstract>
      <url hash="0093d670">2022.emnlp-main.623</url>
      <bibkey>fabbri-etal-2022-improving</bibkey>
    </paper>
    <paper id="624">
      <title>Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing</title>
      <author><first>Linlu</first><last>Qiu</last></author>
      <author><first>Peter</first><last>Shaw</last></author>
      <author><first>Panupong</first><last>Pasupat</last></author>
      <author><first>Tianze</first><last>Shi</last></author>
      <author><first>Jonathan</first><last>Herzig</last></author>
      <author><first>Emily</first><last>Pitler</last></author>
      <author><first>Fei</first><last>Sha</last></author>
      <author><first>Kristina</first><last>Toutanova</last></author>
      <pages>9157-9179</pages>
      <abstract>Despite their strong performance on many tasks, pre-trained language models have been shown to struggle on out-of-distribution compositional generalization. Meanwhile, recent work has shown considerable improvements on many NLP tasks from model scaling. Can scaling up model size also improve compositional generalization in semantic parsing? We evaluate encoder-decoder models up to 11B parameters and decoder-only models up to 540B parameters, and compare model scaling curves for three different methods for applying a pre-trained language model to a new task: fine-tuning all parameters, prompt tuning, and in-context learning. We observe that fine-tuning generally has flat or negative scaling curves on out-of-distribution compositional generalization in semantic parsing evaluations. In-context learning has positive scaling curves, but is generally outperformed by much smaller fine-tuned models. Prompt-tuning can outperform fine-tuning, suggesting further potential improvements from scaling as it exhibits a more positive scaling curve. Additionally, we identify several error trends that vary with model scale. For example, larger models are generally better at modeling the syntax of the output space, but are also more prone to certain types of overfitting. Overall, our study highlights limitations of current techniques for effectively leveraging model scale for compositional generalization, while our analysis also suggests promising directions for future work.</abstract>
      <url hash="ce0ed65a">2022.emnlp-main.624</url>
      <bibkey>qiu-etal-2022-evaluating</bibkey>
    </paper>
    <paper id="625">
      <title>“<fixed-case>I</fixed-case>’m sorry to hear that”: Finding New Biases in Language Models with a Holistic Descriptor Dataset</title>
      <author><first>Eric Michael</first><last>Smith</last></author>
      <author><first>Melissa</first><last>Hall</last></author>
      <author><first>Melanie</first><last>Kambadur</last></author>
      <author><first>Eleonora</first><last>Presani</last></author>
      <author><first>Adina</first><last>Williams</last></author>
      <pages>9180-9211</pages>
      <abstract>As language models grow in popularity, it becomes increasingly important to clearly measure all possible markers of demographic identity in order to avoid perpetuating existing societal harms. Many datasets for measuring bias currently exist, but they are restricted in their coverage of demographic axes and are commonly used with preset bias tests that presuppose which types of biases models can exhibit. In this work, we present a new, more inclusive bias measurement dataset, HolisticBias, which includes nearly 600 descriptor terms across 13 different demographic axes. HolisticBias was assembled in a participatory process including experts and community members with lived experience of these terms. These descriptors combine with a set of bias measurement templates to produce over 450,000 unique sentence prompts, which we use to explore, identify, and reduce novel forms of bias in several generative models. We demonstrate that HolisticBias is effective at measuring previously undetectable biases in token likelihoods from language models, as well as in an offensiveness classifier. We will invite additions and amendments to the dataset, which we hope will serve as a basis for more easy-to-use and standardized methods for evaluating bias in NLP models.</abstract>
      <url hash="9e515240">2022.emnlp-main.625</url>
      <bibkey>smith-etal-2022-im</bibkey>
    </paper>
    <paper id="626">
      <title>Understanding <fixed-case>ME</fixed-case>? Multimodal Evaluation for Fine-grained Visual Commonsense</title>
      <author><first>Zhecan</first><last>Wang</last></author>
      <author><first>Haoxuan</first><last>You</last></author>
      <author><first>Yicheng</first><last>He</last></author>
      <author><first>Wenhao</first><last>Li</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Shih-Fu</first><last>Chang</last></author>
      <pages>9212-9224</pages>
      <abstract>Visual commonsense understanding requires Vision Language (VL) models to not only understand image and text but also cross-reference in-between to fully integrate and achieve comprehension of the visual scene described. Recently, various approaches have been developed and have achieved high performance on visual commonsense benchmarks. However, it is unclear whether the models really understand the visual scene and underlying commonsense knowledge due to limited evaluation data resources. To provide an in-depth analysis, we present a Multimodal Evaluation (ME) pipeline to automatically generate question-answer pairs to test models’ understanding of the visual scene, text, and related knowledge. We then take a step further to show that training with the ME data boosts the model’s performance in standard VCR evaluation. Lastly, our in-depth analysis and comparison reveal interesting findings: (1) semantically low-level information can assist the learning of high-level information but not the opposite; (2) visual information is generally under utilization compared with text.</abstract>
      <url hash="d74ca47a">2022.emnlp-main.626</url>
      <bibkey>wang-etal-2022-understanding-multimodal</bibkey>
    </paper>
    <paper id="627">
      <title>Semantic Novelty Detection and Characterization in Factual Text Involving Named Entities</title>
      <author><first>Nianzu</first><last>Ma</last></author>
      <author><first>Sahisnu</first><last>Mazumder</last></author>
      <author><first>Alexander</first><last>Politowicz</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <author><first>Eric</first><last>Robertson</last></author>
      <author><first>Scott</first><last>Grigsby</last></author>
      <pages>9225-9252</pages>
      <abstract>Much of the existing work on text novelty detection has been studied at the topic level, i.e., identifying whether the topic of a document or a sentence is novel or not. Little work has been done at the fine-grained semantic level (or contextual level). For example, given that we know Elon Musk is the CEO of a technology company, the sentence “Elon Musk acted in the sitcom The Big Bang Theory” is novel and surprising because normally a CEO would not be an actor. Existing topic-based novelty detection methods work poorly on this problem because they do not perform semantic reasoning involving relations between named entities in the text and their background knowledge. This paper proposes an effective model (called PAT-SND) to solve the problem, which can also characterize the novelty. An annotated dataset is also created. Evaluation shows that PAT-SND outperforms 10 baselines by large margins.</abstract>
      <url hash="89568157">2022.emnlp-main.627</url>
      <bibkey>ma-etal-2022-semantic</bibkey>
    </paper>
    <paper id="628">
      <title><fixed-case>CN</fixed-case>-<fixed-case>A</fixed-case>uto<fixed-case>MIC</fixed-case>: Distilling <fixed-case>C</fixed-case>hinese Commonsense Knowledge from Pretrained Language Models</title>
      <author><first>Chenhao</first><last>Wang</last></author>
      <author><first>Jiachun</first><last>Li</last></author>
      <author><first>Yubo</first><last>Chen</last></author>
      <author><first>Kang</first><last>Liu</last></author>
      <author><first>Jun</first><last>Zhao</last></author>
      <pages>9253-9265</pages>
      <abstract>Commonsense knowledge graphs (CKGs) are increasingly applied in various natural language processing tasks. However, most existing CKGs are limited to English, which hinders related research in non-English languages. Meanwhile, directly generating commonsense knowledge from pretrained language models has recently received attention, yet it has not been explored in non-English languages. In this paper, we propose a large-scale Chinese CKG generated from multilingual PLMs, named as **CN-AutoMIC**, aiming to fill the research gap of non-English CKGs. To improve the efficiency, we propose generate-by-category strategy to reduce invalid generation. To ensure the filtering quality, we develop cascaded filters to discard low-quality results. To further increase the diversity and density, we introduce a bootstrapping iteration process to reuse generated results. Finally, we conduct detailed analyses on CN-AutoMIC from different aspects. Empirical results show the proposed CKG has high quality and diversity, surpassing the direct translation version of similar English CKGs. We also find some interesting deficiency patterns and differences between relations, which reveal pending problems in commonsense knowledge generation. We share the resources and related models for further study.</abstract>
      <url hash="624b0550">2022.emnlp-main.628</url>
      <bibkey>wang-etal-2022-cn</bibkey>
    </paper>
    <paper id="629">
      <title>Calibrating Student Models for Emotion-related Tasks</title>
      <author><first>Mahshid</first><last>Hosseini</last></author>
      <author><first>Cornelia</first><last>Caragea</last></author>
      <pages>9266-9278</pages>
      <abstract>Knowledge Distillation (KD) is an effective method to transfer knowledge from one network (a.k.a. teacher) to another (a.k.a. student). In this paper, we study KD on the emotion-related tasks from a new perspective: calibration. We further explore the impact of the mixup data augmentation technique on the distillation objective and propose to use a simple yet effective mixup method informed by training dynamics for calibrating the student models. Underpinned by the regularization impact of the mixup process by providing better training signals to the student models using training dynamics, our proposed mixup strategy gradually enhances the student model’s calibration while effectively improving its performance. We evaluate the calibration of pre-trained language models through knowledge distillation over three tasks of emotion detection, sentiment analysis, and empathy detection. By conducting extensive experiments on different datasets, with both in-domain and out-of-domain test sets, we demonstrate that student models distilled from teacher models trained using our proposed mixup method obtained the lowest Expected Calibration Errors (ECEs) and best performance on both in-domain and out-of-domain test sets.</abstract>
      <url hash="63644091">2022.emnlp-main.629</url>
      <bibkey>hosseini-caragea-2022-calibrating</bibkey>
    </paper>
    <paper id="630">
      <title>Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation</title>
      <author><first>Tu</first><last>Vu</last></author>
      <author><first>Aditya</first><last>Barua</last></author>
      <author><first>Brian</first><last>Lester</last></author>
      <author><first>Daniel</first><last>Cer</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <author><first>Noah</first><last>Constant</last></author>
      <pages>9279-9300</pages>
      <abstract>In this paper, we explore the challenging problem of performing a generative task in a target language when labeled data is only available in English, using summarization as a case study. We assume a strict setting with no access to parallel data or machine translation and find that common transfer learning approaches struggle in this setting, as a generative multilingual model fine-tuned purely on English catastrophically forgets how to generate non-English. Given the recent rise of parameter-efficient adaptation techniques, we conduct the first investigation into how one such method, prompt tuning (Lester et al., 2021), can overcome catastrophic forgetting to enable zero-shot cross-lingual generation. Our experiments show that parameter-efficient prompt tuning provides gains over standard fine-tuning when transferring between less-related languages, e.g., from English to Thai. However, a significant gap still remains between these methods and fully-supervised baselines. To improve cross-lingual transfer further, we explore several approaches, including: (1) mixing in unlabeled multilingual data, and (2) explicitly factoring prompts into recombinable language and task components. Our approaches can provide further quality gains, suggesting that robust zero-shot cross-lingual generation is within reach.</abstract>
      <url hash="a80afca9">2022.emnlp-main.630</url>
      <bibkey>vu-etal-2022-overcoming</bibkey>
    </paper>
    <paper id="631">
      <title>Improving Large-scale Paraphrase Acquisition and Generation</title>
      <author><first>Yao</first><last>Dou</last></author>
      <author><first>Chao</first><last>Jiang</last></author>
      <author><first>Wei</first><last>Xu</last></author>
      <pages>9301-9323</pages>
      <abstract>This paper addresses the quality issues in existing Twitter-based paraphrase datasets, and discusses the necessity of using two separate definitions of paraphrase for identification and generation tasks. We present a new Multi-Topic Paraphrase in Twitter (MultiPIT) corpus that consists of a total of 130k sentence pairs with crowdsoursing (MultiPIT_crowd) and expert (MultiPIT_expert) annotations using two different paraphrase definitions for paraphrase identification, in addition to a multi-reference test set (MultiPIT_NMR) and a large automatically constructed training set (MultiPIT_Auto) for paraphrase generation. With improved data annotation quality and task-specific paraphrase definition, the best pre-trained language model fine-tuned on our dataset achieves the state-of-the-art performance of 84.2 F1 for automatic paraphrase identification. Furthermore, our empirical results also demonstrate that the paraphrase generation models trained on MultiPIT_Auto generate more diverse and high-quality paraphrases compared to their counterparts fine-tuned on other corpora such as Quora, MSCOCO, and ParaNMT.</abstract>
      <url hash="8f8da0fe">2022.emnlp-main.631</url>
      <bibkey>dou-etal-2022-improving</bibkey>
    </paper>
    <paper id="632">
      <title>Entropy- and Distance-Based Predictors From <fixed-case>GPT</fixed-case>-2 Attention Patterns Predict Reading Times Over and Above <fixed-case>GPT</fixed-case>-2 Surprisal</title>
      <author><first>Byung-Doh</first><last>Oh</last></author>
      <author><first>William</first><last>Schuler</last></author>
      <pages>9324-9334</pages>
      <abstract>Transformer-based large language models are trained to make predictions about the next word by aggregating representations of previous tokens through their self-attention mechanism. In the field of cognitive modeling, such attention patterns have recently been interpreted as embodying the process of cue-based retrieval, in which attention over multiple targets is taken to generate interference and latency during retrieval. Under this framework, this work first defines an entropy-based predictor that quantifies the diffuseness of self-attention, as well as distance-based predictors that capture the incremental change in attention patterns across timesteps. Moreover, following recent studies that question the informativeness of attention weights, we also experiment with alternative methods for incorporating vector norms into attention weights. Regression experiments using predictors calculated from the GPT-2 language model show that these predictors deliver a substantially better fit to held-out self-paced reading and eye-tracking data over a rigorous baseline including GPT-2 surprisal.</abstract>
      <url hash="306ae8aa">2022.emnlp-main.632</url>
      <bibkey>oh-schuler-2022-entropy</bibkey>
    </paper>
    <paper id="633">
      <title>A Survey of Computational Framing Analysis Approaches</title>
      <author><first>Mohammad</first><last>Ali</last></author>
      <author><first>Naeemul</first><last>Hassan</last></author>
      <pages>9335-9348</pages>
      <abstract>Framing analysis is predominantly qualitative and quantitative, examining a small dataset with manual coding. Easy access to digital data in the last two decades prompts scholars in both computation and social sciences to utilize various computational methods to explore frames in large-scale datasets. The growing scholarship, however, lacks a comprehensive understanding and resources of computational framing analysis methods. Aiming to address the gap, this article surveys existing computational framing analysis approaches and puts them together. The research is expected to help scholars and journalists gain a deeper understanding of how frames are being explored computationally, better equip them to analyze frames in large-scale datasets, and, finally, work on advancing methodological approaches.</abstract>
      <url hash="b0fd12f0">2022.emnlp-main.633</url>
      <bibkey>ali-hassan-2022-survey</bibkey>
    </paper>
    <paper id="634">
      <title>Learning Cross-Task Dependencies for Joint Extraction of Entities, Events, Event Arguments, and Relations</title>
      <author><first>Minh Van</first><last>Nguyen</last></author>
      <author><first>Bonan</first><last>Min</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Thien</first><last>Nguyen</last></author>
      <pages>9349-9360</pages>
      <abstract>Extracting entities, events, event arguments, and relations (i.e., task instances) from text represents four main challenging tasks in information extraction (IE), which have been solved jointly (JointIE) to boost the overall performance for IE. As such, previous work often leverages two types of dependencies between the tasks, i.e., cross-instance and cross-type dependencies representing relatedness between task instances and correlations between information types of the tasks. However, the cross-task dependencies in prior work are not optimal as they are only designed manually according to some task heuristics. To address this issue, we propose a novel model for JointIE that aims to learn cross-task dependencies from data. In particular, we treat each task instance as a node in a dependency graph where edges between the instances are inferred through information from different layers of a pretrained language model (e.g., BERT). Furthermore, we utilize the Chow-Liu algorithm to learn a dependency tree between information types for JointIE by seeking to approximate the joint distribution of the types from data. Finally, the Chow-Liu dependency tree is used to generate cross-type patterns, serving as anchor knowledge to guide the learning of representations and dependencies between instances for JointIE. Experimental results show that our proposed model significantly outperforms strong JointIE baselines over four datasets with different languages.</abstract>
      <url hash="ee44f126">2022.emnlp-main.634</url>
      <bibkey>nguyen-etal-2022-learning</bibkey>
    </paper>
    <paper id="635">
      <title>Don’t Copy the Teacher: Data and Model Challenges in Embodied Dialogue</title>
      <author><first>So Yeon</first><last>Min</last></author>
      <author><first>Hao</first><last>Zhu</last></author>
      <author><first>Ruslan</first><last>Salakhutdinov</last></author>
      <author><first>Yonatan</first><last>Bisk</last></author>
      <pages>9361-9368</pages>
      <abstract>Embodied dialogue instruction following requires an agent to complete a complex sequence of tasks from a natural language exchange. The recent introduction of benchmarks raises the question of how best to train and evaluate models for this multi-turn, multi-agent, long-horizon task. This paper contributes to that conversation, by arguing that imitation learning (IL) and related low-level metrics are actually misleading and do not align with the goals of embodied dialogue research and may hinder progress.We provide empirical comparisons of metrics, analysis of three models, and make suggestions for how the field might best progress. First, we observe that models trained with IL take spurious actions during evaluation. Second, we find that existing models fail to ground query utterances, which are essential for task completion. Third, we argue evaluation should focus on higher-level semantic goals. We will release code to additionally filter the data and benchmark models for improved evaluation.</abstract>
      <url hash="54e27c07">2022.emnlp-main.635</url>
      <bibkey>min-etal-2022-dont</bibkey>
    </paper>
    <paper id="636">
      <title><fixed-case>ALFRED</fixed-case>-<fixed-case>L</fixed-case>: Investigating the Role of Language for Action Learning in Interactive Visual Environments</title>
      <author><first>Arjun</first><last>Akula</last></author>
      <author><first>Spandana</first><last>Gella</last></author>
      <author><first>Aishwarya</first><last>Padmakumar</last></author>
      <author><first>Mahdi</first><last>Namazifar</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <author><first>Jesse</first><last>Thomason</last></author>
      <author><first>Dilek</first><last>Hakkani-Tur</last></author>
      <pages>9369-9378</pages>
      <abstract>Embodied Vision and Language Task Completion requires an embodied agent to interpret natural language instructions and egocentric visual observations to navigate through and interact with environments. In this work, we examine ALFRED, a challenging benchmark for embodied task completion, with the goal of gaining insight into how effectively models utilize language. We find evidence that sequence-to-sequence and transformer-based models trained on this benchmark are not sufficiently sensitive to changes in input language instructions. Next, we construct a new test split – ALFRED-L to test whether ALFRED models can generalize to task structures not seen during training that intuitively require the same types of language understanding required in ALFRED. Evaluation of existing models on ALFRED-L suggests that (a) models are overly reliant on the sequence in which objects are visited in typical ALFRED trajectories and fail to adapt to modifications of this sequence and (b) models trained with additional augmented trajectories are able to adapt relatively better to such changes in input language instructions.</abstract>
      <url hash="f707031a">2022.emnlp-main.636</url>
      <bibkey>akula-etal-2022-alfred</bibkey>
    </paper>
    <paper id="637">
      <title>Dungeons and Dragons as a Dialog Challenge for Artificial Intelligence</title>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <author><first>Gaurav Singh</first><last>Tomar</last></author>
      <author><first>Lara</first><last>Martin</last></author>
      <author><first>Daphne</first><last>Ippolito</last></author>
      <author><first>Suma</first><last>Bailis</last></author>
      <author><first>David</first><last>Reitter</last></author>
      <pages>9379-9393</pages>
      <abstract>AI researchers have posited Dungeons and Dragons (D&amp;D) as a challenge problem to test systems on various language-related capabilities. In this paper, we frame D&amp;D specifically as a dialogue system challenge, where the tasks are to both generate the next conversational turn in the game and predict the state of the game given the dialogue history. We create a gameplay dataset consisting of nearly 900 games, with a total of 7,000 players, 800,000 dialogue turns, 500,000 dice rolls, and 58 million words. We automatically annotate the data with partial state information about the game play. We train a large language model (LM) to generate the next game turn, conditioning it on different information. The LM can respond as a particular character or as the player who runs the game—i.e., the Dungeon Master (DM). It is trained to produce dialogue that is either in-character (roleplaying in the fictional world) or out-of-character (discussing rules or strategy). We perform a human evaluation to determine what factors make the generated output plausible and interesting. We further perform an automatic evaluation to determine how well the model can predict the game state given the history and examine how well tracking the game state improves its ability to produce plausible conversational output.</abstract>
      <url hash="48c5d9eb">2022.emnlp-main.637</url>
      <bibkey>callison-burch-etal-2022-dungeons</bibkey>
    </paper>
    <paper id="638">
      <title>Unsupervised Entity Linking with Guided Summarization and Multiple-Choice Selection</title>
      <author><first>Young Min</first><last>Cho</last></author>
      <author><first>Li</first><last>Zhang</last></author>
      <author><first>Chris</first><last>Callison-Burch</last></author>
      <pages>9394-9401</pages>
      <abstract>Entity linking, the task of linking potentially ambiguous mentions in texts to corresponding knowledge-base entities, is an important component for language understanding. We address two challenge in entity linking: how to leverage wider contexts surrounding a mention, and how to deal with limited training data. We propose a fully unsupervised model called SumMC that first generates a guided summary of the contexts conditioning on the mention, and then casts the task to a multiple-choice problem where the model chooses an entity from a list of candidates. In addition to evaluating our model on existing datasets that focus on named entities, we create a new dataset that links noun phrases from WikiHow to Wikidata. We show that our SumMC model achieves state-of-the-art unsupervised performance on our new dataset and on exiting datasets.</abstract>
      <url hash="6fe0c9d8">2022.emnlp-main.638</url>
      <bibkey>cho-etal-2022-unsupervised</bibkey>
    </paper>
    <paper id="639">
      <title>Weakly-Supervised Temporal Article Grounding</title>
      <author><first>Long</first><last>Chen</last></author>
      <author><first>Yulei</first><last>Niu</last></author>
      <author><first>Brian</first><last>Chen</last></author>
      <author><first>Xudong</first><last>Lin</last></author>
      <author><first>Guangxing</first><last>Han</last></author>
      <author><first>Christopher</first><last>Thomas</last></author>
      <author><first>Hammad</first><last>Ayyubi</last></author>
      <author><first>Heng</first><last>Ji</last></author>
      <author><first>Shih-Fu</first><last>Chang</last></author>
      <pages>9402-9413</pages>
      <abstract>Given a long untrimmed video and natural language queries, video grounding (VG) aims to temporally localize the semantically-aligned video segments. Almost all existing VG work holds two simple but unrealistic assumptions: 1) All query sentences can be grounded in the corresponding video. 2) All query sentences for the same video are always at the same semantic scale. Unfortunately, both assumptions make today’s VG models fail to work in practice. For example, in real-world multimodal assets (eg, news articles), most of the sentences in the article can not be grounded in their affiliated videos, and they typically have rich hierarchical relations (ie, at different semantic scales). To this end, we propose a new challenging grounding task: Weakly-Supervised temporal Article Grounding (WSAG). Specifically, given an article and a relevant video, WSAG aims to localize all “groundable” sentences to the video, and these sentences are possibly at different semantic scales. Accordingly, we collect the first WSAG dataset to facilitate this task: YouwikiHow, which borrows the inherent multi-scale descriptions in wikiHow articles and plentiful YouTube videos. In addition, we propose a simple but effective method DualMIL for WSAG, which consists of a two-level MIL loss and a single-/cross- sentence constraint loss. These training objectives are carefully designed for these relaxed assumptions. Extensive ablations have verified the effectiveness of DualMIL.</abstract>
      <url hash="4fa904f9">2022.emnlp-main.639</url>
      <bibkey>chen-etal-2022-weakly</bibkey>
    </paper>
    <paper id="640">
      <title>Exploring Dual Encoder Architectures for Question Answering</title>
      <author><first>Zhe</first><last>Dong</last></author>
      <author><first>Jianmo</first><last>Ni</last></author>
      <author><first>Dan</first><last>Bikel</last></author>
      <author><first>Enrique</first><last>Alfonseca</last></author>
      <author><first>Yuan</first><last>Wang</last></author>
      <author><first>Chen</first><last>Qu</last></author>
      <author><first>Imed</first><last>Zitouni</last></author>
      <pages>9414-9419</pages>
      <abstract>Dual encoders have been used for question-answering (QA) and information retrieval (IR) tasks with good results. There are two major types of dual encoders, Siamese Dual Encoders (SDE), with parameters shared across two encoders, and Asymmetric Dual Encoder (ADE), with two distinctly parameterized encoders. In this work, we explore the dual encoder architectures for QA retrieval tasks. By evaluating on MS MARCO, open domain NQ, and the MultiReQA benchmarks, we show that SDE performs significantly better than ADE. We further propose three different improved versions of ADEs. Based on the evaluation of QA retrieval tasks and direct analysis of the embeddings, we demonstrate that sharing parameters in projection layers would enable ADEs to perform competitively with SDEs.</abstract>
      <url hash="7e08fc34">2022.emnlp-main.640</url>
      <bibkey>dong-etal-2022-exploring</bibkey>
    </paper>
    <paper id="641">
      <title>ar<fixed-case>X</fixed-case>iv<fixed-case>E</fixed-case>dits: Understanding the Human Revision Process in Scientific Writing</title>
      <author><first>Chao</first><last>Jiang</last></author>
      <author><first>Wei</first><last>Xu</last></author>
      <author><first>Samuel</first><last>Stevens</last></author>
      <pages>9420-9435</pages>
      <abstract>Scientific publications are the primary means to communicate research discoveries, where the writing quality is of crucial importance. However, prior work studying the human editing process in this domain mainly focused on the abstract or introduction sections, resulting in an incomplete picture. In this work, we provide a complete computational framework for studying text revision in scientific writing. We first introduce arXivEdits, a new annotated corpus of 751 full papers from arXiv with gold sentence alignment across their multiple versions of revision, as well as fine-grained span-level edits and their underlying intentions for 1,000 sentence pairs. It supports our data-driven analysis to unveil the common strategies practiced by researchers for revising their papers. To scale up the analysis, we also develop automatic methods to extract revision at document-, sentence-, and word-levels. A neural CRF sentence alignment model trained on our corpus achieves 93.8 F1, enabling the reliable matching of sentences between different versions. We formulate the edit extraction task as a span alignment problem, and our proposed method extracts more fine-grained and explainable edits, compared to the commonly used diff algorithm. An intention classifier trained on our dataset achieves 78.9 F1 on the fine-grained intent classification task. Our data and system are released at tiny.one/arxivedits.</abstract>
      <url hash="40f9c393">2022.emnlp-main.641</url>
      <bibkey>jiang-etal-2022-arxivedits</bibkey>
    </paper>
    <paper id="642">
      <title>Why Do You Feel This Way? Summarizing Triggers of Emotions in Social Media Posts</title>
      <author><first>Hongli</first><last>Zhan</last></author>
      <author><first>Tiberiu</first><last>Sosea</last></author>
      <author><first>Cornelia</first><last>Caragea</last></author>
      <author><first>Junyi Jessy</first><last>Li</last></author>
      <pages>9436-9453</pages>
      <abstract>Crises such as the COVID-19 pandemic continuously threaten our world and emotionally affect billions of people worldwide in distinct ways. Understanding the triggers leading to people’s emotions is of crucial importance. Social media posts can be a good source of such analysis, yet these texts tend to be charged with multiple emotions, with triggers scattering across multiple sentences. This paper takes a novel angle, namely, emotion detection and trigger summarization, aiming to both detect perceived emotions in text, and summarize events and their appraisals that trigger each emotion. To support this goal, we introduce CovidET (Emotions and their Triggers during Covid-19), a dataset of ~1,900 English Reddit posts related to COVID-19, which contains manual annotations of perceived emotions and abstractive summaries of their triggers described in the post. We develop strong baselines to jointly detect emotions and summarize emotion triggers. Our analyses show that CovidET presents new challenges in emotion-specific summarization, as well as multi-emotion detection in long social media posts.</abstract>
      <url hash="a0445e3f">2022.emnlp-main.642</url>
      <bibkey>zhan-etal-2022-feel</bibkey>
    </paper>
    <paper id="643">
      <title>Analogical Math Word Problems Solving with Enhanced Problem-Solution Association</title>
      <author><first>Zhenwen</first><last>Liang</last></author>
      <author><first>Jipeng</first><last>Zhang</last></author>
      <author><first>Xiangliang</first><last>Zhang</last></author>
      <pages>9454-9464</pages>
      <abstract>Math word problem (MWP) solving is an important task in question answering which requires human-like reasoning ability. Analogical reasoning has long been used in mathematical education, as it enables students to apply common relational structures of mathematical situations to solve new problems. In this paper, we propose to build a novel MWP solver by leveraging analogical MWPs, which advance the solver’s generalization ability across different kinds of MWPs. The key idea, named analogy identification, is to associate the analogical MWP pairs in a latent space, i.e., encoding an MWP close to another analogical MWP, while leaving away from the non-analogical ones. Moreover, a solution discriminator is integrated into the MWP solver to enhance the association between an MWP and its true solution. The evaluation results verify that our proposed analogical learning strategy promotes the performance of MWP-BERT on Math23k over the state-of-the-art model Generate2Rank, with 5 times fewer parameters in the encoder. We also find that our model has a stronger generalization ability in solving difficult MWPs due to the analogical learning from easy MWPs.</abstract>
      <url hash="4192bb82">2022.emnlp-main.643</url>
      <bibkey>liang-etal-2022-analogical</bibkey>
    </paper>
    <paper id="644">
      <title>Towards Teachable Reasoning Systems: Using a Dynamic Memory of User Feedback for Continual System Improvement</title>
      <author><first>Bhavana</first><last>Dalvi Mishra</last></author>
      <author><first>Oyvind</first><last>Tafjord</last></author>
      <author><first>Peter</first><last>Clark</last></author>
      <pages>9465-9480</pages>
      <abstract>Our goal is a teachable reasoning system for question-answering (QA), where a user can interact with faithful answer explanations, and correct its errors so that the system improves over time. Our approach is to augment a QA model with a dynamic memory of user feedback, containing user-supplied corrections toerroneous model beliefs that users identify during interaction. Retrievals from memory are used as additional context for QA, to help avoid previous mistakes in similar new situations - a novel application of memory-based continuous learning. With simulated feedback, we find that our system (called TeachMe) continually improves with time, and without model retraining, requiring feedback on only 25% of training examples to reach within 1% of the upper-bound (feedback on all examples). Similarly, in experiments with real users, we observe a similar trend, with performance improving by over 15% on a hidden test set after teaching. This suggests new opportunities for using frozen language models in an interactive setting where users can inspect, debug, and correct the model’s beliefs, leading to improved system’s performance over time.</abstract>
      <url hash="dc0dbf43">2022.emnlp-main.644</url>
      <bibkey>dalvi-mishra-etal-2022-towards</bibkey>
    </paper>
    <paper id="645">
      <title>Knowledge Transfer from Answer Ranking to Answer Generation</title>
      <author><first>Matteo</first><last>Gabburo</last></author>
      <author><first>Rik</first><last>Koncel-Kedziorski</last></author>
      <author><first>Siddhant</first><last>Garg</last></author>
      <author><first>Luca</first><last>Soldaini</last></author>
      <author><first>Alessandro</first><last>Moschitti</last></author>
      <pages>9481-9495</pages>
      <abstract>Recent studies show that Question Answering (QA) based on Answer Sentence Selection (AS2) can be improved by generating an improved answer from the top-k ranked answer sentences (termed GenQA). This allows for synthesizing the information from multiple candidates into a concise, natural-sounding answer. However, creating large-scale supervised training data for GenQA models is very challenging. In this paper, we propose to train a GenQA model by transferring knowledge from a trained AS2 model, to overcome the aforementioned issue. First, we use an AS2 model to produce a ranking over answer candidates for a set of questions. Then, we use the top ranked candidate as the generation target, and the next k top ranked candidates as context for training a GenQA model. We also propose to use the AS2 model prediction scores for loss weighting and score-conditioned input/output shaping, to aid the knowledge transfer. Our evaluation on three public and one large industrial datasets demonstrates the superiority of our approach over the AS2 baseline, and GenQA trained using supervised data.</abstract>
      <url hash="c7f00338">2022.emnlp-main.645</url>
      <bibkey>gabburo-etal-2022-knowledge</bibkey>
    </paper>
    <paper id="646">
      <title>Perturbation Augmentation for Fairer <fixed-case>NLP</fixed-case></title>
      <author><first>Rebecca</first><last>Qian</last></author>
      <author><first>Candace</first><last>Ross</last></author>
      <author><first>Jude</first><last>Fernandes</last></author>
      <author><first>Eric Michael</first><last>Smith</last></author>
      <author><first>Douwe</first><last>Kiela</last></author>
      <author><first>Adina</first><last>Williams</last></author>
      <pages>9496-9521</pages>
      <abstract>Unwanted and often harmful social biases are becoming ever more salient in NLP research, affecting both models and datasets. In this work, we ask whether training on demographically perturbed data leads to fairer language models. We collect a large dataset of human annotated text perturbations and train a neural perturbation model, which we show outperforms heuristic alternatives. We find that (i) language models (LMs) pre-trained on demographically perturbed corpora are typically more fair, and (ii) LMs finetuned on perturbed GLUE datasets exhibit less demographic bias on downstream tasks, and (iii) fairness improvements do not come at the expense of performance on downstream tasks. Lastly, we discuss outstanding questions about how best to evaluate the (un)fairness of large language models. We hope that this exploration of neural demographic perturbation will help drive more improvement towards fairer NLP.</abstract>
      <url hash="358beb52">2022.emnlp-main.646</url>
      <bibkey>qian-etal-2022-perturbation</bibkey>
    </paper>
    <paper id="647">
      <title>Automatic Document Selection for Efficient Encoder Pretraining</title>
      <author><first>Yukun</first><last>Feng</last></author>
      <author><first>Patrick</first><last>Xia</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <author><first>João</first><last>Sedoc</last></author>
      <pages>9522-9530</pages>
      <abstract>Building pretrained language models is considered expensive and data-intensive, but must we increase dataset size to achieve better performance? We propose an alternative to larger training sets by automatically identifying smaller yet domain-representative subsets. We extend Cynical Data Selection, a statistical sentence scoring method that conditions on a representative target domain corpus. As an example, we treat the OntoNotes corpus as a target domain and pretrain a RoBERTa-like encoder from a cynically selected subset of the Pile. On both perplexity and across several downstream tasks in the target domain, it consistently outperforms random selection with 20x less data, 3x fewer training iterations, and 2x less estimated cloud compute cost, validating the recipe of automatic document selection for LM pretraining.</abstract>
      <url hash="3ba9c19e">2022.emnlp-main.647</url>
      <bibkey>feng-etal-2022-automatic</bibkey>
    </paper>
    <paper id="648">
      <title>The Aligned Multimodal Movie Treebank: An audio, video, dependency-parse treebank</title>
      <author><first>Adam</first><last>Yaari</last></author>
      <author><first>Jan</first><last>DeWitt</last></author>
      <author><first>Henry</first><last>Hu</last></author>
      <author><first>Bennett</first><last>Stankovits</last></author>
      <author><first>Sue</first><last>Felshin</last></author>
      <author><first>Yevgeni</first><last>Berzak</last></author>
      <author><first>Helena</first><last>Aparicio</last></author>
      <author><first>Boris</first><last>Katz</last></author>
      <author><first>Ignacio</first><last>Cases</last></author>
      <author><first>Andrei</first><last>Barbu</last></author>
      <pages>9531-9539</pages>
      <abstract>Treebanks have traditionally included only text and were derived from written sources such as newspapers or the web. We introduce the Aligned Multimodal Movie Treebank (AMMT), an English language treebank derived from dialog in Hollywood movies which includes transcriptions of the audio-visual streams with word-level alignment, as well as part of speech tags and dependency parses in the Universal Dependencies formalism. AMMT consists of 31,264 sentences and 218,090 words, that will amount to the 3rd largest UD English treebank and the only multimodal treebank in UD. To help with the web-based annotation effort, we also introduce the Efficient Audio Alignment Annotator (EAAA), a companion tool that enables annotators to significantly speed-up their annotation processes.</abstract>
      <url hash="0451e230">2022.emnlp-main.648</url>
      <bibkey>yaari-etal-2022-aligned</bibkey>
    </paper>
    <paper id="649">
      <title><fixed-case>DEMETR</fixed-case>: Diagnosing Evaluation Metrics for Translation</title>
      <author><first>Marzena</first><last>Karpinska</last></author>
      <author><first>Nishant</first><last>Raj</last></author>
      <author><first>Katherine</first><last>Thai</last></author>
      <author><first>Yixiao</first><last>Song</last></author>
      <author><first>Ankita</first><last>Gupta</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <pages>9540-9561</pages>
      <abstract>While machine translation evaluation metrics based on string overlap (e.g., BLEU) have their limitations, their computations are transparent: the BLEU score assigned to a particular candidate translation can be traced back to the presence or absence of certain words. The operations of newer learned metrics (e.g., BLEURT, COMET), which leverage pretrained language models to achieve higher correlations with human quality judgments than BLEU, are opaque in comparison. In this paper, we shed light on the behavior of these learned metrics by creating DEMETR, a diagnostic dataset with 31K English examples (translated from 10 source languages) for evaluating the sensitivity of MT evaluation metrics to 35 different linguistic perturbations spanning semantic, syntactic, and morphological error categories. All perturbations were carefully designed to form minimal pairs with the actual translation (i.e., differ in only one aspect). We find that learned metrics perform substantially better than string-based metrics on DEMETR. Additionally, learned metrics differ in their sensitivity to various phenomena (e.g., BERTScore is sensitive to untranslated words but relatively insensitive to gender manipulation, while COMET is much more sensitive to word repetition than to aspectual changes). We publicly release DEMETR to spur more informed future development of machine translation evaluation metrics</abstract>
      <url hash="ec12ab88">2022.emnlp-main.649</url>
      <bibkey>karpinska-etal-2022-demetr</bibkey>
    </paper>
    <paper id="650">
      <title>Empowering Language Models with Knowledge Graph Reasoning for Open-Domain Question Answering</title>
      <author><first>Ziniu</first><last>Hu</last></author>
      <author><first>Yichong</first><last>Xu</last></author>
      <author><first>Wenhao</first><last>Yu</last></author>
      <author><first>Shuohang</first><last>Wang</last></author>
      <author><first>Ziyi</first><last>Yang</last></author>
      <author><first>Chenguang</first><last>Zhu</last></author>
      <author><first>Kai-Wei</first><last>Chang</last></author>
      <author><first>Yizhou</first><last>Sun</last></author>
      <pages>9562-9581</pages>
      <abstract>Answering open-domain questions requires world knowledge about in-context entities. As pre-trained Language Models (LMs) lack the power to store all required knowledge, external knowledge sources, such as knowledge graphs, are often used to augment LMs. In this work, we propose knOwledge REasOning empowered Language Model(OREO-LM), which consists of a novel Knowledge Interaction Layer that can be flexibly plugged into existing Transformer-based LMs to interact with a differentiable Knowledge Graph Reasoning module collaboratively. In this way, LM guides KG to walk towards the desired answer, while the retrieved knowledge improves LM.By adopting OREO-LM to RoBERTa and T5, we show significant performance gain, achieving state-of-art results in the Closed-Book setting. The performance enhancement is mainly from the KG reasoning’s capacity to infer missing relational facts. In addition, OREO-LM provides reasoning paths as rationales to interpret the model’s decision.</abstract>
      <url hash="c5a6ecad">2022.emnlp-main.650</url>
      <bibkey>hu-etal-2022-empowering</bibkey>
    </paper>
    <paper id="651">
      <title>Debiasing Pretrained Text Encoders by Paying Attention to Paying Attention</title>
      <author><first>Yacine</first><last>Gaci</last></author>
      <author><first>Boualem</first><last>Benatallah</last></author>
      <author><first>Fabio</first><last>Casati</last></author>
      <author><first>Khalid</first><last>Benabdeslem</last></author>
      <pages>9582-9602</pages>
      <abstract>Natural Language Processing (NLP) models are found to exhibit discriminatory stereotypes across many social constructs, e.g. gender and race. In comparison to the progress made in reducing bias from static word embeddings, fairness in sentence-level text encoders received little consideration despite their wider applicability in contemporary NLP tasks. In this paper, we propose a debiasing method for pre-trained text encoders that both reduces social stereotypes, and inflicts next to no semantic damage. Unlike previous studies that directly manipulate the embeddings, we suggest to dive deeper into the operation of these encoders, and pay more attention to the way they pay attention to different social groups. We find that stereotypes are also encoded in the attention layer. Then, we work on model debiasing by redistributing the attention scores of a text encoder such that it forgets any preference to historically advantaged groups, and attends to all social classes with the same intensity. Our experiments confirm that reducing bias from attention effectively mitigates it from the model’s text representations.</abstract>
      <url hash="dcf5beee">2022.emnlp-main.651</url>
      <bibkey>gaci-etal-2022-debiasing</bibkey>
    </paper>
    <paper id="652">
      <title><fixed-case>MEE</fixed-case>: A Novel Multilingual Event Extraction Dataset</title>
      <author><first>Amir</first><last>Pouran Ben Veyseh</last></author>
      <author><first>Javid</first><last>Ebrahimi</last></author>
      <author><first>Franck</first><last>Dernoncourt</last></author>
      <author><first>Thien</first><last>Nguyen</last></author>
      <pages>9603-9613</pages>
      <abstract>Event Extraction (EE) is one of the fundamental tasks in Information Extraction (IE) that aims to recognize event mentions and their arguments (i.e., participants) from text. Due to its importance, extensive methods and resources have been developed for Event Extraction. However, one limitation of current research for EE involves the under-exploration for non-English languages in which the lack of high-quality multilingual EE datasets for model training and evaluation has been the main hindrance. To address this limitation, we propose a novel Multilingual Event Extraction dataset (MEE) that provides annotation for more than 50K event mentions in 8 typologically different languages. MEE comprehensively annotates data for entity mentions, event triggers and event arguments. We conduct extensive experiments on the proposed dataset to reveal challenges and opportunities for multilingual EE. To foster future research in this direction, our dataset will be publicly available.</abstract>
      <url hash="4418c469">2022.emnlp-main.652</url>
      <bibkey>pouran-ben-veyseh-etal-2022-mee</bibkey>
    </paper>
    <paper id="653">
      <title><fixed-case>R</fixed-case>obust<fixed-case>LR</fixed-case>: A Diagnostic Benchmark for Evaluating Logical Robustness of Deductive Reasoners</title>
      <author><first>Soumya</first><last>Sanyal</last></author>
      <author><first>Zeyi</first><last>Liao</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <pages>9614-9631</pages>
      <abstract>Transformers have been shown to be able to perform deductive reasoning on inputs containing rules and statements written in the English natural language. However, it is unclear if these models indeed follow rigorous logical reasoning to arrive at the prediction or rely on spurious correlation patterns in making decisions. A strong deductive reasoning model should consistently understand the semantics of different logical operators. To this end, we present RobustLR, a diagnostic benchmark that evaluates the robustness of language models to minimal logical edits in the inputs and different logical equivalence conditions. In our experiments with RoBERTa, T5, and GPT3 we show that the models trained on deductive reasoning datasets do not perform consistently on the RobustLR test set, thus showing that the models are not robust to our proposed logical perturbations. Further, we observe that the models find it especially hard to learn logical negation operators. Our results demonstrate the shortcomings of current language models in logical reasoning and call for the development of better inductive biases to teach the logical semantics to language models. All the datasets and code base have been made publicly available.</abstract>
      <url hash="742f3c65">2022.emnlp-main.653</url>
      <bibkey>sanyal-etal-2022-robustlr</bibkey>
    </paper>
    <paper id="654">
      <title>Evaluating and Improving Factuality in Multimodal Abstractive Summarization</title>
      <author><first>David</first><last>Wan</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>9632-9648</pages>
      <abstract>Current metrics for evaluating factuality for abstractive document summarization have achieved high correlations with human judgment, but they do not account for the vision modality and thus are not adequate for vision-and-language summarization. We propose CLIPBERTSCORE, a simple weighted combination of CLIPScore and BERTScore to leverage the robustness and strong factuality detection performance between image-summary and document-summary, respectively. Next, due to the lack of meta-evaluation benchmarks to evaluate the quality of multimodal factuality metrics, we collect human judgments of factuality with respect to documents and images. We show that this simple combination of two metrics in the zero-shot setting achieves higher correlations than existing factuality metrics for document summarization, outperforms an existing multimodal summarization metric, and performs competitively with strong multimodal factuality metrics specifically fine-tuned for the task. Our thorough analysis demonstrates the robustness and high correlation of CLIPBERTSCORE and its components on four factuality metric-evaluation benchmarks. Finally, we demonstrate two practical downstream applications of our CLIPBERTSCORE metric: for selecting important images to focus on during training, and as a reward for reinforcement learning to improve factuality of multimodal summary generation w.r.t automatic and human evaluation.</abstract>
      <url hash="a35f4d5e">2022.emnlp-main.654</url>
      <bibkey>wan-bansal-2022-evaluating</bibkey>
    </paper>
    <paper id="655">
      <title>Referee: Reference-Free Sentence Summarization with Sharper Controllability through Symbolic Knowledge Distillation</title>
      <author><first>Melanie</first><last>Sclar</last></author>
      <author><first>Peter</first><last>West</last></author>
      <author><first>Sachin</first><last>Kumar</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <pages>9649-9668</pages>
      <abstract>We present Referee, a novel framework for sentence summarization that can be trained reference-free (i.e., requiring no gold summaries for supervision), while allowing direct control for compression ratio. Our work is the first to demonstrate that reference-free, controlled sentence summarization is feasible via the conceptual framework of Symbolic Knowledge Distillation (West et al., 2022), where latent knowledge in pre-trained language models is distilled via explicit examples sampled from the teacher models, further purified with three types of filters: length, fidelity, and Information Bottleneck. Moreover, we uniquely propose iterative distillation of knowledge, where student models from the previous iteration of distillation serve as teacher models in the next iteration. Starting off from a relatively modest set of GPT3-generated summaries, we demonstrate how iterative knowledge distillation can lead to considerably smaller, but better summarizers with sharper controllability. A useful by-product of this iterative distillation process is a high-quality dataset of sentence-summary pairs with varying degrees of compression ratios. Empirical results demonstrate that the final student models vastly outperform the much larger GPT3-Instruct model in terms of the controllability of compression ratios, without compromising the quality of resulting summarization.</abstract>
      <url hash="7abf3dbe">2022.emnlp-main.655</url>
      <bibkey>sclar-etal-2022-referee</bibkey>
    </paper>
    <paper id="656">
      <title>Algorithms for Weighted Pushdown Automata</title>
      <author><first>Alexandra</first><last>Butoi</last></author>
      <author><first>Brian</first><last>DuSell</last></author>
      <author><first>Tim</first><last>Vieira</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <author><first>David</first><last>Chiang</last></author>
      <pages>9669-9680</pages>
      <abstract>Weighted pushdown automata (WPDAs) are at the core of many natural language processing tasks, like syntax-based statistical machine translation and transition-based dependency parsing. As most existing dynamic programming algorithms are designed for context-free grammars (CFGs), algorithms for PDAs often resort to a PDA-to-CFG conversion. In this paper, we develop novel algorithms that operate directly on WPDAs. Our algorithms are inspired by Lang’s algorithm, but use a more general definition of pushdown automaton and either reduce the space requirements by a factor of |Gamma| (the size of the stack alphabet) or reduce the runtime by a factor of more than |Q| (the number of states). When run on the same class of PDAs as Lang’s algorithm, our algorithm is both more space-efficient by a factor of |Gamma| and more time-efficient by a factor of |Q| x |Gamma|.</abstract>
      <url hash="f1b25795">2022.emnlp-main.656</url>
      <bibkey>butoi-etal-2022-algorithms</bibkey>
    </paper>
    <paper id="657">
      <title><fixed-case>MABEL</fixed-case>: Attenuating Gender Bias using Textual Entailment Data</title>
      <author><first>Jacqueline</first><last>He</last></author>
      <author><first>Mengzhou</first><last>Xia</last></author>
      <author><first>Christiane</first><last>Fellbaum</last></author>
      <author><first>Danqi</first><last>Chen</last></author>
      <pages>9681-9702</pages>
      <abstract>Pre-trained language models encode undesirable social biases, which are further exacerbated in downstream use. To this end, we propose MABEL (a Method for Attenuating Gender Bias using Entailment Labels), an intermediate pre-training approach for mitigating gender bias in contextualized representations. Key to our approach is the use of a contrastive learning objective on counterfactually augmented, gender-balanced entailment pairs from natural language inference (NLI) datasets. We also introduce an alignment regularizer that pulls identical entailment pairs along opposite gender directions closer. We extensively evaluate our approach on intrinsic and extrinsic metrics, and show that MABEL outperforms previous task-agnostic debiasing approaches in terms of fairness. It also preserves task performance after fine-tuning on downstream tasks. Together, these findings demonstrate the suitability of NLI data as an effective means of bias mitigation, as opposed to only using unlabeled sentences in the literature. Finally, we identify that existing approaches often use evaluation settings that are insufficient or inconsistent. We make an effort to reproduce and compare previous methods, and call for unifying the evaluation settings across gender debiasing methods for better future comparison.</abstract>
      <url hash="35efa2b2">2022.emnlp-main.657</url>
      <bibkey>he-etal-2022-mabel</bibkey>
    </paper>
    <paper id="658">
      <title>Breakpoint Transformers for Modeling and Tracking Intermediate Beliefs</title>
      <author><first>Kyle</first><last>Richardson</last></author>
      <author><first>Ronen</first><last>Tamari</last></author>
      <author><first>Oren</first><last>Sultan</last></author>
      <author><first>Dafna</first><last>Shahaf</last></author>
      <author><first>Reut</first><last>Tsarfaty</last></author>
      <author><first>Ashish</first><last>Sabharwal</last></author>
      <pages>9703-9719</pages>
      <abstract>Can we teach models designed for language understanding tasks to track and improve their beliefs through intermediate points in text? Besides making their inner workings more transparent, this would also help make models more reliable and consistent. To this end, we propose a representation learning framework called breakpoint modeling that allows for efficient and robust learning of this type. Given any text encoder and data marked with intermediate states (breakpoints) along with corresponding textual queries viewed as true/false propositions (i.e., the candidate intermediate beliefs of a model), our approach trains models in an efficient and end-to-end fashion to build intermediate representations that facilitate direct querying and training of beliefs at arbitrary points in text, alongside solving other end-tasks. We evaluate breakpoint modeling on a diverse set of NLU tasks including relation reasoning on Cluttr and narrative understanding on bAbI. Using novel proposition prediction tasks alongside these end-tasks, we show the benefit of our T5-based breakpoint transformer over strong conventional representation learning approaches in terms of processing efficiency, belief accuracy, and belief consistency, all with minimal to no degradation on the end-task. To show the feasibility of incorporating our belief tracker into more complex reasoning pipelines, we also obtain state-of-the-art performance on the three-tiered reasoning challenge for the recent TRIP benchmark (23-32% absolute improvement on Tasks 2-3).</abstract>
      <url hash="9f28ef65">2022.emnlp-main.658</url>
      <bibkey>richardson-etal-2022-breakpoint</bibkey>
    </paper>
    <paper id="659">
      <title>Late Fusion with Triplet Margin Objective for Multimodal Ideology Prediction and Analysis</title>
      <author><first>Changyuan</first><last>Qiu</last></author>
      <author><first>Winston</first><last>Wu</last></author>
      <author><first>Xinliang Frederick</first><last>Zhang</last></author>
      <author><first>Lu</first><last>Wang</last></author>
      <pages>9720-9736</pages>
      <abstract>Prior work on ideology prediction has largely focused on single modalities, i.e., text or images. In this work, we introduce the task of multimodal ideology prediction, where a model predicts binary or five-point scale ideological leanings, given a text-image pair with political content. We first collect five new large-scale datasets with English documents and images along with their ideological leanings, covering news articles from a wide range of mainstream media in US and social media posts from Reddit and Twitter. We conduct in-depth analyses on news articles and reveal differences in image content and usage across the political spectrum. Furthermore, we perform extensive experiments and ablation studies, demonstrating the effectiveness of targeted pretraining objectives on different model components. Our best-performing model, a late-fusion architecture pretrained with a triplet objective over multimodal content, outperforms the state-of-the-art text-only model by almost 4% and a strong multimodal baseline with no pretraining by over 3%.</abstract>
      <url hash="8dacbc2c">2022.emnlp-main.659</url>
      <bibkey>qiu-etal-2022-late</bibkey>
    </paper>
    <paper id="660">
      <title>Leveraging <fixed-case>QA</fixed-case> Datasets to Improve Generative Data Augmentation</title>
      <author><first>Dheeraj</first><last>Mekala</last></author>
      <author><first>Tu</first><last>Vu</last></author>
      <author><first>Timo</first><last>Schick</last></author>
      <author><first>Jingbo</first><last>Shang</last></author>
      <pages>9737-9750</pages>
      <abstract>The ability of generative language models (GLMs) to generate text has improved considerably in the last few years, enabling their use for generative data augmentation. In this work, we propose CONDA, an approach to further improve GLM’s ability to generate synthetic data by reformulating data generation as context generation for a given question-answer (QA) pair and leveraging QA datasets for training context generators. Then, we cast downstream tasks into the same question answering format and adapt the fine-tuned context generators to the target task domain. Finally, we use the fine-tuned GLM to generate relevant contexts, which are in turn used as synthetic training data for their corresponding tasks. We perform extensive experiments on multiple classification datasets and demonstrate substantial improvements in performance for both few- and zero-shot settings. Our analysis reveals that QA datasets that require high-level reasoning abilities (e.g., abstractive and common-sense QA datasets) tend to give the best boost in performance in both few-shot and zero-shot settings.</abstract>
      <url hash="2ab66279">2022.emnlp-main.660</url>
      <bibkey>mekala-etal-2022-leveraging</bibkey>
    </paper>
    <paper id="661">
      <title>Meta-Learning Fast Weight Language Models</title>
      <author><first>Kevin</first><last>Clark</last></author>
      <author><first>Kelvin</first><last>Guu</last></author>
      <author><first>Ming-Wei</first><last>Chang</last></author>
      <author><first>Panupong</first><last>Pasupat</last></author>
      <author><first>Geoffrey</first><last>Hinton</last></author>
      <author><first>Mohammad</first><last>Norouzi</last></author>
      <pages>9751-9757</pages>
      <abstract>Dynamic evaluation of language models (LMs) adapts model parameters at test time using gradient information from previous tokens and substantially improves LM performance. However, it requires over 3x more compute than standard inference. We present Fast Weight Layers (FWLs), a neural component that provides the benefits of dynamic evaluation much more efficiently by expressing gradient updates as linear attention. A key improvement over dynamic evaluation is that FWLs can also be applied at training time, so the model learns to make good use of gradient updates. FWLs can easily be added on top of existing transformer models, require relatively little extra compute or memory to run, and significantly improve language modeling perplexity.</abstract>
      <url hash="d2edb881">2022.emnlp-main.661</url>
      <bibkey>clark-etal-2022-meta</bibkey>
    </paper>
    <paper id="662">
      <title><fixed-case>CTL</fixed-case>++: Evaluating Generalization on Never-Seen Compositional Patterns of Known Functions, and Compatibility of Neural Representations</title>
      <author><first>Róbert</first><last>Csordás</last></author>
      <author><first>Kazuki</first><last>Irie</last></author>
      <author><first>Juergen</first><last>Schmidhuber</last></author>
      <pages>9758-9767</pages>
      <abstract>Well-designed diagnostic tasks have played a key role in studying the failure of neural nets (NNs) to generalize systematically. Famous examples include SCAN and Compositional Table Lookup (CTL). Here we introduce CTL++, a new diagnostic dataset based on compositions of unary symbolic functions. While the original CTL is used to test length generalization or productivity, CTL++ is designed to test systematicity of NNs, that is, their capability to generalize to unseen compositions of known functions. CTL++ splits functions into groups and tests performance on group elements composed in a way not seen during training. We show that recent CTL-solving Transformer variants fail on CTL++. The simplicity of the task design allows for fine-grained control of task difficulty, as well as many insightful analyses. For example, we measure how much overlap between groups is needed by tested NNs for learning to compose. We also visualize how learned symbol representations in outputs of functions from different groups are compatible in case of success but not in case of failure. These results provide insights into failure cases reported on more complex compositions in the natural language domain. Our code is public.</abstract>
      <url hash="2f64abad">2022.emnlp-main.662</url>
      <bibkey>csordas-etal-2022-ctl</bibkey>
    </paper>
    <paper id="663">
      <title>Learning with Rejection for Abstractive Text Summarization</title>
      <author><first>Meng</first><last>Cao</last></author>
      <author><first>Yue</first><last>Dong</last></author>
      <author><first>Jingyi</first><last>He</last></author>
      <author><first>Jackie Chi Kit</first><last>Cheung</last></author>
      <pages>9768-9780</pages>
      <abstract>State-of-the-art abstractive summarization systems frequently hallucinate content that is not supported by the source document, mainly due to noise in the training dataset.Existing methods opt to drop the noisy samples or tokens from the training set entirely, reducing the effective training set size and creating an artificial propensity to copy words from the source. In this work, we propose a training objective for abstractive summarization based on rejection learning, in which the model learns whether or not to reject potentially noisy tokens. We further propose a regularized decoding objective that penalizes non-factual candidate summaries during inference by using the rejection probability learned during training.We show that our method considerably improves the factuality of generated summaries in automatic and human evaluations when compared to five baseline models, and that it does so while increasing the abstractiveness of the generated summaries.</abstract>
      <url hash="ef395e09">2022.emnlp-main.663</url>
      <bibkey>cao-etal-2022-learning</bibkey>
    </paper>
    <paper id="664">
      <title>Adaptive Label Smoothing with Self-Knowledge in Natural Language Generation</title>
      <author><first>Dongkyu</first><last>Lee</last></author>
      <author><first>Ka Chun</first><last>Cheung</last></author>
      <author><first>Nevin</first><last>Zhang</last></author>
      <pages>9781-9792</pages>
      <abstract>Overconfidence has been shown to impair generalization and calibration of a neural network. Previous studies remedy this issue by adding a regularization term to a loss function, preventing a model from making a peaked distribution. Label smoothing smoothes target labels with a pre-defined prior label distribution; as a result, a model is learned to maximize the likelihood of predicting the soft label. Nonetheless, the amount of smoothing is the same in all samples and remains fixed in training. In other words, label smoothing does not reflect the change in probability distribution mapped by a model over the course of training. To address this issue, we propose a regularization scheme that brings dynamic nature into the smoothing parameter by taking model probability distribution into account, thereby varying the parameter per instance. A model in training self-regulates the extent of smoothing on the fly during forward propagation. Furthermore, inspired by recent work in bridging label smoothing and knowledge distillation, our work utilizes self-knowledge as a prior label distribution in softening target labels, and presents theoretical support for the regularization effect by knowledge distillation and the dynamic smoothing parameter. Our regularizer is validated comprehensively, and the result illustrates marked improvements in model generalization and calibration, enhancing robustness and trustworthiness of a model.</abstract>
      <url hash="87c80dc6">2022.emnlp-main.664</url>
      <bibkey>lee-etal-2022-adaptive</bibkey>
    </paper>
    <paper id="665">
      <title>Hard Gate Knowledge Distillation - Leverage Calibration for Robust and Reliable Language Model</title>
      <author><first>Dongkyu</first><last>Lee</last></author>
      <author><first>Zhiliang</first><last>Tian</last></author>
      <author><first>Yingxiu</first><last>Zhao</last></author>
      <author><first>Ka Chun</first><last>Cheung</last></author>
      <author><first>Nevin</first><last>Zhang</last></author>
      <pages>9793-9803</pages>
      <abstract>In knowledge distillation, a student model is trained with supervisions from both knowledge from a teacher and observations drawn from a training data distribution. Knowledge of a teacher is considered a subject that holds inter-class relations which send a meaningful supervision to a student; hence, much effort has been put to find such knowledge to be distilled. In this paper, we explore a question that has been given little attention: “when to distill such knowledge.” The question is answered in our work with the concept of model calibration; we view a teacher model not only as a source of knowledge but also as a gauge to detect miscalibration of a student. This simple and yet novel view leads to a hard gate knowledge distillation scheme that switches between learning from a teacher model and training data. We verify the gating mechanism in the context of natural language generation at both the token-level and the sentence-level. Empirical comparisons with strong baselines show that hard gate knowledge distillation not only improves model generalization, but also significantly lowers model calibration error.</abstract>
      <url hash="dc49c794">2022.emnlp-main.665</url>
      <bibkey>lee-etal-2022-hard</bibkey>
    </paper>
    <paper id="666">
      <title>Are All Spurious Features in Natural Language Alike? An Analysis through a Causal Lens</title>
      <author><first>Nitish</first><last>Joshi</last></author>
      <author><first>Xiang</first><last>Pan</last></author>
      <author><first>He</first><last>He</last></author>
      <pages>9804-9817</pages>
      <abstract>The term ‘spurious correlations’ has been used in NLP to informally denote any undesirable feature-label correlations. However, a correlation can be undesirable because (i) the feature is irrelevant to the label (e.g. punctuation in a review), or (ii) the feature’s effect on the label depends on the context (e.g. negation words in a review), which is ubiquitous in language tasks. In case (i), we want the model to be invariant to the feature, which is neither necessary nor sufficient for prediction. But in case (ii), even an ideal model (e.g. humans) must rely on the feature, since it is necessary (but not sufficient) for prediction. Therefore, a more fine-grained treatment of spurious features is needed to specify the desired model behavior. We formalize this distinction using a causal model and probabilities of necessity and sufficiency, which delineates the causal relations between a feature and a label. We then show that this distinction helps explain results of existing debiasing methods on different spurious features, and demystifies surprising results such as the encoding of spurious features in model representations after debiasing.</abstract>
      <url hash="af380bb4">2022.emnlp-main.666</url>
      <bibkey>joshi-etal-2022-spurious</bibkey>
    </paper>
    <paper id="667">
      <title>Correcting Diverse Factual Errors in Abstractive Summarization via Post-Editing and Language Model Infilling</title>
      <author><first>Vidhisha</first><last>Balachandran</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <author><first>William</first><last>Cohen</last></author>
      <author><first>Yulia</first><last>Tsvetkov</last></author>
      <pages>9818-9830</pages>
      <abstract>Abstractive summarization models often generate inconsistent summaries containing factual errors or hallucinated content. Recent works focus on correcting factual errors in generated summaries via post-editing. Such correction models are trained using adversarial non-factual summaries constructed using heuristic rules for injecting errors. However, generating non-factual summaries using heuristics often does not generalize well to actual model errors. In this work, we propose to generate hard, representative synthetic examples of non-factual summaries through infilling language models. With this data, we train a more robust fact-correction model to post-edit the summaries to improve factual consistency. Through quantitative and qualitative experiments on two popular summarization datasets— CNN/DM and XSum—we show that our approach vastly outperforms prior methods in correcting erroneous summaries. Our model—FactEdit—improves factuality scores by over ~11 points on CNN/DM and over ~31 points on XSum on average across multiple summarization models, producing more factual summaries while maintaining competitive summarization quality.</abstract>
      <url hash="7d272f16">2022.emnlp-main.667</url>
      <bibkey>balachandran-etal-2022-correcting</bibkey>
    </paper>
    <paper id="668">
      <title>Coordinated Topic Modeling</title>
      <author><first>Pritom Saha</first><last>Akash</last></author>
      <author><first>Jie</first><last>Huang</last></author>
      <author><first>Kevin Chen-Chuan</first><last>Chang</last></author>
      <pages>9831-9843</pages>
      <abstract>We propose a new problem called coordinated topic modeling that imitates human behavior while describing a text corpus. It considers a set of well-defined topics like the axes of a semantic space with a reference representation. It then uses the axes to model a corpus for easily understandable representation. This new task helps represent a corpus more interpretably by reusing existing knowledge and benefits the corpora comparison task. We design ECTM, an embedding-based coordinated topic model that effectively uses the reference representation to capture the target corpus-specific aspects while maintaining each topic’s global semantics. In ECTM, we introduce the topic- and document-level supervision with a self-training mechanism to solve the problem. Finally, extensive experiments on multiple domains show the superiority of our model over other baselines.</abstract>
      <url hash="e6e7d167">2022.emnlp-main.668</url>
      <bibkey>akash-etal-2022-coordinated</bibkey>
    </paper>
    <paper id="669">
      <title>Large Dual Encoders Are Generalizable Retrievers</title>
      <author><first>Jianmo</first><last>Ni</last></author>
      <author><first>Chen</first><last>Qu</last></author>
      <author><first>Jing</first><last>Lu</last></author>
      <author><first>Zhuyun</first><last>Dai</last></author>
      <author><first>Gustavo</first><last>Hernandez Abrego</last></author>
      <author><first>Ji</first><last>Ma</last></author>
      <author><first>Vincent</first><last>Zhao</last></author>
      <author><first>Yi</first><last>Luan</last></author>
      <author><first>Keith</first><last>Hall</last></author>
      <author><first>Ming-Wei</first><last>Chang</last></author>
      <author><first>Yinfei</first><last>Yang</last></author>
      <pages>9844-9855</pages>
      <abstract>It has been shown that dual encoders trained on one domain often fail to generalize to other domains for retrieval tasks. One widespread belief is that the bottleneck layer of a dual encoder, where the final score is simply a dot-product between a query vector and a passage vector, is too limited compared to models with fine-grained interactions between the query and the passage. In this paper, we challenge this belief by scaling up the size of the dual encoder model <i>while keeping the bottleneck layer as a single dot-product with a fixed size.</i> With multi-stage training, scaling up the model size brings significant improvement on a variety of retrieval tasks, especially for out-of-domain generalization. We further analyze the impact of the bottleneck layer and demonstrate diminishing improvement when scaling up the embedding size. Experimental results show that our dual encoders, <b>G</b>eneralizable <b>T</b>5-based dense <b>R</b>etrievers (GTR), outperform previous sparse and dense retrievers on the BEIR dataset significantly. Most surprisingly, our ablation study finds that GTR is very data efficient, as it only needs 10% of MS Marco supervised data to match the out-of-domain performance of using all supervised data.</abstract>
      <url hash="484be2c7">2022.emnlp-main.669</url>
      <bibkey>ni-etal-2022-large</bibkey>
    </paper>
    <paper id="670">
      <title><fixed-case>CRIPP</fixed-case>-<fixed-case>VQA</fixed-case>: Counterfactual Reasoning about Implicit Physical Properties via Video Question Answering</title>
      <author><first>Maitreya</first><last>Patel</last></author>
      <author><first>Tejas</first><last>Gokhale</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <author><first>Yezhou</first><last>Yang</last></author>
      <pages>9856-9870</pages>
      <abstract>Videos often capture objects, their visible properties, their motion, and the interactions between different objects. Objects also have physical properties such as mass, which the imaging pipeline is unable to directly capture. However, these properties can be estimated by utilizing cues from relative object motion and the dynamics introduced by collisions. In this paper, we introduce CRIPP-VQA, a new video question answering dataset for reasoning about the implicit physical properties of objects in a scene. CRIPP-VQA contains videos of objects in motion, annotated with questions that involve counterfactual reasoning about the effect of actions, questions about planning in order to reach a goal, and descriptive questions about visible properties of objects. The CRIPP-VQA test set enables evaluation under several out-of-distribution settings – videos with objects with masses, coefficients of friction, and initial velocities that are not observed in the training distribution. Our experiments reveal a surprising and significant performance gap in terms of answering questions about implicit properties (the focus of this paper) and explicit properties of objects (the focus of prior work).</abstract>
      <url hash="fd764c9d">2022.emnlp-main.670</url>
      <bibkey>patel-etal-2022-cripp</bibkey>
    </paper>
    <paper id="671">
      <title>Entity-centered Cross-document Relation Extraction</title>
      <author><first>Fengqi</first><last>Wang</last></author>
      <author><first>Fei</first><last>Li</last></author>
      <author><first>Hao</first><last>Fei</last></author>
      <author><first>Jingye</first><last>Li</last></author>
      <author><first>Shengqiong</first><last>Wu</last></author>
      <author><first>Fangfang</first><last>Su</last></author>
      <author><first>Wenxuan</first><last>Shi</last></author>
      <author><first>Donghong</first><last>Ji</last></author>
      <author><first>Bo</first><last>Cai</last></author>
      <pages>9871-9881</pages>
      <abstract>Relation Extraction (RE) is a fundamental task of information extraction, which has attracted a large amount of research attention. Previous studies focus on extracting the relations within a sentence or document, while currently researchers begin to explore cross-document RE. However, current cross-document RE methods directly utilize text snippets surrounding target entities in multiple given documents, which brings considerable noisy and non-relevant sentences. Moreover, they utilize all the text paths in a document bag in a coarse-grained way, without considering the connections between these text paths.In this paper, we aim to address both of these shortages and push the state-of-the-art for cross-document RE. First, we focus on input construction for our RE model and propose an entity-based document-context filter to retain useful information in the given documents by using the bridge entities in the text paths. Second, we propose a cross-document RE model based on cross-path entity relation attention, which allow the entity relations across text paths to interact with each other. We compare our cross-document RE method with the state-of-the-art methods in the dataset CodRED. Our method outperforms them by at least 10% in F1, thus demonstrating its effectiveness.</abstract>
      <url hash="d7df8b4e">2022.emnlp-main.671</url>
      <bibkey>wang-etal-2022-entity</bibkey>
    </paper>
    <paper id="672">
      <title>Exploring Document-Level Literary Machine Translation with Parallel Paragraphs from World Literature</title>
      <author><first>Katherine</first><last>Thai</last></author>
      <author><first>Marzena</first><last>Karpinska</last></author>
      <author><first>Kalpesh</first><last>Krishna</last></author>
      <author><first>Bill</first><last>Ray</last></author>
      <author><first>Moira</first><last>Inghilleri</last></author>
      <author><first>John</first><last>Wieting</last></author>
      <author><first>Mohit</first><last>Iyyer</last></author>
      <pages>9882-9902</pages>
      <abstract>Literary translation is a culturally significant task, but it is bottlenecked by the small number of qualified literary translators relative to the many untranslated works published around the world. Machine translation (MT) holds potential to complement the work of human translators by improving both training procedures and their overall efficiency. Literary translation is less constrained than more traditional MT settings since translators must balance meaning equivalence, readability, and critical interpretability in the target language. This property, along with the complex discourse-level context present in literary texts, also makes literary MT more challenging to computationally model and evaluate. To explore this task, we collect a dataset (Par3) of non-English language novels in the public domain, each aligned at the paragraph level to both human and automatic English translations. Using Par3, we discover that expert literary translators prefer reference human translations over machine-translated paragraphs at a rate of 84%, while state-of-the-art automatic MT metrics do not correlate with those preferences. The experts note that MT outputs contain not only mistranslations, but also discourse-disrupting errors and stylistic inconsistencies. To address these problems, we train a post-editing model whose output is preferred over normal MT output at a rate of 69% by experts. We publicly release Par3 to spur future research into literary MT.</abstract>
      <url hash="5d0ee56b">2022.emnlp-main.672</url>
      <bibkey>thai-etal-2022-exploring</bibkey>
    </paper>
    <paper id="673">
      <title>Label-aware Multi-level Contrastive Learning for Cross-lingual Spoken Language Understanding</title>
      <author><first>Shining</first><last>Liang</last></author>
      <author><first>Linjun</first><last>Shou</last></author>
      <author><first>Jian</first><last>Pei</last></author>
      <author><first>Ming</first><last>Gong</last></author>
      <author><first>Wanli</first><last>Zuo</last></author>
      <author><first>Xianglin</first><last>Zuo</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <pages>9903-9918</pages>
      <abstract>Despite the great success of spoken language understanding (SLU) in high-resource languages, it remains challenging in low-resource languages mainly due to the lack of labeled training data. The recent multilingual code-switching approach achieves better alignments of model representations across languages by constructing a mixed-language context in zero-shot cross-lingual SLU. However, current code-switching methods are limited to implicit alignment and disregard the inherent semantic structure in SLU, i.e., the hierarchical inclusion of utterances, slots and words. In this paper, we propose to model the utterance-slot-word structure by a multi-level contrastive learning framework at the utterance, slot and word levels to facilitate explicit alignment. Novel code-switching schemes are introduced to generate hard negative examples for our contrastive learning framework. Furthermore, we develop a label-aware joint model leveraging label semantics to enhance the implicit alignment and feed to contrastive learning. Our experimental results show that our proposed methods significantly improve the performance compared with the strong baselines on two zero-shot cross-lingual SLU benchmark datasets.</abstract>
      <url hash="9f0e00df">2022.emnlp-main.673</url>
      <bibkey>liang-etal-2022-label</bibkey>
    </paper>
    <paper id="674">
      <title>Polyglot Prompt: Multilingual Multitask Prompt Training</title>
      <author><first>Jinlan</first><last>Fu</last></author>
      <author><first>See-Kiong</first><last>Ng</last></author>
      <author><first>Pengfei</first><last>Liu</last></author>
      <pages>9919-9935</pages>
      <abstract>This paper aims for a potential architectural improvement for multilingual learning and asks: Can different tasks from different languages be modeled in a monolithic framework, i.e. without any task/language-specific module? The benefit of achieving this could open new doors for future multilingual research, including allowing systems trained on low resources to be further assisted by other languages as well as other tasks. We approach this goal by developing a learning framework named Polyglot Prompting to exploit prompting methods for learning a unified semantic space for different languages and tasks with multilingual prompt engineering. We performed a comprehensive evaluation of 6 tasks, namely topic classification, sentiment classification, named entity recognition, question answering, natural language inference, and summarization, covering 24 datasets and 49 languages. The experimental results demonstrated the efficacy of multilingual multitask prompt-based learning and led to inspiring observations. We also present an interpretable multilingual evaluation methodology and show how the proposed framework, multilingual multitask prompt training, works. We release all datasets prompted in the best setting and code.</abstract>
      <url hash="e45e4c7f">2022.emnlp-main.674</url>
      <bibkey>fu-etal-2022-polyglot</bibkey>
    </paper>
    <paper id="675">
      <title><fixed-case>V</fixed-case>is<fixed-case>T</fixed-case>o<fixed-case>T</fixed-case>: Vision-Augmented Table-to-Text Generation</title>
      <author><first>Prajwal</first><last>Gatti</last></author>
      <author><first>Anand</first><last>Mishra</last></author>
      <author><first>Manish</first><last>Gupta</last></author>
      <author><first>Mithun</first><last>Das Gupta</last></author>
      <pages>9936-9949</pages>
      <abstract>Table-to-text generation has been widely studied in the Natural Language Processing community in the recent years. We give a new perspective to this problem by incorporating signals from both tables as well as associated images to generate relevant text. While tables contain a structured list of facts, images are a rich source of unstructured visual information. For example, in the tourism domain, images can be used to infer knowledge such as the type of landmark (e.g., church), its architecture (e.g., Ancient Roman), and composition (e.g., white marble). Therefore, in this paper, we introduce the novel task of Vision-augmented Table-To-Text Generation (VisToT, defined as follows: given a table and an associated image, produce a descriptive sentence conditioned on the multimodal input. For the task, we present a novel multimodal table-to-text dataset, WikiLandmarks, covering 73,084 unique world landmarks. Further, we also present a competitive architecture, namely, VT3 that generates accurate sentences conditioned on the image and table pairs. Through extensive analyses and experiments, we show that visual cues from images are helpful in (i) inferring missing information from incomplete or sparse tables, and (ii) strengthening the importance of useful information from noisy tables for natural language generation. We make the code and data publicly available.</abstract>
      <url hash="2b40a448">2022.emnlp-main.675</url>
      <bibkey>gatti-etal-2022-vistot</bibkey>
    </paper>
    <paper id="676">
      <title>Generative Entity-to-Entity Stance Detection with Knowledge Graph Augmentation</title>
      <author><first>Xinliang Frederick</first><last>Zhang</last></author>
      <author><first>Nick</first><last>Beauchamp</last></author>
      <author><first>Lu</first><last>Wang</last></author>
      <pages>9950-9969</pages>
      <abstract>Stance detection is typically framed as predicting the sentiment in a given text towards a target entity. However, this setup overlooks the importance of the source entity, i.e., who is expressing the opinion. In this paper, we emphasize the imperative need for studying interactions among entities when inferring stances. We first introduce a new task, entity-to-entity (E2E) stance detection, which primes models to identify entities in their canonical names and discern stances jointly. To support this study, we curate a new dataset with 10,641 annotations labeled at the sentence level from news articles of different ideological leanings. We present a novel generative framework to allow the generation of canonical names for entities as well as stances among them. We further enhance the model with a graph encoder to summarize entity activities and external knowledge surrounding the entities. Experiments show that our model outperforms strong comparisons by large margins. Further analyses demonstrate the usefulness of E2E stance detection for understanding media quotation and stance landscape as well as inferring entity ideology.</abstract>
      <url hash="9f602c22">2022.emnlp-main.676</url>
      <bibkey>zhang-etal-2022-generative</bibkey>
    </paper>
    <paper id="677">
      <title>Symptom Identification for Interpretable Detection of Multiple Mental Disorders on Social Media</title>
      <author><first>Zhiling</first><last>Zhang</last></author>
      <author><first>Siyuan</first><last>Chen</last></author>
      <author><first>Mengyue</first><last>Wu</last></author>
      <author><first>Kenny</first><last>Zhu</last></author>
      <pages>9970-9985</pages>
      <abstract>Mental disease detection (MDD) from social media has suffered from poor generalizability and interpretability, due to lack of symptom modeling. This paper introduces PsySym, the first annotated symptom identification corpus of multiple psychiatric disorders, to facilitate further research progress. PsySym is annotated according to a knowledge graph of the 38 symptom classes related to 7 mental diseases complied from established clinical manuals and scales, and a novel annotation framework for diversity and quality. Experiments show that symptom-assisted MDD enabled by PsySym can outperform strong pure-text baselines. We also exhibit the convincing MDD explanations provided by symptom predictions with case studies, and point to their further potential applications.</abstract>
      <url hash="974d353c">2022.emnlp-main.677</url>
      <bibkey>zhang-etal-2022-symptom</bibkey>
    </paper>
    <paper id="678">
      <title>Improving Iterative Text Revision by Learning Where to Edit from Other Revision Tasks</title>
      <author><first>Zae Myung</first><last>Kim</last></author>
      <author><first>Wanyu</first><last>Du</last></author>
      <author><first>Vipul</first><last>Raheja</last></author>
      <author><first>Dhruv</first><last>Kumar</last></author>
      <author><first>Dongyeop</first><last>Kang</last></author>
      <pages>9986-9999</pages>
      <abstract>Iterative text revision improves text quality by fixing grammatical errors, rephrasing for better readability or contextual appropriateness, or reorganizing sentence structures throughout a document.Most recent research has focused on understanding and classifying different types of edits in the iterative revision process from human-written text instead of building accurate and robust systems for iterative text revision.In this work, we aim to build an end-to-end text revision system that can iteratively generate helpful edits by explicitly detecting editable spans (where-to-edit) with their corresponding edit intents and then instructing a revision model to revise the detected edit spans.Leveraging datasets from other related text editing NLP tasks, combined with the specification of editable spans, leads our system to more accurately model the process of iterative text refinement, as evidenced by empirical results and human evaluations.Our system significantly outperforms previous baselines on our text revision tasks and other standard text revision tasks, including grammatical error correction, text simplification, sentence fusion, and style transfer.Through extensive qualitative and quantitative analysis, we make vital connections between edit intentions and writing quality, and better computational modeling of iterative text revisions.</abstract>
      <url hash="ba548918">2022.emnlp-main.678</url>
      <bibkey>kim-etal-2022-improving</bibkey>
    </paper>
    <paper id="679">
      <title><fixed-case>CONQRR</fixed-case>: Conversational Query Rewriting for Retrieval with Reinforcement Learning</title>
      <author><first>Zeqiu</first><last>Wu</last></author>
      <author><first>Yi</first><last>Luan</last></author>
      <author><first>Hannah</first><last>Rashkin</last></author>
      <author><first>David</first><last>Reitter</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <author><first>Mari</first><last>Ostendorf</last></author>
      <author><first>Gaurav Singh</first><last>Tomar</last></author>
      <pages>10000-10014</pages>
      <abstract>Compared to standard retrieval tasks, passage retrieval for conversational question answering (CQA) poses new challenges in understanding the current user question, as each question needs to be interpreted within the dialogue context. Moreover, it can be expensive to re-train well-established retrievers such as search engines that are originally developed for non-conversational queries. To facilitate their use, we develop a query rewriting model CONQRR that rewrites a conversational question in the context into a standalone question. It is trained with a novel reward function to directly optimize towards retrieval using reinforcement learning and can be adapted to any off-the-shelf retriever. CONQRR achieves state-of-the-art results on a recent open-domain CQA dataset containing conversations from three different sources, and is effective for two different off-the-shelf retrievers. Our extensive analysis also shows the robustness of CONQRR to out-of-domain dialogues as well as to zero query rewriting supervision.</abstract>
      <url hash="ced88c72">2022.emnlp-main.679</url>
      <bibkey>wu-etal-2022-conqrr</bibkey>
    </paper>
    <paper id="680">
      <title>Specializing Multi-domain <fixed-case>NMT</fixed-case> via Penalizing Low Mutual Information</title>
      <author><first>Jiyoung</first><last>Lee</last></author>
      <author><first>Hantae</first><last>Kim</last></author>
      <author><first>Hyunchang</first><last>Cho</last></author>
      <author><first>Edward</first><last>Choi</last></author>
      <author><first>Cheonbok</first><last>Park</last></author>
      <pages>10015-10026</pages>
      <abstract>Multi-domain Neural Machine Translation (NMT) trains a single model with multiple domains. It is appealing because of its efficacy in handling multiple domains within one model. An ideal multi-domain NMT learns distinctive domain characteristics simultaneously, however, grasping the domain peculiarity is a non-trivial task. In this paper, we investigate domain-specific information through the lens of mutual information (MI) and propose a new objective that penalizes low MI to become higher.Our method achieved the state-of-the-art performance among the current competitive multi-domain NMT models. Also, we show our objective promotes low MI to be higher resulting in domain-specialized multi-domain NMT.</abstract>
      <url hash="e7470d87">2022.emnlp-main.680</url>
      <bibkey>lee-etal-2022-specializing</bibkey>
    </paper>
    <paper id="681">
      <title>A Simple Contrastive Learning Framework for Interactive Argument Pair Identification via Argument-Context Extraction</title>
      <author><first>Lida</first><last>Shi</last></author>
      <author><first>Fausto</first><last>Giunchiglia</last></author>
      <author><first>Rui</first><last>Song</last></author>
      <author><first>Daqian</first><last>Shi</last></author>
      <author><first>Tongtong</first><last>Liu</last></author>
      <author><first>Xiaolei</first><last>Diao</last></author>
      <author><first>Hao</first><last>Xu</last></author>
      <pages>10027-10039</pages>
      <abstract>Interactive argument pair identification is an emerging research task for argument mining, aiming to identify whether two arguments are interactively related. It is pointed out that the context of the argument is essential to improve identification performance. However, current context-based methods achieve limited improvements since the entire context typically contains much irrelevant information. In this paper, we propose a simple contrastive learning framework to solve this problem by extracting valuable information from the context. This framework can construct hard argument-context samples and obtain a robust and uniform representation by introducing contrastive learning. We also propose an argument-context extraction module to enhance information extraction by discarding irrelevant blocks. The experimental results show that our method achieves the state-of-the-art performance on the benchmark dataset. Further analysis demonstrates the effectiveness of our proposed modules and visually displays more compact semantic representations.</abstract>
      <url hash="13cebdaf">2022.emnlp-main.681</url>
      <bibkey>shi-etal-2022-simple</bibkey>
    </paper>
    <paper id="682">
      <title>Sentence-level Media Bias Analysis Informed by Discourse Structures</title>
      <author><first>Yuanyuan</first><last>Lei</last></author>
      <author><first>Ruihong</first><last>Huang</last></author>
      <author><first>Lu</first><last>Wang</last></author>
      <author><first>Nick</first><last>Beauchamp</last></author>
      <pages>10040-10050</pages>
      <abstract>As polarization continues to rise among both the public and the news media, increasing attention has been devoted to detecting media bias. Most recent work in the NLP community, however, identify bias at the level of individual articles. However, each article itself comprises multiple sentences, which vary in their ideological bias. In this paper, we aim to identify sentences within an article that can illuminate and explain the overall bias of the entire article. We show that understanding the discourse role of a sentence in telling a news story, as well as its relation with nearby sentences, can reveal the ideological leanings of an author even when the sentence itself appears merely neutral. In particular, we consider using a functional news discourse structure and PDTB discourse relations to inform bias sentence identification, and distill the auxiliary knowledge from the two types of discourse structure into our bias sentence identification system. Experimental results on benchmark datasets show that incorporating both the global functional discourse structure and local rhetorical discourse relations can effectively increase the recall of bias sentence identification by 8.27% - 8.62%, as well as increase the precision by 2.82% - 3.48%.</abstract>
      <url hash="758b3132">2022.emnlp-main.682</url>
      <bibkey>lei-etal-2022-sentence</bibkey>
    </paper>
    <paper id="683">
      <title>Towards Efficient Dialogue Pre-training with Transferable and Interpretable Latent Structure</title>
      <author><first>Xueliang</first><last>Zhao</last></author>
      <author><first>Lemao</first><last>Liu</last></author>
      <author><first>Tingchen</first><last>Fu</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <author><first>Rui</first><last>Yan</last></author>
      <pages>10051-10063</pages>
      <abstract>With the availability of massive general-domain dialogue data, pre-trained dialogue generation appears to be super appealing to transfer knowledge from the general domain to downstream applications. In most existing work, such transferable ability is mainly obtained by fitting a large model with hundreds of millions of parameters on massive data in an exhaustive way, leading to inefficient running and poor interpretability. This paper proposes a novel dialogue generation model with a latent structure that is easily transferable from the general domain to downstream tasks in a lightweight and transparent way. Experiments on two benchmarks validate the effectiveness of the proposed model. Thanks to the transferable latent structure, our model is able to yield better dialogue responses than four strong baselines in terms of both automatic and human evaluations, and our model with about 22% parameters particularly delivers a 5x speedup in running time compared with the strongest baseline. Moreover, the proposed model is explainable by interpreting the discrete latent variables.</abstract>
      <url hash="4cf9c9bd">2022.emnlp-main.683</url>
      <bibkey>zhao-etal-2022-towards</bibkey>
    </paper>
    <paper id="684">
      <title>An Empirical Revisiting of Linguistic Knowledge Fusion in Language Understanding Tasks</title>
      <author><first>Changlong</first><last>Yu</last></author>
      <author><first>Tianyi</first><last>Xiao</last></author>
      <author><first>Lingpeng</first><last>Kong</last></author>
      <author><first>Yangqiu</first><last>Song</last></author>
      <author><first>Wilfred</first><last>Ng</last></author>
      <pages>10064-10070</pages>
      <abstract>Though linguistic knowledge emerges during large-scale language model pretraining, recent work attempt to explicitly incorporate human-defined linguistic priors into task-specific fine-tuning. Infusing language models with syntactic or semantic knowledge from parsers has shown improvements on many language understanding tasks. To further investigate the effectiveness of structural linguistic priors, we conduct empirical study of replacing parsed graphs or trees with trivial ones (rarely carrying linguistic knowledge e.g., balanced tree) for tasks in the GLUE benchmark. Encoding with trivial graphs achieves competitive or even better performance in fully-supervised and few-shot settings. It reveals that the gains might not be significantly attributed to explicit linguistic priors but rather to more feature interactions brought by fusion layers. Hence we call for attention to using trivial graphs as necessary baselines to design advanced knowledge fusion methods in the future.</abstract>
      <url hash="e091a88f">2022.emnlp-main.684</url>
      <bibkey>yu-etal-2022-empirical</bibkey>
    </paper>
    <paper id="685">
      <title>Unsupervised Non-transferable Text Classification</title>
      <author><first>Guangtao</first><last>Zeng</last></author>
      <author><first>Wei</first><last>Lu</last></author>
      <pages>10071-10084</pages>
      <abstract>Training a good deep learning model requires substantial data and computing resources, which makes the resulting neural model a valuable intellectual property. To prevent the neural network from being undesirably exploited, non-transferable learning has been proposed to reduce the model generalization ability in specific target domains. However, existing approaches require labeled data for the target domain which can be difficult to obtain. Furthermore, they do not have the mechanism to still recover the model’s ability to access the target domain.In this paper, we propose a novel unsupervised non-transferable learning method for the text classification task that does not require annotated target domain data. We further introduce a secret key component in our approach for recovering the access to the target domain, where we design both an explicit and an implicit method for doing so. Extensive experiments demonstrate the effectiveness of our approach.</abstract>
      <url hash="bc3e37ac">2022.emnlp-main.685</url>
      <bibkey>zeng-lu-2022-unsupervised</bibkey>
    </paper>
    <paper id="686">
      <title>Adaptive Contrastive Learning on Multimodal Transformer for Review Helpfulness Prediction</title>
      <author><first>Thong</first><last>Nguyen</last></author>
      <author><first>Xiaobao</first><last>Wu</last></author>
      <author><first>Anh Tuan</first><last>Luu</last></author>
      <author><first>Zhen</first><last>Hai</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <pages>10085-10096</pages>
      <abstract>Modern Review Helpfulness Prediction systems are dependent upon multiple modalities, typically texts and images. Unfortunately, those contemporary approaches pay scarce attention to polish representations of cross-modal relations and tend to suffer from inferior optimization. This might cause harm to model’s predictions in numerous cases. To overcome the aforementioned issues, we propose Multi-modal Contrastive Learning for Multimodal Review Helpfulness Prediction (MRHP) problem, concentrating on mutual information between input modalities to explicitly elaborate cross-modal relations. In addition, we introduce Adaptive Weighting scheme for our contrastive learning approach in order to increase flexibility in optimization. Lastly, we propose Multimodal Interaction module to address the unalignment nature of multimodal data, thereby assisting the model in producing more reasonable multimodal representations. Experimental results show that our method outperforms prior baselines and achieves state-of-the-art results on two publicly available benchmark datasets for MRHP problem.</abstract>
      <url hash="fed0aa14">2022.emnlp-main.686</url>
      <bibkey>nguyen-etal-2022-adaptive</bibkey>
    </paper>
    <paper id="687">
      <title>Adaptive Token-level Cross-lingual Feature Mixing for Multilingual Neural Machine Translation</title>
      <author><first>Junpeng</first><last>Liu</last></author>
      <author><first>Kaiyu</first><last>Huang</last></author>
      <author><first>Jiuyi</first><last>Li</last></author>
      <author><first>Huan</first><last>Liu</last></author>
      <author><first>Jinsong</first><last>Su</last></author>
      <author><first>Degen</first><last>Huang</last></author>
      <pages>10097-10113</pages>
      <abstract>Multilingual neural machine translation aims to translate multiple language pairs in a single model and has shown great success thanks to the knowledge transfer across languages with the shared parameters. Despite promising, this share-all paradigm suffers from insufficient ability to capture language-specific features. Currently, the common practice is to insert or search language-specific networks to balance the shared and specific features. However, those two types of features are not sufficient enough to model the complex commonality and divergence across languages, such as the locally shared features among similar languages, which leads to sub-optimal transfer, especially in massively multilingual translation. In this paper, we propose a novel token-level feature mixing method that enables the model to capture different features and dynamically determine the feature sharing across languages. Based on the observation that the tokens in the multilingual model are usually shared by different languages, we we insert a feature mixing layer into each Transformer sublayer and model each token representation as a mix of different features, with a proportion indicating its feature preference. In this way, we can perform fine-grained feature sharing and achieve better multilingual transfer. Experimental results on multilingual datasets show that our method outperforms various strong baselines and can be extended to zero-shot translation. Further analyses reveal that our method can capture different linguistic features and bridge the representation gap across languages.</abstract>
      <url hash="aec7963b">2022.emnlp-main.687</url>
      <bibkey>liu-etal-2022-adaptive</bibkey>
    </paper>
    <paper id="688">
      <title>A Dataset for Hyper-Relational Extraction and a Cube-Filling Approach</title>
      <author><first>Yew Ken</first><last>Chia</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Sharifah Mahani</first><last>Aljunied</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <author><first>Soujanya</first><last>Poria</last></author>
      <pages>10114-10133</pages>
      <abstract>Relation extraction has the potential for large-scale knowledge graph construction, but current methods do not consider the qualifier attributes for each relation triplet, such as time, quantity or location. The qualifiers form hyper-relational facts which better capture the rich and complex knowledge graph structure. For example, the relation triplet (Leonard Parker, Educated At, Harvard University) can be factually enriched by including the qualifier (End Time, 1967). Hence, we propose the task of hyper-relational extraction to extract more specific and complete facts from text. To support the task, we construct HyperRED, a large-scale and general-purpose dataset. Existing models cannot perform hyper-relational extraction as it requires a model to consider the interaction between three entities. Hence, we propose CubeRE, a cube-filling model inspired by table-filling approaches and explicitly considers the interaction between relation triplets and qualifiers. To improve model scalability and reduce negative class imbalance, we further propose a cube-pruning method. Our experiments show that CubeRE outperforms strong baselines and reveal possible directions for future research. Our code and data are available at github.com/declare-lab/HyperRED.</abstract>
      <url hash="ff506618">2022.emnlp-main.688</url>
      <bibkey>chia-etal-2022-dataset</bibkey>
    </paper>
    <paper id="689">
      <title>Low-resource Neural Machine Translation with Cross-modal Alignment</title>
      <author><first>Zhe</first><last>Yang</last></author>
      <author><first>Qingkai</first><last>Fang</last></author>
      <author><first>Yang</first><last>Feng</last></author>
      <pages>10134-10146</pages>
      <abstract>How to achieve neural machine translation with limited parallel data? Existing techniques often rely on large-scale monolingual corpus, which is impractical for some low-resource languages. In this paper, we turn to connect several low-resource languages to a particular high-resource one by additional visual modality. Specifically, we propose a cross-modal contrastive learning method to learn a shared space for all languages, where both a coarse-grained sentence-level objective and a fine-grained token-level one are introduced. Experimental results and further analysis show that our method can effectively learn the cross-modal and cross-lingual alignment with a small amount of image-text pairs, and achieves significant improvements over the text-only baseline under both zero-shot and few-shot scenarios.</abstract>
      <url hash="0c711b6f">2022.emnlp-main.689</url>
      <bibkey>yang-etal-2022-low</bibkey>
    </paper>
    <paper id="690">
      <title>Prompt-based Distribution Alignment for Domain Generalization in Text Classification</title>
      <author><first>Chen</first><last>Jia</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>10147-10157</pages>
      <abstract>Prompt-based learning (a.k.a. prompting) achieves high performance by bridging the gap between the objectives of language modeling and downstream tasks. Domain generalization ability can be improved by prompting since classification across different domains can be unified into the prediction of the same set of label words. The remaining challenge for domain generalization by prompting comes from discrepancies between the data distribution of different domains. To improve domain generalization with prompting, we learn distributional invariance across source domains via two alignment regularization loss functions. The first is vocabulary distribution alignment, which uses a Kullback-Leibler divergence regularization on source-domain vocabulary distributions. The second is feature distribution alignment, which uses a novel adversarial training strategy to learn domain invariant representation across source domains. Experiments on sentiment analysis and natural language inference show the effectiveness of our method and achieve state-of-the-art results on six datasets.</abstract>
      <url hash="5911083b">2022.emnlp-main.690</url>
      <bibkey>jia-zhang-2022-prompt</bibkey>
    </paper>
    <paper id="691">
      <title>Two is Better than Many? Binary Classification as an Effective Approach to Multi-Choice Question Answering</title>
      <author><first>Deepanway</first><last>Ghosal</last></author>
      <author><first>Navonil</first><last>Majumder</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <author><first>Soujanya</first><last>Poria</last></author>
      <pages>10158-10166</pages>
      <abstract>We propose a simple refactoring of multi-choice question answering (MCQA) tasks as a series of binary classifications. The MCQA task is generally performed by scoring each (question, answer) pair normalized over all the pairs, and then selecting the answer from the pair that yield the highest score. For n answer choices, this is equivalent to an n-class classification setup where only one class (true answer) is correct. We instead show that classifying (question, true answer) as positive instances and (question, false answer) as negative instances is significantly more effective across various models and datasets. We show the efficacy of our proposed approach in different tasks – abductive reasoning, commonsense question answering, science question answering, and sentence completion. Our DeBERTa binary classification model reaches the top or close to the top performance on public leaderboards for these tasks. The source code of the proposed approach is available at https://github.com/declare-lab/TEAM.</abstract>
      <url hash="6447e1c6">2022.emnlp-main.691</url>
      <bibkey>ghosal-etal-2022-two</bibkey>
    </paper>
    <paper id="692">
      <title><fixed-case>HEGEL</fixed-case>: Hypergraph Transformer for Long Document Summarization</title>
      <author><first>Haopeng</first><last>Zhang</last></author>
      <author><first>Xiao</first><last>Liu</last></author>
      <author><first>Jiawei</first><last>Zhang</last></author>
      <pages>10167-10176</pages>
      <abstract>Extractive summarization for long documents is challenging due to the extended structured input context. The long-distance sentence dependency hinders cross-sentence relations modeling, the critical step of extractive summarization. This paper proposes HEGEL, a hypergraph neural network for long document summarization by capturing high-order cross-sentence relations. HEGEL updates and learns effective sentence representations with hypergraph transformer layers and fuses different types of sentence dependencies, including latent topics, keywords coreference, and section structure. We validate HEGEL by conducting extensive experiments on two benchmark datasets, and experimental results demonstrate the effectiveness and efficiency of HEGEL.</abstract>
      <url hash="c778c0f5">2022.emnlp-main.692</url>
      <bibkey>zhang-etal-2022-hegel</bibkey>
    </paper>
    <paper id="693">
      <title>Adapting a Language Model While Preserving its General Knowledge</title>
      <author><first>Zixuan</first><last>Ke</last></author>
      <author><first>Yijia</first><last>Shao</last></author>
      <author><first>Haowei</first><last>Lin</last></author>
      <author><first>Hu</first><last>Xu</last></author>
      <author><first>Lei</first><last>Shu</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <pages>10177-10188</pages>
      <abstract>Domain-adaptive pre-training (or DA-training for short), also known as post-training, aimsto train a pre-trained general-purpose language model (LM) using an unlabeled corpus of aparticular domain to adapt the LM so that end-tasks in the domain can give improved performances. However, existing DA-training methods are in some sense blind as they do not explicitly identify what knowledge in the LM should be preserved and what should be changed by the domain corpus. This paper shows that the existing methods are suboptimal and proposes a novel method to perform a more informed adaptation of the knowledge in the LM by (1) soft-masking the attention heads based on their importance to best preserve the general knowledge in the LM and (2) contrasting the representations of the general and the full (both general and domain knowledge) to learn an integrated representation with both general and domain-specific knowledge. Experimental results will demonstrate the effectiveness of the proposed approach.</abstract>
      <url hash="2147e8f3">2022.emnlp-main.693</url>
      <bibkey>ke-etal-2022-adapting</bibkey>
    </paper>
    <paper id="694">
      <title>Human Guided Exploitation of Interpretable Attention Patterns in Summarization and Topic Segmentation</title>
      <author><first>Raymond</first><last>Li</last></author>
      <author><first>Wen</first><last>Xiao</last></author>
      <author><first>Linzi</first><last>Xing</last></author>
      <author><first>Lanjun</first><last>Wang</last></author>
      <author><first>Gabriel</first><last>Murray</last></author>
      <author><first>Giuseppe</first><last>Carenini</last></author>
      <pages>10189-10204</pages>
      <abstract>The multi-head self-attention mechanism of the transformer model has been thoroughly investigated recently. In one vein of study, researchers are interested in understanding why and how transformers work. In another vein, researchers propose new attention augmentation methods to make transformers more accurate, efficient and interpretable. In this paper, we combine these two lines of research in a human-in-the-loop pipeline to first discover important task-specific attention patterns. Then those patterns are injected, not only to smaller models, but also to the original model. The benefits of our pipeline and discovered patterns are demonstrated in two case studies with extractive summarization and topic segmentation. After discovering interpretable patterns in BERT-based models fine-tuned for the two downstream tasks, experiments indicate that when we inject the patterns into attention heads, the models show considerable improvements in accuracy and efficiency.</abstract>
      <url hash="f4885132">2022.emnlp-main.694</url>
      <bibkey>li-etal-2022-human</bibkey>
    </paper>
    <paper id="695">
      <title>Continual Training of Language Models for Few-Shot Learning</title>
      <author><first>Zixuan</first><last>Ke</last></author>
      <author><first>Haowei</first><last>Lin</last></author>
      <author><first>Yijia</first><last>Shao</last></author>
      <author><first>Hu</first><last>Xu</last></author>
      <author><first>Lei</first><last>Shu</last></author>
      <author><first>Bing</first><last>Liu</last></author>
      <pages>10205-10216</pages>
      <abstract>Recent work on applying large language models (LMs) achieves impressive performance in many NLP applications. Adapting or posttraining an LM using an unlabeled domain corpus can produce even better performance for end-tasks in the domain. This paper proposes the problem of continually extending an LM by incrementally post-train the LM with a sequence of unlabeled domain corpora to expand its knowledge without forgetting its previous skills. The goal is to improve the few-shot end-task learning in these domains. The resulting system is called CPT (Continual PostTraining), which to our knowledge, is the first continual post-training system. Experimental results verify its effectiveness.</abstract>
      <url hash="be28097d">2022.emnlp-main.695</url>
      <bibkey>ke-etal-2022-continual</bibkey>
    </paper>
    <paper id="696">
      <title>Dictionary-Assisted Supervised Contrastive Learning</title>
      <author><first>Patrick</first><last>Wu</last></author>
      <author><first>Richard</first><last>Bonneau</last></author>
      <author><first>Joshua</first><last>Tucker</last></author>
      <author><first>Jonathan</first><last>Nagler</last></author>
      <pages>10217-10235</pages>
      <abstract>Text analysis in the social sciences often involves using specialized dictionaries to reason with abstract concepts, such as perceptions about the economy or abuse on social media. These dictionaries allow researchers to impart domain knowledge and note subtle usages of words relating to a concept(s) of interest. We introduce the dictionary-assisted supervised contrastive learning (DASCL) objective, allowing researchers to leverage specialized dictionaries when fine-tuning pretrained language models. The text is first keyword simplified: a common, fixed token replaces any word in the corpus that appears in the dictionary(ies) relevant to the concept of interest. During fine-tuning, a supervised contrastive objective draws closer the embeddings of the original and keyword-simplified texts of the same class while pushing further apart the embeddings of different classes. The keyword-simplified texts of the same class are more textually similar than their original text counterparts, which additionally draws the embeddings of the same class closer together. Combining DASCL and cross-entropy improves classification performance metrics in few-shot learning settings and social science applications compared to using cross-entropy alone and alternative contrastive and data augmentation methods.</abstract>
      <url hash="c151c6e7">2022.emnlp-main.696</url>
      <bibkey>wu-etal-2022-dictionary</bibkey>
    </paper>
    <paper id="697">
      <title>Fine-Tuning Pre-trained Transformers into Decaying Fast Weights</title>
      <author><first>Huanru Henry</first><last>Mao</last></author>
      <pages>10236-10242</pages>
      <abstract>Autoregressive Transformers are strong language models but incur O(T) complexity during per-token generation due to the self-attention mechanism. Recent work proposes kernel-based methods to approximate causal self-attention by replacing it with recurrent formulations with various update rules and feature maps to achieve O(1) time and memory complexity. We explore these approaches and find that they are unnecessarily complex, and propose a simple alternative - decaying fast weights - that runs fast on GPU, outperforms prior methods, and retains 99% of attention’s performance for GPT-2. We also show competitive performance on WikiText-103 against more complex attention substitutes.</abstract>
      <url hash="8dfa9083">2022.emnlp-main.697</url>
      <bibkey>mao-2022-fine</bibkey>
    </paper>
    <paper id="698">
      <title><fixed-case>PRO</fixed-case>-<fixed-case>CS</fixed-case> : An Instance-Based Prompt Composition Technique for Code-Switched Tasks</title>
      <author><first>Srijan</first><last>Bansal</last></author>
      <author><first>Suraj</first><last>Tripathi</last></author>
      <author><first>Sumit</first><last>Agarwal</last></author>
      <author><first>Teruko</first><last>Mitamura</last></author>
      <author><first>Eric</first><last>Nyberg</last></author>
      <pages>10243-10255</pages>
      <abstract>Code-switched (CS) data is ubiquitous in today’s globalized world, but the dearth of annotated datasets in code-switching poses a significant challenge for learning diverse tasks across different language pairs. Parameter-efficient prompt-tuning approaches conditioned on frozen language models have shown promise for transfer learning in limited-resource setups. In this paper, we propose a novel instance-based prompt composition technique, PRO-CS, for CS tasks that combine language and task knowledge. We compare our approach with prompt-tuning and fine-tuning for code-switched tasks on 10 datasets across 4 language pairs. Our model outperforms the prompt-tuning approach by significant margins across all datasets and outperforms or remains at par with fine-tuning by using just 0.18% of total parameters. We also achieve competitive results when compared with the fine-tuned model in the low-resource cross-lingual and cross-task setting, indicating the effectiveness of our approach to incorporate new code-switched tasks.</abstract>
      <url hash="001a5be6">2022.emnlp-main.698</url>
      <bibkey>bansal-etal-2022-pro</bibkey>
    </paper>
    <paper id="699">
      <title><fixed-case>S</fixed-case>ent<fixed-case>BS</fixed-case>: Sentence-level Beam Search for Controllable Summarization</title>
      <author><first>Chenhui</first><last>Shen</last></author>
      <author><first>Liying</first><last>Cheng</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <author><first>Yang</first><last>You</last></author>
      <author><first>Luo</first><last>Si</last></author>
      <pages>10256-10265</pages>
      <abstract>A wide range of control perspectives have been explored in controllable text generation. Structure-controlled summarization is recently proposed as a useful and interesting research direction. However, current structure-controlling methods have limited effectiveness in enforcing the desired structure. To address this limitation, we propose a sentence-level beam search generation method (SentBS), where evaluation is conducted throughout the generation process to select suitable sentences for subsequent generations. We experiment with different combinations of decoding methods to be used as sub-components by SentBS and evaluate results on the structure-controlled dataset MReD. Experiments show that all explored combinations for SentBS can improve the agreement between the generated text and the desired structure, with the best method significantly reducing the structural discrepancies suffered by the existing model, by approximately 68%.</abstract>
      <url hash="e63a6d79">2022.emnlp-main.699</url>
      <bibkey>shen-etal-2022-sentbs</bibkey>
    </paper>
    <paper id="700">
      <title>A Fine-grained <fixed-case>C</fixed-case>hinese Software Privacy Policy Dataset for Sequence Labeling and Regulation Compliant Identification</title>
      <author><first>Kaifa</first><last>Zhao</last></author>
      <author><first>Le</first><last>Yu</last></author>
      <author><first>Shiyao</first><last>Zhou</last></author>
      <author><first>Jing</first><last>Li</last></author>
      <author><first>Xiapu</first><last>Luo</last></author>
      <author><first>Yat Fei Aemon</first><last>Chiu</last></author>
      <author><first>Yutong</first><last>Liu</last></author>
      <pages>10266-10277</pages>
      <abstract>Privacy protection raises great attention on both legal levels and user awareness. To protect user privacy, countries enact laws and regulations requiring software privacy policies to regulate their behavior. However, privacy policies are written in professional languages with many legal terms and software jargon that prevent users from understanding and even reading them. It is necessary and urgent to use NLP techniques to analyze privacy policies. However, existing datasets ignore law requirements and are limited to English. In this paper, we construct the first Chinese privacy policy dataset, namely CA4P-483, to facilitate the sequence labeling tasks and regulation compliance identification between privacy policies and software. Our dataset includes 483 Chinese Android application privacy policies, over 11K sentences, and 52K fine-grained annotations. We evaluate families of robust and representative baseline models on our dataset. Based on baseline performance, we provide findings and potential research directions on our dataset. Finally, we investigate the potential applications of CA4P-483 combing regulation requirements and program analysis.</abstract>
      <url hash="eb0dec4b">2022.emnlp-main.700</url>
      <bibkey>zhao-etal-2022-fine-grained</bibkey>
    </paper>
    <paper id="701">
      <title>Saving Dense Retriever from Shortcut Dependency in Conversational Search</title>
      <author><first>Sungdong</first><last>Kim</last></author>
      <author><first>Gangwoo</first><last>Kim</last></author>
      <pages>10278-10287</pages>
      <abstract>Conversational search (CS) needs a holistic understanding of conversational inputs to retrieve relevant passages. In this paper, we demonstrate the existence of a <i>retrieval shortcut</i> in CS, which causes models to retrieve passages solely relying on partial history while disregarding the latest question. With in-depth analysis, we first show that naively trained dense retrievers heavily exploit the shortcut and hence perform poorly when asked to answer history-independent questions. To build more robust models against shortcut dependency, we explore various hard negative mining strategies. Experimental results show that training with the model-based hard negatives effectively mitigates the dependency on the shortcut, significantly improving dense retrievers on recent CS benchmarks. In particular, our retriever outperforms the previous state-of-the-art model by 11.0 in Recall@10 on QReCC.</abstract>
      <url hash="71749921">2022.emnlp-main.701</url>
      <bibkey>kim-kim-2022-saving</bibkey>
    </paper>
    <paper id="702">
      <title>Graph-Induced Transformers for Efficient Multi-Hop Question Answering</title>
      <author><first>Giwon</first><last>Hong</last></author>
      <author><first>Jeonghwan</first><last>Kim</last></author>
      <author><first>Junmo</first><last>Kang</last></author>
      <author><first>Sung-Hyon</first><last>Myaeng</last></author>
      <pages>10288-10294</pages>
      <abstract>A graph is a suitable data structure to represent the structural information of text. Recently, multi-hop question answering (MHQA) tasks, which require inter-paragraph/sentence linkages, have come to exploit such properties of a graph. Previous approaches to MHQA relied on leveraging the graph information along with the pre-trained language model (PLM) encoders. However, this trend exhibits the following drawbacks: (i) sample inefficiency while training in a low-resource setting; (ii) lack of reusability due to changes in the model structure or input. Our work proposes the Graph-Induced Transformer (GIT) that applies graph-derived attention patterns directly into a PLM, without the need to employ external graph modules. GIT can leverage the useful inductive bias of graphs while retaining the unperturbed Transformer structure and parameters. Our experiments on HotpotQA successfully demonstrate both the sample efficient characteristic of GIT and its capacity to replace the graph modules while preserving model performance.</abstract>
      <url hash="9d763e02">2022.emnlp-main.702</url>
      <bibkey>hong-etal-2022-graph</bibkey>
    </paper>
    <paper id="703">
      <title><fixed-case>D</fixed-case>isco<fixed-case>S</fixed-case>ense: Commonsense Reasoning with Discourse Connectives</title>
      <author><first>Prajjwal</first><last>Bhargava</last></author>
      <author><first>Vincent</first><last>Ng</last></author>
      <pages>10295-10310</pages>
      <abstract>We present DiscoSense, a benchmark for commonsense reasoning via understanding a wide variety of discourse connectives. We generate compelling distractors in DiscoSense using Conditional Adversarial Filtering, an extension of Adversarial Filtering that employs conditional generation. We show that state-of-the-art pre-trained language models struggle to perform well on DiscoSense, which makes this dataset ideal for evaluating next-generation commonsense reasoning systems.</abstract>
      <url hash="9bc841d2">2022.emnlp-main.703</url>
      <bibkey>bhargava-ng-2022-discosense</bibkey>
    </paper>
    <paper id="704">
      <title>Boosting Document-Level Relation Extraction by Mining and Injecting Logical Rules</title>
      <author><first>Shengda</first><last>Fan</last></author>
      <author><first>Shasha</first><last>Mo</last></author>
      <author><first>Jianwei</first><last>Niu</last></author>
      <pages>10311-10323</pages>
      <abstract>Document-level relation extraction (DocRE) aims at extracting relations of all entity pairs in a document. A key challenge to DocRE lies in the complex interdependency between the relations of entity pairs. Unlike most prior efforts focusing on implicitly powerful representations, the recently proposed LogiRE (Ru et al., 2021) explicitly captures the interdependency by learning logical rules. However, LogiRE requires extra parameterized modules to reason merely after training backbones, and this disjointed optimization of backbones and extra modules may lead to sub-optimal results. In this paper, we propose MILR, a logic enhanced framework that boosts DocRE by Mining and Injecting Logical Rules. MILR first mines logical rules from annotations based on frequencies. Then in training, consistency regularizationis leveraged as an auxiliary loss to penalize instances that violate mined rules. Finally, MILR infers from a global perspective based on integer programming. Compared with LogiRE, MILR does not introduce extra parameters and injects logical rules during both training and inference. Extensive experiments on two benchmarks demonstrate that MILR not only improves the relation extraction performance (1.1%-3.8% F1) but also makes predictions more logically consistent (over 4.5% Logic). More importantly, MILR also consistently outperforms LogiRE on both counts. Code is available at https://github.com/XingYing-stack/MILR.</abstract>
      <url hash="75d4ac8e">2022.emnlp-main.704</url>
      <bibkey>fan-etal-2022-boosting</bibkey>
    </paper>
    <paper id="705">
      <title><fixed-case>MOCHA</fixed-case>: A Multi-Task Training Approach for Coherent Text Generation from Cognitive Perspective</title>
      <author><first>Zhe</first><last>Hu</last></author>
      <author><first>Hou Pong</first><last>Chan</last></author>
      <author><first>Lifu</first><last>Huang</last></author>
      <pages>10324-10334</pages>
      <abstract>Teaching neural models to generate narrative coherent texts is a critical problem. Recent pre-trained language models have achieved promising results, but there is still a gap between human written texts and machine-generated outputs. In this work, we propose a novel multi-task training strategy for long text generation grounded on the cognitive theory of writing, which empowers the model to learn essential subskills needed for writing including planning and reviewing besides end-to-end generation. We extensively evaluate our model on three open-ended generation tasks including story generation, news article writing and argument generation. Experiments show that our model achieves better results on both few-shot and fully-supervised settings than strong baselines, and human evaluations confirm that our model can generate more coherent outputs.</abstract>
      <url hash="ef3c9e72">2022.emnlp-main.705</url>
      <bibkey>hu-etal-2022-mocha</bibkey>
    </paper>
    <paper id="706">
      <title>Variational Autoencoder with Disentanglement Priors for Low-Resource Task-Specific Natural Language Generation</title>
      <author><first>Zhuang</first><last>Li</last></author>
      <author><first>Lizhen</first><last>Qu</last></author>
      <author><first>Qiongkai</first><last>Xu</last></author>
      <author><first>Tongtong</first><last>Wu</last></author>
      <author><first>Tianyang</first><last>Zhan</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <pages>10335-10356</pages>
      <abstract>In this paper, we propose a variational autoencoder with disentanglement priors, VAE-Dprior, for task-specific natural language generation with none or a handful of task-specific labeled examples. In order to tackle compositional generalization across tasks, our model performs disentangled representation learning by introducing a conditional prior for the latent content space and another conditional prior for the latent label space. Both types of priors satisfy a novel property called <tex-math>\epsilon</tex-math>-disentangled. We show both empirically and theoretically that the novel priors can disentangle representations even without specific regularizations as in the prior work. The content prior enables directly sampling diverse content representations from the content space learned from the seen tasks, and fuse them with the representations of novel tasks for generating semantically diverse texts in the low-resource settings. Our extensive experiments demonstrate the superior performance of our model over competitive baselines in terms of i) data augmentation in continuous zero/few-shot learning, and ii) text style transfer in the few-shot setting.</abstract>
      <url hash="486e1ade">2022.emnlp-main.706</url>
      <bibkey>li-etal-2022-variational-autoencoder</bibkey>
    </paper>
    <paper id="707">
      <title><fixed-case>CISLR</fixed-case>: Corpus for <fixed-case>I</fixed-case>ndian <fixed-case>S</fixed-case>ign <fixed-case>L</fixed-case>anguage Recognition</title>
      <author><first>Abhinav</first><last>Joshi</last></author>
      <author><first>Ashwani</first><last>Bhat</last></author>
      <author><first>Pradeep</first><last>S</last></author>
      <author><first>Priya</first><last>Gole</last></author>
      <author><first>Shashwat</first><last>Gupta</last></author>
      <author><first>Shreyansh</first><last>Agarwal</last></author>
      <author><first>Ashutosh</first><last>Modi</last></author>
      <pages>10357-10366</pages>
      <abstract>Indian Sign Language, though used by a diverse community, still lacks well-annotated resources for developing systems that would enable sign language processing. In recent years researchers have actively worked for sign languages like American Sign Languages, however, Indian Sign language is still far from data-driven tasks like machine translation. To address this gap, in this paper, we introduce a new dataset CISLR (Corpus for Indian Sign Language Recognition) for word-level recognition in Indian Sign Language using videos. The corpus has a large vocabulary of around 4700 words covering different topics and domains. Further, we propose a baseline model for word recognition from sign language videos. To handle the low resource problem in the Indian Sign Language, the proposed model consists of a prototype-based one-shot learner that leverages resource rich American Sign Language to learn generalized features for improving predictions in Indian Sign Language. Our experiments show that gesture features learned in another sign language can help perform one-shot predictions in CISLR.</abstract>
      <url hash="e394b516">2022.emnlp-main.707</url>
      <bibkey>joshi-etal-2022-cislr</bibkey>
    </paper>
    <paper id="708">
      <title>Mask the Correct Tokens: An Embarrassingly Simple Approach for Error Correction</title>
      <author><first>Kai</first><last>Shen</last></author>
      <author><first>Yichong</first><last>Leng</last></author>
      <author><first>Xu</first><last>Tan</last></author>
      <author><first>Siliang</first><last>Tang</last></author>
      <author><first>Yuan</first><last>Zhang</last></author>
      <author><first>Wenjie</first><last>Liu</last></author>
      <author><first>Edward</first><last>Lin</last></author>
      <pages>10367-10380</pages>
      <abstract>Text error correction aims to correct the errors in text sequences such as those typed by humans or generated by speech recognition models.Previous error correction methods usually take the source (incorrect) sentence as encoder input and generate the target (correct) sentence through the decoder. Since the error rate of the incorrect sentence is usually low (e.g., 10%), the correction model can only learn to correct on limited error tokens but trivially copy on most tokens (correct tokens), which harms the effective training of error correction. In this paper, we argue that the correct tokens should be better utilized to facilitate effective training and then propose a simple yet effective masking strategy to achieve this goal.Specifically, we randomly mask out a part of the correct tokens in the source sentence and let the model learn to not only correct the original error tokens but also predict the masked tokens based on their context information. Our method enjoys several advantages: 1) it alleviates trivial copy; 2) it leverages effective training signals from correct tokens; 3) it is a plug-and-play module and can be applied to different models and tasks. Experiments on spelling error correction and speech recognition error correction on Mandarin datasets and grammar error correction on English datasets with both autoregressive and non-autoregressive generation models show that our method improves the correctionaccuracy consistently.</abstract>
      <url hash="d6998fff">2022.emnlp-main.708</url>
      <bibkey>shen-etal-2022-mask</bibkey>
    </paper>
    <paper id="709">
      <title><fixed-case>AMAL</fixed-case>: Meta Knowledge-Driven Few-Shot Adapter Learning</title>
      <author><first>S. K.</first><last>Hong</last></author>
      <author><first>Tae Young</first><last>Jang</last></author>
      <pages>10381-10389</pages>
      <abstract>NLP has advanced greatly together with the proliferation of Transformer-based pre-trained language models. To adapt to a downstream task, the pre-trained language models need to be fine-tuned with a sufficient supply of annotated examples. In recent years, Adapter-based fine-tuning methods have expanded the applicability of pre-trained language models by substantially lowering the required amount of annotated examples. However, existing Adapter-based methods still fail to yield meaningful results in the few-shot regime where only a few annotated examples are provided. In this study, we present a meta-learning-driven low-rank adapter pooling method, called AMAL, for leveraging pre-trained language models even with just a few data points. We evaluate our method on five text classification benchmark datasets. The results show that AMAL significantly outperforms previous few-shot learning methods and achieves a new state-of-the-art.</abstract>
      <url hash="6c8acd4a">2022.emnlp-main.709</url>
      <bibkey>hong-jang-2022-amal</bibkey>
    </paper>
    <paper id="710">
      <title>Discourse Context Predictability Effects in <fixed-case>H</fixed-case>indi Word Order</title>
      <author><first>Sidharth</first><last>Ranjan</last></author>
      <author><first>Marten</first><last>van Schijndel</last></author>
      <author><first>Sumeet</first><last>Agarwal</last></author>
      <author><first>Rajakrishnan</first><last>Rajkumar</last></author>
      <pages>10390-10406</pages>
      <abstract>We test the hypothesis that discourse predictability influences Hindi syntactic choice. While prior work has shown that a number of factors (e.g., information status, dependency length, and syntactic surprisal) influence Hindi word order preferences, the role of discourse predictability is underexplored in the literature. Inspired by prior work on syntactic priming, we investigate how the words and syntactic structures in a sentence influence the word order of the following sentences. Specifically, we extract sentences from the Hindi-Urdu Treebank corpus (HUTB), permute the preverbal constituents of those sentences, and build a classifier to predict which sentences actually occurred in the corpus against artificially generated distractors. The classifier uses a number of discourse-based features and cognitive features to make its predictions, including dependency length, surprisal, and information status. We find that information status and LSTM-based discourse predictability influence word order choices, especially for non-canonical object-fronted orders. We conclude by situating our results within the broader syntactic priming literature.</abstract>
      <url hash="c9b3aaa7">2022.emnlp-main.710</url>
      <bibkey>ranjan-etal-2022-discourse</bibkey>
    </paper>
    <paper id="711">
      <title>“Covid vaccine is against Covid but <fixed-case>O</fixed-case>xford vaccine is made at <fixed-case>O</fixed-case>xford!” Semantic Interpretation of Proper Noun Compounds</title>
      <author><first>Keshav</first><last>Kolluru</last></author>
      <author><first>Gabriel</first><last>Stanovsky</last></author>
      <author><first>Mausam</first><last>-</last></author>
      <pages>10407-10420</pages>
      <abstract>Proper noun compounds, e.g., “Covid vaccine”, convey information in a succinct manner (a “Covid vaccine” is a “vaccine that immunizes against the Covid disease”). These are commonly used in short-form domains, such as news headlines, but are largely ignored in information-seeking applications. To address this limitation, we release a new manually annotated dataset, ProNCI, consisting of 22.5K proper noun compounds along with their free-form semantic interpretations. ProNCI is 60 times larger than prior noun compound datasets and also includes non-compositional examples, which have not been previously explored. We experiment with various neural models for automatically generating the semantic interpretations from proper noun compounds, ranging from few-shot prompting to supervised learning, with varying degrees of knowledge about the constituent nouns. We find that adding targeted knowledge, particularly about the common noun, results in performance gains of upto 2.8%. Finally, we integrate our model generated interpretations with an existing Open IE system and observe an 7.5% increase in yield at a precision of 85%. The dataset and code are available at https://github.com/dair-iitd/pronci.</abstract>
      <url hash="fe39dc98">2022.emnlp-main.711</url>
      <bibkey>kolluru-etal-2022-covid</bibkey>
    </paper>
    <paper id="712">
      <title>Context Limitations Make Neural Language Models More Human-Like</title>
      <author><first>Tatsuki</first><last>Kuribayashi</last></author>
      <author><first>Yohei</first><last>Oseki</last></author>
      <author><first>Ana</first><last>Brassard</last></author>
      <author><first>Kentaro</first><last>Inui</last></author>
      <pages>10421-10436</pages>
      <abstract>Language models (LMs) have been used in cognitive modeling as well as engineering studies—they compute information-theoretic complexity metrics that simulate humans’ cognitive load during reading.This study highlights a limitation of modern neural LMs as the model of choice for this purpose: there is a discrepancy between their context access capacities and that of humans.Our results showed that constraining the LMs’ context access improved their simulation of human reading behavior.We also showed that LM-human gaps in context access were associated with specific syntactic constructions; incorporating syntactic biases into LMs’ context access might enhance their cognitive plausibility.</abstract>
      <url hash="ac6a47eb">2022.emnlp-main.712</url>
      <bibkey>kuribayashi-etal-2022-context</bibkey>
    </paper>
    <paper id="713">
      <title>A Generative Model for End-to-End Argument Mining with Reconstructed Positional Encoding and Constrained Pointer Mechanism</title>
      <author><first>Jianzhu</first><last>Bao</last></author>
      <author><first>Yuhang</first><last>He</last></author>
      <author><first>Yang</first><last>Sun</last></author>
      <author><first>Bin</first><last>Liang</last></author>
      <author><first>Jiachen</first><last>Du</last></author>
      <author><first>Bing</first><last>Qin</last></author>
      <author><first>Min</first><last>Yang</last></author>
      <author><first>Ruifeng</first><last>Xu</last></author>
      <pages>10437-10449</pages>
      <abstract>Argument mining (AM) is a challenging task as it requires recognizing the complex argumentation structures involving multiple subtasks.To handle all subtasks of AM in an end-to-end fashion, previous works generally transform AM into a dependency parsing task.However, such methods largely require complex pre- and post-processing to realize the task transformation.In this paper, we investigate the end-to-end AM task from a novel perspective by proposing a generative framework, in which the expected outputs of AM are framed as a simple target sequence. Then, we employ a pre-trained sequence-to-sequence language model with a constrained pointer mechanism (CPM) to model the clues for all the subtasks of AM in the light of the target sequence. Furthermore, we devise a reconstructed positional encoding (RPE) to alleviate the order biases induced by the autoregressive generation paradigm.Experimental results show that our proposed framework achieves new state-of-the-art performance on two AM benchmarks.</abstract>
      <url hash="cc611659">2022.emnlp-main.713</url>
      <bibkey>bao-etal-2022-generative</bibkey>
    </paper>
    <paper id="714">
      <title>Reflect, Not Reflex: Inference-Based Common Ground Improves Dialogue Response Quality</title>
      <author><first>Pei</first><last>Zhou</last></author>
      <author><first>Hyundong</first><last>Cho</last></author>
      <author><first>Pegah</first><last>Jandaghi</last></author>
      <author><first>Dong-Ho</first><last>Lee</last></author>
      <author><first>Bill Yuchen</first><last>Lin</last></author>
      <author><first>Jay</first><last>Pujara</last></author>
      <author><first>Xiang</first><last>Ren</last></author>
      <pages>10450-10468</pages>
      <abstract>Human communication relies on common ground (CG), the mutual knowledge and beliefs shared by participants, to produce coherent and interesting conversations. In this paper, we demonstrate that current response generation (RG) models produce generic and dull responses in dialogues because they act reflexively, failing to explicitly model CG, both due to the lack of CG in training data and the standard RG training procedure. We introduce Reflect, a dataset that annotates dialogues with explicit CG (materialized as inferences approximating shared knowledge and beliefs) and solicits 9k diverse human-generated responses each following one common ground. Using Reflect, we showcase the limitations of current dialogue data and RG models: less than half of the responses in current data is rated as high quality (sensible, specific, and interesting) and models trained using this data have even lower quality, while most Reflect responses are judged high quality. Next, we analyze whether CG can help models produce better quality responses by using Reflect CG to guide RG models. Surprisingly, we find that simply prompting GPT3 to “think” about CG generates 30% more quality responses, showing promising benefits to integrating CG into the RG process.</abstract>
      <url hash="ce214fc1">2022.emnlp-main.714</url>
      <bibkey>zhou-etal-2022-reflect</bibkey>
    </paper>
    <paper id="715">
      <title><fixed-case>F</fixed-case>low<fixed-case>E</fixed-case>val: A Consensus-Based Dialogue Evaluation Framework Using Segment Act Flows</title>
      <author><first>Jianqiao</first><last>Zhao</last></author>
      <author><first>Yanyang</first><last>Li</last></author>
      <author><first>Wanyu</first><last>Du</last></author>
      <author><first>Yangfeng</first><last>Ji</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <author><first>Michael</first><last>Lyu</last></author>
      <author><first>Liwei</first><last>Wang</last></author>
      <pages>10469-10483</pages>
      <abstract>Despite recent progress in open-domain dialogue evaluation, how to develop automatic metrics remains an open problem. We explore the potential of dialogue evaluation featuring dialog act information, which was hardly explicitly modeled in previous methods. However, defined at the utterance level in general, dialog act is of coarse granularity, as an utterance can contain multiple segments possessing different functions. Hence, we propose segment act, an extension of dialog act from utterance level to segment level, and crowdsource a large-scale dataset for it. To utilize segment act flows, sequences of segment acts, for evaluation, we develop the first consensus-based dialogue evaluation framework, FlowEval. This framework provides a reference-free approach for dialog evaluation by finding pseudo-references. Extensive experiments against strong baselines on three benchmark datasets demonstrate the effectiveness and other desirable characteristics of our FlowEval, pointing out a potential path for better dialogue evaluation.</abstract>
      <url hash="6573cd38">2022.emnlp-main.715</url>
      <bibkey>zhao-etal-2022-floweval</bibkey>
    </paper>
    <paper id="716">
      <title><fixed-case>F</fixed-case>a<fixed-case>D</fixed-case>-<fixed-case>VLP</fixed-case>: Fashion Vision-and-Language Pre-training towards Unified Retrieval and Captioning</title>
      <author><first>Suvir</first><last>Mirchandani</last></author>
      <author><first>Licheng</first><last>Yu</last></author>
      <author><first>Mengjiao</first><last>Wang</last></author>
      <author><first>Animesh</first><last>Sinha</last></author>
      <author><first>Wenwen</first><last>Jiang</last></author>
      <author><first>Tao</first><last>Xiang</last></author>
      <author><first>Ning</first><last>Zhang</last></author>
      <pages>10484-10497</pages>
      <abstract>Multimodal tasks in the fashion domain have significant potential for e-commerce, but involve challenging vision-and-language learning problems—e.g., retrieving a fashion item given a reference image plus text feedback from a user. Prior works on multimodal fashion tasks have either been limited by the data in individual benchmarks, or have leveraged generic vision-and-language pre-training but have not taken advantage of the characteristics of fashion data. Additionally, these works have mainly been restricted to multimodal understanding tasks. To address these gaps, we make two key contributions. First, we propose a novel fashion-specific pre-training framework based on weakly-supervised triplets constructed from fashion image-text pairs. We show the triplet-based tasks are an effective addition to standard multimodal pre-training tasks. Second, we propose a flexible decoder-based model architecture capable of both fashion retrieval and captioning tasks. Together, our model design and pre-training approach are competitive on a diverse set of fashion tasks, including cross-modal retrieval, image retrieval with text feedback, image captioning, relative image captioning, and multimodal categorization.</abstract>
      <url hash="e43a480c">2022.emnlp-main.716</url>
      <bibkey>mirchandani-etal-2022-fad</bibkey>
    </paper>
    <paper id="717">
      <title><fixed-case>MM</fixed-case>-Align: Learning Optimal Transport-based Alignment Dynamics for Fast and Accurate Inference on Missing Modality Sequences</title>
      <author><first>Wei</first><last>Han</last></author>
      <author><first>Hui</first><last>Chen</last></author>
      <author><first>Min-Yen</first><last>Kan</last></author>
      <author><first>Soujanya</first><last>Poria</last></author>
      <pages>10498-10511</pages>
      <abstract>Existing multimodal tasks mostly target at the complete input modality setting, i.e., each modality is either complete or completely missing in both training and test sets. However, the randomly missing situations have still been underexplored. In this paper, we present a novel approach named MM-Align to address the missing-modality inference problem. Concretely, we propose 1) an alignment dynamics learning module based on the theory of optimal transport (OT) for missing data imputation; 2) a denoising training algorithm to enhance the quality of imputation as well as the accuracy of model predictions. Compared with previous generative methods which devote to restoring the missing inputs, MM-Align learns to capture and imitate the alignment dynamics between modality sequences. Results of comprehensive experiments on two multimodal tasks empirically demonstrate that our method can perform more accurate and faster inference and alleviate the overfitting issue under different missing conditions.</abstract>
      <url hash="cac335b6">2022.emnlp-main.717</url>
      <bibkey>han-etal-2022-mm</bibkey>
    </paper>
    <paper id="718">
      <title>Evaluating the Knowledge Dependency of Questions</title>
      <author><first>Hyeongdon</first><last>Moon</last></author>
      <author><first>Yoonseok</first><last>Yang</last></author>
      <author><first>Hangyeol</first><last>Yu</last></author>
      <author><first>Seunghyun</first><last>Lee</last></author>
      <author><first>Myeongho</first><last>Jeong</last></author>
      <author><first>Juneyoung</first><last>Park</last></author>
      <author><first>Jamin</first><last>Shin</last></author>
      <author><first>Minsam</first><last>Kim</last></author>
      <author><first>Seungtaek</first><last>Choi</last></author>
      <pages>10512-10526</pages>
      <abstract>The automatic generation of Multiple Choice Questions (MCQ) has the potential to reduce the time educators spend on student assessment significantly. However, existing evaluation metrics for MCQ generation, such as BLEU, ROUGE, and METEOR, focus on the n-gram based similarity of the generated MCQ to the gold sample in the dataset and disregard their educational value.They fail to evaluate the MCQ’s ability to assess the student’s knowledge of the corresponding target fact. To tackle this issue, we propose a novel automatic evaluation metric, coined Knowledge Dependent Answerability (KDA), which measures the MCQ’s answerability given knowledge of the target fact. Specifically, we first show how to measure KDA based on student responses from a human survey.Then, we propose two automatic evaluation metrics, KDA_disc and KDA_cont, that approximate KDA by leveraging pre-trained language models to imitate students’ problem-solving behavior.Through our human studies, we show that KDA_disc and KDA_soft have strong correlations with both (1) KDA and (2) usability in an actual classroom setting, labeled by experts. Furthermore, when combined with n-gram based similarity metrics, KDA_disc and KDA_cont are shown to have a strong predictive power for various expert-labeled MCQ quality measures.</abstract>
      <url hash="31d0f98f">2022.emnlp-main.718</url>
      <bibkey>moon-etal-2022-evaluating</bibkey>
    </paper>
    <paper id="719">
      <title><fixed-case>M</fixed-case>o<fixed-case>SE</fixed-case>: Modality Split and Ensemble for Multimodal Knowledge Graph Completion</title>
      <author><first>Yu</first><last>Zhao</last></author>
      <author><first>Xiangrui</first><last>Cai</last></author>
      <author><first>Yike</first><last>Wu</last></author>
      <author><first>Haiwei</first><last>Zhang</last></author>
      <author><first>Ying</first><last>Zhang</last></author>
      <author><first>Guoqing</first><last>Zhao</last></author>
      <author><first>Ning</first><last>Jiang</last></author>
      <pages>10527-10536</pages>
      <abstract>Multimodal knowledge graph completion (MKGC) aims to predict missing entities in MKGs. Previous works usually share relation representation across modalities. This results in mutual interference between modalities during training, since for a pair of entities, the relation from one modality probably contradicts that from another modality. Furthermore, making a unified prediction based on the shared relation representation treats the input in different modalities equally, while their importance to the MKGC task should be different. In this paper, we propose MoSE, a Modality Split representation learning and Ensemble inference framework for MKGC. Specifically, in the training phase, we learn modality-split relation embeddings for each modality instead of a single modality-shared one, which alleviates the modality interference. Based on these embeddings, in the inference phase, we first make modality-split predictions and then exploit various ensemble methods to combine the predictions with different weights, which models the modality importance dynamically. Experimental results on three KG datasets show that MoSE outperforms state-of-the-art MKGC methods. Codes are available at https://github.com/OreOZhao/MoSE4MKGC.</abstract>
      <url hash="c0c4355f">2022.emnlp-main.719</url>
      <bibkey>zhao-etal-2022-mose</bibkey>
    </paper>
    <paper id="720">
      <title>Entropy-Based Vocabulary Substitution for Incremental Learning in Multilingual Neural Machine Translation</title>
      <author><first>Kaiyu</first><last>Huang</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Jin</first><last>Ma</last></author>
      <author id="yang-liu-icsi"><first>Yang</first><last>Liu</last></author>
      <pages>10537-10550</pages>
      <abstract>In a practical real-world scenario, the longstanding goal is that a universal multilingual translation model can be incrementally updated when new language pairs arrive. Specifically, the initial vocabulary only covers some of the words in new languages, which hurts the translation quality for incremental learning. Although existing approaches attempt to address this issue by replacing the original vocabulary with a rebuilt vocabulary or constructing independent language-specific vocabularies, these methods can not meet the following three demands simultaneously: (1) High translation quality for original and incremental languages, (2) low cost for model training, (3) low time overhead for preprocessing. In this work, we propose an entropy-based vocabulary substitution (EVS) method that just needs to walk through new language pairs for incremental learning in a large-scale multilingual data updating while remaining the size of the vocabulary. Our method has access to learn new knowledge from updated training samples incrementally while keeping high translation quality for original language pairs, alleviating the issue of catastrophic forgetting. Results of experiments show that EVS can achieve better performance and save excess overhead for incremental learning in the multilingual machine translation task.</abstract>
      <url hash="30e399be">2022.emnlp-main.720</url>
      <bibkey>huang-etal-2022-entropy</bibkey>
    </paper>
    <paper id="721">
      <title>Eliciting Knowledge from Large Pre-Trained Models for Unsupervised Knowledge-Grounded Conversation</title>
      <author><first>Yanyang</first><last>Li</last></author>
      <author><first>Jianqiao</first><last>Zhao</last></author>
      <author><first>Michael</first><last>Lyu</last></author>
      <author><first>Liwei</first><last>Wang</last></author>
      <pages>10551-10564</pages>
      <abstract>Recent advances in large-scale pre-training provide large models with the potential to learn knowledge from the raw text. It is thus natural to ask whether it is possible to leverage these large models as knowledge bases for downstream tasks. In this work, we answer the aforementioned question in unsupervised knowledge-grounded conversation. We explore various methods that best elicit knowledge from large models. Our human study indicates that, though hallucinations exist, large models post the unique advantage of being able to output common sense and summarize facts that cannot be directly retrieved from the search engine. To better exploit such generated knowledge in dialogue generation, we treat the generated knowledge as a noisy knowledge source and propose the posterior-based reweighing as well as the noisy training strategy. Empirical results on two benchmarks show advantages over the state-of-the-art methods.</abstract>
      <url hash="63e35736">2022.emnlp-main.721</url>
      <bibkey>li-etal-2022-eliciting</bibkey>
    </paper>
    <paper id="722">
      <title>An Unsupervised, Geometric and Syntax-aware Quantification of Polysemy</title>
      <author><first>Anmol</first><last>Goel</last></author>
      <author><first>Charu</first><last>Sharma</last></author>
      <author><first>Ponnurangam</first><last>Kumaraguru</last></author>
      <pages>10565-10574</pages>
      <abstract>Polysemy is the phenomenon where a single word form possesses two or more related senses. It is an extremely ubiquitous part of natural language and analyzing it has sparked rich discussions in the linguistics, psychology and philosophy communities alike. With scarce attention paid to polysemy in computational linguistics, and even scarcer attention toward quantifying polysemy, in this paper, we propose a novel, unsupervised framework to compute and estimate polysemy scores for words in multiple languages. We infuse our proposed quantification with syntactic knowledge in the form of dependency structures. This informs the final polysemy scores of the lexicon motivated by recent linguistic findings that suggest there is an implicit relation between syntax and ambiguity/polysemy. We adopt a graph based approach by computing the discrete Ollivier Ricci curvature on a graph of the contextual nearest neighbors. We test our framework on curated datasets controlling for different sense distributions of words in 3 typologically diverse languages - English, French and Spanish. The effectiveness of our framework is demonstrated by significant correlations of our quantification with expert human annotated language resources like WordNet. We observe a 0.3 point increase in the correlation coefficient as compared to previous quantification studies in English. Our research leverages contextual language models and syntactic structures to empirically support the widely held theoretical linguistic notion that syntax is intricately linked to ambiguity/polysemy.</abstract>
      <url hash="d1b82d39">2022.emnlp-main.722</url>
      <bibkey>goel-etal-2022-unsupervised</bibkey>
    </paper>
    <paper id="723">
      <title>Reorder and then Parse, Fast and Accurate Discontinuous Constituency Parsing</title>
      <author><first>Kailai</first><last>Sun</last></author>
      <author><first>Zuchao</first><last>Li</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <pages>10575-10588</pages>
      <abstract>Discontinuous constituency parsing is still kept developing for its efficiency and accuracy are far behind its continuous counterparts. Motivated by the observation that a discontinuous constituent tree can be simply transformed into a pseudo-continuous one by artificially reordering words in the sentence, we propose a novel reordering method, thereby construct fast and accurate discontinuous constituency parsing systems working in continuous way. Specifically, we model the relative position changes of words as a list of actions. By parsing and performing this actions, the corresponding pseudo-continuous sequence is derived. Discontinuous parse tree can be further inferred via integrating a high-performance pseudo-continuous constituency parser. Our systems are evaluated on three classical discontinuous constituency treebanks, achieving new state-of-the-art on two treebanks and showing a distinct advantage in speed.</abstract>
      <url hash="58283422">2022.emnlp-main.723</url>
      <bibkey>sun-etal-2022-reorder</bibkey>
    </paper>
    <paper id="724">
      <title>Making Science Simple: Corpora for the Lay Summarisation of Scientific Literature</title>
      <author><first>Tomas</first><last>Goldsack</last></author>
      <author><first>Zhihao</first><last>Zhang</last></author>
      <author><first>Chenghua</first><last>Lin</last></author>
      <author><first>Carolina</first><last>Scarton</last></author>
      <pages>10589-10604</pages>
      <abstract>Lay summarisation aims to jointly summarise and simplify a given text, thus making its content more comprehensible to non-experts.Automatic approaches for lay summarisation can provide significant value in broadening access to scientific literature, enabling a greater degree of both interdisciplinary knowledge sharing and public understanding when it comes to research findings. However, current corpora for this task are limited in their size and scope, hindering the development of broadly applicable data-driven approaches. Aiming to rectify these issues, we present two novel lay summarisation datasets, PLOS (large-scale) and eLife (medium-scale), each of which contains biomedical journal articles alongside expert-written lay summaries.We provide a thorough characterisation of our lay summaries, highlighting differing levels of readability and abstractivenessbetween datasets that can be leveraged to support the needs of different applications.Finally, we benchmark our datasets using mainstream summarisation approaches and perform a manual evaluation with domain experts, demonstrating their utility and casting light on the key challenges of this task.</abstract>
      <url hash="b6fb991e">2022.emnlp-main.724</url>
      <bibkey>goldsack-etal-2022-making</bibkey>
    </paper>
    <paper id="725">
      <title>Looking at the Overlooked: An Analysis on the Word-Overlap Bias in Natural Language Inference</title>
      <author><first>Sara</first><last>Rajaee</last></author>
      <author><first>Yadollah</first><last>Yaghoobzadeh</last></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last></author>
      <pages>10605-10616</pages>
      <abstract>It has been shown that NLI models are usually biased with respect to the word-overlap between the premise and the hypothesis, as they take this feature as a primary cue for predicting the entailment label. In this paper, we focus on an overlooked aspect of the overlap bias in the NLI models: the reverse word-overlap bias. Our experimental results demonstrate that current NLI systems are also highly biased towards the non-entailment label on instances with low overlap and that existing debiasing methods, which are reportedly successful on challenge datasets, are generally ineffective in addressing this category of bias.Through a set of analyses, we investigate the reasons for the emergence of the overlap bias and the role of minority examples in mitigating this bias.For the former, we find that the word overlap bias does not stem from pre-training, and in the latter, we observe that in contrast to the accepted assumption, eliminating minority examples does not affect the generalizability of debiasing methods with respect to the overlap bias.</abstract>
      <url hash="f40bf624">2022.emnlp-main.725</url>
      <bibkey>rajaee-etal-2022-looking</bibkey>
    </paper>
    <paper id="726">
      <title>An Empirical Study on the Transferability of Transformer Modules in Parameter-efficient Fine-tuning</title>
      <author><first>Mohammad</first><last>AkbarTajari</last></author>
      <author><first>Sara</first><last>Rajaee</last></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last></author>
      <pages>10617-10625</pages>
      <abstract>Parameter-efficient fine-tuning has garnered lots of attention in recent studies.On this subject, we investigate the capability of different transformer modules in transferring knowledge from a pre-trained model to a downstream task. Our empirical results suggest that every transformer module is a winning ticket such that fine-tuning the specific module while the rest of the network is frozen achieves a comparable performance to the full fine-tuning case. Among different modules in LMs, LayerNorms exhibit a significant capacity for transfer learning to the extent that with only 0.003% updateable parameters in the layer-wise analysis, they can show acceptable performance on various target tasks.We argue that the performance of LayerNorms could be attributed to their high-magnitude weights compared to other components in a pre-trained model.</abstract>
      <url hash="60f3a3fe">2022.emnlp-main.726</url>
      <bibkey>akbartajari-etal-2022-empirical</bibkey>
    </paper>
    <paper id="727">
      <title><fixed-case>CODER</fixed-case>: An efficient framework for improving retrieval through <fixed-case>CO</fixed-case>ntextual Document Embedding Reranking</title>
      <author><first>George</first><last>Zerveas</last></author>
      <author><first>Navid</first><last>Rekabsaz</last></author>
      <author><first>Daniel</first><last>Cohen</last></author>
      <author><first>Carsten</first><last>Eickhoff</last></author>
      <pages>10626-10644</pages>
      <abstract>Contrastive learning has been the dominant approach to training dense retrieval models. In this work, we investigate the impact of ranking context - an often overlooked aspect of learning dense retrieval models. In particular, we examine the effect of its constituent parts: jointly scoring a large number of negatives per query, using retrieved (query-specific) instead of random negatives, and a fully list-wise loss.To incorporate these factors into training, we introduce Contextual Document Embedding Reranking (CODER), a highly efficient retrieval framework. When reranking, it incurs only a negligible computational overhead on top of a first-stage method at run time (approx. 5 ms delay per query), allowing it to be easily combined with any state-of-the-art dual encoder method. Models trained through CODER can also be used as stand-alone retrievers.Evaluating CODER in a large set of experiments on the MS MARCO and TripClick collections, we show that the contextual reranking of precomputed document embeddings leads to a significant improvement in retrieval performance. This improvement becomes even more pronounced when more relevance information per query is available, shown in the TripClick collection, where we establish new state-of-the-art results by a large margin.</abstract>
      <url hash="ad215797">2022.emnlp-main.727</url>
      <bibkey>zerveas-etal-2022-coder</bibkey>
    </paper>
    <paper id="728">
      <title><fixed-case>A</fixed-case>dapter<fixed-case>S</fixed-case>hare: Task Correlation Modeling with Adapter Differentiation</title>
      <author><first>Zhi</first><last>Chen</last></author>
      <author><first>Bei</first><last>Chen</last></author>
      <author><first>Lu</first><last>Chen</last></author>
      <author><first>Kai</first><last>Yu</last></author>
      <author><first>Jian-Guang</first><last>Lou</last></author>
      <pages>10645-10651</pages>
      <abstract>Thanks to the development of pre-trained language models, multitask learning (MTL) methods achieve a great success in natural language understanding area.However, current MTL methods pay more attention to task selection or model design to fuse as much knowledge as possible, while intrinsic task correlation is often neglected. It is important to learn sharing strategy among multiple tasks rather than sharing everything.%The MTL model is directly shared among all the tasks. %For example, in traditional MTL methods, the last classification layers or the decoder layers are manually separated. More deeply, In this paper, we propose AdapterShare, an adapter differentiation method to explicitly model the task correlation among multiple tasks. AdapterShare is automatically learned based on the gradients on tiny held-out validation data. Compared to single-task learning and fully shared MTL methods, our proposed method obtains obvious performance improvement. Compared to the existing MTL method AdapterFusion, AdapterShare achieves absolute 1.90 average points improvement on five dialogue understanding tasks and 2.33 points gain on NLU tasks.</abstract>
      <url hash="0a251b63">2022.emnlp-main.728</url>
      <bibkey>chen-etal-2022-adaptershare</bibkey>
    </paper>
    <paper id="729">
      <title>Rethinking Task-Specific Knowledge Distillation: Contextualized Corpus as Better Textbook</title>
      <author><first>Chang</first><last>Liu</last></author>
      <author><first>Chongyang</first><last>Tao</last></author>
      <author><first>Jianxin</first><last>Liang</last></author>
      <author><first>Tao</first><last>Shen</last></author>
      <author><first>Jiazhan</first><last>Feng</last></author>
      <author><first>Quzhe</first><last>Huang</last></author>
      <author><first>Dongyan</first><last>Zhao</last></author>
      <pages>10652-10658</pages>
      <abstract>Knowledge distillation has been proven effective when customizing small language models for specific tasks. Here, a corpus as ‘textbook’ plays an indispensable role, only through which the teacher can teach the student. Prevailing methods adopt a two-stage distillation paradigm: general distillation first with task-agnostic general corpus and task-specific distillation next with augmented task-specific corpus. We argue that such a paradigm may not be optimal. In general distillation, it’s extravagant to let the diverse but desultory general knowledge overwhelms the limited model capacity of the student. While in task-specific distillation, the task corpus is usually limited and narrow, preventing the student from learning enough knowledge. To mitigate the issues in the two gapped corpora, we present a better textbook for the student to learn: contextualized corpus that contextualizes task corpus with large-scale general corpus through relevance-based text retrieval. Experimental results on GLUE benchmark demonstrate that contextualized corpus is the better textbook compared with jointly using general corpus and augmented task-specific corpus. Surprisingly, it enables task-specific distillation from scratch without general distillation while maintaining comparable performance, making it more flexible to customize the student model with desired model size under various computation constraints.</abstract>
      <url hash="ec13219f">2022.emnlp-main.729</url>
      <bibkey>liu-etal-2022-rethinking-task</bibkey>
    </paper>
    <paper id="730">
      <title>Recovering Gold from Black Sand: Multilingual Dense Passage Retrieval with Hard and False Negative Samples</title>
      <author><first>Tianhao</first><last>Shen</last></author>
      <author><first>Mingtong</first><last>Liu</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <author><first>Deyi</first><last>Xiong</last></author>
      <pages>10659-10670</pages>
      <abstract>Negative samples have not been efficiently explored in multilingual dense passage retrieval. In this paper, we propose a novel multilingual dense passage retrieval framework, mHFN, to recover and utilize hard and false negative samples. mHFN consists of three key components: 1) a multilingual hard negative sample augmentation module that allows knowledge of indistinguishable passages to be shared across multiple languages and synthesizes new hard negative samples by interpolating representations of queries and existing hard negative samples, 2) a multilingual negative sample cache queue that stores negative samples from previous batches in each language to increase the number of multilingual negative samples used in training beyond the batch size limit, and 3) a lightweight adaptive false negative sample filter that uses generated pseudo labels to separate unlabeled false negative samples and converts them into positive passages in training. We evaluate mHFN on Mr. TyDi, a high-quality multilingual dense passage retrieval dataset covering eleven typologically diverse languages, and experimental results show that mHFN outperforms strong sparse, dense and hybrid baselines and achieves new state-of-the-art performance on all languages. Our source code is available at https://github.com/Magnetic2014/mHFN.</abstract>
      <url hash="e2bd72fa">2022.emnlp-main.730</url>
      <bibkey>shen-etal-2022-recovering</bibkey>
    </paper>
    <paper id="731">
      <title>The “Problem” of Human Label Variation: On Ground Truth in Data, Modeling and Evaluation</title>
      <author><first>Barbara</first><last>Plank</last></author>
      <pages>10671-10682</pages>
      <abstract>Human variation in labeling is often considered noise. Annotation projects for machine learning (ML) aim at minimizing human label variation, with the assumption to maximize data quality and in turn optimize and maximize machine learning metrics. However, thisconventional practice assumes that there exists a *ground truth*, and neglects that there exists genuine human variation in labeling due to disagreement, subjectivity in annotation or multiple plausible answers.In this position paper, we argue that this big open problem of <i>human label variation</i> persists and critically needs more attention to move our field forward. This is because human label variation impacts all stages of the ML pipeline: *data, modeling and evaluation*. However, few works consider all of these dimensions jointly; and existing research is fragmented. We reconcile different previously proposed notions of human label variation, provide a repository of publicly-available datasets with un-aggregated labels, depict approaches proposed so far, identify gaps and suggest ways forward. As datasets are becoming increasingly available, we hope that this synthesized view on the “problem” will lead to an open discussion on possible strategies to devise fundamentally new directions.</abstract>
      <url hash="15e302fe">2022.emnlp-main.731</url>
      <bibkey>plank-2022-problem</bibkey>
    </paper>
    <paper id="732">
      <title>Quality Scoring of Source Words in Neural Translation Models</title>
      <author><first>Priyesh</first><last>Jain</last></author>
      <author><first>Sunita</first><last>Sarawagi</last></author>
      <author><first>Tushar</first><last>Tomar</last></author>
      <pages>10683-10691</pages>
      <abstract>Word-level quality scores on input source sentences can provide useful feedback to an end-user when translating into an unfamiliar target language. Recent approaches either require training special word-scoring models based on synthetic data or require repeated invocation of the translation model. We propose a simple approach based on comparing the difference of probabilities from two language models. The basic premise of our method is to reason how well each source word is explained by the target sentence as against the source language model. Our approach provides up to five points higher F1 scores and is significantly faster than the state of the art methods on three language pairs. Also, our method does not require training any new model. We release a public dataset on word omissions and mistranslations on a new language pair.</abstract>
      <url hash="c5fc9572">2022.emnlp-main.732</url>
      <bibkey>jain-etal-2022-quality</bibkey>
    </paper>
    <paper id="733">
      <title>Pneg: Prompt-based Negative Response Generation for Dialogue Response Selection Task</title>
      <author><first>Nyoungwoo</first><last>Lee</last></author>
      <author><first>ChaeHun</first><last>Park</last></author>
      <author><first>Ho-Jin</first><last>Choi</last></author>
      <author><first>Jaegul</first><last>Choo</last></author>
      <pages>10692-10703</pages>
      <abstract>In retrieval-based dialogue systems, a response selection model acts as a ranker to select the most appropriate response among several candidates. However, such selection models tend to rely on context-response content similarity, which makes models vulnerable to adversarial responses that are semantically similar but not relevant to the dialogue context. Recent studies have shown that leveraging these adversarial responses as negative training samples is useful for improving the discriminating power of the selection model. Nevertheless, collecting human-written adversarial responses is expensive, and existing synthesizing methods often have limited scalability. To overcome these limitations, this paper proposes a simple but efficient method for generating adversarial negative responses leveraging a large-scale language model. Experimental results on dialogue selection tasks show that our method outperforms other methods of synthesizing adversarial negative responses. These results suggest that our method can be an effective alternative to human annotators in generating adversarial responses. Our code and dataset will be released if the paper is accepted.</abstract>
      <url hash="6c53ae5e">2022.emnlp-main.733</url>
      <bibkey>lee-etal-2022-pneg</bibkey>
    </paper>
    <paper id="734">
      <title>Facilitating Contrastive Learning of Discourse Relational Senses by Exploiting the Hierarchy of Sense Relations</title>
      <author><first>Wanqiu</first><last>Long</last></author>
      <author><first>Bonnie</first><last>Webber</last></author>
      <pages>10704-10716</pages>
      <abstract>Implicit discourse relation recognition is a challenging task that involves identifying the sense or senses that hold between two adjacent spans of text, in the absense of an explicit connective between them. In both PDTB-2 (prasad et al., 2008) and PDTB-3 (Webber et al., 2019), discourse relational senses are organized into a three-level hierarchy ranging from four broad top-level senses, to more specific senses below them. Most previous work on implicitf discourse relation recognition have used the sense hierarchy simply to indicate what sense labels were available. Here we do more — incorporating the sense hierarchy into the recognition process itself and using it to select the negative examples used in contrastive learning. With no additional effort, the approach achieves state-of-the-art performance on the task. Our code is released inhttps://github.com/wanqiulong 0923/Contrastive_IDRR.</abstract>
      <url hash="a8391b0d">2022.emnlp-main.734</url>
      <bibkey>long-webber-2022-facilitating</bibkey>
    </paper>
    <paper id="735">
      <title>Simplified Graph Learning for Inductive Short Text Classification</title>
      <author><first>Kaixin</first><last>Zheng</last></author>
      <author><first>Yaqing</first><last>Wang</last></author>
      <author><first>Quanming</first><last>Yao</last></author>
      <author><first>Dejing</first><last>Dou</last></author>
      <pages>10717-10724</pages>
      <abstract>Short text classification (STC) is hard as short texts lack context information and labeled data is not enough. Graph neural networks obtain the state-of-the-art on STC since they can merge various auxiliary information via the message passing framework. However, existing works conduct transductive learning, which requires retraining to accommodate new samples and takes large memory. In this paper, we present SimpleSTC which handles inductive STC problem but only leverages words. We construct word graph from an external large corpus to compensate for the lack of semantic information, and learn text graph to handle the lack of labeled data. Results show that SimpleSTC obtains state-of-the-art performance with lower memory consumption and faster inference speed.</abstract>
      <url hash="bc116479">2022.emnlp-main.735</url>
      <bibkey>zheng-etal-2022-simplified</bibkey>
    </paper>
    <paper id="736">
      <title>Don’t Stop Fine-Tuning: On Training Regimes for Few-Shot Cross-Lingual Transfer with Multilingual Language Models</title>
      <author><first>Fabian David</first><last>Schmidt</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Goran</first><last>Glavaš</last></author>
      <pages>10725-10742</pages>
      <abstract>A large body of recent work highlights the fallacies of zero-shot cross-lingual transfer (ZS-XLT) with large multilingual language models. Namely, their performance varies substantially for different target languages and is the weakest where needed the most: for low-resource languages distant to the source language. One remedy is few-shot transfer (FS-XLT), where leveraging only a few task-annotated instances in the target language(s) may yield sizable performance gains. However, FS-XLT also succumbs to large variation, as models easily overfit to the small datasets. In this work, we present a systematic study focused on a spectrum of FS-XLT fine-tuning regimes, analyzing key properties such as effectiveness, (in)stability, and modularity. We conduct extensive experiments on both higher-level (NLI, paraphrasing) and lower-level tasks (NER, POS), presenting new FS-XLT strategies that yield both improved and more stable FS-XLT across the board. Our findings challenge established FS-XLT methods: e.g., we propose to replace sequential fine-tuning with joint fine-tuning on source and target language instances, offering consistent gains with different number of shots (including resource-rich scenarios). We also show that further gains can be achieved with multi-stage FS-XLT training in which joint multilingual fine-tuning precedes the bilingual source-target specialization.</abstract>
      <url hash="bddb53e1">2022.emnlp-main.736</url>
      <bibkey>schmidt-etal-2022-dont</bibkey>
    </paper>
    <paper id="737">
      <title>Towards Compositional Generalization in Code Search</title>
      <author><first>Hojae</first><last>Han</last></author>
      <author><first>Seung-won</first><last>Hwang</last></author>
      <author><first>Shuai</first><last>Lu</last></author>
      <author><first>Nan</first><last>Duan</last></author>
      <author><first>Seungtaek</first><last>Choi</last></author>
      <pages>10743-10750</pages>
      <abstract>We study compositional generalization, which aims to generalize on unseen combinations of seen structural elements, for code search. Unlike existing approaches of partially pursuing this goal, we study how to extract structural elements, which we name a template that directly targets compositional generalization. Thus we propose CTBERT, or Code Template BERT, representing codes using automatically extracted templates as building blocks. We empirically validate CTBERT on two public code search benchmarks, AdvTest and CSN. Further, we show that templates are complementary to data flow graphs in GraphCodeBERT, by enhancing structural context around variables.</abstract>
      <url hash="827a92be">2022.emnlp-main.737</url>
      <bibkey>han-etal-2022-towards</bibkey>
    </paper>
    <paper id="738">
      <title>Towards relation extraction from speech</title>
      <author><first>Tongtong</first><last>Wu</last></author>
      <author><first>Guitao</first><last>Wang</last></author>
      <author><first>Jinming</first><last>Zhao</last></author>
      <author><first>Zhaoran</first><last>Liu</last></author>
      <author><first>Guilin</first><last>Qi</last></author>
      <author><first>Yuan-Fang</first><last>Li</last></author>
      <author><first>Gholamreza</first><last>Haffari</last></author>
      <pages>10751-10762</pages>
      <abstract>Relation extraction typically aims to extract semantic relationships between entities from the unstructured text.One of the most essential data sources for relation extraction is the spoken language, such as interviews and dialogues.However, the error propagation introduced in automatic speech recognition (ASR) has been ignored in relation extraction, and the end-to-end speech-based relation extraction method has been rarely explored.In this paper, we propose a new listening information extraction task, i.e., speech relation extraction.We construct the training dataset for speech relation extraction via text-to-speech systems, and we construct the testing dataset via crowd-sourcing with native English speakers.We explore speech relation extraction via two approaches: the pipeline approach conducting text-based extraction with a pretrained ASR module, and the end2end approach via a new proposed encoder-decoder model, or what we called SpeechRE.We conduct comprehensive experiments to distinguish the challenges in speech relation extraction, which may shed light on future explorations. We share the code and data on https://github.com/wutong8023/SpeechRE.</abstract>
      <url hash="3ed8cca1">2022.emnlp-main.738</url>
      <bibkey>wu-etal-2022-towards-relation</bibkey>
    </paper>
    <paper id="739">
      <title>Structural Constraints and Natural Language Inference for End-to-End Flowchart Grounded Dialog Response Generation</title>
      <author><first>Dinesh</first><last>Raghu</last></author>
      <author><first>Suraj</first><last>Joshi</last></author>
      <author><first>Sachindra</first><last>Joshi</last></author>
      <author><first>Mausam</first><last>-</last></author>
      <pages>10763-10774</pages>
      <abstract>Flowchart grounded dialog systems converse with users by following a given flowchart and a corpus of FAQs. The existing state-of-the-art approach (Raghu et al, 2021) for learning such a dialog system, named FLONET, has two main limitations. (1) It uses a Retrieval Augmented Generation (RAG) framework which represents a flowchart as a bag of nodes. By doing so, it loses the connectivity structure between nodes that can aid in better response generation. (2) Typically dialogs progress with the agent asking polar (Y/N) questions, but users often respond indirectly without the explicit use of polar words. In such cases, it fails to understand the correct polarity of the answer. To overcome these issues, we propose Structure-Aware FLONET (SA-FLONET) which infuses structural constraints derived from the connectivity structure of flowcharts into the RAG framework. It uses natural language inference to better predict the polarity of indirect Y/N answers. We find that SA-FLONET outperforms FLONET, with a success rate improvement of 68% and 123% in flowchart grounded response generation and zero-shot flowchart grounded response generation tasks respectively.</abstract>
      <url hash="b87ab674">2022.emnlp-main.739</url>
      <bibkey>raghu-etal-2022-structural</bibkey>
    </paper>
    <paper id="740">
      <title><fixed-case>SLICER</fixed-case>: Sliced Fine-Tuning for Low-Resource Cross-Lingual Transfer for Named Entity Recognition</title>
      <author><first>Fabian David</first><last>Schmidt</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <author><first>Goran</first><last>Glavaš</last></author>
      <pages>10775-10785</pages>
      <abstract>Large multilingual language models generally demonstrate impressive results in zero-shot cross-lingual transfer, yet often fail to successfully transfer to low-resource languages, even for token-level prediction tasks like named entity recognition (NER). In this work, we introduce a simple yet highly effective approach for improving zero-shot transfer for NER to low-resource languages. We observe that NER fine-tuning in the source language decontextualizes token representations, i.e., tokens increasingly attend to themselves. This increased reliance on token information itself, we hypothesize, triggers a type of overfitting to properties that NE tokens within the source languages share, but are generally not present in NE mentions of target languages. As a remedy, we propose a simple yet very effective sliced fine-tuning for NER (SLICER) that forces stronger token contextualization in the Transformer: we divide the transformed token representations and classifier into disjoint slices that are then independently classified during training. We evaluate SLICER on two standard benchmarks for NER that involve low-resource languages, WikiANN and MasakhaNER, and show that it (i) indeed reduces decontextualization (i.e., extent to which NE tokens attend to themselves), consequently (ii) yielding consistent transfer gains, especially prominent for low-resource target languages distant from the source language.</abstract>
      <url hash="19316e66">2022.emnlp-main.740</url>
      <bibkey>schmidt-etal-2022-slicer</bibkey>
    </paper>
    <paper id="741">
      <title><fixed-case>E</fixed-case>dge<fixed-case>F</fixed-case>ormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation</title>
      <author><first>Tao</first><last>Ge</last></author>
      <author><first>Si-Qing</first><last>Chen</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <pages>10786-10798</pages>
      <abstract>We introduce EdgeFormer – a parameter-efficient Transformer for on-device seq2seq generation under the strict computation and memory constraints. Compared with the previous parameter-efficient Transformers, EdgeFormer applies two novel principles for cost-effective parameterization, allowing it to perform better given the same parameter budget; moreover, EdgeFormer is further enhanced by layer adaptation innovation that is proposed for improving the network with shared layers.Extensive experiments show EdgeFormer can effectively outperform previous parameter-efficient Transformer baselines and achieve competitive results under both the computation and memory constraints. Given the promising results, we release EdgeLM – the pretrained version of EdgeFormer, which is the first publicly available pretrained on-device seq2seq model that can be easily fine-tuned for seq2seq tasks with strong results, facilitating on-device seq2seq generation in practice.</abstract>
      <url hash="37cc477c">2022.emnlp-main.741</url>
      <bibkey>ge-etal-2022-edgeformer</bibkey>
    </paper>
    <paper id="742">
      <title>End-to-End Unsupervised Vision-and-Language Pre-training with Referring Expression Matching</title>
      <author><first>Chi</first><last>Chen</last></author>
      <author><first>Peng</first><last>Li</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author id="yang-liu-ict"><first>Yang</first><last>Liu</last></author>
      <pages>10799-10810</pages>
      <abstract>Recently there has been an emerging interest in unsupervised vision-and-language pre-training (VLP) that learns multimodal representations without parallel image-caption data. These pioneering works significantly reduce the cost of VLP on data collection and achieve promising results compared to supervised VLP. However, existing unsupervised VLP methods take as input pre-extracted region-based visual features from external object detectors, which both limits flexibility and reduces computational efficiency. In this paper, we explore end-to-end unsupervised VLP with a vision encoder to directly encode images. The vision encoder is pre-trained on image-only data and jointly optimized during multimodal pre-training. To further enhance the learned cross-modal features, we propose a novel pre-training task that predicts which patches contain an object referred to in natural language from the encoded visual features. Extensive experiments on four vision-and-language tasks show that our approach outperforms previous unsupervised VLP methods and obtains new state-of-the-art results.</abstract>
      <url hash="4a7fe234">2022.emnlp-main.742</url>
      <bibkey>chen-etal-2022-end</bibkey>
    </paper>
    <paper id="743">
      <title>Faithful Knowledge Graph Explanations in Commonsense Question Answering</title>
      <author><first>Guy</first><last>Aglionby</last></author>
      <author><first>Simone</first><last>Teufel</last></author>
      <pages>10811-10817</pages>
      <abstract>Knowledge graphs are commonly used as sources of information in commonsense question answering, and can also be used to express explanations for the model’s answer choice. A common way of incorporating facts from the graph is to encode them separately from the question, and then combine the two representations to select an answer. In this paper, we argue that highly faithful graph-based explanations cannot be extracted from existing models of this type. Such explanations will not include reasoning done by the transformer encoding the question, so will be incomplete. We confirm this theory with a novel proxy measure for faithfulness and propose two architecture changes to address the problem. Our findings suggest a path forward for developing architectures for faithful graph-based explanations.</abstract>
      <url hash="fce68090">2022.emnlp-main.743</url>
      <bibkey>aglionby-teufel-2022-faithful</bibkey>
    </paper>
    <paper id="744">
      <title><fixed-case>KOLD</fixed-case>: <fixed-case>K</fixed-case>orean Offensive Language Dataset</title>
      <author><first>Younghoon</first><last>Jeong</last></author>
      <author><first>Juhyun</first><last>Oh</last></author>
      <author><first>Jongwon</first><last>Lee</last></author>
      <author><first>Jaimeen</first><last>Ahn</last></author>
      <author><first>Jihyung</first><last>Moon</last></author>
      <author><first>Sungjoon</first><last>Park</last></author>
      <author><first>Alice</first><last>Oh</last></author>
      <pages>10818-10833</pages>
      <abstract>Recent directions for offensive language detection are hierarchical modeling, identifying the type and the target of offensive language, and interpretability with offensive span annotation and prediction. These improvements are focused on English and do not transfer well to other languages because of cultural and linguistic differences. In this paper, we present the Korean Offensive Language Dataset (KOLD) comprising 40,429 comments, which are annotated hierarchically with the type and the target of offensive language, accompanied by annotations of the corresponding text spans. We collect the comments from NAVER news and YouTube platform and provide the titles of the articles and videos as the context information for the annotation process. We use these annotated comments as training data for Korean BERT and RoBERTa models and find that they are effective at offensiveness detection, target classification, and target span detection while having room for improvement for target group classification and offensive span detection. We discover that the target group distribution differs drastically from the existing English datasets, and observe that providing the context information improves the model performance in offensiveness detection (+0.3), target classification (+1.5), and target group classification (+13.1). We publicly release the dataset and baseline models.</abstract>
      <url hash="3d92a228">2022.emnlp-main.744</url>
      <bibkey>jeong-etal-2022-kold</bibkey>
    </paper>
    <paper id="745">
      <title>Evade the Trap of Mediocrity: Promoting Diversity and Novelty in Text Generation via Concentrating Attention</title>
      <author><first>Wenhao</first><last>Li</last></author>
      <author><first>Xiaoyuan</first><last>Yi</last></author>
      <author><first>Jinyi</first><last>Hu</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <author><first>Xing</first><last>Xie</last></author>
      <pages>10834-10858</pages>
      <abstract>Recently, powerful Transformer architectures have proven superior in generating high-quality sentences. Nevertheless, these models tend to produce dull high-frequency phrases, severely hurting the diversity and novelty of generated text. In this work, we dig into the intrinsic mechanism of this problem and found that sparser attention values in Transformer could improve diversity. To understand such a phenomenon, we first conduct both empirical and theoretical analysis and then attribute it to representation degeneration caused by the attentive mixture of the hidden states during training. We term this process the Trap of Mediocrity. To escape from such a trap, we introduce a novel attention regularization loss to control the sharpness of the attention distribution, which is transparent to model structures and can be easily implemented within 20 lines of python code. We prove that this method could be mathematically regarded as learning a Bayesian approximation of posterior attention. Experiments show that our method improved the diversity and novelty of the generated text while maintaining comparable quality on a variety of conditional and unconditional generation tasks.</abstract>
      <url hash="11bdd8f5">2022.emnlp-main.745</url>
      <bibkey>li-etal-2022-evade</bibkey>
    </paper>
    <paper id="746">
      <title>The better your Syntax, the better your Semantics? Probing Pretrained Language Models for the <fixed-case>E</fixed-case>nglish Comparative Correlative</title>
      <author><first>Leonie</first><last>Weissweiler</last></author>
      <author><first>Valentin</first><last>Hofmann</last></author>
      <author><first>Abdullatif</first><last>Köksal</last></author>
      <author><first>Hinrich</first><last>Schütze</last></author>
      <pages>10859-10882</pages>
      <abstract>Construction Grammar (CxG) is a paradigm from cognitive linguistics emphasising the connection between syntax and semantics. Rather than rules that operate on lexical items, it posits constructions as the central building blocks of language, i.e., linguistic units of different granularity that combine syntax and semantics. As a first step towards assessing the compatibility of CxG with the syntactic and semantic knowledge demonstrated by state-of-the-art pretrained language models (PLMs), we present an investigation of their capability to classify and understand one of the most commonly studied constructions, the English comparative correlative (CC). We conduct experiments examining the classification accuracy of a syntactic probe on the one hand and the models’ behaviour in a semantic application task on the other, with BERT, RoBERTa, and DeBERTa as the example PLMs. Our results show that all three investigated PLMs are able to recognise the structure of the CC but fail to use its meaning. While human-like performance of PLMs on many NLP tasks has been alleged, this indicates that PLMs still suffer from substantial shortcomings in central domains of linguistic knowledge.</abstract>
      <url hash="b6a48ef9">2022.emnlp-main.746</url>
      <bibkey>weissweiler-etal-2022-better</bibkey>
    </paper>
    <paper id="747">
      <title><fixed-case>P</fixed-case>roof<fixed-case>I</fixed-case>nfer: Generating Proof via Iterative Hierarchical Inference</title>
      <author><first>Zichu</first><last>Fei</last></author>
      <author><first>Qi</first><last>Zhang</last></author>
      <author><first>Xin</first><last>Zhou</last></author>
      <author><first>Tao</first><last>Gui</last></author>
      <author><first>Xuanjing</first><last>Huang</last></author>
      <pages>10883-10892</pages>
      <abstract>Proof generation focuses on deductive reasoning: given a hypothesis and a set of theories, including some supporting facts and logical rules expressed in natural language, the model generates a proof tree indicating how to deduce the hypothesis from given theories.Current models with state-of-the-art performance employ the stepwise method that adds an individual node to the proof step-by-step.However, these methods actually focus on generating several proof paths rather than a whole tree.During generation, they focus on the most relevant areas of the currently generated node while neglecting the rest of the proof tree. To address this problem, we propose ProofInfer, which generates the proof tree via iterative hierarchical inference.At each step, ProofInfer adds the entire layer to the proof, where all nodes in this layer are generated simultaneously. Since the conventional autoregressive generation architecture cannot simultaneously predict multiple nodes, ProofInfer employs text-to-text paradigm.To this end, we propose a divide-and-conquer algorithm to encode the proof tree as the plain text without losing structure information.Experimental results show that ProofInfer significantly improves performance on several widely-used datasets.In addition, ProofInfer still performs well with data-limited, achieving comparable performance to the state-of-the-art model with about 40% of the training data.</abstract>
      <url hash="28f61c6b">2022.emnlp-main.747</url>
      <bibkey>fei-etal-2022-proofinfer</bibkey>
    </paper>
    <paper id="748">
      <title><fixed-case>ECTS</fixed-case>um: A New Benchmark Dataset For Bullet Point Summarization of Long Earnings Call Transcripts</title>
      <author><first>Rajdeep</first><last>Mukherjee</last></author>
      <author><first>Abhinav</first><last>Bohra</last></author>
      <author><first>Akash</first><last>Banerjee</last></author>
      <author><first>Soumya</first><last>Sharma</last></author>
      <author><first>Manjunath</first><last>Hegde</last></author>
      <author><first>Afreen</first><last>Shaikh</last></author>
      <author><first>Shivani</first><last>Shrivastava</last></author>
      <author><first>Koustuv</first><last>Dasgupta</last></author>
      <author><first>Niloy</first><last>Ganguly</last></author>
      <author><first>Saptarshi</first><last>Ghosh</last></author>
      <author><first>Pawan</first><last>Goyal</last></author>
      <pages>10893-10906</pages>
      <abstract>Despite tremendous progress in automatic summarization, state-of-the-art methods are predominantly trained to excel in summarizing short newswire articles, or documents with strong layout biases such as scientific articles or government reports. Efficient techniques to summarize financial documents, discussing facts and figures, have largely been unexplored, majorly due to the unavailability of suitable datasets. In this work, we present ECTSum, a new dataset with transcripts of earnings calls (ECTs), hosted by publicly traded companies, as documents, and experts-written short telegram-style bullet point summaries derived from corresponding Reuters articles. ECTs are long unstructured documents without any prescribed length limit or format. We benchmark our dataset with state-of-the-art summarization methods across various metrics evaluating the content quality and factual consistency of the generated summaries. Finally, we present a simple yet effective approach, ECT-BPS, to generate a set of bullet points that precisely capture the important facts discussed in the calls.</abstract>
      <url hash="f81a0b83">2022.emnlp-main.748</url>
      <bibkey>mukherjee-etal-2022-ectsum</bibkey>
    </paper>
    <paper id="749">
      <title>Cross-domain Generalization for <fixed-case>AMR</fixed-case> Parsing</title>
      <author><first>Xuefeng</first><last>Bai</last></author>
      <author><first>Sen</first><last>Yang</last></author>
      <author><first>Leyang</first><last>Cui</last></author>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Yue</first><last>Zhang</last></author>
      <pages>10907-10921</pages>
      <abstract>Abstract Meaning Representation (AMR) parsing aims to predict an AMR graph from textual input. Recently, there has been notable growth in AMR parsing performance. However, most existing work focuses on improving the performance in the specific domain, ignoring the potential domain dependence of AMR parsing systems. To address this, we extensively evaluate five representative AMR parsers on five domains and analyze challenges to cross-domain AMR parsing. We observe that challenges to cross-domain AMR parsing mainly arise from the distribution shift of words and AMR concepts. Based on our observation, we investigate two approaches to reduce the domain distribution divergence of text and AMR features, respectively. Experimental results on two out-of-domain test sets show the superiority of our method.</abstract>
      <url hash="58bca03f">2022.emnlp-main.749</url>
      <bibkey>bai-etal-2022-cross</bibkey>
    </paper>
    <paper id="750">
      <title><fixed-case>C</fixed-case>ite<fixed-case>S</fixed-case>um: Citation Text-guided Scientific Extreme Summarization and Domain Adaptation with Limited Supervision</title>
      <author><first>Yuning</first><last>Mao</last></author>
      <author><first>Ming</first><last>Zhong</last></author>
      <author><first>Jiawei</first><last>Han</last></author>
      <pages>10922-10935</pages>
      <abstract>Scientific extreme summarization (TLDR) aims to form ultra-short summaries of scientific papers. Previous efforts on curating scientific TLDR datasets failed to scale up due to the heavy human annotation and domain expertise required. In this paper, we propose a simple yet effective approach to automatically extracting TLDR summaries for scientific papers from their citation texts. Based on the proposed approach, we create a new benchmark CiteSum without human annotation, which is around 30 times larger than the previous human-curated dataset SciTLDR. We conduct a comprehensive analysis of CiteSum, examining its data characteristics and establishing strong baselines. We further demonstrate the usefulness of CiteSum by adapting models pre-trained on CiteSum (named CITES) to new tasks and domains with limited supervision. For scientific extreme summarization, CITES outperforms most fully-supervised methods on SciTLDR without any fine-tuning and obtains state-of-the-art results with only 128 examples. For news extreme summarization, CITES achieves significant gains on XSum over its base model (not pre-trained on CiteSum), e.g., +7.2 ROUGE-1 zero-shot performance and state-of-the-art few-shot performance. For news headline generation, CITES performs the best among unsupervised and zero-shot methods on Gigaword.</abstract>
      <url hash="68cc02b0">2022.emnlp-main.750</url>
      <bibkey>mao-etal-2022-citesum</bibkey>
    </paper>
    <paper id="751">
      <title><fixed-case>FETA</fixed-case>: A Benchmark for Few-Sample Task Transfer in Open-Domain Dialogue</title>
      <author><first>Alon</first><last>Albalak</last></author>
      <author><first>Yi-Lin</first><last>Tuan</last></author>
      <author><first>Pegah</first><last>Jandaghi</last></author>
      <author><first>Connor</first><last>Pryor</last></author>
      <author><first>Luke</first><last>Yoffe</last></author>
      <author><first>Deepak</first><last>Ramachandran</last></author>
      <author><first>Lise</first><last>Getoor</last></author>
      <author><first>Jay</first><last>Pujara</last></author>
      <author><first>William Yang</first><last>Wang</last></author>
      <pages>10936-10953</pages>
      <abstract>Task transfer, transferring knowledge contained in related tasks, holds the promise of reducing the quantity of labeled data required to fine-tune language models. Dialogue understanding encompasses many diverse tasks, yet task transfer has not been thoroughly studied in conversational AI. This work explores conversational task transfer by introducing FETA: a benchmark for FEw-sample TAsk transfer in open-domain dialogue.FETA contains two underlying sets of conversations upon which there are 10 and 7 tasks annotated, enabling the study of intra-dataset task transfer; task transfer without domain adaptation. We utilize three popular language models and three learning algorithms to analyze the transferability between 132 source-target task pairs and create a baseline for future work.We run experiments in the single- and multi-source settings and report valuable findings, e.g., most performance trends are model-specific, and span extraction and multiple-choice tasks benefit the most from task transfer.In addition to task transfer, FETA can be a valuable resource for future research into the efficiency and generalizability of pre-training datasets and model architectures, as well as for learning settings such as continual and multitask learning.</abstract>
      <url hash="3c104cd1">2022.emnlp-main.751</url>
      <bibkey>albalak-etal-2022-feta</bibkey>
    </paper>
    <paper id="752">
      <title>Do Children Texts Hold The Key To Commonsense Knowledge?</title>
      <author><first>Julien</first><last>Romero</last></author>
      <author><first>Simon</first><last>Razniewski</last></author>
      <pages>10954-10959</pages>
      <abstract>Compiling comprehensive repositories of commonsense knowledge is a long-standing problem in AI. Many concerns revolve around the issue of reporting bias, i.e., that frequency in text sources is not a good proxy for relevance or truth. This paper explores whether children’s texts hold the key to commonsense knowledge compilation, based on the hypothesis that such content makes fewer assumptions on the reader’s knowledge, and therefore spells out commonsense more explicitly. An analysis with several corpora shows that children’s texts indeed contain much more, and more typical commonsense assertions. Moreover, experiments show that this advantage can be leveraged in popular language-model-based commonsense knowledge extraction settings, where task-unspecific fine-tuning on small amounts of children texts (childBERT) already yields significant improvements. This provides a refreshing perspective different from the common trend of deriving progress from ever larger models and corpora.</abstract>
      <url hash="4ed0efd7">2022.emnlp-main.752</url>
      <bibkey>romero-razniewski-2022-children</bibkey>
    </paper>
    <paper id="753">
      <title>On the Limitations of Reference-Free Evaluations of Generated Text</title>
      <author><first>Daniel</first><last>Deutsch</last></author>
      <author><first>Rotem</first><last>Dror</last></author>
      <author><first>Dan</first><last>Roth</last></author>
      <pages>10960-10977</pages>
      <abstract>There is significant interest in developing evaluation metrics which accurately estimate the quality of generated text without the aid of a human-written reference text, which can be time consuming and expensive to collect or entirely unavailable in online applications. However, in this work, we demonstrate that these reference-free metrics are inherently biased and limited in their ability to evaluate generated text, and we argue that they should not be used to measure progress on tasks like machine translation or summarization. We show how reference-free metrics are equivalent to using one generation model to evaluate another, which has several limitations: (1) the metrics can be optimized at test time to find the approximate best-possible output, (2) they are inherently biased toward models which are more similar to their own, and (3) they can be biased against higher-quality outputs, including those written by humans. Therefore, we recommend that reference-free metrics should be used as diagnostic tools for analyzing and understanding model behavior instead of measures of how well models perform a task, in which the goal is to achieve as high of a score as possible.</abstract>
      <url hash="27766bc2">2022.emnlp-main.753</url>
      <bibkey>deutsch-etal-2022-limitations</bibkey>
    </paper>
    <paper id="754">
      <title>Sampling-Based Approximations to Minimum <fixed-case>B</fixed-case>ayes Risk Decoding for Neural Machine Translation</title>
      <author><first>Bryan</first><last>Eikema</last></author>
      <author><first>Wilker</first><last>Aziz</last></author>
      <pages>10978-10993</pages>
      <abstract>In NMT we search for the mode of the model distribution to form predictions. The mode and other high-probability translations found by beam search have been shown to often be inadequate in a number of ways. This prevents improving translation quality through better search, as these idiosyncratic translations end up selected by the decoding algorithm, a problem known as the beam search curse. Recently, an approximation to minimum Bayes risk (MBR) decoding has been proposed as an alternative decision rule that would likely not suffer from the same problems. We analyse this approximation and establish that it has no equivalent to the beam search curse. We then design approximations that decouple the cost of exploration from the cost of robust estimation of expected utility. This allows for much larger hypothesis spaces, which we show to be beneficial. We also show that mode-seeking strategies can aid in constructing compact sets of promising hypotheses and that MBR is effective in identifying good translations in them. We conduct experiments on three language pairs varying in amounts of resources available: English into and from German, Romanian, and Nepali.</abstract>
      <url hash="d8c7c9c0">2022.emnlp-main.754</url>
      <bibkey>eikema-aziz-2022-sampling</bibkey>
    </paper>
    <paper id="755">
      <title><fixed-case>I</fixed-case>ndic<fixed-case>XNLI</fixed-case>: Evaluating Multilingual Inference for <fixed-case>I</fixed-case>ndian Languages</title>
      <author><first>Divyanshu</first><last>Aggarwal</last></author>
      <author><first>Vivek</first><last>Gupta</last></author>
      <author><first>Anoop</first><last>Kunchukuttan</last></author>
      <pages>10994-11006</pages>
      <abstract>While Indic NLP has made rapid advances recently in terms of the availability of corpora and pre-trained models, benchmark datasets on standard NLU tasks are limited. To this end, we introduce INDICXNLI, an NLI dataset for 11 Indic languages. It has been created by high-quality machine translation of the original English XNLI dataset and our analysis attests to the quality of INDICXNLI. By finetuning different pre-trained LMs on this INDICXNLI, we analyze various cross-lingual transfer techniques with respect to the impact of the choice of language models, languages, multi-linguality, mix-language input, etc. These experiments provide us with useful insights into the behaviour of pre-trained models for a diverse set of languages.</abstract>
      <url hash="2dc8c006">2022.emnlp-main.755</url>
      <bibkey>aggarwal-etal-2022-indicxnli</bibkey>
    </paper>
    <paper id="756">
      <title>Model Cascading: Towards Jointly Improving Efficiency and Accuracy of <fixed-case>NLP</fixed-case> Systems</title>
      <author><first>Neeraj</first><last>Varshney</last></author>
      <author><first>Chitta</first><last>Baral</last></author>
      <pages>11007-11021</pages>
      <abstract>Do all instances need inference through the big models for a correct prediction? Perhaps not; some instances are easy and can be answered correctly by even small capacity models. This provides opportunities for improving the computational efficiency of systems. In this work, we present an explorative study on ‘model cascading’, a simple technique that utilizes a collection of models of varying capacities to accurately yet efficiently output predictions. Through comprehensive experiments in multiple task settings that differ in the number of models available for cascading (K value), we show that cascading improves both the computational efficiency and the prediction accuracy. For instance, in K=3 setting, cascading saves up to 88.93% computation cost and consistently achieves superior prediction accuracy with an improvement of up to 2.18%. We also study the impact of introducing additional models in the cascade and show that it further increases the efficiency improvements. Finally, we hope that our work will facilitate development of efficient NLP systems making their widespread adoption in real-world applications possible.</abstract>
      <url hash="6b1e3884">2022.emnlp-main.756</url>
      <bibkey>varshney-baral-2022-model</bibkey>
    </paper>
    <paper id="757">
      <title>Semantic Simplification for Sentiment Classification</title>
      <author><first>Xiaotong</first><last>Jiang</last></author>
      <author><first>Zhongqing</first><last>Wang</last></author>
      <author><first>Guodong</first><last>Zhou</last></author>
      <pages>11022-11032</pages>
      <abstract>Recent work on document-level sentiment classification has shown that the sentiment in the original text is often hard to capture, since the sentiment is usually either expressed implicitly or shifted due to the occurrences of negation and rhetorical words. To this end, we enhance the original text with a sentiment-driven simplified clause to intensify its sentiment. The simplified clause shares the same opinion with the original text but expresses the opinion much more simply. Meanwhile, we employ Abstract Meaning Representation (AMR) for generating simplified clauses, since AMR explicitly provides core semantic knowledge, and potentially offers core concepts and explicit structures of original texts. Empirical studies show the effectiveness of our proposed model over several strong baselines. The results also indicate the importance of simplified clauses for sentiment classification.</abstract>
      <url hash="da4f719f">2022.emnlp-main.757</url>
      <bibkey>jiang-etal-2022-semantic</bibkey>
    </paper>
    <paper id="758">
      <title><fixed-case>XP</fixed-case>rompt: Exploring the Extreme of Prompt Tuning</title>
      <author><first>Fang</first><last>Ma</last></author>
      <author><first>Chen</first><last>Zhang</last></author>
      <author><first>Lei</first><last>Ren</last></author>
      <author><first>Jingang</first><last>Wang</last></author>
      <author><first>Qifan</first><last>Wang</last></author>
      <author><first>Wei</first><last>Wu</last></author>
      <author><first>Xiaojun</first><last>Quan</last></author>
      <author><first>Dawei</first><last>Song</last></author>
      <pages>11033-11047</pages>
      <abstract>Prompt tuning learns soft prompts to condition the frozen Pre-trained Language Models (PLMs) for performing downstream tasks in a parameter-efficient manner. While prompt tuning has gradually reached the performance level of fine-tuning as the model scale increases, there is still a large performance gap between prompt tuning and fine-tuning for models of moderate and small scales (typically less than 11B parameters). In this paper, we empirically show that the trained prompt tokens can have a negative impact on a downstream task and thus degrade its performance. To bridge the gap, we propose a novel Prompt tuning model with an eXtremely small scale (XPrompt) under the regime of lottery tickets hypothesis. Specifically, XPrompt eliminates the negative prompt tokens at different granularity levels through a hierarchical structured pruning, yielding a more parameter-efficient prompt yet with a competitive performance. Comprehensive experiments are carried out on the SuperGLUE tasks, and the results indicate that XPrompt is able to close the performance gap at smaller model scales.</abstract>
      <url hash="cad63123">2022.emnlp-main.758</url>
      <bibkey>ma-etal-2022-xprompt</bibkey>
    </paper>
    <paper id="759">
      <title>Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</title>
      <author><first>Sewon</first><last>Min</last></author>
      <author><first>Xinxi</first><last>Lyu</last></author>
      <author><first>Ari</first><last>Holtzman</last></author>
      <author><first>Mikel</first><last>Artetxe</last></author>
      <author><first>Mike</first><last>Lewis</last></author>
      <author><first>Hannaneh</first><last>Hajishirzi</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <pages>11048-11064</pages>
      <abstract>Large language models (LMs) are able to in-context learn—perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required—randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.</abstract>
      <url hash="01ff06b6">2022.emnlp-main.759</url>
      <bibkey>min-etal-2022-rethinking</bibkey>
    </paper>
    <paper id="760">
      <title>The Curious Case of Control</title>
      <author><first>Elias</first><last>Stengel-Eskin</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <pages>11065-11076</pages>
      <abstract>Children acquiring English make systematic errors on subject control sentences even after they have reached near-adult competence (Chomsky, 1969), possibly due to heuristics based on semantic roles (Maratsos, 1974).Given the advanced fluency of large generative language models, we ask whether model outputs are consistent with these heuristics, and to what degree different models are consistent with each other. We find that models can be categorized by behavior into three separate groups, with broad differences between the groups. The outputs of models in the largest group are consistent with positional heuristics that succeed on subject control but fail on object control. This result is surprising, given that object control is orders of magnitude more frequent in the text data used to train such models. We examine to what degree the models are sensitive to prompting with agent-patient information, finding that raising the salience of agent and patient relations results in significant changes in the outputs of most models. Based on this observation, we leverage an existing dataset of semantic proto-role annotations (White et al. 2020) to explore the connections between control and labeling event participants with properties typically associated with agents and patients.</abstract>
      <url hash="481f0473">2022.emnlp-main.760</url>
      <bibkey>stengel-eskin-van-durme-2022-curious</bibkey>
    </paper>
    <paper id="761">
      <title><fixed-case>SHARE</fixed-case>: a System for Hierarchical Assistive Recipe Editing</title>
      <author><first>Shuyang</first><last>Li</last></author>
      <author><first>Yufei</first><last>Li</last></author>
      <author><first>Jianmo</first><last>Ni</last></author>
      <author><first>Julian</first><last>McAuley</last></author>
      <pages>11077-11090</pages>
      <abstract>The large population of home cooks with dietary restrictions is under-served by existing cooking resources and recipe generation models. To help them, we propose the task of controllable recipe editing: adapt a base recipe to satisfy a user-specified dietary constraint. This task is challenging, and cannot be adequately solved with human-written ingredient substitution rules or existing end-to-end recipe generation models. We tackle this problem with SHARE: a System for Hierarchical Assistive Recipe Editing, which performs simultaneous ingredient substitution before generating natural-language steps using the edited ingredients. By decoupling ingredient and step editing, our step generator can explicitly integrate the available ingredients. Experiments on the novel RecipePairs dataset—83K pairs of similar recipes where each recipe satisfies one of seven dietary constraints—demonstrate that SHARE produces convincing, coherent recipes that are appropriate for a target dietary constraint. We further show through human evaluations and real-world cooking trials that recipes edited by SHARE can be easily followed by home cooks to create appealing dishes.</abstract>
      <url hash="64ef7ca4">2022.emnlp-main.761</url>
      <bibkey>li-etal-2022-share</bibkey>
    </paper>
    <paper id="762">
      <title><fixed-case>IM</fixed-case>^2: an Interpretable and Multi-category Integrated Metric Framework for Automatic Dialogue Evaluation</title>
      <author><first>Zhihua</first><last>Jiang</last></author>
      <author><first>Guanghui</first><last>Ye</last></author>
      <author><first>Dongning</first><last>Rao</last></author>
      <author><first>Di</first><last>Wang</last></author>
      <author><first>Xin</first><last>Miao</last></author>
      <pages>11091-11103</pages>
      <abstract>Evaluation metrics shine the light on the best models and thus strongly influence the research directions, such as the recently developed dialogue metrics USR, FED, and GRADE. However, most current metrics evaluate the dialogue data as isolated and static because they only focus on a single quality or several qualities. To mitigate the problem, this paper proposes an interpretable, multi-faceted, and controllable framework IM^2 (Interpretable and Multi-category Integrated Metric) to combine a large number of metrics which are good at measuring different qualities. The IM^2 framework first divides current popular dialogue qualities into different categories and then applies or proposes dialogue metrics to measure the qualities within each category and finally generates an overall IM^2 score. An initial version of IM^2 was submitted to the AAAI 2022 Track5.1@DSTC10 challenge and took the 2^nd place on both of the development and test leaderboard. After the competition, we develop more metrics and improve the performance of our model. We compare IM^2 with other 13 current dialogue metrics and experimental results show that IM^2 correlates more strongly with human judgments than any of them on each evaluated dataset.</abstract>
      <url hash="fb6f3db6">2022.emnlp-main.762</url>
      <bibkey>jiang-etal-2022-im</bibkey>
    </paper>
    <paper id="763">
      <title><fixed-case>PEVL</fixed-case>: Position-enhanced Pre-training and Prompt Tuning for Vision-language Models</title>
      <author><first>Yuan</first><last>Yao</last></author>
      <author><first>Qianyu</first><last>Chen</last></author>
      <author><first>Ao</first><last>Zhang</last></author>
      <author><first>Wei</first><last>Ji</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Tat-Seng</first><last>Chua</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>11104-11117</pages>
      <abstract>Vision-language pre-training (VLP) has shown impressive performance on a wide range of cross-modal tasks, where VLP models without reliance on object detectors are becoming the mainstream due to their superior computation efficiency and competitive performance. However, the removal of object detectors also deprives the capability of VLP models in explicit object modeling, which is essential to various position-sensitive vision-language (VL) tasks, such as referring expression comprehension and visual commonsense reasoning. To address the challenge, we introduce PEVL that enhances the pre-training and prompt tuning of VLP models with explicit object position modeling. Specifically, PEVL reformulates discretized object positions and language in a unified language modeling framework, which facilitates explicit VL alignment during pre-training, and also enables flexible prompt tuning for various downstream tasks. We show that PEVL enables state-of-the-art performance of detector-free VLP models on position-sensitive tasks such as referring expression comprehension and phrase grounding, and also improves the performance on position-insensitive tasks with grounded inputs. We make the data and code for this paper publicly available at https://github.com/thunlp/PEVL.</abstract>
      <url hash="11a0b0f9">2022.emnlp-main.763</url>
      <bibkey>yao-etal-2022-pevl</bibkey>
    </paper>
    <paper id="764">
      <title>Pre-training Language Models with Deterministic Factual Knowledge</title>
      <author><first>Shaobo</first><last>Li</last></author>
      <author><first>Xiaoguang</first><last>Li</last></author>
      <author><first>Lifeng</first><last>Shang</last></author>
      <author><first>Chengjie</first><last>Sun</last></author>
      <author><first>Bingquan</first><last>Liu</last></author>
      <author><first>Zhenzhou</first><last>Ji</last></author>
      <author><first>Xin</first><last>Jiang</last></author>
      <author><first>Qun</first><last>Liu</last></author>
      <pages>11118-11131</pages>
      <abstract>Previous works show that Pre-trained Language Models (PLMs) can capture factual knowledge. However, some analyses reveal that PLMs fail to perform it robustly, e.g., being sensitive to the changes of prompts when extracting factual knowledge. To mitigate this issue, we propose to let PLMs learn the deterministic relationship between the remaining context and the masked content. The deterministic relationship ensures that the masked factual content can be deterministically inferable based on the existing clues in the context. That would provide more stable patterns for PLMs to capture factual knowledge than randomly masking. Two pre-training tasks are further introduced to motivate PLMs to rely on the deterministic relationship when filling masks. Specifically, we use an external Knowledge Base (KB) to identify deterministic relationships and continuously pre-train PLMs with the proposed methods. The factual knowledge probing experiments indicate that the continuously pre-trained PLMs achieve better robustness in factual knowledge capturing. Further experiments on question-answering datasets show that trying to learn a deterministic relationship with the proposed methods can also help other knowledge-intensive tasks.</abstract>
      <url hash="ebcd2dfa">2022.emnlp-main.764</url>
      <bibkey>li-etal-2022-pre-training</bibkey>
    </paper>
    <paper id="765">
      <title>Finding Skill Neurons in Pre-trained Transformer-based Language Models</title>
      <author><first>Xiaozhi</first><last>Wang</last></author>
      <author><first>Kaiyue</first><last>Wen</last></author>
      <author><first>Zhengyan</first><last>Zhang</last></author>
      <author><first>Lei</first><last>Hou</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Juanzi</first><last>Li</last></author>
      <pages>11132-11152</pages>
      <abstract>Transformer-based pre-trained language models have demonstrated superior performance on various natural language processing tasks. However, it remains unclear how the skills required to handle these tasks distribute among model parameters. In this paper, we find that after prompt tuning for specific tasks, the activations of some neurons within pre-trained Transformers are highly predictive of the task labels. We dub these neurons skill neurons and confirm they encode task-specific skills by finding that: (1) Skill neurons are crucial for handling tasks. Performances of pre-trained Transformers on a task significantly drop when corresponding skill neurons are perturbed. (2) Skill neurons are task-specific. Similar tasks tend to have similar distributions of skill neurons. Furthermore, we demonstrate the skill neurons are most likely generated in pre-training rather than fine-tuning by showing that the skill neurons found with prompt tuning are also crucial for other fine-tuning methods freezing neuron weights, such as the adapter-based tuning and BitFit. We also explore the applications of skill neurons, including accelerating Transformers with network pruning and building better transferability indicators. These findings may promote further research on understanding Transformers. The source code can be obtained from https://github.com/THU-KEG/Skill-Neuron.</abstract>
      <url hash="25802695">2022.emnlp-main.765</url>
      <bibkey>wang-etal-2022-finding-skill</bibkey>
    </paper>
    <paper id="766">
      <title>Prompt Conditioned <fixed-case>VAE</fixed-case>: Enhancing Generative Replay for Lifelong Learning in Task-Oriented Dialogue</title>
      <author><first>Yingxiu</first><last>Zhao</last></author>
      <author><first>Yinhe</first><last>Zheng</last></author>
      <author><first>Zhiliang</first><last>Tian</last></author>
      <author><first>Chang</first><last>Gao</last></author>
      <author><first>Jian</first><last>Sun</last></author>
      <author><first>Nevin L.</first><last>Zhang</last></author>
      <pages>11153-11169</pages>
      <abstract>Lifelong learning (LL) is vital for advanced task-oriented dialogue (ToD) systems. To address the catastrophic forgetting issue of LL, generative replay methods are widely employed to consolidate past knowledge with generated pseudo samples. However, most existing generative replay methods use only a single task-specific token to control their models. This scheme is usually not strong enough to constrain the generative model due to insufficient information involved. In this paper, we propose a novel method, prompt conditioned VAE for lifelong learning (PCLL), to enhance generative replay by incorporating tasks’ statistics. PCLL captures task-specific distributions with a conditional variational autoencoder, conditioned on natural language prompts to guide the pseudo-sample generation. Moreover, it leverages a distillation process to further consolidate past knowledge by alleviating the noise in pseudo samples. Experiments on natural language understanding tasks of ToD systems demonstrate that PCLL significantly outperforms competitive baselines in building lifelong learning models.</abstract>
      <url hash="67bc64b9">2022.emnlp-main.766</url>
      <bibkey>zhao-etal-2022-prompt</bibkey>
    </paper>
    <paper id="767">
      <title><fixed-case>P</fixed-case>re<fixed-case>Q</fixed-case>u<fixed-case>EL</fixed-case>: Quality Estimation of Machine Translation Outputs in Advance</title>
      <author><first>Shachar</first><last>Don-Yehiya</last></author>
      <author><first>Leshem</first><last>Choshen</last></author>
      <author><first>Omri</first><last>Abend</last></author>
      <pages>11170-11183</pages>
      <abstract>We present the task of PreQuEL, Pre-(Quality-Estimation) Learning. A PreQuEL system predicts how well a given sentence will be translated, without recourse to the actual translation, thus eschewing unnecessary resource allocation when translation quality is bound to be low. PreQuEL can be defined relative to a given MT system (e.g., some industry service) or generally relative to the state-of-the-art.From a theoretical perspective, PreQuEL places the focus on the source text, tracing properties, possibly linguistic features, that make a sentence harder to machine translate.We develop a baseline model for the task and analyze its performance. We also develop a data augmentation method (from parallel corpora), that improves results substantially. We show that this augmentation method can improve the performance of the Quality-Estimation task as well.We investigate the properties of the input text that our model is sensitive to, by testing it on challenge sets and different languages. We conclude that it is aware of syntactic and semantic distinctions, and correlates and even over-emphasizes the importance of standard NLP features.</abstract>
      <url hash="7ce1f52b">2022.emnlp-main.767</url>
      <bibkey>don-yehiya-etal-2022-prequel</bibkey>
    </paper>
    <paper id="768">
      <title>Can Transformers Reason in Fragments of Natural Language?</title>
      <author><first>Viktor</first><last>Schlegel</last></author>
      <author><first>Kamen</first><last>Pavlov</last></author>
      <author><first>Ian</first><last>Pratt-Hartmann</last></author>
      <pages>11184-11199</pages>
      <abstract>State-of-the-art deep-learning-based approaches to Natural Language Processing (NLP) are credited with various capabilities that involve reasoning with natural language texts. %However, reasoning in this setting is often ill-defined and shallow. In this paper we carry out a large-scale empirical study investigating the detection of formally valid inferences in controlled fragments of natural language for which the satisfiability problem becomes increasingly complex. We find that, while transformer-based language models perform surprisingly well in these scenarios, a deeper analysis reveals that they appear to overfit to superficial patterns in the data rather than acquiring the logical principles governing the reasoning in these fragments.</abstract>
      <url hash="fd4a050f">2022.emnlp-main.768</url>
      <bibkey>schlegel-etal-2022-transformers</bibkey>
    </paper>
    <paper id="769">
      <title>Textless Speech Emotion Conversion using Discrete &amp; Decomposed Representations</title>
      <author><first>Felix</first><last>Kreuk</last></author>
      <author><first>Adam</first><last>Polyak</last></author>
      <author><first>Jade</first><last>Copet</last></author>
      <author><first>Eugene</first><last>Kharitonov</last></author>
      <author><first>Tu Anh</first><last>Nguyen</last></author>
      <author><first>Morgan</first><last>Rivière</last></author>
      <author><first>Wei-Ning</first><last>Hsu</last></author>
      <author><first>Abdelrahman</first><last>Mohamed</last></author>
      <author><first>Emmanuel</first><last>Dupoux</last></author>
      <author><first>Yossi</first><last>Adi</last></author>
      <pages>11200-11214</pages>
      <abstract>Speech emotion conversion is the task of modifying the perceived emotion of a speech utterance while preserving the lexical content and speaker identity. In this study, we cast the problem of emotion conversion as a spoken language translation task. We use a decomposition of the speech signal into discrete learned representations, consisting of phonetic-content units, prosodic features, speaker, and emotion. First, we modify the speech content by translating the phonetic-content units to a target emotion, and then predict the prosodic features based on these units. Finally, the speech waveform is generated by feeding the predicted representations into a neural vocoder. Such a paradigm allows us to go beyond spectral and parametric changes of the signal, and model non-verbal vocalizations, such as laughter insertion, yawning removal, etc. We demonstrate objectively and subjectively that the proposed method is vastly superior to current approaches and even beats text-based systems in terms of perceived emotion and audio quality. We rigorously evaluate all components of such a complex system and conclude with an extensive model analysis and ablation study to better emphasize the architectural choices, strengths and weaknesses of the proposed method. Samples are available under the following link: https://speechbot.github.io/emotion</abstract>
      <url hash="6d7f9bbd">2022.emnlp-main.769</url>
      <bibkey>kreuk-etal-2022-textless</bibkey>
    </paper>
    <paper id="770">
      <title>Textual Backdoor Attacks Can Be More Harmful via Two Simple Tricks</title>
      <author><first>Yangyi</first><last>Chen</last></author>
      <author><first>Fanchao</first><last>Qi</last></author>
      <author><first>Hongcheng</first><last>Gao</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>11215-11221</pages>
      <abstract>Backdoor attacks are a kind of emergent security threat in deep learning. After being injected with a backdoor, a deep neural model will behave normally on standard inputs but give adversary-specified predictions once the input contains specific backdoor triggers. In this paper, we find two simple tricks that can make existing textual backdoor attacks much more harmful. The first trick is to add an extra training task to distinguish poisoned and clean data during the training of the victim model, and the second one is to use all the clean training data rather than remove the original clean data corresponding to the poisoned data. These two tricks are universally applicable to different attack models. We conduct experiments in three tough situations including clean data fine-tuning, low-poisoning-rate, and label-consistent attacks. Experimental results show that the two tricks can significantly improve attack performance. This paper exhibits the great potential harmfulness of backdoor attacks. All the code and data can be obtained at <url>https://github.com/thunlp/StyleAttack</url>.</abstract>
      <url hash="2f209381">2022.emnlp-main.770</url>
      <bibkey>chen-etal-2022-textual</bibkey>
    </paper>
    <paper id="771">
      <title>Why Should Adversarial Perturbations be Imperceptible? Rethink the Research Paradigm in Adversarial <fixed-case>NLP</fixed-case></title>
      <author><first>Yangyi</first><last>Chen</last></author>
      <author><first>Hongcheng</first><last>Gao</last></author>
      <author><first>Ganqu</first><last>Cui</last></author>
      <author><first>Fanchao</first><last>Qi</last></author>
      <author><first>Longtao</first><last>Huang</last></author>
      <author><first>Zhiyuan</first><last>Liu</last></author>
      <author><first>Maosong</first><last>Sun</last></author>
      <pages>11222-11237</pages>
      <abstract>Textual adversarial samples play important roles in multiple subfields of NLP research, including security, evaluation, explainability, and data augmentation. However, most work mixes all these roles, obscuring the problem definitions and research goals of the security role that aims to reveal the practical concerns of NLP models. In this paper, we rethink the research paradigm of textual adversarial samples in security scenarios. We discuss the deficiencies in previous work and propose our suggestions that the research on the Security-oriented adversarial NLP (SoadNLP) should: (1) evaluate their methods on security tasks to demonstrate the real-world concerns; (2) consider real-world attackers’ goals, instead of developing impractical methods. To this end, we first collect, process, and release a security datasets collection Advbench. Then, we reformalize the task and adjust the emphasis on different goals in SoadNLP. Next, we propose a simple method based on heuristic rules that can easily fulfill the actual adversarial goals to simulate real-world attack methods. We conduct experiments on both the attack and the defense sides on Advbench. Experimental results show that our method has higher practical value, indicating that the research paradigm in SoadNLP may start from our new benchmark. All the code and data of Advbench can be obtained at <url>https://github.com/thunlp/Advbench</url>.</abstract>
      <url hash="d116681a">2022.emnlp-main.771</url>
      <bibkey>chen-etal-2022-adversarial</bibkey>
    </paper>
    <paper id="772">
      <title>Retrieval Augmented Visual Question Answering with Outside Knowledge</title>
      <author><first>Weizhe</first><last>Lin</last></author>
      <author id="bill-byrne-ucsd"><first>Bill</first><last>Byrne</last></author>
      <pages>11238-11254</pages>
      <abstract>Outside-Knowledge Visual Question Answering (OK-VQA) is a challenging VQA task that requires retrieval of external knowledge to answer questions about images. Recent OK-VQA systems use Dense Passage Retrieval (DPR) to retrieve documents from external knowledge bases, such as Wikipedia, but with DPR trained separately from answer generation, introducing a potential limit on the overall system performance.Instead, we propose a joint training scheme which includes differentiable DPR integrated with answer generation so that the system can be trained in an end-to-end fashion. Our experiments show that our scheme outperforms recent OK-VQA systems with strong DPR for retrieval. We also introduce new diagnostic metrics to analyze how retrieval and generation interact. The strong retrieval ability of our model significantly reduces the number of retrieved documents needed in training, yielding significant benefits in answer quality and computation required for training.</abstract>
      <url hash="72d86168">2022.emnlp-main.772</url>
      <bibkey>lin-byrne-2022-retrieval</bibkey>
    </paper>
    <paper id="773">
      <title>Instance Regularization for Discriminative Language Model Pre-training</title>
      <author><first>Zhuosheng</first><last>Zhang</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <author><first>Ming</first><last>Zhou</last></author>
      <pages>11255-11265</pages>
      <abstract>Discriminative pre-trained language models (PrLMs) can be generalized as denoising auto-encoders that work with two procedures, ennoising and denoising. First, an ennoising process corrupts texts with arbitrary noising functions to construct training instances. Then, a denoising language model is trained to restore the corrupted tokens. Existing studies have made progress by optimizing independent strategies of either ennoising or denosing. They treat training instances equally throughout the training process, with little attention on the individual contribution of those instances. To model explicit signals of instance contribution, this work proposes to estimate the complexity of restoring the original sentences from corrupted ones in language model pre-training. The estimations involve the corruption degree in the ennoising data construction process and the prediction confidence in the denoising counterpart. Experimental results on natural language understanding and reading comprehension benchmarks show that our approach improves pre-training efficiency, effectiveness, and robustness. Code is publicly available at https://github.com/cooelf/InstanceReg.</abstract>
      <url hash="c2997ae8">2022.emnlp-main.773</url>
      <bibkey>zhang-etal-2022-instance</bibkey>
    </paper>
    <paper id="774">
      <title><fixed-case>G</fixed-case>uo<fixed-case>F</fixed-case>eng: A Benchmark for Zero Pronoun Recovery and Translation</title>
      <author><first>Mingzhou</first><last>Xu</last></author>
      <author><first>Longyue</first><last>Wang</last></author>
      <author><first>Derek F.</first><last>Wong</last></author>
      <author><first>Hongye</first><last>Liu</last></author>
      <author><first>Linfeng</first><last>Song</last></author>
      <author><first>Lidia S.</first><last>Chao</last></author>
      <author><first>Shuming</first><last>Shi</last></author>
      <author><first>Zhaopeng</first><last>Tu</last></author>
      <pages>11266-11278</pages>
      <abstract>The phenomenon of zero pronoun (ZP) has attracted increasing interest in the machine translation (MT) community due to its importance and difficulty. However, previous studies generally evaluate the quality of translating ZPs with BLEU scores on MT testsets, which is not expressive or sensitive enough for accurate assessment. To bridge the data and evaluation gaps, we propose a benchmark testset for target evaluation on Chinese-English ZP translation. The human-annotated testset covers five challenging genres, which reveal different characteristics of ZPs for comprehensive evaluation. We systematically revisit eight advanced models on ZP translation and identify current challenges for future exploration. We release data, code, models and annotation guidelines, which we hope can significantly promote research in this field (https://github.com/longyuewangdcu/mZPRT).</abstract>
      <url hash="29337b73">2022.emnlp-main.774</url>
      <bibkey>xu-etal-2022-guofeng</bibkey>
    </paper>
    <paper id="775">
      <title><fixed-case>S</fixed-case>cience<fixed-case>W</fixed-case>orld: Is your Agent Smarter than a 5th Grader?</title>
      <author><first>Ruoyao</first><last>Wang</last></author>
      <author><first>Peter</first><last>Jansen</last></author>
      <author><first>Marc-Alexandre</first><last>Côté</last></author>
      <author><first>Prithviraj</first><last>Ammanabrolu</last></author>
      <pages>11279-11298</pages>
      <abstract>We present ScienceWorld, a benchmark to test agents’ scientific reasoning abilities in a new interactive text environment at the level of a standard elementary school science curriculum. Despite the transformer-based progress seen in question-answering and scientific text processing, we find that current models cannot reason about or explain learned science concepts in novel contexts. For instance, models can easily answer what the conductivity of a known material is but struggle when asked how they would conduct an experiment in a grounded environment to find the conductivity of an unknown material. This begs the question of whether current models are simply retrieving answers by way of seeing a large number of similar examples or if they have learned to reason about concepts in a reusable manner. We hypothesize that agents need to be grounded in interactive environments to achieve such reasoning capabilities. Our experiments provide empirical evidence supporting this hypothesis – showing that a 1.5 million parameter agent trained interactively for 100k steps outperforms a 11 billion parameter model statically trained for scientific question-answering and reasoning from millions of expert demonstrations.</abstract>
      <url hash="a8975fbf">2022.emnlp-main.775</url>
      <bibkey>wang-etal-2022-scienceworld</bibkey>
    </paper>
    <paper id="776">
      <title>Improving Embeddings Representations for Comparing Higher Education Curricula: A Use Case in Computing</title>
      <author><first>Jeffri</first><last>Murrugarra-Llerena</last></author>
      <author><first>Fernando</first><last>Alva-Manchego</last></author>
      <author><first>Nils</first><last>Murrugarra-LLerena</last></author>
      <pages>11299-11307</pages>
      <abstract>We propose an approach for comparing curricula of study programs in higher education. Pre-trained word embeddings are fine-tuned in a study program classification task, where each curriculum is represented by the names and content of its courses. By combining metric learning with a novel course-guided attention mechanism, our method obtains more accurate curriculum representations than strong baselines. Experiments on a new dataset with curricula of computing programs demonstrate the intuitive power of our approach via attention weights, topic modeling, and embeddings visualizations. We also present a use case comparing computing curricula from USA and Latin America to showcase the capabilities of our improved embeddings representations.</abstract>
      <url hash="2fea8ce8">2022.emnlp-main.776</url>
      <bibkey>murrugarra-llerena-etal-2022-improving</bibkey>
    </paper>
    <paper id="777">
      <title>Mitigating Spurious Correlation in Natural Language Understanding with Counterfactual Inference</title>
      <author><first>Can</first><last>Udomcharoenchaikit</last></author>
      <author><first>Wuttikorn</first><last>Ponwitayarat</last></author>
      <author><first>Patomporn</first><last>Payoungkhamdee</last></author>
      <author><first>Kanruethai</first><last>Masuk</last></author>
      <author><first>Weerayut</first><last>Buaphet</last></author>
      <author><first>Ekapol</first><last>Chuangsuwanich</last></author>
      <author><first>Sarana</first><last>Nutanong</last></author>
      <pages>11308-11321</pages>
      <abstract>Despite their promising results on standard benchmarks, NLU models are still prone to make predictions based on shortcuts caused by unintended bias in the dataset. For example, an NLI model may use lexical overlap as a shortcut to make entailment predictions due to repetitive data generation patterns from annotators, also called annotation artifacts. In this paper, we propose a causal analysis framework to help debias NLU models. We show that (1) by defining causal relationships, we can introspect how much annotation artifacts affect the outcomes. (2) We can utilize counterfactual inference to mitigate bias with this knowledge. We found that viewing a model as a treatment can mitigate bias more effectively than viewing annotation artifacts as treatment. (3) In addition to bias mitigation, we can interpret how much each debiasing strategy is affected by annotation artifacts. Our experimental results show that using counterfactual inference can improve out-of-distribution performance in all settings while maintaining high in-distribution performance.</abstract>
      <url hash="519d2804">2022.emnlp-main.777</url>
      <bibkey>udomcharoenchaikit-etal-2022-mitigating</bibkey>
    </paper>
    <paper id="778">
      <title>End-to-End Neural Discourse Deixis Resolution in Dialogue</title>
      <author><first>Shengjie</first><last>Li</last></author>
      <author><first>Vincent</first><last>Ng</last></author>
      <pages>11322-11334</pages>
      <abstract>We adapt Lee et al.’s (2018) span-based entity coreference model to the task of end-to-end discourse deixis resolution in dialogue, specifically by proposing extensions to their model that exploit task-specific characteristics. The resulting model, dd-utt, achieves state-of-the-art results on the four datasets in the CODI-CRAC 2021 shared task.</abstract>
      <url hash="bb40e0be">2022.emnlp-main.778</url>
      <bibkey>li-ng-2022-end</bibkey>
    </paper>
    <paper id="779">
      <title>Balancing out Bias: Achieving Fairness Through Balanced Training</title>
      <author><first>Xudong</first><last>Han</last></author>
      <author><first>Timothy</first><last>Baldwin</last></author>
      <author><first>Trevor</first><last>Cohn</last></author>
      <pages>11335-11350</pages>
      <abstract>Group bias in natural language processing tasks manifests as disparities in system error rates across texts authorized by different demographic groups, typically disadvantaging minority groups. Dataset balancing has been shown to be effective at mitigating bias, however existing approaches do not directly account for correlations between author demographics and linguistic variables, limiting their effectiveness. To achieve Equal Opportunity fairness, such as equal job opportunity without regard to demographics, this paper introduces a simple, but highly effective, objective for countering bias using balanced training.We extend the method in the form of a gated model, which incorporates protected attributes as input, and show that it is effective at reducing bias in predictions through demographic input perturbation, outperforming all other bias mitigation techniques when combined with balanced training.</abstract>
      <url hash="b6b5a048">2022.emnlp-main.779</url>
      <bibkey>han-etal-2022-balancing</bibkey>
    </paper>
    <paper id="780">
      <title>Prompting <fixed-case>ELECTRA</fixed-case>: Few-Shot Learning with Discriminative Pre-Trained Models</title>
      <author><first>Mengzhou</first><last>Xia</last></author>
      <author><first>Mikel</first><last>Artetxe</last></author>
      <author><first>Jingfei</first><last>Du</last></author>
      <author><first>Danqi</first><last>Chen</last></author>
      <author><first>Veselin</first><last>Stoyanov</last></author>
      <pages>11351-11361</pages>
      <abstract>Pre-trained masked language models successfully perform few-shot learning by formulating downstream tasks as text infilling. How- ever, as a strong alternative in full-shot settings, discriminative pre-trained models like ELECTRA do not fit into the paradigm. In this work, we adapt prompt-based few-shot learning to ELECTRA and show that it outperforms masked language models in a wide range of tasks. ELECTRA is pre-trained to distinguish if a token is generated or original. We naturally extend that to prompt-based few-shot learning by training to score the originality of the target options without introducing new parameters. Our method can be easily adapted to tasks involving multi-token predictions without extra computation overhead. Analysis shows that ELECTRA learns distributions that align better with downstream tasks.</abstract>
      <url hash="a7f9542f">2022.emnlp-main.780</url>
      <bibkey>xia-etal-2022-prompting</bibkey>
    </paper>
    <paper id="781">
      <title>Identifying Physical Object Use in Sentences</title>
      <author><first>Tianyu</first><last>Jiang</last></author>
      <author><first>Ellen</first><last>Riloff</last></author>
      <pages>11362-11372</pages>
      <abstract>Commonsense knowledge about the typicalfunctions of physical objects allows people tomake inferences during sentence understanding.For example, we infer that “Sam enjoyedthe book” means that Sam enjoyed reading thebook, even though the action is implicit. Priorresearch has focused on learning the prototypicalfunctions of physical objects in order toenable inferences about implicit actions. Butmany sentences refer to objects even when theyare not used (e.g., “The book fell”). We arguethat NLP systems need to recognize whether anobject is being used before inferring how theobject is used. We define a new task called ObjectUse Classification that determines whethera physical object mentioned in a sentence wasused or likely will be used. We introduce a newdataset for this task and present a classificationmodel that exploits data augmentation methodsand FrameNet when fine-tuning a pre-trainedlanguage model. We also show that object useclassification combined with knowledge aboutthe prototypical functions of objects has thepotential to yield very good inferences aboutimplicit and anticipated actions.</abstract>
      <url hash="59918d1a">2022.emnlp-main.781</url>
      <bibkey>jiang-riloff-2022-identifying</bibkey>
    </paper>
    <paper id="782">
      <title><fixed-case>CD</fixed-case>ialog: A Multi-turn Covid-19 Conversation Dataset for Entity-Aware Dialog Generation</title>
      <author><first>Deeksha</first><last>Varshney</last></author>
      <author><first>Aizan</first><last>Zafar</last></author>
      <author><first>Niranshu</first><last>Behera</last></author>
      <author><first>Asif</first><last>Ekbal</last></author>
      <pages>11373-11385</pages>
      <abstract/>
      <url hash="d65d42a6">2022.emnlp-main.782</url>
      <bibkey>varshney-etal-2022-cdialog</bibkey>
    </paper>
    <paper id="783">
      <title>Robustifying Sentiment Classification by Maximally Exploiting Few Counterfactuals</title>
      <author><first>Maarten</first><last>De Raedt</last></author>
      <author><first>Fréderic</first><last>Godin</last></author>
      <author><first>Chris</first><last>Develder</last></author>
      <author><first>Thomas</first><last>Demeester</last></author>
      <pages>11386-11400</pages>
      <abstract>For text classification tasks, finetuned language models perform remarkably well. Yet, they tend to rely on spurious patterns in training data, thus limiting their performance on out-of-distribution (OOD) test data. Among recent models aiming to avoid this spurious pattern problem, adding extra counterfactual samples to the training data has proven to be very effective. Yet, counterfactual data generation is costly since it relies on human annotation. Thus, we propose a novel solution that only requires annotation of a small fraction (e.g., 1%) of the original training data, and uses automatic generation of extra counterfactuals in an encoding vector space. We demonstrate the effectiveness of our approach in sentiment classification, using IMDb data for training and other sets for OOD tests (i.e., Amazon, SemEval and Yelp). We achieve noticeable accuracy improvements by adding only 1% manual counterfactuals: +3% compared to adding +100% in-distribution training samples, +1.3% compared to alternate counterfactual approaches.</abstract>
      <url hash="beddaae0">2022.emnlp-main.783</url>
      <bibkey>de-raedt-etal-2022-robustifying</bibkey>
    </paper>
    <paper id="784">
      <title>Data-Efficient Playlist Captioning With Musical and Linguistic Knowledge</title>
      <author><first>Giovanni</first><last>Gabbolini</last></author>
      <author><first>Romain</first><last>Hennequin</last></author>
      <author><first>Elena</first><last>Epure</last></author>
      <pages>11401-11415</pages>
      <abstract>Music streaming services feature billions of playlists created by users, professional editors or algorithms. In this content overload scenario, it is crucial to characterise playlists, so that music can be effectively organised and accessed. Playlist titles and descriptions are proposed in natural language either manually by music editors and users or automatically from pre-defined templates. However, the former is time-consuming while the latter is limited by the vocabulary and covered music themes. In this work, we propose PlayNTell, a data-efficient multi-modal encoder-decoder model for automatic playlist captioning. Compared to existing music captioning algorithms, PlayNTell leverages also linguistic and musical knowledge to generate correct and thematic captions. We benchmark PlayNTell on a new editorial playlists dataset collected from two major music streaming services.PlayNTell yields 2x-3x higher BLEU@4 and CIDEr than state of the art captioning algorithms.</abstract>
      <url hash="9843bc09">2022.emnlp-main.784</url>
      <bibkey>gabbolini-etal-2022-data</bibkey>
    </paper>
    <paper id="785">
      <title>Improved grammatical error correction by ranking elementary edits</title>
      <author><first>Alexey</first><last>Sorokin</last></author>
      <pages>11416-11429</pages>
      <abstract>We offer a two-stage reranking method for grammatical error correction: the first model serves as edit generator, while the second classifies the proposed edits as correct or false. We show how to use both encoder-decoder and sequence labeling models for the first step of our pipeline. We achieve state-of-the-art quality on BEA 2019 English dataset even using weak BERT-GEC edit generator. Combining our roberta-base scorer with state-of-the-art GECToR edit generator, we surpass GECToR by 2-3%. With a larger model we establish a new SOTA on BEA development and test sets. Our model also sets a new SOTA on Russian, despite using smaller models and less data than the previous approaches.</abstract>
      <url hash="ab9065cb">2022.emnlp-main.785</url>
      <bibkey>sorokin-2022-improved</bibkey>
    </paper>
    <paper id="786">
      <title>Improving Tokenisation by Alternative Treatment of Spaces</title>
      <author><first>Edward</first><last>Gow-Smith</last></author>
      <author><first>Harish</first><last>Tayyar Madabushi</last></author>
      <author><first>Carolina</first><last>Scarton</last></author>
      <author><first>Aline</first><last>Villavicencio</last></author>
      <pages>11430-11443</pages>
      <abstract/>
      <url hash="4f9a846b">2022.emnlp-main.786</url>
      <bibkey>gow-smith-etal-2022-improving</bibkey>
    </paper>
    <paper id="787">
      <title><fixed-case>GENIE</fixed-case>: Toward Reproducible and Standardized Human Evaluation for Text Generation</title>
      <author><first>Daniel</first><last>Khashabi</last></author>
      <author><first>Gabriel</first><last>Stanovsky</last></author>
      <author><first>Jonathan</first><last>Bragg</last></author>
      <author><first>Nicholas</first><last>Lourie</last></author>
      <author><first>Jungo</first><last>Kasai</last></author>
      <author><first>Yejin</first><last>Choi</last></author>
      <author><first>Noah A.</first><last>Smith</last></author>
      <author><first>Daniel</first><last>Weld</last></author>
      <pages>11444-11458</pages>
      <abstract>While often assumed a gold standard, effective human evaluation of text generation remains an important, open area for research.We revisit this problem with a focus on producing consistent evaluations that are reproducible—over time and across different populations. We study this goal in different stages of the human evaluation pipeline. In particular, we consider design choices for the annotation interface used to elicit human judgments and their impact on reproducibility. Furthermore, we develop an automated mechanism for maintaining annotator quality via a probabilistic model that detects and excludes noisy annotators. Putting these lessons together, we introduce GENIE: a system for running standardized human evaluations across different generation tasks.We instantiate GENIE with datasets representing four core challenges in text generation: machine translation, summarization, commonsense reasoning, and machine comprehension.For each task, GENIE offers a leaderboard that automatically crowdsources annotations for submissions, evaluating them along axes such as correctness, conciseness, and fluency.We have made the GENIE leaderboards publicly available, and have already ranked 50 submissions from 10 different research groups. We hope GENIE encourages further progress toward effective, standardized evaluations for text generation.</abstract>
      <url hash="a1335298">2022.emnlp-main.787</url>
      <bibkey>khashabi-etal-2022-genie</bibkey>
    </paper>
    <paper id="788">
      <title>Attentional Probe: Estimating a Module’s Functional Potential</title>
      <author><first>Tiago</first><last>Pimentel</last></author>
      <author><first>Josef</first><last>Valvoda</last></author>
      <author><first>Niklas</first><last>Stoehr</last></author>
      <author><first>Ryan</first><last>Cotterell</last></author>
      <pages>11459-11472</pages>
      <abstract/>
      <url hash="fbec6a23">2022.emnlp-main.788</url>
      <bibkey>pimentel-etal-2022-attentional</bibkey>
    </paper>
    <paper id="789">
      <title>When More Data Hurts: A Troubling Quirk in Developing Broad-Coverage Natural Language Understanding Systems</title>
      <author><first>Elias</first><last>Stengel-Eskin</last></author>
      <author><first>Emmanouil Antonios</first><last>Platanios</last></author>
      <author><first>Adam</first><last>Pauls</last></author>
      <author><first>Sam</first><last>Thomson</last></author>
      <author><first>Hao</first><last>Fang</last></author>
      <author><first>Benjamin</first><last>Van Durme</last></author>
      <author><first>Jason</first><last>Eisner</last></author>
      <author><first>Yu</first><last>Su</last></author>
      <pages>11473-11487</pages>
      <abstract>In natural language understanding (NLU) production systems, users’ evolving needs necessitate the addition of new features over time, indexed by new symbols added to the meaning representation space. This requires additional training data and results in ever-growing datasets. We present the first systematic investigation into this incremental symbol learning scenario. Our analysis reveals a troubling quirk in building broad-coverage NLU systems: as the training dataset grows, performance on a small set of new symbols often decreases. We show that this trend holds for multiple mainstream models on two common NLU tasks: intent recognition and semantic parsing. Rejecting class imbalance as the sole culprit, we reveal that the trend is closely associated with an effect we call source signal dilution, where strong lexical cues for the new symbol become diluted as the training dataset grows. Selectively dropping training examples to prevent dilution often reverses the trend, showing the over-reliance of mainstream neural NLU models on simple lexical cues.</abstract>
      <url hash="432a3573">2022.emnlp-main.789</url>
      <bibkey>stengel-eskin-etal-2022-data</bibkey>
    </paper>
    <paper id="790">
      <title>Zero-shot Cross-lingual Transfer of Prompt-based Tuning with a Unified Multilingual Prompt</title>
      <author><first>Lianzhe</first><last>Huang</last></author>
      <author><first>Shuming</first><last>Ma</last></author>
      <author><first>Dongdong</first><last>Zhang</last></author>
      <author><first>Furu</first><last>Wei</last></author>
      <author><first>Houfeng</first><last>Wang</last></author>
      <pages>11488-11497</pages>
      <abstract>Prompt-based tuning has been proven effective for pretrained language models (PLMs). While most of the existing work focuses on the monolingual prompts, we study the multilingual prompts for multilingual PLMs, especially in the zero-shot cross-lingual setting. To alleviate the effort of designing different prompts for multiple languages, we propose a novel model that uses a unified prompt for all languages, called UniPrompt. Different from the discrete prompts and soft prompts, the unified prompt is model-based and language-agnostic. Specifically, the unified prompt is initialized by a multilingual PLM to produce language-independent representation, after which is fused with the text input. During inference, the prompts can be pre-computed so that no extra computation cost is needed. To collocate with the unified prompt, we propose a new initialization method for the target label word to further improve the model’s transferability across languages. Extensive experiments show that our proposed methods can significantly outperform the strong baselines across different languages. We release data and code to facilitate future research.</abstract>
      <url hash="ecb4d7c4">2022.emnlp-main.790</url>
      <bibkey>huang-etal-2022-zero</bibkey>
    </paper>
    <paper id="791">
      <title>Three Real-World Datasets and Neural Computational Models for Classification Tasks in Patent Landscaping</title>
      <author><first>Subhash</first><last>Pujari</last></author>
      <author><first>Jannik</first><last>Strötgen</last></author>
      <author><first>Mark</first><last>Giereth</last></author>
      <author><first>Michael</first><last>Gertz</last></author>
      <author><first>Annemarie</first><last>Friedrich</last></author>
      <pages>11498-11513</pages>
      <abstract>Patent Landscaping, one of the central tasks of intellectual property management, includes selecting and grouping patents according to user-defined technical or application-oriented criteria. While recent transformer-based models have been shown to be effective for classifying patents into taxonomies such as CPC or IPC, there is yet little research on how to support real-world Patent Landscape Studies (PLSs) using natural language processing methods. With this paper, we release three labeled datasets for PLS-oriented classification tasks covering two diverse domains. We provide a qualitative analysis and report detailed corpus statistics.Most research on neural models for patents has been restricted to leveraging titles and abstracts. We compare strong neural and non-neural baselines, proposing a novel model that takes into account textual information from the patents’ full texts as well as embeddings created based on the patents’ CPC labels. We find that for PLS-oriented classification tasks, going beyond title and abstract is crucial, CPC labels are an effective source of information, and combining all features yields the best results.</abstract>
      <url hash="83a06885">2022.emnlp-main.791</url>
      <bibkey>pujari-etal-2022-three</bibkey>
    </paper>
    <paper id="792">
      <title>Topic Modeling With Topological Data Analysis</title>
      <author><first>Ciarán</first><last>Byrne</last></author>
      <author><first>Danijela</first><last>Horak</last></author>
      <author><first>Karo</first><last>Moilanen</last></author>
      <author><first>Amandla</first><last>Mabona</last></author>
      <pages>11514-11533</pages>
      <abstract>Recent unsupervised topic modelling ap-proaches that use clustering techniques onword, token or document embeddings can ex-tract coherent topics. A common limitationof such approaches is that they reveal noth-ing about inter-topic relationships which areessential in many real-world application do-mains. We present an unsupervised topic mod-elling method which harnesses TopologicalData Analysis (TDA) to extract a topologicalskeleton of the manifold upon which contextu-alised word embeddings lie. We demonstratethat our approach, which performs on par witha recent baseline, is able to construct a networkof coherent topics together with meaningfulrelationships between them.</abstract>
      <url hash="899ac29e">2022.emnlp-main.792</url>
      <bibkey>byrne-etal-2022-topic</bibkey>
    </paper>
    <paper id="793">
      <title>Predicting Fine-Tuning Performance with Probing</title>
      <author><first>Zining</first><last>Zhu</last></author>
      <author><first>Soroosh</first><last>Shahtalebi</last></author>
      <author><first>Frank</first><last>Rudzicz</last></author>
      <pages>11534-11547</pages>
      <abstract>Large NLP models have recently shown impressive performance in language understanding tasks, typically evaluated by their fine-tuned performance. Alternatively, probing has received increasing attention as being a lightweight method for interpreting the intrinsic mechanisms of large NLP models. In probing, post-hoc classifiers are trained on “out-of-domain” datasets that diagnose specific abilities. While probing the language models has led to insightful findings, they appear disjointed from the development of models. This paper explores the utility of probing deep NLP models to extract a proxy signal widely used in model development – the fine-tuning performance. We find that it is possible to use the accuracies of only three probing tests to predict the fine-tuning performance with errors 40% - 80% smaller than baselines. We further discuss possible avenues where probing can empower the development of deep NLP models.</abstract>
      <url hash="92e4b68e">2022.emnlp-main.793</url>
      <bibkey>zhu-etal-2022-predicting</bibkey>
    </paper>
    <paper id="794">
      <title>Diverse Parallel Data Synthesis for Cross-Database Adaptation of Text-to-<fixed-case>SQL</fixed-case> Parsers</title>
      <author><first>Abhijeet</first><last>Awasthi</last></author>
      <author><first>Ashutosh</first><last>Sathe</last></author>
      <author><first>Sunita</first><last>Sarawagi</last></author>
      <pages>11548-11562</pages>
      <abstract>Text-to-SQL parsers typically struggle with databases unseen during the train time. Adapting Text-to-SQL parsers to new database schemas is a challenging problem owing to a vast diversity of schemas and zero availability of natural language queries in new schemas. We present ReFill, a framework for synthesizing high-quality and textually diverse parallel datasets for adapting Text-to-SQL parsers. Unlike prior methods that utilize SQL-to-Text generation, ReFill learns to retrieve-and-edit text queries in existing schemas and transfer them to the new schema. ReFill utilizes a simple method for retrieving diverse existing text, masking their schema-specific tokens, and refilling with tokens relevant to the new schema. We show that this process leads to significantly more diverse text queries than achievable by standard SQL-to-Text generation models. Through experiments on several databases, we show that adapting a parser by finetuning it on datasets synthesized by ReFill consistently outperforms prior data-augmentation methods.</abstract>
      <url hash="c743940d">2022.emnlp-main.794</url>
      <bibkey>awasthi-etal-2022-diverse</bibkey>
    </paper>
    <paper id="795">
      <title>Agent-Specific Deontic Modality Detection in Legal Language</title>
      <author><first>Abhilasha</first><last>Sancheti</last></author>
      <author><first>Aparna</first><last>Garimella</last></author>
      <author><first>Balaji Vasan</first><last>Srinivasan</last></author>
      <author><first>Rachel</first><last>Rudinger</last></author>
      <pages>11563-11579</pages>
      <abstract>Legal documents are typically long and written in legalese, which makes it particularly difficult for laypeople to understand their rights and duties. While natural language understanding technologies can be valuable in supporting such understanding in the legal domain, the limited availability of datasets annotated for deontic modalities in the legal domain, due to the cost of hiring experts and privacy issues, is a bottleneck. To this end, we introduce, LEXDEMOD, a corpus of English contracts annotatedwith deontic modality expressed with respect to a contracting party or agent along with the modal triggers. We benchmark this dataset on two tasks: (i) agent-specific multi-label deontic modality classification, and (ii) agent-specific deontic modality and trigger span detection using Transformer-based (Vaswani et al., 2017) language models. Transfer learning experiments show that the linguistic diversity of modal expressions in LEXDEMOD generalizes reasonably from lease to employment andrental agreements. A small case study indicates that a model trained on LEXDEMOD can detect red flags with high recall. We believe our work offers a new research direction for deontic modality detection in the legal domain.</abstract>
      <url hash="13600346">2022.emnlp-main.795</url>
      <bibkey>sancheti-etal-2022-agent</bibkey>
    </paper>
    <paper id="796">
      <title><fixed-case>COLD</fixed-case>: A Benchmark for <fixed-case>C</fixed-case>hinese Offensive Language Detection</title>
      <author><first>Jiawen</first><last>Deng</last></author>
      <author><first>Jingyan</first><last>Zhou</last></author>
      <author><first>Hao</first><last>Sun</last></author>
      <author><first>Chujie</first><last>Zheng</last></author>
      <author><first>Fei</first><last>Mi</last></author>
      <author><first>Helen</first><last>Meng</last></author>
      <author><first>Minlie</first><last>Huang</last></author>
      <pages>11580-11599</pages>
      <abstract>Offensive language detection is increasingly crucial for maintaining a civilized social media platform and deploying pre-trained language models. However, this task in Chinese is still under exploration due to the scarcity of reliable datasets. To this end, we propose a benchmark –COLD for Chinese offensive language analysis, including a Chinese Offensive Language Dataset –COLDATASET and a baseline detector –COLDETECTOR which is trained on the dataset. We show that the COLD benchmark contributes to Chinese offensive language detection which is challenging for existing resources. We then deploy the COLDETECTOR and conduct detailed analyses on popular Chinese pre-trained language models. We first analyze the offensiveness of existing generative models and show that these models inevitably expose varying degrees of offensive issues. Furthermore, we investigate the factors that influence the offensive generations, and we find that anti-bias contents and keywords referring to certain groups or revealing negative attitudes trigger offensive outputs easier.</abstract>
      <url hash="4d05e42a">2022.emnlp-main.796</url>
      <bibkey>deng-etal-2022-cold</bibkey>
    </paper>
    <paper id="797">
      <title>Fixing Model Bugs with Natural Language Patches</title>
      <author><first>Shikhar</first><last>Murty</last></author>
      <author><first>Christopher</first><last>Manning</last></author>
      <author><first>Scott</first><last>Lundberg</last></author>
      <author><first>Marco Tulio</first><last>Ribeiro</last></author>
      <pages>11600-11613</pages>
      <abstract>Current approaches for fixing systematic problems in NLP models (e.g., regex patches, finetuning on more data) are either brittle, or labor-intensive and liable to shortcuts. In contrast, humans often provide corrections to each other through natural language. Taking inspiration from this, we explore natural language patches—declarative statements that allow developers to provide corrective feedback at the right level of abstraction, either overriding the model (“if a review gives 2 stars, the sentiment is negative”) or providing additional information the model may lack (“if something is described as the bomb, then it is good”). We model the task of determining if a patch applies separately from the task of integrating patch information, and show that with a small amount of synthetic data, we can teach models to effectively use real patches on real data—1 to 7 patches improve accuracy by ~1–4 accuracy points on different slices of a sentiment analysis dataset, and F1 by 7 points on a relation extraction dataset. Finally, we show that finetuning on as many as 100 labeled examples may be needed to match the performance of a small set of language patches.</abstract>
      <url hash="4818e907">2022.emnlp-main.797</url>
      <bibkey>murty-etal-2022-fixing</bibkey>
    </paper>
    <paper id="798">
      <title><fixed-case>W</fixed-case>e<fixed-case>D</fixed-case>ef: Weakly Supervised Backdoor Defense for Text Classification</title>
      <author><first>Lesheng</first><last>Jin</last></author>
      <author><first>Zihan</first><last>Wang</last></author>
      <author><first>Jingbo</first><last>Shang</last></author>
      <pages>11614-11626</pages>
      <abstract>Existing backdoor defense methods are only effective for limited trigger types. To defend different trigger types at once, we start from the class-irrelevant nature of the poisoning process and propose a novel weakly supervised backdoor defense framework WeDef. Recent advances in weak supervision make it possible to train a reasonably accurate text classifier using only a small number of user-provided, class-indicative seed words. Such seed words shall be considered independent of the triggers. Therefore, a weakly supervised text classifier trained by only the poisoned documents without their labels will likely have no backdoor. Inspired by this observation, in WeDef, we define the reliability of samples based on whether the predictions of the weak classifier agree with their labels in the poisoned training set. We further improve the results through a two-phase sanitization: (1) iteratively refine the weak classifier based on the reliable samples and (2) train a binary poison classifier by distinguishing the most unreliable samples from the most reliable samples. Finally, we train the sanitized model on the samples that the poison classifier predicts as benign. Extensive experiments show that WeDef is effective against popular trigger-based attacks (e.g., words, sentences, and paraphrases), outperforming existing defense methods.</abstract>
      <url hash="d5b35786">2022.emnlp-main.798</url>
      <bibkey>jin-etal-2022-wedef</bibkey>
    </paper>
    <paper id="799">
      <title>Interventional Training for Out-Of-Distribution Natural Language Understanding</title>
      <author><first>Sicheng</first><last>Yu</last></author>
      <author><first>Jing</first><last>Jiang</last></author>
      <author><first>Hao</first><last>Zhang</last></author>
      <author><first>Yulei</first><last>Niu</last></author>
      <author><first>Qianru</first><last>Sun</last></author>
      <author><first>Lidong</first><last>Bing</last></author>
      <pages>11627-11638</pages>
      <abstract>Out-of-distribution (OOD) settings are used to measure a model’s performance when the distribution of the test data is different from that of the training data. NLU models are known to suffer in OOD. We study this issue from the perspective of causality, which sees confounding bias as the reason for models to learn spurious correlations. While a common solution is to perform intervention, existing methods handle only known and single confounder, but in many NLU tasks the confounders can be both unknown and multifactorial. In this paper, we propose a novel interventional training method called Bottom-up Automatic Intervention (BAI) that performs multi-granular intervention with identified multifactorial confounders. Our experiments on three NLU tasks, namely, natural language inference, fact verification and paraphrase identification, show the effectiveness of BAI for tackling OOD settings.</abstract>
      <url hash="47500038">2022.emnlp-main.799</url>
      <bibkey>yu-etal-2022-interventional</bibkey>
    </paper>
    <paper id="800">
      <title>Pseudo-Relevance for Enhancing Document Representation</title>
      <author><first>Jihyuk</first><last>Kim</last></author>
      <author><first>Seung-won</first><last>Hwang</last></author>
      <author><first>Seoho</first><last>Song</last></author>
      <author><first>Hyeseon</first><last>Ko</last></author>
      <author><first>Young-In</first><last>Song</last></author>
      <pages>11639-11652</pages>
      <abstract>This paper studies how to enhance the document representation for the bi-encoder approach in dense document retrieval. The bi-encoder, separately encoding a query and a document as a single vector, is favored for high efficiency in large-scale information retrieval, compared to more effective but complex architectures. To combine the strength of the two, the multi-vector representation of documents for bi-encoder, such as ColBERT preserving all token embeddings, has been widely adopted. Our contribution is to reduce the size of the multi-vector representation, without compromising the effectiveness, supervised by query logs. Our proposed solution decreases the latency and the memory footprint, up to 8- and 3-fold, validated on MSMARCO and real-world search query logs.</abstract>
      <url hash="5e559b5e">2022.emnlp-main.800</url>
      <bibkey>kim-etal-2022-pseudo</bibkey>
    </paper>
    <paper id="801">
      <title><fixed-case>Z</fixed-case>ero<fixed-case>G</fixed-case>en: Efficient Zero-shot Learning via Dataset Generation</title>
      <author><first>Jiacheng</first><last>Ye</last></author>
      <author><first>Jiahui</first><last>Gao</last></author>
      <author><first>Qintong</first><last>Li</last></author>
      <author><first>Hang</first><last>Xu</last></author>
      <author><first>Jiangtao</first><last>Feng</last></author>
      <author><first>Zhiyong</first><last>Wu</last></author>
      <author><first>Tao</first><last>Yu</last></author>
      <author><first>Lingpeng</first><last>Kong</last></author>
      <pages>11653-11669</pages>
      <abstract>There is a growing interest in dataset generation recently due to the superior generative capacity of large pre-trained language models (PLMs). In this paper, we study a flexible and efficient zero-short learning method, ZeroGen.Given a zero-shot task, we first generate a dataset from scratch using PLMs in an unsupervised manner. Then, we train a tiny task model (e.g., LSTM) under the supervision of the synthesized dataset. This approach allows highly efficient inference as the final task model only has orders of magnitude fewer parameters comparing to PLMs (e.g., GPT2-XL).Apart from being annotation-free and efficient, we argue that ZeroGen can also provide useful insights from the perspective of data-free model-agnostic knowledge distillation, and unreferenced text generation evaluation. Experiments and analysis on different NLP tasks, namely, text classification, question answering, and natural language inference, show the effectiveness of ZeroGen.</abstract>
      <url hash="18e4638a">2022.emnlp-main.801</url>
      <bibkey>ye-etal-2022-zerogen</bibkey>
    </paper>
    <paper id="802">
      <title>Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings</title>
      <author><first>Malte</first><last>Ostendorff</last></author>
      <author><first>Nils</first><last>Rethmeier</last></author>
      <author><first>Isabelle</first><last>Augenstein</last></author>
      <author><first>Bela</first><last>Gipp</last></author>
      <author><first>Georg</first><last>Rehm</last></author>
      <pages>11670-11688</pages>
      <abstract>Learning scientific document representations can be substantially improved through contrastive learning objectives, where the challenge lies in creating positive and negative training samples that encode the desired similarity semantics. Prior work relies on discrete citation relations to generate contrast samples. However, discrete citations enforce a hard cut-off to similarity. This is counter-intuitive to similarity-based learning and ignores that scientific papers can be very similar despite lacking a direct citation - a core problem of finding related research. Instead, we use controlled nearest neighbor sampling over citation graph embeddings for contrastive learning. This control allows us to learn continuous similarity, to sample hard-to-learn negatives and positives, and also to avoid collisions between negative and positive samples by controlling the sampling margin between them. The resulting method SciNCL outperforms the state-of-the-art on the SciDocs benchmark. Furthermore, we demonstrate that it can train (or tune) language models sample-efficiently and that it can be combined with recent training-efficient methods. Perhaps surprisingly, even training a general-domain language model this way outperforms baselines pretrained in-domain.</abstract>
      <url hash="f0eac7f0">2022.emnlp-main.802</url>
      <bibkey>ostendorff-etal-2022-neighborhood</bibkey>
    </paper>
    <paper id="803">
      <title><fixed-case>SPE</fixed-case>: Symmetrical Prompt Enhancement for Fact Probing</title>
      <author><first>Yiyuan</first><last>Li</last></author>
      <author><first>Tong</first><last>Che</last></author>
      <author><first>Yezhen</first><last>Wang</last></author>
      <author><first>Zhengbao</first><last>Jiang</last></author>
      <author><first>Caiming</first><last>Xiong</last></author>
      <author><first>Snigdha</first><last>Chaturvedi</last></author>
      <pages>11689-11698</pages>
      <abstract>Pretrained language models (PLMs) have been shown to accumulate factual knowledge during pretraining (Petroni et al. 2019). Recent works probe PLMs for the extent of this knowledge through prompts either in discrete or continuous forms. However, these methods do not consider symmetry of the task: object prediction and subject prediction. In this work, we propose Symmetrical Prompt Enhancement (SPE), a continuous prompt-based method for factual probing in PLMs that leverages the symmetry of the task by constructing symmetrical prompts for subject and object prediction. Our results on a popular factual probing dataset, LAMA, show significant improvement of SPE over previous probing methods.</abstract>
      <url hash="e56cbb41">2022.emnlp-main.803</url>
      <bibkey>li-etal-2022-spe</bibkey>
    </paper>
    <paper id="804">
      <title>Efficient Large Scale Language Modeling with Mixtures of Experts</title>
      <author><first>Mikel</first><last>Artetxe</last></author>
      <author><first>Shruti</first><last>Bhosale</last></author>
      <author><first>Naman</first><last>Goyal</last></author>
      <author><first>Todor</first><last>Mihaylov</last></author>
      <author><first>Myle</first><last>Ott</last></author>
      <author><first>Sam</first><last>Shleifer</last></author>
      <author><first>Xi Victoria</first><last>Lin</last></author>
      <author><first>Jingfei</first><last>Du</last></author>
      <author><first>Srinivasan</first><last>Iyer</last></author>
      <author><first>Ramakanth</first><last>Pasunuru</last></author>
      <author><first>Giridharan</first><last>Anantharaman</last></author>
      <author><first>Xian</first><last>Li</last></author>
      <author><first>Shuohui</first><last>Chen</last></author>
      <author><first>Halil</first><last>Akin</last></author>
      <author><first>Mandeep</first><last>Baines</last></author>
      <author><first>Louis</first><last>Martin</last></author>
      <author><first>Xing</first><last>Zhou</last></author>
      <author><first>Punit Singh</first><last>Koura</last></author>
      <author><first>Brian</first><last>O’Horo</last></author>
      <author><first>Jeffrey</first><last>Wang</last></author>
      <author><first>Luke</first><last>Zettlemoyer</last></author>
      <author><first>Mona</first><last>Diab</last></author>
      <author><first>Zornitsa</first><last>Kozareva</last></author>
      <author><first>Veselin</first><last>Stoyanov</last></author>
      <pages>11699-11732</pages>
      <abstract>Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language modeling, zero- and few-shot priming, and full-shot fine-tuning. With the exception of fine-tuning, we find MoEs to be substantially more compute efficient. At more modest training budgets, MoEs can match the performance of dense models using ~4 times less compute. This gap narrows at scale, but our largest MoE model (1.1T parameters) consistently outperforms a compute-equivalent dense model (6.7B parameters). Overall, this performance gap varies greatly across tasks and domains, suggesting that MoE and dense models generalize differently in ways that are worthy of future study. We make our code and models publicly available for research use.</abstract>
      <url hash="dd3c64e6">2022.emnlp-main.804</url>
      <bibkey>artetxe-etal-2022-efficient</bibkey>
    </paper>
    <paper id="805">
      <title><fixed-case>M</fixed-case>ed<fixed-case>JE</fixed-case>x: A Medical Jargon Extraction Model with <fixed-case>W</fixed-case>iki’s Hyperlink Span and Contextualized Masked Language Model Score</title>
      <author><first>Sunjae</first><last>Kwon</last></author>
      <author><first>Zonghai</first><last>Yao</last></author>
      <author><first>Harmon</first><last>Jordan</last></author>
      <author><first>David</first><last>Levy</last></author>
      <author><first>Brian</first><last>Corner</last></author>
      <author><first>Hong</first><last>Yu</last></author>
      <pages>11733-11751</pages>
      <abstract>This paper proposes a new natural language processing (NLP) application for identifying medical jargon terms potentially difficult for patients to comprehend from electronic health record (EHR) notes. We first present a novel and publicly available dataset with expert-annotated medical jargon terms from 18K+ EHR note sentences (MedJ). Then, we introduce a novel medical jargon extraction (MedJEx) model which has been shown to outperform existing state-of-the-art NLP models. First, MedJEx improved the overall performance when it was trained on an auxiliary Wikipedia hyperlink span dataset, where hyperlink spans provide additional Wikipedia articles to explain the spans (or terms), and then fine-tuned on the annotated MedJ data. Secondly, we found that a contextualized masked language model score was beneficial for detecting domain-specific unfamiliar jargon terms. Moreover, our results show that training on the auxiliary Wikipedia hyperlink span datasets improved six out of eight biomedical named entity recognition benchmark datasets. MedJEx is publicly available.</abstract>
      <url hash="b47776ee">2022.emnlp-main.805</url>
      <bibkey>kwon-etal-2022-medjex</bibkey>
    </paper>
    <paper id="806">
      <title>Discourse Comprehension: A Question Answering Framework to Represent Sentence Connections</title>
      <author><first>Wei-Jen</first><last>Ko</last></author>
      <author><first>Cutter</first><last>Dalton</last></author>
      <author><first>Mark</first><last>Simmons</last></author>
      <author><first>Eliza</first><last>Fisher</last></author>
      <author><first>Greg</first><last>Durrett</last></author>
      <author><first>Junyi Jessy</first><last>Li</last></author>
      <pages>11752-11764</pages>
      <abstract>While there has been substantial progress in text comprehension through simple factoid question answering, more holistic comprehension of a discourse still presents a major challenge (Dunietz et al., 2020). Someone critically reflecting on a text as they read it will pose curiosity-driven, often open-ended questions, which reflect deep understanding of the content and require complex reasoning to answer (Ko et al., 2020; Westera et al., 2020). A key challenge in building and evaluating models for this type of discourse comprehension is the lack of annotated data, especially since collecting answers to such questions requires high cognitive load for annotators.This paper presents a novel paradigm that enables scalable data collection targeting the comprehension of news documents, viewing these questions through the lens of discourse. The resulting corpus, DCQA (Discourse Comprehension by Question Answering), captures both discourse and semantic links between sentences in the form of free-form, open-ended questions. On an evaluation set that we annotated on questions from Ko et al. (2020), we show that DCQA provides valuable supervision for answering open-ended questions. We additionally design pre-training methods utilizing existing question-answering resources, and use synthetic data to accommodate unanswerable questions.</abstract>
      <url hash="ea72dbe7">2022.emnlp-main.806</url>
      <bibkey>ko-etal-2022-discourse</bibkey>
    </paper>
    <paper id="807">
      <title>Learning to Generate Overlap Summaries through Noisy Synthetic Data</title>
      <author><first>Naman</first><last>Bansal</last></author>
      <author><first>Mousumi</first><last>Akter</last></author>
      <author><first>Shubhra Kanti</first><last>Karmaker Santu</last></author>
      <pages>11765-11777</pages>
      <abstract>Semantic Overlap Summarization (SOS) is a novel and relatively under-explored seq-to-seq task which entails summarizing common information from multiple alternate narratives. One of the major challenges for solving this task is the lack of existing datasets for supervised training. To address this challenge, we propose a novel data augmentation technique, which allows us to create large amount of synthetic data for training a seq-to-seq model that can perform the SOS task. Through extensive experiments using narratives from the news domain, we show that the models fine-tuned using the synthetic dataset provide significant performance improvements over the pre-trained vanilla summarization techniques and are close to the models fine-tuned on the golden training data; which essentially demonstrates the effectiveness of out proposed data augmentation technique for training seq-to-seq models on the SOS task.</abstract>
      <url hash="1405ff1b">2022.emnlp-main.807</url>
      <bibkey>bansal-etal-2022-learning</bibkey>
    </paper>
    <paper id="808">
      <title>Mutual Exclusivity Training and Primitive Augmentation to Induce Compositionality</title>
      <author><first>Yichen</first><last>Jiang</last></author>
      <author><first>Xiang</first><last>Zhou</last></author>
      <author><first>Mohit</first><last>Bansal</last></author>
      <pages>11778-11793</pages>
      <abstract>Recent datasets expose the lack of the systematic generalization ability in standard sequence-to-sequence models. In this work, we analyze this behavior of seq2seq models and identify two contributing factors: a lack of mutual exclusivity bias (one target sequence can only be mapped to one source sequence), and the tendency to memorize whole examples rather than separating structures from contents. We propose two techniques to address these two issues respectively: Mutual Exclusivity Training that prevents the model from producing seen generations when facing novel examples via an unlikelihood-based loss, and prim2primX data augmentation that automatically diversifies the arguments of every syntactic function to prevent memorizing and provide a compositional inductive bias without exposing test-set data. Combining these two techniques, we show substantial empirical improvements using standard sequence-to-sequence models (LSTMs and Transformers) on two widely-used compositionality datasets: SCAN and COGS. Finally, we provide analysis characterizing the improvements as well as the remaining challenges, and provide detailed ablations of our method.</abstract>
      <url hash="2808f9c4">2022.emnlp-main.808</url>
      <bibkey>jiang-etal-2022-mutual</bibkey>
    </paper>
    <paper id="809">
      <title>Directions for <fixed-case>NLP</fixed-case> Practices Applied to Online Hate Speech Detection</title>
      <author><first>Paula</first><last>Fortuna</last></author>
      <author><first>Monica</first><last>Dominguez</last></author>
      <author><first>Leo</first><last>Wanner</last></author>
      <author><first>Zeerak</first><last>Talat</last></author>
      <pages>11794-11805</pages>
      <abstract>Addressing hate speech in online spaces has been conceptualized as a classification task that uses Natural Language Processing (NLP) techniques. Through this conceptualization, the hate speech detection task has relied on common conventions and practices from NLP. For instance, inter-annotator agreement is conceptualized as a way to measure dataset quality and certain metrics and benchmarks are used to assure model generalization. However, hate speech is a deeply complex and situated concept that eludes such static and disembodied practices. In this position paper, we critically reflect on these methodologies for hate speech detection, we argue that many conventions in NLP are poorly suited for the problem and encourage researchers to develop methods that are more appropriate for the task.</abstract>
      <url hash="6cf03779">2022.emnlp-main.809</url>
      <bibkey>fortuna-etal-2022-directions</bibkey>
    </paper>
    <paper id="810">
      <title>Pre-training Transformer Models with Sentence-Level Objectives for Answer Sentence Selection</title>
      <author><first>Luca</first><last>Di Liello</last></author>
      <author><first>Siddhant</first><last>Garg</last></author>
      <author><first>Luca</first><last>Soldaini</last></author>
      <author><first>Alessandro</first><last>Moschitti</last></author>
      <pages>11806-11816</pages>
      <abstract>An important task for designing QA systems is answer sentence selection (AS2): selecting the sentence containing (or constituting) the answer to a question from a set of retrieved relevant documents. In this paper, we propose three novel sentence-level transformer pre-training objectives that incorporate paragraph-level semantics within and across documents, to improve the performance of transformers for AS2, and mitigate the requirement of large labeled datasets. Specifically, the model is tasked to predict whether: (i) two sentences are extracted from the same paragraph, (ii) a given sentence is extracted from a given paragraph, and (iii) two paragraphs are extracted from the same document. Our experiments on three public and one industrial AS2 datasets demonstrate the empirical superiority of our pre-trained transformers over baseline models such as RoBERTa and ELECTRA for AS2.</abstract>
      <url hash="874dcf73">2022.emnlp-main.810</url>
      <bibkey>di-liello-etal-2022-pre</bibkey>
    </paper>
    <paper id="811">
      <title><fixed-case>O</fixed-case>pen<fixed-case>CQA</fixed-case>: Open-ended Question Answering with Charts</title>
      <author><first>Shankar</first><last>Kantharaj</last></author>
      <author><first>Xuan Long</first><last>Do</last></author>
      <author><first>Rixie Tiffany</first><last>Leong</last></author>
      <author><first>Jia Qing</first><last>Tan</last></author>
      <author><first>Enamul</first><last>Hoque</last></author>
      <author><first>Shafiq</first><last>Joty</last></author>
      <pages>11817-11837</pages>
      <abstract>Charts are very popular to analyze data and convey important insights. People often analyze visualizations to answer open-ended questions that require explanatory answers. Answering such questions are often difficult and time-consuming as it requires a lot of cognitive and perceptual efforts. To address this challenge, we introduce a new task called OpenCQA, where the goal is to answer an open-ended question about a chart with descriptive texts. We present the annotation process and an in-depth analysis of our dataset. We implement and evaluate a set of baselines under three practical settings. In the first setting, a chart and the accompanying article is provided as input to the model. The second setting provides only the relevant paragraph(s) to the chart instead of the entire article, whereas the third setting requires the model to generate an answer solely based on the chart. Our analysis of the results show that the top performing models generally produce fluent and coherent text while they struggle to perform complex logical and arithmetic reasoning.</abstract>
      <url hash="848acb6c">2022.emnlp-main.811</url>
      <bibkey>kantharaj-etal-2022-opencqa</bibkey>
    </paper>
    <paper id="812">
      <title>A Systematic Investigation of Commonsense Knowledge in Large Language Models</title>
      <author><first>Xiang Lorraine</first><last>Li</last></author>
      <author><first>Adhiguna</first><last>Kuncoro</last></author>
      <author><first>Jordan</first><last>Hoffmann</last></author>
      <author><first>Cyprien</first><last>de Masson d’Autume</last></author>
      <author><first>Phil</first><last>Blunsom</last></author>
      <author><first>Aida</first><last>Nematzadeh</last></author>
      <pages>11838-11855</pages>
      <abstract>Language models (LMs) trained on large amounts of data have shown impressive performance on many NLP tasks under the zero-shot and few-shot setup. Here we aim to better understand the extent to which such models learn commonsense knowledge — a critical component of many NLP applications. We conduct a systematic and rigorous zero-shot and few-shot commonsense evaluation of large pre-trained LMs, where we: (i) carefully control for the LMs’ ability to exploit potential surface cues and annotation artefacts, and (ii) account for variations in performance that arise from factors that are not related to commonsense knowledge. Our findings highlight the limitations of pre-trained LMs in acquiring commonsense knowledge without task-specific supervision; furthermore, using larger models or few-shot evaluation is insufficient to achieve human-level commonsense performance.</abstract>
      <url hash="280c4c82">2022.emnlp-main.812</url>
      <bibkey>li-etal-2022-systematic</bibkey>
    </paper>
    <paper id="813">
      <title>Transforming Sequence Tagging Into A <fixed-case>S</fixed-case>eq2<fixed-case>S</fixed-case>eq Task</title>
      <author><first>Karthik</first><last>Raman</last></author>
      <author><first>Iftekhar</first><last>Naim</last></author>
      <author><first>Jiecao</first><last>Chen</last></author>
      <author><first>Kazuma</first><last>Hashimoto</last></author>
      <author><first>Kiran</first><last>Yalasangi</last></author>
      <author><first>Krishna</first><last>Srinivasan</last></author>
      <pages>11856-11874</pages>
      <abstract>Pretrained, large, generative language models (LMs) have had great success in a wide range of sequence tagging and structured prediction tasks. Casting a sequence tagging task as a Seq2Seq one requires deciding the formats of the input and output sequences. However, we lack a principled understanding of the trade-offs associated with these formats (such as the effect on model accuracy, sequence length, multilingual generalization, hallucination). In this paper, we rigorously study different formats one could use for casting input text sentences and their output labels into the input and target (i.e., output) of a Seq2Seq model. Along the way, we introduce a new format, which we show to to be both simpler and more effective. Additionally the new format demonstrates significant gains in the multilingual settings – both zero-shot transfer learning and joint training. Lastly, we find that the new format is more robust and almost completely devoid of hallucination – an issue we find common in existing formats. With well over a 1000 experiments studying 14 different formats, over 7 diverse public benchmarks – including 3 multilingual datasets spanning 7 languages – we believe our findings provide a strong empirical basis in understanding how we should tackle sequence tagging tasks.</abstract>
      <url hash="591c55f4">2022.emnlp-main.813</url>
      <bibkey>raman-etal-2022-transforming</bibkey>
    </paper>
    <paper id="814">
      <title><fixed-case>C</fixed-case>ycle<fixed-case>KQR</fixed-case>: Unsupervised Bidirectional Keyword-Question Rewriting</title>
      <author><first>Andrea</first><last>Iovine</last></author>
      <author><first>Anjie</first><last>Fang</last></author>
      <author><first>Besnik</first><last>Fetahu</last></author>
      <author><first>Jie</first><last>Zhao</last></author>
      <author><first>Oleg</first><last>Rokhlenko</last></author>
      <author><first>Shervin</first><last>Malmasi</last></author>
      <pages>11875-11886</pages>
      <abstract>Users expect their queries to be answered by search systems, regardless of the query’s surface form, which include keyword queries and natural questions. Natural Language Understanding (NLU) components of Search and QA systems may fail to correctly interpret semantically equivalent inputs if this deviates from how the system was trained, leading to suboptimal understanding capabilities. We propose the keyword-question rewriting task to improve query understanding capabilities of NLU systems for all surface forms. To achieve this, we present CycleKQR, an unsupervised approach, enabling effective rewriting between keyword and question queries using non-parallel data.Empirically we show the impact on QA performance of unfamiliar query forms for open domain and Knowledge Base QA systems (trained on either keywords or natural language questions). We demonstrate how CycleKQR significantly improves QA performance by rewriting queries into the appropriate form, while at the same time retaining the original semantic meaning of input queries, allowing CycleKQR to improve performance by up to 3% over supervised baselines. Finally, we release a datasetof 66k keyword-question pairs.</abstract>
      <url hash="5b540cf0">2022.emnlp-main.814</url>
      <bibkey>iovine-etal-2022-cyclekqr</bibkey>
    </paper>
    <paper id="815">
      <title>Model Criticism for Long-Form Text Generation</title>
      <author><first>Yuntian</first><last>Deng</last></author>
      <author><first>Volodymyr</first><last>Kuleshov</last></author>
      <author><first>Alexander</first><last>Rush</last></author>
      <pages>11887-11912</pages>
      <abstract>Language models have demonstrated the ability to generate highly fluent text; however, it remains unclear whether their output retains coherent high-level structure (e.g., story progression). Here, we propose to apply a statistical tool, model criticism in latent space, to evaluate the high-level structure of the generated text. Model criticism compares the distributions between real and generated data in a latent space obtained according to an assumptive generative process. Different generative processes identify specific failure modes of the underlying model. We perform experiments on three representative aspects of high-level discourse—coherence, coreference, and topicality—and find that transformer-based language models are able to capture topical structures but have a harder time maintaining structural coherence or modeling coreference.</abstract>
      <url hash="176e9a9d">2022.emnlp-main.815</url>
      <bibkey>deng-etal-2022-model</bibkey>
    </paper>
    <paper id="816">
      <title>Improving Faithfulness by Augmenting Negative Summaries from Fake Documents</title>
      <author><first>Tianshu</first><last>Wang</last></author>
      <author><first>Faisal</first><last>Ladhak</last></author>
      <author><first>Esin</first><last>Durmus</last></author>
      <author><first>He</first><last>He</last></author>
      <pages>11913-11921</pages>
      <abstract>Current abstractive summarization systems tend to hallucinate content that is unfaithful to the source document, posing a risk of misinformation. To mitigate hallucination, we must teach the model to distinguish hallucinated summaries from faithful ones. However, the commonly used maximum likelihood training does not disentangle factual errors from other model errors. To address this issue,we propose a back-translation-style approach to augment negative samples that mimic factual errors made by the model. Specifically, we train an elaboration model that generates hallucinated documents given the reference summaries, and then generates negative summaries from the fake documents. We incorporate the negative samples into training through a controlled generator, which produces faithful/unfaithful summaries conditioned on the control codes. Additionally, we find that adding textual entailment data through multitasking further boosts the performance. Experiments on three datasets (XSum, Gigaword, and WikiHow) show that our method consistently improves faithfulness without sacrificing informativeness according to both human and automatic evaluation</abstract>
      <url hash="651a6771">2022.emnlp-main.816</url>
      <bibkey>wang-etal-2022-improving</bibkey>
    </paper>
    <paper id="817">
      <title>Joint Completion and Alignment of Multilingual Knowledge Graphs</title>
      <author><first>Soumen</first><last>Chakrabarti</last></author>
      <author><first>Harkanwar</first><last>Singh</last></author>
      <author><first>Shubham</first><last>Lohiya</last></author>
      <author><first>Prachi</first><last>Jain</last></author>
      <author><first>Mausam</first><last>-</last></author>
      <pages>11922-11938</pages>
      <abstract>Knowledge Graph Completion (KGC) predicts missing facts in an incomplete Knowledge Graph (KG). Multilingual KGs associate entities and relations with surface forms written in different languages. An entity or relation may be associated with distinct IDs in different KGs, necessitating entity alignment (EA) and relation alignment (RA). Many effective algorithms have been proposed for completion and alignment as separate tasks. Here we show that these tasks are synergistic and best solved together. Our multitask approach starts with a state-of-the-art KG embedding scheme, but adds a novel relation representation based on sets of embeddings of (subject, object) entity pairs. This representation leads to a new relation alignment loss term based on a maximal bipartite matching between two sets of embedding vectors. This loss is combined with traditional KGC loss and optionally, losses based on text embeddings of entity (and relation) names. In experiments over KGs in seven languages, we find that our system achieves large improvements in KGC compared to a strong completion model that combines known facts in all languages. It also outperforms strong EA and RA baselines, underscoring the value of joint alignment and completion.</abstract>
      <url hash="822ce953">2022.emnlp-main.817</url>
      <bibkey>chakrabarti-etal-2022-joint</bibkey>
    </paper>
    <paper id="818">
      <title>Offer a Different Perspective: Modeling the Belief Alignment of Arguments in Multi-party Debates</title>
      <author><first>Suzanna</first><last>Sia</last></author>
      <author><first>Kokil</first><last>Jaidka</last></author>
      <author><first>Hansin</first><last>Ahuja</last></author>
      <author><first>Niyati</first><last>Chhaya</last></author>
      <author><first>Kevin</first><last>Duh</last></author>
      <pages>11939-11950</pages>
      <abstract>In contexts where debate and deliberation are the norm, the participants are regularly presented with new information that conflicts with their original beliefs. When required to update their beliefs (belief alignment), they may choose arguments that align with their worldview (confirmation bias). We test this and competing hypotheses in a constraint-based modeling approach to predict the winning arguments in multi-party interactions in the Reddit Change My View and Intelligence Squared debates datasets. We adopt a hierarchical generative Variational Autoencoder as our model and impose structural constraints that reflect competing hypotheses about the nature of argumentation. Our findings suggest that in most settings, predictive models that anticipate winning arguments to be further from the initial argument of the opinion holder are more likely to succeed.</abstract>
      <url hash="017617cd">2022.emnlp-main.818</url>
      <bibkey>sia-etal-2022-offer</bibkey>
    </paper>
    <paper id="819">
      <title>A Federated Approach to Predicting Emojis in <fixed-case>H</fixed-case>indi Tweets</title>
      <author><first>Deep</first><last>Gandhi</last></author>
      <author><first>Jash</first><last>Mehta</last></author>
      <author><first>Nirali</first><last>Parekh</last></author>
      <author><first>Karan</first><last>Waghela</last></author>
      <author><first>Lynette</first><last>D’Mello</last></author>
      <author><first>Zeerak</first><last>Talat</last></author>
      <pages>11951-11961</pages>
      <abstract>The use of emojis affords a visual modality to, often private, textual communication.The task of predicting emojis however provides a challenge for machine learning as emoji use tends to cluster into the frequently used and the rarely used emojis.Much of the machine learning research on emoji use has focused on high resource languages and has conceptualised the task of predicting emojis around traditional server-side machine learning approaches.However, traditional machine learning approaches for private communication can introduce privacy concerns, as these approaches require all data to be transmitted to a central storage.In this paper, we seek to address the dual concerns of emphasising high resource languages for emoji prediction and risking the privacy of people’s data.We introduce a new dataset of 118k tweets (augmented from 25k unique tweets) for emoji prediction in Hindi, and propose a modification to the federated learning algorithm, CausalFedGSD, which aims to strike a balance between model performance and user privacy. We show that our approach obtains comparative scores with more complex centralised models while reducing the amount of data required to optimise the models and minimising risks to user privacy.</abstract>
      <url hash="93e7d7de">2022.emnlp-main.819</url>
      <bibkey>gandhi-etal-2022-federated</bibkey>
    </paper>
    <paper id="820">
      <title>Injecting Domain Knowledge in Language Models for Task-oriented Dialogue Systems</title>
      <author><first>Denis</first><last>Emelin</last></author>
      <author><first>Daniele</first><last>Bonadiman</last></author>
      <author><first>Sawsan</first><last>Alqahtani</last></author>
      <author><first>Yi</first><last>Zhang</last></author>
      <author><first>Saab</first><last>Mansour</last></author>
      <pages>11962-11974</pages>
      <abstract>Pre-trained language models (PLM) have advanced the state-of-the-art across NLP applications, but lack domain-specific knowledge that does not naturally occur in pre-training data. Previous studies augmented PLMs with symbolic knowledge for different downstream NLP tasks. However, knowledge bases (KBs) utilized in these studies are usually large-scale and static, in contrast to small, domain-specific, and modifiable knowledge bases that are prominent in real-world task-oriented dialogue (TOD) systems. In this paper, we showcase the advantages of injecting domain-specific knowledge prior to fine-tuning on TOD tasks. To this end, we utilize light-weight adapters that can be easily integrated with PLMs and serve as a repository for facts learned from different KBs. To measure the efficacy of proposed knowledge injection methods, we introduce Knowledge Probing using Response Selection (KPRS) – a probe designed specifically for TOD models. Experiments on KPRS and the response generation task show improvements of knowledge injection with adapters over strong baselines.</abstract>
      <url hash="3eb936f2">2022.emnlp-main.820</url>
      <bibkey>emelin-etal-2022-injecting</bibkey>
    </paper>
    <paper id="821">
      <title><fixed-case>TASA</fixed-case>: Deceiving Question Answering Models by Twin Answer Sentences Attack</title>
      <author><first>Yu</first><last>Cao</last></author>
      <author><first>Dianqi</first><last>Li</last></author>
      <author><first>Meng</first><last>Fang</last></author>
      <author><first>Tianyi</first><last>Zhou</last></author>
      <author><first>Jun</first><last>Gao</last></author>
      <author><first>Yibing</first><last>Zhan</last></author>
      <author><first>Dacheng</first><last>Tao</last></author>
      <pages>11975-11992</pages>
      <abstract>We present Twin Answer Sentences Attack (TASA), an adversarial attack method for question answering (QA) models that produces fluent and grammatical adversarial contexts while maintaining gold answers. Despite phenomenal progress on general adversarial attacks, few works have investigated the vulnerability and attack specifically for QA models. In this work, we first explore the biases in the existing models and discover that they mainly rely on keyword matching between the question and context, and ignore the relevant contextual relations for answer prediction.Based on two biases above, TASA attacks the target model in two folds: (1) lowering the model’s confidence on the gold answer with a perturbed answer sentence; (2) misguiding the model towards a wrong answer with a distracting answer sentence. Equipped with designed beam search and filtering methods, TASA can generate more effective attacks than existing textual attack methods while sustaining the quality of contexts, in extensive experiments on five QA datasets and human evaluations.</abstract>
      <url hash="3b69c783">2022.emnlp-main.821</url>
      <bibkey>cao-etal-2022-tasa</bibkey>
    </paper>
    <paper id="822">
      <title>Improving Low-Resource Languages in Pre-Trained Multilingual Language Models</title>
      <author><first>Viktor</first><last>Hangya</last></author>
      <author><first>Hossain Shaikh</first><last>Saadi</last></author>
      <author><first>Alexander</first><last>Fraser</last></author>
      <pages>11993-12006</pages>
      <abstract>Pre-trained multilingual language models are the foundation of many NLP approaches, including cross-lingual transfer solutions. However, languages with small available monolingual corpora are often not well-supported by these models leading to poor performance. We propose an unsupervised approach to improve the cross-lingual representations of low-resource languages by bootstrapping word translation pairs from monolingual corpora and using them to improve language alignment in pre-trained language models. We perform experiments on nine languages, using contextual word retrieval and zero-shot named entity recognition to measure both intrinsic cross-lingual word representation quality and downstream task performance, showing improvements on both tasks. Our results show that it is possible to improve pre-trained multilingual language models by relying only on non-parallel resources.</abstract>
      <url hash="3582b984">2022.emnlp-main.822</url>
      <bibkey>hangya-etal-2022-improving</bibkey>
    </paper>
    <paper id="823">
      <title><fixed-case>SCROLLS</fixed-case>: Standardized <fixed-case>C</fixed-case>ompa<fixed-case>R</fixed-case>ison Over Long Language Sequences</title>
      <author><first>Uri</first><last>Shaham</last></author>
      <author><first>Elad</first><last>Segal</last></author>
      <author><first>Maor</first><last>Ivgi</last></author>
      <author><first>Avia</first><last>Efrat</last></author>
      <author><first>Ori</first><last>Yoran</last></author>
      <author><first>Adi</first><last>Haviv</last></author>
      <author><first>Ankit</first><last>Gupta</last></author>
      <author><first>Wenhan</first><last>Xiong</last></author>
      <author><first>Mor</first><last>Geva</last></author>
      <author><first>Jonathan</first><last>Berant</last></author>
      <author><first>Omer</first><last>Levy</last></author>
      <pages>12007-12021</pages>
      <abstract>NLP benchmarks have largely focused on short texts, such as sentences and paragraphs, even though long texts comprise a considerable amount of natural language in the wild. We introduce SCROLLS, a suite of tasks that require reasoning over long texts. We examine existing long-text datasets, and handpick ones where the text is naturally long, while prioritizing tasks that involve synthesizing information across the input. SCROLLS contains summarization, question answering, and natural language inference tasks, covering multiple domains, including literature, science, business, and entertainment. Initial baselines, including Longformer Encoder-Decoder, indicate that there is ample room for improvement on SCROLLS. We make all datasets available in a unified text-to-text format and host a live leaderboard to facilitate research on model architecture and pretraining methods.</abstract>
      <url hash="8fa7cf97">2022.emnlp-main.823</url>
      <bibkey>shaham-etal-2022-scrolls</bibkey>
    </paper>
    <paper id="824">
      <title><fixed-case>PAR</fixed-case>: Political Actor Representation Learning with Social Context and Expert Knowledge</title>
      <author><first>Shangbin</first><last>Feng</last></author>
      <author><first>Zhaoxuan</first><last>Tan</last></author>
      <author><first>Zilong</first><last>Chen</last></author>
      <author><first>Ningnan</first><last>Wang</last></author>
      <author><first>Peisheng</first><last>Yu</last></author>
      <author><first>Qinghua</first><last>Zheng</last></author>
      <author><first>Xiaojun</first><last>Chang</last></author>
      <author><first>Minnan</first><last>Luo</last></author>
      <pages>12022-12036</pages>
      <abstract>Modeling the ideological perspectives of political actors is an essential task in computational political science with applications in many downstream tasks. Existing approaches are generally limited to textual data and voting records, while they neglect the rich social context and valuable expert knowledge for holistic ideological analysis. In this paper, we propose PAR, a Political Actor Representation learning framework that jointly leverages social context and expert knowledge. Specifically, we retrieve and extract factual statements about legislators to leverage social context information. We then construct a heterogeneous information network to incorporate social context and use relational graph neural networks to learn legislator representations. Finally, we train PAR with three objectives to align representation learning with expert knowledge, model ideological stance consistency, and simulate the echo chamber phenomenon. Extensive experiments demonstrate that PAR is better at augmenting political text understanding and successfully advances the state-of-the-art in political perspective detection and roll call vote prediction. Further analysis proves that PAR learns representations that reflect the political reality and provide new insights into political behavior.</abstract>
      <url hash="44f74d05">2022.emnlp-main.824</url>
      <bibkey>feng-etal-2022-par</bibkey>
    </paper>
    <paper id="825">
      <title><fixed-case>JDDC</fixed-case> 2.1: A Multimodal <fixed-case>C</fixed-case>hinese Dialogue Dataset with Joint Tasks of Query Rewriting, Response Generation, Discourse Parsing, and Summarization</title>
      <author><first>Nan</first><last>Zhao</last></author>
      <author><first>Haoran</first><last>Li</last></author>
      <author><first>Youzheng</first><last>Wu</last></author>
      <author><first>Xiaodong</first><last>He</last></author>
      <pages>12037-12051</pages>
      <abstract>The popularity of multimodal dialogue has stimulated the need for a new generation of dialogue agents with multimodal interactivity.When users communicate with customer service, they may express their requirements by means of text, images, or even videos. Visual information usually acts as discriminators for product models, or indicators of product failures, which play an important role in the E-commerce scenario.On the other hand, detailed information provided by the images is limited, and typically, customer service systems cannot understand the intent of users without the input text.Thus, bridging the gap between the image and text is crucial for communicating with customers.In this paper, we construct JDDC 2.1, a large-scale multimodal multi-turn dialogue dataset collected from a mainstream Chinese E-commerce platform, containing about 246K dialogue sessions, 3M utterances, and 507K images, along with product knowledge bases and image category annotations. Over our dataset, we jointly define four tasks: the multimodal dialogue response generation task,the multimodal query rewriting task, the multimodal dialogue discourse parsing task, and the multimodal dialogue summarization task.JDDC 2.1 is the first corpus with annotations for all the above tasks over the same dialogue sessions, which facilitates the comprehensive research around the dialogue.In addition, we present several text-only and multimodal baselines and show the importance of visual information for these tasks. Our dataset and implements will be publicly available.</abstract>
      <url hash="52cfbb2d">2022.emnlp-main.825</url>
      <bibkey>zhao-etal-2022-jddc</bibkey>
    </paper>
    <paper id="826">
      <title><fixed-case>PCL</fixed-case>: Peer-Contrastive Learning with Diverse Augmentations for Unsupervised Sentence Embeddings</title>
      <author><first>Qiyu</first><last>Wu</last></author>
      <author><first>Chongyang</first><last>Tao</last></author>
      <author><first>Tao</first><last>Shen</last></author>
      <author><first>Can</first><last>Xu</last></author>
      <author><first>Xiubo</first><last>Geng</last></author>
      <author><first>Daxin</first><last>Jiang</last></author>
      <pages>12052-12066</pages>
      <abstract>Learning sentence embeddings in an unsupervised manner is fundamental in natural language processing. Recent common practice is to couple pre-trained language models with unsupervised contrastive learning, whose success relies on augmenting a sentence with a semantically-close positive instance to construct contrastive pairs. Nonetheless, existing approaches usually depend on a mono-augmenting strategy, which causes learning shortcuts towards the augmenting biases and thus corrupts the quality of sentence embeddings. A straightforward solution is resorting to more diverse positives from a multi-augmenting strategy, while an open question remains about how to unsupervisedly learn from the diverse positives but with uneven augmenting qualities in the text field. As one answer, we propose a novel Peer-Contrastive Learning (PCL) with diverse augmentations. PCL constructs diverse contrastive positives and negatives at the group level for unsupervised sentence embeddings. PCL performs peer-positive contrast as well as peer-network cooperation, which offers an inherent anti-bias ability and an effective way to learn from diverse augmentations. Experiments on STS benchmarks verify the effectiveness of PCL against its competitors in unsupervised sentence embeddings.</abstract>
      <url hash="e4dbc5ee">2022.emnlp-main.826</url>
      <bibkey>wu-etal-2022-pcl</bibkey>
    </paper>
    <paper id="827">
      <title>Digging Errors in <fixed-case>NMT</fixed-case>: Evaluating and Understanding Model Errors from Partial Hypothesis Space</title>
      <author><first>Jianhao</first><last>Yan</last></author>
      <author><first>Chenming</first><last>Wu</last></author>
      <author><first>Fandong</first><last>Meng</last></author>
      <author><first>Jie</first><last>Zhou</last></author>
      <pages>12067-12085</pages>
      <abstract>Solid evaluation of neural machine translation (NMT) is key to its understanding and improvement. Current evaluation of an NMT system is usually built upon a heuristic decoding algorithm (e.g., beam search) and an evaluation metric assessing similarity between the translation and golden reference. However, this system-level evaluation framework is limited by evaluating only one best hypothesis and search errors brought by heuristic decoding algorithms. To better understand NMT models, we propose a novel evaluation protocol, which defines model errors with model’s ranking capability over hypothesis space. To tackle the problem of exponentially large space, we propose two approximation methods, top region evaluation along with an exact top-k decoding algorithm, which finds top-ranked hypotheses in the whole hypothesis space, and Monte Carlo sampling evaluation, which simulates hypothesis space from a broader perspective. To quantify errors, we define our NMT model errors by measuring distance between the hypothesis array ranked by the model and the ideally ranked hypothesis array. After confirming the strong correlation with human judgment, we apply our evaluation to various NMT benchmarks and model architectures. We show that the state-of-the-art Transformer models face serious ranking issues and only perform at the random chance level in the top region. We further analyze model errors on architectures with different depths and widths, as well as different data-augmentation techniques, showing how these factors affect model errors. Finally, we connect model errors with the search algorithms and provide interesting findings of beam search inductive bias and correlation with Minimum Bayes Risk (MBR) decoding.</abstract>
      <url hash="3cda41a2">2022.emnlp-main.827</url>
      <bibkey>yan-etal-2022-digging</bibkey>
    </paper>
    <paper id="828">
      <title><fixed-case>D</fixed-case>ialog<fixed-case>C</fixed-case>onv: A Lightweight Fully Convolutional Network for Multi-view Response Selection</title>
      <author><first>Yongkang</first><last>Liu</last></author>
      <author><first>Shi</first><last>Feng</last></author>
      <author><first>Wei</first><last>Gao</last></author>
      <author><first>Daling</first><last>Wang</last></author>
      <author><first>Yifei</first><last>Zhang</last></author>
      <pages>12086-12098</pages>
      <abstract>Current end-to-end retrieval-based dialogue systems are mainly based on Recurrent Neural Networks or Transformers with attention mechanisms. Although promising results have been achieved, these models often suffer from slow inference or huge number of parameters. In this paper, we propose a novel lightweight fully convolutional architecture, called DialogConv, for response selection. DialogConv is exclusively built on top of convolution to extract matching features of context and response. Dialogues are modeled in 3D views, where DialogConv performs convolution operations on embedding view, word view and utterance view to capture richer semantic information from multiple contextual views. On the four benchmark datasets, compared with state-of-the-art baselines, DialogConv is on average about 8.5x smaller in size, and 79.39x and 10.64x faster on CPU and GPU devices, respectively. At the same time, DialogConv achieves the competitive effectiveness of response selection.</abstract>
      <url hash="15e5bcd9">2022.emnlp-main.828</url>
      <bibkey>liu-etal-2022-dialogconv</bibkey>
    </paper>
  </volume>
  <volume id="tutorials" ingest-date="2022-12-08">
    <meta>
      <booktitle>Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts</booktitle>
      <editor><first>Samhaa R.</first><last>El-Beltagy</last></editor>
      <editor><first>Xipeng</first><last>Qiu</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Abu Dubai, UAE</address>
      <month>December</month>
      <year>2022</year>
      <url hash="47b288fc">2022.emnlp-tutorials</url>
      <venue>emnlp</venue>
    </meta>
    <frontmatter>
      <url hash="b19e894e">2022.emnlp-tutorials.0</url>
      <bibkey>emnlp-2022-2022-empirical</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Non-Autoregressive Models for Fast Sequence Generation</title>
      <author><first>Yang</first><last>Feng</last></author>
      <author><first>Chenze</first><last>Shao</last></author>
      <pages>1-8</pages>
      <abstract>Autoregressive (AR) models have achieved great success in various sequence generation tasks. However, AR models can only generate target sequence word-by-word due to the AR mechanism and hence suffer from slow inference. Recently, non-autoregressive (NAR) models, which generate all the tokens in parallel by removing the sequential dependencies within the target sequence, have received increasing attention in sequence generation tasks such as neural machine translation (NMT), automatic speech recognition (ASR), and text to speech (TTS). In this tutorial, we will provide a comprehensive introduction to non-autoregressive sequence generation.</abstract>
      <url hash="704f1720">2022.emnlp-tutorials.1</url>
      <bibkey>feng-shao-2022-non</bibkey>
    </paper>
    <paper id="2">
      <title>Modular and Parameter-Efficient Fine-Tuning for <fixed-case>NLP</fixed-case> Models</title>
      <author><first>Sebastian</first><last>Ruder</last></author>
      <author><first>Jonas</first><last>Pfeiffer</last></author>
      <author><first>Ivan</first><last>Vulić</last></author>
      <pages>9-10</pages>
      <abstract>State-of-the-art language models in NLP perform best when fine-tuned even on small datasets, but due to their increasing size, fine-tuning and downstream usage have become extremely compute-intensive. Being able to efficiently and effectively fine-tune the largest pre-trained models is thus key in order to reap the benefits of the latest advances in NLP. In this tutorial, we provide a comprehensive overview of parameter-efficient fine-tuning methods. We highlight their similarities and differences by presenting them in a unified view. We explore the benefits and usage scenarios of a neglected property of such parameter-efficient models—modularity—such as composition of modules to deal with previously unseen data conditions. We finally highlight how both properties——parameter efficiency and modularity——can be useful in the real-world setting of adapting pre-trained models to under-represented languages and domains with scarce annotated data for several downstream applications.</abstract>
      <url hash="36b51093">2022.emnlp-tutorials.2</url>
      <bibkey>ruder-etal-2022-modular</bibkey>
    </paper>
    <paper id="3">
      <title>Meaning Representations for Natural Languages: Design, Models and Applications</title>
      <author><first>Jeffrey</first><last>Flanigan</last></author>
      <author><first>Ishan</first><last>Jindal</last></author>
      <author><first>Yunyao</first><last>Li</last></author>
      <author><first>Tim</first><last>O’Gorman</last></author>
      <author><first>Martha</first><last>Palmer</last></author>
      <author><first>Nianwen</first><last>Xue</last></author>
      <pages>11-16</pages>
      <abstract>This tutorial reviews the design of common meaning representations, SoTA models for predicting meaning representations, and the applications of meaning representations in a wide range of downstream NLP tasks and real-world applications. Reporting by a diverse team of NLP researchers from academia and industry with extensive experience in designing, building and using meaning representations, our tutorial has three components: (1) an introduction to common meaning representations, including basic concepts and design challenges; (2) a review of SoTA methods on building models for meaning representations; and (3) an overview of applications of meaning representations in downstream NLP tasks and real-world applications. We will also present qualitative comparisons of common meaning representations and a quantitative study on how their differences impact model performance. Finally, we will share best practices in choosing the right meaning representation for downstream tasks.</abstract>
      <url hash="fd615cd7">2022.emnlp-tutorials.3</url>
      <bibkey>flanigan-etal-2022-meaning</bibkey>
    </paper>
    <paper id="4">
      <title><fixed-case>C</fixed-case>ausal<fixed-case>NLP</fixed-case> Tutorial: An Introduction to Causality for Natural Language Processing</title>
      <author><first>Zhijing</first><last>Jin</last></author>
      <author><first>Amir</first><last>Feder</last></author>
      <author><first>Kun</first><last>Zhang</last></author>
      <pages>17-22</pages>
      <abstract>Causal inference is becoming an increasingly important topic in deep learning, with the potential to help with critical deep learning problems such as model robustness, interpretability, and fairness. In addition, causality is naturally widely used in various disciplines of science, to discover causal relationships among variables and estimate causal effects of interest. In this tutorial, we introduce the fundamentals of causal discovery and causal effect estimation to the natural language processing (NLP) audience, provide an overview of causal perspectives to NLP problems, and aim to inspire novel approaches to NLP further. This tutorial is inclusive to a variety of audiences and is expected to facilitate the community’s developments in formulating and addressing new, important NLP problems in light of emerging causal principles and methodologies.</abstract>
      <url hash="030415e4">2022.emnlp-tutorials.4</url>
      <bibkey>jin-etal-2022-causalnlp</bibkey>
    </paper>
    <paper id="5">
      <title>Emergent Language-Based Coordination In Deep Multi-Agent Systems</title>
      <author><first>Marco</first><last>Baroni</last></author>
      <author><first>Roberto</first><last>Dessi</last></author>
      <author><first>Angeliki</first><last>Lazaridou</last></author>
      <pages>23-29</pages>
      <abstract>Large pre-trained deep networks are the standard building blocks of modern AI applications. This raises fundamental questions about how to control their behaviour and how to make them efficiently interact with each other. Deep net emergent communication tackles these challenges by studying how to induce communication protocols between neural network agents, and how to include humans in the communication loop. Traditionally, this research had focussed on relatively small-scale experiments where two networks had to develop a discrete code from scratch for referential communication. However, with the rise of large pre-trained language models that can work well on many tasks, the emphasis is now shifting on how to let these models interact through a language-like channel to engage in more complex behaviors. By reviewing several representative papers, we will provide an introduction to deep net emergent communication, we will cover various central topics from the present and recent past, as well as discussing current shortcomings and suggest future directions. The presentation is complemented by a hands-on section where participants will implement and analyze two emergent communications setups from the literature. The tutorial should be of interest to researchers wanting to develop more flexible AI systems, but also to cognitive scientists and linguists interested in the evolution of communication systems.</abstract>
      <url hash="55b474b9">2022.emnlp-tutorials.5</url>
      <bibkey>baroni-etal-2022-emergent</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>A</fixed-case>rabic Natural Language Processing</title>
      <author><first>Nizar</first><last>Habash</last></author>
      <pages>30-35</pages>
      <abstract>The Arabic language continues to be the focus of an increasing number of projects in natural language processing (NLP) and computational linguistics (CL). This tutorial provides NLP/CL system developers and researchers (computer scientists and linguists alike) with the necessary background information for working with Arabic in its various forms: Classical, Modern Standard and Dialectal. We discuss various Arabic linguistic phenomena and review the state-of-the-art in Arabic processing from enabling technologies and resources, to common tasks and applications. The tutorial will explain important concepts, common wisdom, and common pitfalls in Arabic processing. Given the wide range of possible issues, we invite tutorial attendees to bring up interesting challenges and problems they are working on to discuss during the tutorial.</abstract>
      <url hash="4e54867c">2022.emnlp-tutorials.6</url>
      <bibkey>habash-2022-arabic</bibkey>
    </paper>
  </volume>
  <event id="emnlp-2022">
    <colocated>2022.findings-emnlp</colocated>
    <colocated>2022.conll-1</colocated>
    <colocated>2022.blackboxnlp-1</colocated>
    <colocated>2022.case-1</colocated>
  </event>
</collection>
