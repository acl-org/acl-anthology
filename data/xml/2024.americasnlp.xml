<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.americasnlp">
  <volume id="1" ingest-date="2024-06-21" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 4th Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP 2024)</booktitle>
      <editor><first>Manuel</first><last>Mager</last></editor>
      <editor><first>Abteen</first><last>Ebrahimi</last></editor>
      <editor><first>Shruti</first><last>Rijhwani</last></editor>
      <editor><first>Arturo</first><last>Oncevay</last></editor>
      <editor><first>Luis</first><last>Chiruzzo</last></editor>
      <editor><first>Robert</first><last>Pugh</last></editor>
      <editor><first>Katharina</first><last>von der Wense</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Mexico City, Mexico</address>
      <month>June</month>
      <year>2024</year>
      <url hash="fee45f5e">2024.americasnlp-1</url>
      <venue>americasnlp</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="1d5f1e96">2024.americasnlp-1.0</url>
      <bibkey>americasnlp-2024-natural</bibkey>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>NLP</fixed-case> for Language Documentation: Two Reasons for the Gap between Theory and Practice</title>
      <author><first>Luke</first><last>Gessler</last><affiliation>University of Colorado, Boulder</affiliation></author>
      <author><first>Katharina</first><last>von der Wense</last><affiliation>University of Colorado Boulder</affiliation></author>
      <pages>1-6</pages>
      <abstract>Both NLP researchers and linguists have expressed a desire to use language technologies in language documentation, but most documentary work still proceeds without them, presenting a lost opportunity to hasten the preservation of the world’s endangered languages, such as those spoken in Latin America. In this work, we empirically measure two factors that have previously been identified as explanations of this low utilization: curricular offerings in graduate programs, and rates of interdisciplinary collaboration in publications related to NLP in language documentation. Our findings verify the claim that interdisciplinary training and collaborations are scarce and support the view that interdisciplinary curricular offerings facilitate interdisciplinary collaborations.</abstract>
      <url hash="a13f5591">2024.americasnlp-1.1</url>
      <bibkey>gessler-von-der-wense-2024-nlp</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.1</doi>
    </paper>
    <paper id="2">
      <title>Translation systems for low-resource Colombian Indigenous languages, a first step towards cultural preservation</title>
      <author><first>Juan</first><last>Prieto</last><affiliation>Universidad de Los Andes</affiliation></author>
      <author><first>Cristian</first><last>Martinez</last><affiliation>Universidad de Los Andes</affiliation></author>
      <author><first>Melissa</first><last>Robles</last><affiliation>Universidad de Los Andes</affiliation></author>
      <author><first>Alberto</first><last>Moreno</last><affiliation>Universidad de Los Andes</affiliation></author>
      <author><first>Sara</first><last>Palacios</last><affiliation>Universidad de Los Andes</affiliation></author>
      <author><first>Rubén</first><last>Manrique</last><affiliation>Universidad de Los Andes</affiliation></author>
      <pages>7-14</pages>
      <abstract>The use of machine learning and Natural Language Processing (NLP) technologies can assist in the preservation and revitalization of indigenous languages, particularly those classified as “low-resource.” Given the increasing digitization of information, the development of translation tools for these languages is of significant importance. These tools not only facilitate better access to digital resources for indigenous communities but also stimulate language preservation efforts and potentially foster more inclusive, equitable societies, as demonstrated by the AmericasNLP workshop since 2021. The focus of this paper is Colombia, a country home to 65 distinct indigenous languages, presenting a vast spectrum of linguistic characteristics. This cultural and linguistic diversity is an inherent pillar of the nation’s identity, and safeguarding it has been increasingly challenging given the dwindling number of native speakers and the communities’ inclination towards oral traditions. Considering this context, scattered initiatives exist to develop translation systems for these languages. However, these endeavors suffer from a lack of consolidated, comparable data. This paper consolidates a dataset of parallel data in four Colombian indigenous languages - Wayuunaiki, Arhuaco, Inga, and Nasa - gathered from existing digital resources. It also presents the creation of baseline models for future translation and comparison, ultimately serving as a catalyst for incorporating more digital resources progressively.</abstract>
      <url hash="58207cec">2024.americasnlp-1.2</url>
      <attachment type="SupplementaryMaterial" hash="465f4638">2024.americasnlp-1.2.SupplementaryMaterial.zip</attachment>
      <bibkey>prieto-etal-2024-translation</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.2</doi>
    </paper>
    <paper id="3">
      <title>Word-level prediction in <fixed-case>P</fixed-case>lains <fixed-case>C</fixed-case>ree: First steps</title>
      <author><first>Olga</first><last>Kriukova</last><affiliation>University of Saskatchewan</affiliation></author>
      <author><first>Antti</first><last>Arppe</last><affiliation>University of Alberta</affiliation></author>
      <pages>15-23</pages>
      <abstract>Plains Cree (nêhiyawêwin) is a morphologically complex and predominantly prefixing language. The combinatory potential of inflectional and derivational/lexical prefixes and verb stems in Plains Cree makes it challenging for traditional auto-completion (or word suggestion) approaches to handle. The lack of a large corpus of Plains Cree also complicates the situation. This study attempts to investigate how well a BiLSTM model trained on a small Cree corpus can handle a word suggestion task. Moreover, this study evaluates whether the use of semantically and morphosyntactically refined Word2Vec embeddings can improve the overall accuracy and quality of BiLSTM suggestions. The results show that some models trained with the refined vectors provide semantically and morphosyntactically better suggestions. They are also more accurate in predictions of content words. The model trained with the non-refined vectors, in contrast, was better at predicting conjunctions, particles, and other non-inflecting words. The models trained with different refined vector combinations provide the expected next word among top-10 predictions in 36.73 to 37.88% of cases (depending on the model).</abstract>
      <url hash="e6730ca0">2024.americasnlp-1.3</url>
      <bibkey>kriukova-arppe-2024-word</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.3</doi>
    </paper>
    <paper id="4">
      <title>Mapping ‘when’-clauses in <fixed-case>L</fixed-case>atin <fixed-case>A</fixed-case>merican and <fixed-case>C</fixed-case>aribbean languages: an experiment in subtoken-based typology</title>
      <author><first>Nilo</first><last>Pedrazzini</last><affiliation>The Alan Turing Institute</affiliation></author>
      <pages>24-33</pages>
      <abstract>Languages can encode temporal subordination lexically, via subordinating conjunctions, and morphologically, by marking the relation on the predicate. Systematic cross-linguistic variation among the former can be studied using well-established token-based typological approaches to token-aligned parallel corpora. Variation among different morphological means is instead much harder to tackle and therefore more poorly understood, despite being predominant in several language groups. This paper explores variation in the expression of generic temporal subordination (‘when’-clauses) among the languages of Latin America and the Caribbean, where morphological marking is particularly common. It presents probabilistic semantic maps computed on the basis of the languages of the region, thus avoiding bias towards the many world’s languages that exclusively use lexified connectors, incorporating associations between character in/i-grams and English iwhen/i. The approach allows capturing morphological clause-linkage devices in addition to lexified connectors, paving the way for larger-scale, strategy-agnostic analyses of typological variation in temporal subordination.</abstract>
      <url hash="51fcb7c5">2024.americasnlp-1.4</url>
      <attachment type="SupplementaryMaterial" hash="b615ad57">2024.americasnlp-1.4.SupplementaryMaterial.zip</attachment>
      <bibkey>pedrazzini-2024-mapping</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.4</doi>
      <revision id="1" href="2024.americasnlp-1.4v1" hash="79db931f"/>
      <revision id="2" href="2024.americasnlp-1.4v2" hash="51fcb7c5" date="2024-10-18">Compressed PDF version.</revision>
    </paper>
    <paper id="5">
      <title>Comparing <fixed-case>LLM</fixed-case> prompting with Cross-lingual transfer performance on Indigenous and Low-resource <fixed-case>B</fixed-case>razilian Languages</title>
      <author><first>David Ifeoluwa</first><last>Adelani</last><affiliation>University College London</affiliation></author>
      <author><first>A. Seza</first><last>Doğruöz</last><affiliation>Universiteit Gent</affiliation></author>
      <author><first>André</first><last>Coneglian</last><affiliation>Federal university of Minas gerais</affiliation></author>
      <author><first>Atul Kr.</first><last>Ojha</last><affiliation>Data Science Institute, Unit for Linguistic Data, University of Galway</affiliation></author>
      <pages>34-41</pages>
      <abstract>Large Language Models are transforming NLP for a lot of tasks. However, how LLMs perform NLP tasks for LRLs is less explored. In alliance with the theme track of the NAACL’24, we focus on 12 low-resource languages (LRLs) from Brazil, 2 LRLs from Africa and 2 high-resource languages (HRLs) (e.g., English and Brazilian Portuguese). Our results indicate that the LLMs perform worse for the labeling of LRLs in comparison to HRLs in general. We explain the reasons behind this failure and provide an error analyses through examples from 2 Brazilian LRLs.</abstract>
      <url hash="6c6f245b">2024.americasnlp-1.5</url>
      <bibkey>adelani-etal-2024-comparing</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.5</doi>
    </paper>
    <paper id="6">
      <title>Analyzing Finetuned Vision Models for <fixed-case>M</fixed-case>ixtec Codex Interpretation</title>
      <author><first>Alexander</first><last>Webber</last><affiliation>University of Florida</affiliation></author>
      <author><first>Zachary</first><last>Sayers</last><affiliation>University of Florida</affiliation></author>
      <author><first>Amy</first><last>Wu</last><affiliation>University of Florida</affiliation></author>
      <author><first>Elizabeth</first><last>Thorner</last><affiliation>University of Florida</affiliation></author>
      <author><first>Justin</first><last>Witter</last><affiliation>University of Florida</affiliation></author>
      <author><first>Gabriel</first><last>Ayoubi</last><affiliation>University of Florida</affiliation></author>
      <author><first>Christan</first><last>Grant</last><affiliation>University of Florida</affiliation></author>
      <pages>42-49</pages>
      <abstract>Throughout history, pictorial record-keeping has been used to document events, stories, and concepts. A popular example of this is the Tzolk’in Maya Calendar. The pre-Columbian Mixtec society also recorded many works through graphical media called codices that depict both stories and real events. Mixtec codices are unique because the depicted scenes are highly structured within and across documents. As a first effort toward translation, we created two binary classification tasks over Mixtec codices, namely, gender and pose. The composition of figures within a codex is essential for understanding the codex’s narrative. We labeled a dataset with around 1300 figures drawn from three codices of varying qualities. We finetuned the Visual Geometry Group 16 (VGG-16) and Vision Transformer 16 (ViT-16) models, measured their performance, and compared learned features with expert opinions found in literature. The results show that when finetuned, both VGG and ViT perform well, with the transformer-based architecture (ViT) outperforming the CNN-based architecture (VGG) at higher learning rates. We are releasing this work to allow collaboration with the Mixtec community and domain scientists.</abstract>
      <url hash="528d355d">2024.americasnlp-1.6</url>
      <bibkey>webber-etal-2024-analyzing</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.6</doi>
    </paper>
    <paper id="7">
      <title>A New Benchmark for <fixed-case>K</fixed-case>alaallisut-<fixed-case>D</fixed-case>anish Neural Machine Translation</title>
      <author><first>Ross</first><last>Kristensen-Mclachlan</last><affiliation>Aarhus University</affiliation></author>
      <author><first>Johanne</first><last>Nedergård</last><affiliation>University of Copenhagen</affiliation></author>
      <pages>50-55</pages>
      <abstract>Kalaallisut, also known as (West) Greenlandic, poses a number of unique challenges to contemporary natural language processing (NLP). In particular, the language has historically lacked benchmarking datasets and robust evaluation of specific NLP tasks, such as neural machine translation (NMT). In this paper, we present a new benchmark dataset for Greenlandic to Danish NMT comprising over 1.2m words of Greenlandic and 2.1m words of parallel Danish translations. We provide initial metrics for models trained on this dataset and conclude by suggesting how these findings can be taken forward to other NLP tasks for the Greenlandic language.</abstract>
      <url hash="c85c6cc9">2024.americasnlp-1.7</url>
      <attachment type="SupplementaryMaterial" hash="2e7e08a7">2024.americasnlp-1.7.SupplementaryMaterial.zip</attachment>
      <bibkey>kristensen-mclachlan-nedergard-2024-new</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.7</doi>
    </paper>
    <paper id="8">
      <title>Morphological Tagging in <fixed-case>B</fixed-case>ribri Using <fixed-case>U</fixed-case>niversal <fixed-case>D</fixed-case>ependency Features</title>
      <author><first>Jessica</first><last>Karson</last><affiliation>University of Zürich</affiliation></author>
      <author><first>Rolando</first><last>Coto-Solano</last><affiliation>Dartmouth College</affiliation></author>
      <pages>56-66</pages>
      <abstract>This paper outlines the Universal Features tagging of a dependency treebank for Bribri, an Indigenous language of Costa Rica. Universal Features are a morphosyntactic tagging component of Universal Dependencies, which is a framework that aims to provide an annotation system inclusive of all languages and their diverse structures (Nivre et al., 2016; de Marneffe et al., 2021). We used a rule-based system to do a first-pass tagging of a treebank of 1572 words. After manual corrections, the treebank contained 3051 morphological features. We then used this morphologically-tagged treebank to train a UDPipe 2 parsing and tagging model. This model has a UFEATS precision of 80.5 ± 3.6, which is a statistically significant improvement upon the previously available FOMA-based morphological tagger for Bribri. An error analysis suggests that missing TAM and case markers are the most common problem for the model. We hope to use this model to expand upon existing treebanks and facilitate the construction of linguistically-annotated corpora for the language.</abstract>
      <url hash="aa7f52ce">2024.americasnlp-1.8</url>
      <bibkey>karson-coto-solano-2024-morphological</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.8</doi>
    </paper>
    <paper id="9">
      <title><fixed-case>LLM</fixed-case>-Assisted Rule Based Machine Translation for Low/No-Resource Languages</title>
      <author><first>Jared</first><last>Coleman</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Bhaskar</first><last>Krishnamachari</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Ruben</first><last>Rosales</last><affiliation>Independent Researcher</affiliation></author>
      <author><first>Khalil</first><last>Iskarous</last><affiliation>University of Southern California</affiliation></author>
      <pages>67-87</pages>
      <abstract>We propose a new paradigm for machine translation that is particularly useful for no-resource languages (those without any publicly available bilingual or monolingual corpora): LLM-RBMT (LLM-Assisted Rule Based Machine Translation). Using the LLM-RBMT paradigm, we design the first language education/revitalization-oriented machine translator for Owens Valley Paiute (OVP), a critically endangered Indigenous American language for which there is virtually no publicly available data. We present a detailed evaluation of the translator’s components: a rule-based sentence builder, an OVP to English translator, and an English to OVP translator. We also discuss the potential of the paradigm, its limitations, and the many avenues for future research that it opens up.</abstract>
      <url hash="1781b341">2024.americasnlp-1.9</url>
      <attachment type="SupplementaryMaterial" hash="4e941c22">2024.americasnlp-1.9.SupplementaryMaterial.zip</attachment>
      <bibkey>coleman-etal-2024-llm</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.9</doi>
    </paper>
    <paper id="10">
      <title>A Concise Survey of <fixed-case>OCR</fixed-case> for Low-Resource Languages</title>
      <author><first>Milind</first><last>Agarwal</last><affiliation>George Mason University</affiliation></author>
      <author><first>Antonios</first><last>Anastasopoulos</last><affiliation>George Mason University</affiliation></author>
      <pages>88-102</pages>
      <abstract>Modern natural language processing (NLP) techniques increasingly require substantial amounts of data to train robust algorithms. Building such technologies for low-resource languages requires focusing on data creation efforts and data-efficient algorithms. For a large number of low-resource languages, especially Indigenous languages of the Americas, this data exists in image-based non-machine-readable documents. This includes scanned copies of comprehensive dictionaries, linguistic field notes, children’s stories, and other textual material. To digitize these resources, Optical Character Recognition (OCR) has played a major role but it comes with certain challenges in low-resource settings. In this paper, we share the first survey of OCR techniques specific to low-resource data creation settings and outline several open challenges, with a special focus on Indigenous Languages of the Americas. Based on experiences and results from previous research, we conclude with recommendations on utilizing and improving OCR for the benefit of computational researchers, linguists, and language communities.</abstract>
      <url hash="8559bd07">2024.americasnlp-1.10</url>
      <bibkey>agarwal-anastasopoulos-2024-concise</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.10</doi>
    </paper>
    <paper id="11">
      <title>Unlocking Knowledge with <fixed-case>OCR</fixed-case>-Driven Document Digitization for <fixed-case>P</fixed-case>eruvian Indigenous Languages</title>
      <author><first>Shadya</first><last>Sanchez Carrera</last><affiliation>Pontificia Universidad Catolica del Peru</affiliation></author>
      <author><first>Roberto</first><last>Zariquiey</last><affiliation>Pontificia Universidad Catolica del Peru</affiliation></author>
      <author><first>Arturo</first><last>Oncevay</last><affiliation>The University of Edinburgh</affiliation></author>
      <pages>103-111</pages>
      <abstract>The current focus on resource-rich languages poses a challenge to linguistic diversity, affecting minority languages with limited digital presence and relatively old published and unpublished resources. In addressing this issue, this study targets the digitalization of old scanned textbooks written in four Peruvian indigenous languages (Asháninka, Shipibo-Konibo, Yanesha, and Yine) using Optical Character Recognition (OCR) technology. This is complemented with text correction methods to minimize extraction errors. Contributions include the creation of an annotated dataset with 454 scanned page images, for a rigorous evaluation, and the development of a module to correct OCR-generated transcription alignments.</abstract>
      <url hash="0cd7af0f">2024.americasnlp-1.11</url>
      <attachment type="SupplementaryMaterial" hash="cf8aa539">2024.americasnlp-1.11.SupplementaryMaterial.zip</attachment>
      <bibkey>sanchez-carrera-etal-2024-unlocking</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.11</doi>
    </paper>
    <paper id="12">
      <title>Awajun-<fixed-case>OP</fixed-case>: Multi-domain dataset for <fixed-case>S</fixed-case>panish–Awajun Machine Translation</title>
      <author><first>Oscar</first><last>Moreno</last><affiliation>Pontifical Catholic University of Peru</affiliation></author>
      <author><first>Yanua</first><last>Atamain</last><affiliation>Instituto de Defensa Legal del Ambiente y Desarrollo Sostenible - IDLADS PERÚ</affiliation></author>
      <author><first>Arturo</first><last>Oncevay</last><affiliation>The University of Edinburgh</affiliation></author>
      <pages>112-120</pages>
      <abstract>We introduce a Spanish-Awajun parallel dataset of 22k high-quality sentence pairs with the help of the journalistic organization Company C. This dataset consists of parallel data obtained from various web sources such as poems, stories, laws, protocols, guidelines, handbooks, the Bible, and news published by Company C. The study also includes an analysis of the dataset’s performance for Spanish-Awajun translation using a Transformer architecture with transfer learning from a parent model, utilizing Spanish-English and Spanish-Finnish as high-resource language-pairs. As far as we know, this is the first Spanish-Awajun machine translation study, and we hope that this work will serve as a starting point for future research on this neglected Peruvian language.</abstract>
      <url hash="d4e7a522">2024.americasnlp-1.12</url>
      <attachment type="SupplementaryMaterial" hash="31cf38be">2024.americasnlp-1.12.SupplementaryMaterial.zip</attachment>
      <bibkey>moreno-etal-2024-awajun</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.12</doi>
    </paper>
    <paper id="13">
      <title>Wav2pos: Exploring syntactic analysis from audio for <fixed-case>H</fixed-case>ighland <fixed-case>P</fixed-case>uebla <fixed-case>N</fixed-case>ahuatl</title>
      <author><first>Robert</first><last>Pugh</last><affiliation>Indiana University</affiliation></author>
      <author><first>Varun</first><last>Sreedhar</last><affiliation>Indiana University</affiliation></author>
      <author><first>Francis</first><last>Tyers</last><affiliation>Indiana University</affiliation></author>
      <pages>121-126</pages>
      <abstract>We describe an approach to part-of-speech tagging from audio with very little human-annotated data, for Highland Puebla Nahuatl, a low-resource language of Mexico. While automatic morphosyntactic analysis is typically trained on annotated textual data, large amounts of text is rarely available for low-resource, marginalized, and/or minority languages, and morphosyntactically-annotated data is even harder to come by. Much of the data from these languages may exist in the form of recordings, often only partially-transcribed or analyzed by field linguists working on language documentation projects. Given this relatively low-availability of text in the low-resource language scenario, we explore end-to-end automated morphosyntactic analysis directly from audio. The experiments described in this paper focus on one piece of morphosyntax, part-of-speech tagging, and builds on existing work in a high-resource setting. We use weak supervision to increase training volume, and explore a few techniques for generating word-level predictions from the acoustic features. Our experiments show promising results, despite less than 400 sentences of audio-aligned, manually-labeled text.</abstract>
      <url hash="32285a23">2024.americasnlp-1.13</url>
      <bibkey>pugh-etal-2024-wav2pos</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.13</doi>
    </paper>
    <paper id="14">
      <title>From Field Linguistics to <fixed-case>NLP</fixed-case>: Creating a curated dataset in Amuzgo language</title>
      <author><first>Antonio</first><last>Reyes</last><affiliation>Universidad Autónoma de Querétaro</affiliation></author>
      <author><first>Hamlet Antonio</first><last>García</last><affiliation>INAH</affiliation></author>
      <pages>127-131</pages>
      <abstract>This article presents an ongoing research on one of the several native languages of the Americas: Amuzgo or jny’on3 nda3 . This language is spoken in Southern Mexico and belongs to the Otomanguean family. Although Amuzgo vitality is stable and there are some available resources, such as grammars, dictionaries, or literature, its digital inclusion is emerging (cf. Eberhard et al. (2024)). In this respect, here is described the creation of a curated dataset in Amuzgo. This resource is intended to contribute the development of tools for scarce resources languages by providing fine-grained linguistic information in different layers: From data collection with native speakers to data annotation. The dataset was built according to the following method: i) data collection in Amuzgo by means of linguistic fieldwork; ii) acoustic data processing; iii) data transcription; iv) glossing and translating data into Spanish; v) semiautomatic alignment of translations; and vi) data systematization. This resource is released as an open access dataset to foster the academic community to explore the richness of this language.</abstract>
      <url hash="5a4f0230">2024.americasnlp-1.14</url>
      <attachment type="SupplementaryMaterial" hash="41d81d6a">2024.americasnlp-1.14.SupplementaryMaterial.zip</attachment>
      <bibkey>reyes-garcia-2024-field</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.14</doi>
    </paper>
    <paper id="15">
      <title>Enenlhet as a case-study to investigate <fixed-case>ASR</fixed-case> model generalizability for language documentation</title>
      <author><first>Éric</first><last>Le Ferrand</last><affiliation>Boston College</affiliation></author>
      <author><first>Raina</first><last>Heaton</last><affiliation>University of Oklahoma</affiliation></author>
      <author><first>Emily</first><last>Prud’hommeaux</last><affiliation>Boston College</affiliation></author>
      <pages>132-137</pages>
      <abstract>Although both linguists and language community members recognize the potential utility of automatic speech recognition (ASR) for documentation, one of the obstacles to using these technologies is the scarcity of data necessary to train effective systems. Recent advances in ASR, particularly the ability to fine-tune large multilingual acoustic models to small amounts of data from a new language, have demonstrated the potential of ASR for transcription. However, many proof-of-concept demonstrations of ASR in low-resource settings rely on a single data collection project, which may yield models that are biased toward that particular data scenario, whether in content, recording quality, transcription conventions, or speaker population. In this paper, we investigate the performance of two state-of-the art ASR architectures for fine-tuning acoustic models to small speech datasets with the goal of transcribing recordings of Enenlhet, an endangered Indigenous language spoken in South America. Our results suggest that while ASR offers utility for generating first-pass transcriptions of speech collected in the course of linguistic fieldwork, individual vocabulary diversity and data quality have an outsized impact on ASR accuracy.</abstract>
      <url hash="69f1c171">2024.americasnlp-1.15</url>
      <bibkey>le-ferrand-etal-2024-enenlhet</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.15</doi>
    </paper>
    <paper id="16">
      <title>Advancing <fixed-case>NMT</fixed-case> for Indigenous Languages: A Case Study on <fixed-case>Y</fixed-case>ucatec <fixed-case>M</fixed-case>ayan and <fixed-case>C</fixed-case>hol</title>
      <author><first>Julio</first><last>Rangel</last><affiliation>RIKEN</affiliation></author>
      <author><first>Norio</first><last>Kobayashi</last><affiliation>RIKEN</affiliation></author>
      <pages>138-142</pages>
      <abstract>This study leverages Spanish-trained large language models (LLMs) to develop neural machine translation (NMT) systems for Mayan languages. For this, we first compile and process a low-resource dataset of 28,135 translation pairs of Chol and Yucatec Mayan extracted from documents in the CPLM Corpus (Martínez et al.). Then, we implement a prompt-based approach to train one-to-many and many-to-many models. By comparing several training strategies for two LLMs, we found that, on average, training multilingual models is better, as shown by the ChrF++ reaching 50 on the test set in the best case. This study reinforces the viability of using LLMs to improve accessibility and preservation for languages with limited digital resources. We share our code, datasets, and models to promote collaboration and progress in this field: https://github.com/RIKEN-DKO/iikim_translator.</abstract>
      <url hash="480859f3">2024.americasnlp-1.16</url>
      <bibkey>rangel-kobayashi-2024-advancing</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.16</doi>
    </paper>
    <paper id="17">
      <title><fixed-case>BSC</fixed-case> Submission to the <fixed-case>A</fixed-case>mericas<fixed-case>NLP</fixed-case> 2024 Shared Task</title>
      <author><first>Javier</first><last>Garcia Gilabert</last><affiliation>Barcelona Super Computing Center</affiliation></author>
      <author><first>Aleix</first><last>Sant</last><affiliation>Barcelona Supercomputing Center</affiliation></author>
      <author><first>Carlos</first><last>Escolano</last><affiliation>Universitat PolitÃ ̈cnica de Catalunya, Barcelona Supercomputing Center</affiliation></author>
      <author><first>Francesca</first><last>De Luca Fornaciari</last><affiliation>BSC Barcelona Supercomputing Center</affiliation></author>
      <author><first>Audrey</first><last>Mash</last><affiliation>BSC</affiliation></author>
      <author><first>Maite</first><last>Melero</last><affiliation>Barcelona Supercomputing Center</affiliation></author>
      <pages>143-149</pages>
      <abstract>This paper describes the BSC’s submission to the AmericasNLP 2024 Shared Task. We participated in the Spanish to Quechua and Spanish to Guarani tasks. In this paper we show that by using LoRA adapters we can achieve similar performance as a full parameter fine-tuning by only training 14.2% of the total number of parameters. Our systems achieved the highest ChrF++ scores and ranked first for both directions in the final results outperforming strong baseline systems in the provided development and test datasets.</abstract>
      <url hash="79d7b285">2024.americasnlp-1.17</url>
      <bibkey>garcia-gilabert-etal-2024-bsc</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.17</doi>
    </paper>
    <paper id="18">
      <title>System Description of the <fixed-case>N</fixed-case>ordics<fixed-case>A</fixed-case>lps Submission to the <fixed-case>A</fixed-case>mericas<fixed-case>NLP</fixed-case> 2024 Machine Translation Shared Task</title>
      <author><first>Joseph</first><last>Attieh</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Zachary</first><last>Hopton</last><affiliation>University of Zurich</affiliation></author>
      <author><first>Yves</first><last>Scherrer</last><affiliation>University of Oslo</affiliation></author>
      <author><first>Tanja</first><last>Samardžić</last><affiliation>University of Zurich</affiliation></author>
      <pages>150-158</pages>
      <abstract>This paper presents the system description of the NordicsAlps team for the AmericasNLP 2024 Machine Translation Shared Task 1. We investigate the effect of tokenization on translation quality by exploring two different tokenization schemes: byte-level and redundancy-driven tokenization. We submitted three runs per language pair. The redundancy-driven tokenization ranked first among all submissions, scoring the highest average chrF2++, chrF, and BLEU metrics (averaged across all languages). These findings demonstrate the importance of carefully tailoring the tokenization strategies of machine translation systems, particularly in resource-constrained scenarios.</abstract>
      <url hash="a74fe49e">2024.americasnlp-1.18</url>
      <bibkey>attieh-etal-2024-system</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.18</doi>
    </paper>
    <paper id="19">
      <title>On the Robustness of Neural Models for Full Sentence Transformation</title>
      <author><first>Michael</first><last>Ginn</last><affiliation>University of Colorado</affiliation></author>
      <author><first>Ali</first><last>Marashian</last><affiliation>University of Colorado at Boulder</affiliation></author>
      <author><first>Bhargav</first><last>Shandilya</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Claire</first><last>Post</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Enora</first><last>Rice</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Juan</first><last>Vásquez</last><affiliation>Department of Computer Science, University of Colorado Boulder</affiliation></author>
      <author><first>Marie</first><last>Mcgregor</last><affiliation>University of Coloado Boulder</affiliation></author>
      <author><first>Matthew</first><last>Buchholz</last><affiliation>University of Colorado–Boulder</affiliation></author>
      <author><first>Mans</first><last>Hulden</last><affiliation>University of Colorado</affiliation></author>
      <author><first>Alexis</first><last>Palmer</last><affiliation>University of Colorado Boulder</affiliation></author>
      <pages>159-173</pages>
      <abstract>This paper describes the LECS Lab submission to the AmericasNLP 2024 Shared Task on the Creation of Educational Materials for Indigenous Languages. The task requires transforming a base sentence with regards to one or more linguistic properties (such as negation or tense). We observe that this task shares many similarities with the well-studied task of word-level morphological inflection, and we explore whether the findings from inflection research are applicable to this task. In particular, we experiment with a number of augmentation strategies, finding that they can significantly benefit performance, but that not all augmented data is necessarily beneficial. Furthermore, we find that our character-level neural models show high variability with regards to performance on unseen data, and may not be the best choice when training data is limited.</abstract>
      <url hash="e8687ded">2024.americasnlp-1.19</url>
      <attachment type="SupplementaryMaterial" hash="b81b6026">2024.americasnlp-1.19.SupplementaryMaterial.zip</attachment>
      <bibkey>ginn-etal-2024-robustness</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.19</doi>
    </paper>
    <paper id="20">
      <title>The unreasonable effectiveness of large language models for low-resource clause-level morphology: In-context generalization or prior exposure?</title>
      <author><first>Coleman</first><last>Haley</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>174-178</pages>
      <abstract>This paper describes the submission of Team “Giving it a Shot” to the AmericasNLP 2024 Shared Task on Creation of Educational Materials for Indigenous Languages. We use a simple few-shot prompting approach with several state of the art large language models, achieving competitive performance on the shared task, with our best system placing third overall. We perform a preliminary analysis to determine to what degree the performance of our model is due to prior exposure to the task languages, finding that generally our performance is better explained as being derived from in-context learning capabilities.</abstract>
      <url hash="55239a87">2024.americasnlp-1.20</url>
      <attachment type="SupplementaryMaterial" hash="a14f4101">2024.americasnlp-1.20.SupplementaryMaterial.zip</attachment>
      <attachment type="SupplementaryMaterial" hash="a14f4101">2024.americasnlp-1.20.SupplementaryMaterial.zip</attachment>
      <bibkey>haley-2024-unreasonable</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.20</doi>
    </paper>
    <paper id="21">
      <title>A Comparison of Fine-Tuning and In-Context Learning for Clause-Level Morphosyntactic Alternation</title>
      <author><first>Jim</first><last>Su</last><affiliation>University of Florida</affiliation></author>
      <author><first>Justin</first><last>Ho</last><affiliation>University of Florida</affiliation></author>
      <author><first>George</first><last>Broadwell</last><affiliation>University of Florida</affiliation></author>
      <author><first>Sarah</first><last>Moeller</last><affiliation>University of Florida</affiliation></author>
      <author><first>Bonnie</first><last>Dorr</last><affiliation>University of Florida</affiliation></author>
      <pages>179-187</pages>
      <abstract>This paper presents our submission to the AmericasNLP 2024 Shared Task on the Creation of Educational Materials for Indigenous Languages. We frame this task as one of morphological inflection generation, treating each sentence as a single word. We investigate and compare two distinct approaches: fine-tuning neural encoder-decoder models such as NLLB- 200, and in-context learning with proprietary large language models (LLMs). Our findings demonstrate that for this task, no one approach is perfect. Anthropic’s Claude 3 Opus, when supplied with grammatical description entries, achieves the highest performance on Bribri among the evaluated models. This outcome corroborates and extends previous research exploring the efficacy of in-context learning in low- resource settings. For Maya, fine-tuning NLLB- 200-3.3B using StemCorrupt augmented data yielded the best performance.</abstract>
      <url hash="a7db5b8b">2024.americasnlp-1.21</url>
      <bibkey>su-etal-2024-comparison</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.21</doi>
    </paper>
    <paper id="22">
      <title>Experiments in Mamba Sequence Modeling and <fixed-case>NLLB</fixed-case>-200 Fine-Tuning for Low Resource Multilingual Machine Translation</title>
      <author><first>Dan</first><last>Degenaro</last><affiliation>Department of Linguistics, Georgetown University</affiliation></author>
      <author><first>Tom</first><last>Lupicki</last><affiliation>Department of Computer Science, Georgetown University</affiliation></author>
      <pages>188-194</pages>
      <abstract>This paper presents DC_DMV’s submission to the AmericasNLP 2024 Shared Task 1: Machine Translation Systems for Indigenous Languages. Our submission consists of two multilingual approaches to building machine translation systems from Spanish to eleven Indigenous languages: fine-tuning the 600M distilled variant of NLLB-200, and an experiment in training from scratch a neural network using the Mamba State Space Modeling architecture. We achieve the best results on the test set for a total of 4 of the language pairs between two checkpoints by fine-tuning NLLB-200, and outperform the baseline score on the test set for 2 languages.</abstract>
      <url hash="31060519">2024.americasnlp-1.22</url>
      <attachment type="SupplementaryMaterial" hash="c6f133e6">2024.americasnlp-1.22.SupplementaryMaterial.zip</attachment>
      <bibkey>degenaro-lupicki-2024-experiments</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.22</doi>
    </paper>
    <paper id="23">
      <title><fixed-case>JGU</fixed-case> Mainz’s Submission to the <fixed-case>A</fixed-case>mericas<fixed-case>NLP</fixed-case> 2024 Shared Task on the Creation of Educational Materials for Indigenous Languages</title>
      <author><first>Minh Duc</first><last>Bui</last><affiliation>University of Mainz</affiliation></author>
      <author><first>Katharina</first><last>von der Wense</last><affiliation>University of Colorado Boulder</affiliation></author>
      <pages>195-200</pages>
      <abstract>In this paper, we present the four systems developed by the Meenzer team from JGU for the AmericasNLP 2024 shared task on the creation of educational materials for Indigenous languages. The task involves accurately applying specific grammatical modifications to given source sentences across three low-resource Indigenous languages: Bribri, Guarani, and Maya. We train two types of model architectures: finetuning a sequence-to-sequence pointer-generator LSTM and finetuning the Mixtral 8x7B model by incorporating in-context examples into the training phase. System 1, an ensemble combining finetuned LSTMs, finetuned Mixtral models, and GPT-4, achieves the best performance on Guarani. Meanwhile, system 4, another ensemble consisting solely of fine-tuned Mixtral models, outperforms all other teams on Maya and secures the second place overall. Additionally, we conduct an ablation study to understand the performance of our system 4.</abstract>
      <url hash="7d65f866">2024.americasnlp-1.23</url>
      <bibkey>bui-von-der-wense-2024-jgu</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.23</doi>
    </paper>
    <paper id="24">
      <title>Applying Linguistic Expertise to <fixed-case>LLM</fixed-case>s for Educational Material Development in Indigenous Languages</title>
      <author><first>Justin</first><last>Vasselli</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Arturo</first><last>Martínez Peguero</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Junehwan</first><last>Sung</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <author><first>Taro</first><last>Watanabe</last><affiliation>Nara Institute of Science and Technology</affiliation></author>
      <pages>201-208</pages>
      <abstract>This paper presents our approach to the AmericasNLP 2024 Shared Task 2 as the JAJ (/dʒæz/) team. The task aimed at creating educational materials for indigenous languages, and we focused on Maya and Bribri. Given the unique linguistic features and challenges of these languages, and the limited size of the training datasets, we developed a hybrid methodology combining rule-based NLP methods with prompt-based techniques. This approach leverages the meta-linguistic capabilities of large language models, enabling us to blend broad, language-agnostic processing with customized solutions. Our approach lays a foundational framework that can be expanded to other indigenous languages languages in future work.</abstract>
      <url hash="ea9fc434">2024.americasnlp-1.24</url>
      <attachment type="SupplementaryMaterial" hash="705e0f89">2024.americasnlp-1.24.SupplementaryMaterial.zip</attachment>
      <bibkey>vasselli-etal-2024-applying</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.24</doi>
    </paper>
    <paper id="25">
      <title>Exploring Very Low-Resource Translation with <fixed-case>LLM</fixed-case>s: The <fixed-case>U</fixed-case>niversity of <fixed-case>E</fixed-case>dinburgh’s Submission to <fixed-case>A</fixed-case>mericas<fixed-case>NLP</fixed-case> 2024 Translation Task</title>
      <author><first>Vivek</first><last>Iyer</last><affiliation>The University of Edinburgh</affiliation></author>
      <author><first>Bhavitvya</first><last>Malik</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Wenhao</first><last>Zhu</last><affiliation>National Key Laboratory for Novel Software Technology, Nanjing University</affiliation></author>
      <author><first>Pavel</first><last>Stepachev</last><affiliation>The University of Edinburgh</affiliation></author>
      <author><first>Pinzhen</first><last>Chen</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Barry</first><last>Haddow</last><affiliation>University of Edinburgh</affiliation></author>
      <author><first>Alexandra</first><last>Birch</last><affiliation>University of Edinburgh</affiliation></author>
      <pages>209-220</pages>
      <abstract>This paper describes the University of Edinburgh’s submission to the AmericasNLP 2024 shared task on the translation of Spanish into 11 indigenous American languages. We explore the ability of multilingual Large Language Models (LLMs) to model low-resource languages by continued pre-training with LoRA, and conduct instruction fine-tuning using a variety of datasets, demonstrating that this improves LLM performance. Furthermore, we demonstrate the efficacy of checkpoint averaging alongside decoding techniques like beam search and sampling, resulting in further improvements. We participate in all 11 translation directions.</abstract>
      <url hash="1f0a7dfd">2024.americasnlp-1.25</url>
      <attachment type="SupplementaryMaterial" hash="7df0ad73">2024.americasnlp-1.25.SupplementaryMaterial.zip</attachment>
      <bibkey>iyer-etal-2024-exploring</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.25</doi>
    </paper>
    <paper id="26">
      <title>The role of morphosyntactic similarity in generating related sentences</title>
      <author><first>Michael</first><last>Hammond</last><affiliation>University of Arizona</affiliation></author>
      <pages>221-223</pages>
      <abstract>In this paper we describe our work on Task~2: Creation of Educational Materials. We tried three approaches, but only the third approach yielded improvement over the baseline system. The first system was a fairly generic transformer model. The second system was our own implementation of the edit tree approach from the baseline system. Our final attempt was a version of the baseline system where if no transformation succeeded, we applied transformations from similar morphosyntactic relations. We describe all three here, but, in the end, we only submitted the third system.</abstract>
      <url hash="0bd8d066">2024.americasnlp-1.26</url>
      <bibkey>hammond-2024-role</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.26</doi>
    </paper>
    <paper id="27">
      <title>Findings of the <fixed-case>A</fixed-case>mericas<fixed-case>NLP</fixed-case> 2024 Shared Task on the Creation of Educational Materials for Indigenous Languages</title>
      <author><first>Luis</first><last>Chiruzzo</last><affiliation>Universidad de la Republica</affiliation></author>
      <author><first>Pavel</first><last>Denisov</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Alejandro</first><last>Molina-Villegas</last><affiliation>CentroGEO, Mexico</affiliation></author>
      <author><first>Silvia</first><last>Fernandez-Sabido</last><affiliation>CentroGEO, Mexico</affiliation></author>
      <author><first>Rolando</first><last>Coto-Solano</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Marvin</first><last>Agüero-Torales</last><affiliation>University of Granada</affiliation></author>
      <author><first>Aldo</first><last>Alvarez</last><affiliation>Universidad de Itapua</affiliation></author>
      <author><first>Samuel</first><last>Canul-Yah</last><affiliation>Independent linguist</affiliation></author>
      <author><first>Lorena</first><last>Hau-Ucán</last><affiliation>SEDECULTA</affiliation></author>
      <author><first>Abteen</first><last>Ebrahimi</last><affiliation>University of Colorado, Boulder</affiliation></author>
      <author><first>Robert</first><last>Pugh</last><affiliation>Indiana University</affiliation></author>
      <author><first>Arturo</first><last>Oncevay</last><affiliation>The University of Edinburgh</affiliation></author>
      <author><first>Shruti</first><last>Rijhwani</last><affiliation>Google</affiliation></author>
      <author><first>Katharina</first><last>von der Wense</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Manuel</first><last>Mager</last><affiliation>Amazon AWS</affiliation></author>
      <pages>224-235</pages>
      <abstract>This paper presents the results of the first shared task about the creation of educational materials for three indigenous languages of the Americas.The task proposes to automatically generate variations of sentences according to linguistic features that could be used for grammar exercises.The languages involved in this task are Bribri, Maya, and Guarani.Seven teams took part in the challenge, submitting a total of 22 systems, obtaining very promising results.</abstract>
      <url hash="ee4667ae">2024.americasnlp-1.27</url>
      <bibkey>chiruzzo-etal-2024-findings</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.27</doi>
    </paper>
    <paper id="28">
      <title>Findings of the <fixed-case>A</fixed-case>mericas<fixed-case>NLP</fixed-case> 2024 Shared Task on Machine Translation into Indigenous Languages</title>
      <author><first>Abteen</first><last>Ebrahimi</last><affiliation>University of Colorado, Boulder</affiliation></author>
      <author><first>Ona</first><last>de Gibert</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Raul</first><last>Vazquez</last><affiliation>University of Helsinki</affiliation></author>
      <author><first>Rolando</first><last>Coto-Solano</last><affiliation>Dartmouth College</affiliation></author>
      <author><first>Pavel</first><last>Denisov</last><affiliation>University of Stuttgart</affiliation></author>
      <author><first>Robert</first><last>Pugh</last><affiliation>Indiana University</affiliation></author>
      <author><first>Manuel</first><last>Mager</last><affiliation>Amazon AWS</affiliation></author>
      <author><first>Arturo</first><last>Oncevay</last><affiliation>The University of Edinburgh</affiliation></author>
      <author><first>Luis</first><last>Chiruzzo</last><affiliation>Universidad de la Republica</affiliation></author>
      <author><first>Katharina</first><last>von der Wense</last><affiliation>University of Colorado Boulder</affiliation></author>
      <author><first>Shruti</first><last>Rijhwani</last><affiliation>Google</affiliation></author>
      <pages>236-246</pages>
      <abstract>This paper presents the findings of the third iteration of the AmericasNLP Shared Task on Machine Translation. This year’s competition features eleven Indigenous languages found across North, Central, and South America. A total of six teams participate with a total of 157 submissions across all languages and models. Two baselines – the Sheffield and Helsinki systems from 2023 – are provided and represent hard-to-beat starting points for the competition. In addition to the baselines, teams are given access to a new repository of training data which consists of data collected by teams in prior shared tasks. Using ChrF++ as the main competition metric, we see improvements over the baseline for 4 languages: Chatino, Guarani, Quechua, and Rarámuri, with performance increases over the best baseline of 4.2 ChrF++. In this work, we present a summary of the submitted systems, results, and a human evaluation of system outputs for Bribri, which consists of both (1) a rating of meaning and fluency and (2) a qualitative error analysis of outputs from the best submitted system.</abstract>
      <url hash="d29b0e56">2024.americasnlp-1.28</url>
      <bibkey>ebrahimi-etal-2024-findings</bibkey>
      <doi>10.18653/v1/2024.americasnlp-1.28</doi>
    </paper>
  </volume>
</collection>
