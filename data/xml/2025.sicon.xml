<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.sicon">
  <volume id="1" ingest-date="2025-07-22" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Third Workshop on Social Influence in Conversations (SICon 2025)</booktitle>
      <editor><first>James</first><last>Hale</last></editor>
      <editor><first>Brian</first><last>Deuksin Kwon</last></editor>
      <editor><first>Ritam</first><last>Dutt</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Vienna, Austria</address>
      <month>July</month>
      <year>2025</year>
      <url hash="ef037b83">2025.sicon-1</url>
      <venue>sicon</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-266-4</isbn>
      <doi>10.18653/v1/2025.sicon-1</doi>
    </meta>
    <frontmatter>
      <url hash="2ce305ef">2025.sicon-1.0</url>
      <bibkey>sicon-ws-2025-1</bibkey>
      <doi>10.18653/v1/2025.sicon-1.0</doi>
    </frontmatter>
    <paper id="1">
      <title><fixed-case>LLM</fixed-case> Roleplay: Simulating Human-Chatbot Interaction</title>
      <author><first>Hovhannes</first><last>Tamoyan</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <author><first>Hendrik</first><last>Schuff</last><affiliation>Zurich Group</affiliation></author>
      <author><first>Iryna</first><last>Gurevych</last><affiliation>Technische Universität Darmstadt</affiliation></author>
      <pages>1-26</pages>
      <abstract>The development of chatbots requires collecting a large number of human-chatbot dialogues to reflect the breadth of users’ sociodemographic backgrounds and conversational goals. However, the resource requirements to conduct the respective user studies can be prohibitively high and often only allow for a narrow analysis of specific dialogue goals and participant demographics. In this paper, we propose LLM Roleplay, the first comprehensive method integrating multi-turn human-chatbot interaction simulation, explicit persona construction from sociodemographic traits, goal-driven dialogue planning, and robust handling of conversational failures, enabling broad utility and reliable dialogue generation. To validate our method, we collect natural human-chatbot dialogues from different sociodemographic groups and conduct a user study to compare these with our generated dialogues. We evaluate the capabilities of state-of-the-art LLMs in maintaining a conversation during their embodiment of a specific persona and find that our method can simulate human-chatbot dialogues with a high indistinguishability rate.</abstract>
      <url hash="d2c19eaa">2025.sicon-1.1</url>
      <bibkey>tamoyan-etal-2025-llm</bibkey>
      <doi>10.18653/v1/2025.sicon-1.1</doi>
    </paper>
    <paper id="2">
      <title>Prompt Refinement or Fine-tuning? Best Practices for using <fixed-case>LLM</fixed-case>s in Computational Social Science Tasks</title>
      <author><first>Anders</first><last>Giovanni Møller</last><affiliation>IT University of Copenhagen</affiliation></author>
      <author><first>Luca</first><last>Maria Aiello</last><affiliation>IT University of Copenhagen</affiliation></author>
      <pages>27-49</pages>
      <abstract>Large Language Models are expressive tools that enable complex tasks of text understanding within Computational Social Science. Their versatility, while beneficial, poses a barrier for establishing standardized best practices within the field. To bring clarity on the values of different strategies, we present an overview of the performance of modern LLM-based classification methods on a benchmark of 23 social knowledge tasks. Our results point to three best practices: prioritize models with larger vocabulary and pre-training corpora; avoid simple zero-shot in favor of AI-enhanced prompting; fine-tune on task-specific data, and consider more complex forms instruction-tuning on multiple datasets only when only training data is more abundant.</abstract>
      <url hash="0e829594">2025.sicon-1.2</url>
      <bibkey>giovanni-moller-maria-aiello-2025-prompt</bibkey>
      <doi>10.18653/v1/2025.sicon-1.2</doi>
    </paper>
    <paper id="3">
      <title><fixed-case>D</fixed-case>ecep<fixed-case>B</fixed-case>ench: Benchmarking Multimodal Deception Detection</title>
      <author><first>Ethan</first><last>Braverman</last><affiliation>Hillsdale High School</affiliation></author>
      <author><first>Vittesh</first><last>Maganti</last><affiliation>Algoverse</affiliation></author>
      <author><first>Nysa</first><last>Lalye</last><affiliation>Algoverse</affiliation></author>
      <author><first>Akhil</first><last>Ganti</last></author>
      <author><first>Michael</first><last>Lu</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Kevin</first><last>Zhu</last><affiliation>Algoverse</affiliation></author>
      <author><first>Vasu</first><last>Sharma</last><affiliation>Facebook</affiliation></author>
      <author><first>Sean</first><last>O’Brien</last><affiliation>Computer Science and Engineering, University of California, San Diego</affiliation></author>
      <pages>50-64</pages>
      <abstract>Deception detection is crucial in domains such as security, forensics, and legal proceedings, as well as to ensure the reliability of AI systems. However, current approaches are limited by the lack of generalizable and interpretable benchmarks built on large and diverse datasets. To address this gap, we introduce DecepBench, a comprehensive and robust benchmark for multimodal deception detection. DecepBench includes an enhanced version of the DOLOS dataset, the largest game-show deception dataset (1,700 labeled video clips with audio). We augment each video clip with transcripts, introducing a third modality (text) and incorporating deception-related features identified in psychological research. We employ explainable methods to evaluate the relevance of key deception cues, providing insights into model limitations and guiding future improvements. Our enhancements to DOLOS, combined with these interpretable analyses, yield improved performance and a deeper understanding of multimodal deception detection.</abstract>
      <url hash="de15f3b8">2025.sicon-1.3</url>
      <bibkey>braverman-etal-2025-decepbench</bibkey>
      <doi>10.18653/v1/2025.sicon-1.3</doi>
    </paper>
    <paper id="4">
      <title>Should <fixed-case>I</fixed-case> go vegan: Evaluating the Persuasiveness of <fixed-case>LLM</fixed-case>s in Persona-Grounded Dialogues</title>
      <author><first>Shruthi</first><last>Chockkalingam</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Seyed</first><last>Hossein Alavi</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Raymond</first><last>T. Ng</last><affiliation>University of British Columbia</affiliation></author>
      <author><first>Vered</first><last>Shwartz</last><affiliation>University of British Columbia</affiliation></author>
      <pages>65-72</pages>
      <abstract>As the use of large language models becomes ever more prevalent, understanding their persuasive abilities, both in ways that can be beneficial and harmful to humans, proves an important task. Previous work has focused on persuasion in the context of negotiations, political debate and advertising. We instead shift the focus to a more realistic setup of a dialogue between a persuadee with an everyday dilemma (e.g., whether to switch to a vegan diet or not) and a persuader with no prior knowledge about the persuadee who is trying to persuade them towards a certain decision based on arguments they feel would be most suited to the persuadee’s persona. We collect and analyze conversations between a human persuadee and either a human persuader or an LLM persuader based on GPT-4. We find that, in this setting, GPT-4 is perceived as both more persuasive and more empathetic, whereas humans are more skilled at discovering new information about the person they are speaking to. This research provides the groundwork for future work predicting the persuasiveness of utterances in conversation across a range of topics.</abstract>
      <url hash="28a4e2d4">2025.sicon-1.4</url>
      <bibkey>chockkalingam-etal-2025-go</bibkey>
      <doi>10.18653/v1/2025.sicon-1.4</doi>
    </paper>
    <paper id="5">
      <title><fixed-case>PROTECT</fixed-case>: Policy-Related Organizational Value Taxonomy for Ethical Compliance and Trust</title>
      <author><first>Avni</first><last>Mittal</last><affiliation>Microsoft</affiliation></author>
      <author><first>Sree</first><last>Hari Nagaralu</last><affiliation>Microsoft</affiliation></author>
      <author><first>Sandipan</first><last>Dandapat</last><affiliation>Microsoft</affiliation></author>
      <pages>73-75</pages>
      <abstract>This paper presents PROTECT, a novel policy-driven organizational value taxonomy designed to enhance ethical compliance and trust within organizations. Drawing on established human value systems and leveraging large language models, PROTECT generates values tailored to organizational contexts and clusters them into a refined taxonomy. This taxonomy serves as the basis for creating a comprehensive dataset of compliance scenarios, each linked to specific values and paired with both compliant and non-compliant responses. By systematically varying value emphasis, we illustrate how different LLM personas emerge, reflecting diverse compliance behaviors. The dataset, directly grounded in the taxonomy, enables consistent evaluation and training of LLMs on value-sensitive tasks. While PROTECT offers a robust foundation for aligning AI systems with organizational standards, our experiments also reveal current limitations in model accuracy, highlighting the need for further improvements. Together, the taxonomy and dataset represent complementary, foundational contributions toward value-aligned AI in organizational settings.</abstract>
      <url hash="001437cc">2025.sicon-1.5</url>
      <bibkey>mittal-etal-2025-protect</bibkey>
      <doi>10.18653/v1/2025.sicon-1.5</doi>
    </paper>
    <paper id="6">
      <title>Too Polite to be Human: Evaluating <fixed-case>LLM</fixed-case> Empathy in <fixed-case>K</fixed-case>orean Conversations via a <fixed-case>DCT</fixed-case>-Based Framework</title>
      <author><first>Seoyoon</first><last>Park</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Jaehee</first><last>Kim</last><affiliation>Yonsei University</affiliation></author>
      <author><first>Hansaem</first><last>Kim</last><affiliation>Yonsei University</affiliation></author>
      <pages>76-89</pages>
      <abstract>As LLMs are increasingly used in global conversational settings, concerns remain about their ability to handle complex sociocultural contexts. This study evaluates LLMs’ empathetic understanding in Korean—a high-context language—using a pragmatics-based Discourse Completion Task (DCT) focused on interpretive judgment rather than generation. We constructed a dataset varying relational hierarchy, intimacy, and emotional valence, and compared responses from proprietary and open-source LLMs to those of Korean speakers. Most LLMs showed over-empathizing tendencies and struggled with ambiguous relational cues. Neither model size nor Korean fine-tuning significantly improved performance. While humans reflected relational nuance and contextual awareness, LLMs relied on surface strategies. These findings underscore LLMs’ limits in socio-pragmatic reasoning and introduce a scalable, culturally flexible framework for evaluating socially-aware AI.</abstract>
      <url hash="98c7a563">2025.sicon-1.6</url>
      <bibkey>park-etal-2025-polite</bibkey>
      <doi>10.18653/v1/2025.sicon-1.6</doi>
    </paper>
    <paper id="7">
      <title>Masculine Defaults via Gendered Discourse in Podcasts and Large Language Models</title>
      <author><first>Maria</first><last>Teleki</last><affiliation>Texas A&amp;M University</affiliation></author>
      <author><first>Xiangjue</first><last>Dong</last><affiliation>Texas A&amp;M University</affiliation></author>
      <author><first>Haoran</first><last>Liu</last><affiliation>Texas A&amp;M University</affiliation></author>
      <author><first>James</first><last>Caverlee</last><affiliation>Texas A&amp;M University</affiliation></author>
      <pages>90-96</pages>
      <abstract>Masculine discourse words are discourse terms that are both socially normative and statistically associated with male speakers. We propose a twofold framework for (i) the large-scale discovery and analysis of gendered discourse words in spoken content via our Gendered Discourse Correlation Framework; and (ii) the measurement of the gender bias associated with these words in LLMs via our Discourse Word-Embedding Association Test. We focus our study on podcasts, a popular and growing form of social media, analyzing 15,117 podcast episodes. We analyze correlations between gender and discourse words – discovered via LDA and BERTopic. We then find that gendered discourse-based masculine defaults exist in the domains of business, technology/politics, and video games, indicating that these gendered discourse words are socially influential. Next, we study the representation of these words from a state-of-the-art LLM embedding model from OpenAI, and find that the masculine discourse words have a more stable and robust representation than the feminine discourse words, which may result in better system performance on downstream tasks for men. Hence, men are rewarded for their discourse patterns with better system performance – and this embedding disparity constitutes a representational harm and a masculine default.</abstract>
      <url hash="fa6bac98">2025.sicon-1.7</url>
      <bibkey>teleki-etal-2025-masculine</bibkey>
      <doi>10.18653/v1/2025.sicon-1.7</doi>
    </paper>
    <paper id="8">
      <title><fixed-case>CLAIM</fixed-case>: An Intent-Driven Multi-Agent Framework for Analyzing Manipulation in Courtroom Dialogues</title>
      <author><first>Disha</first><last>Sheshanarayana</last><affiliation>Manipal University</affiliation></author>
      <author><first>Tanishka</first><last>Magar</last><affiliation>Manipal University</affiliation></author>
      <author><first>Ayushi</first><last>Mittal</last><affiliation>Manipal University Jaipur</affiliation></author>
      <author><first>Neelam</first><last>Chaplot</last><affiliation>Jaipur</affiliation></author>
      <pages>97-108</pages>
      <abstract>Courtrooms are places where lives are determined and fates are sealed, yet they are not impervious to manipulation. Strategic use of manipulation in legal jargon can sway the opinions of judges and affect the decisions. Despite the growing advancements in NLP, its application in detecting and analyzing manipulation within the legal domain remains largely unexplored. Our work addresses this gap by introducing LegalCon, a dataset of 1,063 annotated courtroom conversations labeled for manipulation detection, identification of primary manipulators, and classification of manipulative techniques, with a focus on long conversations. Furthermore, we propose CLAIM, a two-stage, Intent-driven Multi-agent framework designed to enhance manipulation analysis by enabling context-aware and informed decision-making. Our results highlight the potential of incorporating agentic frameworks to improve fairness and transparency in judicial processes. We hope that this contributes to the broader application of NLP in legal discourse analysis and the development of robust tools to support fairness in legal decision-making. Our code and data are available at CLAIM.</abstract>
      <url hash="47f33523">2025.sicon-1.8</url>
      <bibkey>sheshanarayana-etal-2025-unmasking</bibkey>
      <doi>10.18653/v1/2025.sicon-1.8</doi>
    </paper>
    <paper id="9">
      <title>Steering Conversational Large Language Models for Long Emotional Support Conversations</title>
      <author><first>Navid</first><last>Madani</last><affiliation>State University of New York at Buffalo</affiliation></author>
      <author><first>Rohini</first><last>Srihari</last><affiliation>State University of New York at Buffalo</affiliation></author>
      <pages>109-123</pages>
      <abstract>In this study, we address the challenge of consistently following emotional support strategies in long conversations by large language models (LLMs). We introduce the Strategy-Relevant Attention (SRA) metric, a model-agnostic measure designed to evaluate the effectiveness of LLMs in adhering to strategic prompts in emotional support contexts. By analyzing conversations within the Emotional Support Conversations dataset (ESConv) using LLaMA models, we demonstrate that SRA is significantly correlated with a model’s ability to sustain the outlined strategy throughout the interactions. Our findings reveal that the application of SRA-informed prompts leads to enhanced strategic adherence, resulting in conversations that more reliably exhibit the desired emotional support strategies over longer conversations. Furthermore, we contribute a comprehensive, multi-branch synthetic conversation dataset for ESConv, featuring a variety of strategy continuations informed by our optimized prompting method. The code and data are publicly available on our Github.</abstract>
      <url hash="de15f3b8">2025.sicon-1.9</url>
      <bibkey>madani-srihari-2025-steering</bibkey>
      <doi>10.18653/v1/2025.sicon-1.9</doi>
    </paper>
    <paper id="10">
      <title>Text Overlap: An <fixed-case>LLM</fixed-case> with Human-like Conversational Behaviors</title>
      <author><first>JiWoo</first><last>Kim</last><affiliation>Sung Kyun Kwan University</affiliation></author>
      <author><first>Minsuk</first><last>Chang</last><affiliation>Google Deepmind</affiliation></author>
      <author><first>JinYeong</first><last>Bak</last><affiliation>Sung Kyun Kwan University</affiliation></author>
      <pages>124-136</pages>
      <abstract>Traditional text-based human-AI interactions typically follow a strict turn-taking approach. This rigid structure limits conversational flow, unlike natural human conversations, which can freely incorporate overlapping speech. However, our pilot study suggests that even in text-based interfaces, overlapping behaviors such as backchanneling and proactive responses lead to more natural and functional exchanges. Motivated by these findings, we introduce text-based overlapping interactions as a new challenge in human-AI communication, characterized by real-time typing, diverse response types, and interruptions. To enable AI systems to handle such interactions, we define three core tasks: deciding when to overlap, selecting the response type, and generating utterances. We construct a synthetic dataset for these tasks and train OverlapBot, an LLM-driven chatbot designed to engage in text-based overlapping interactions. Quantitative and qualitative evaluations show that OverlapBot increases turn exchanges compared to traditional turn-taking systems, with users making 72% more turns and the chatbot 130% more turns, which is perceived as efficient by end-users. This finding supports overlapping interactions and enhances communicative efficiency and engagement.</abstract>
      <url hash="8d07fbcb">2025.sicon-1.10</url>
      <bibkey>kim-etal-2025-text</bibkey>
      <doi>10.18653/v1/2025.sicon-1.10</doi>
    </paper>
    <paper id="11">
      <title>Social Influence in Consumer Response to Advertising: A Model of Conversational Engagement</title>
      <author><first>Javier</first><last>Marín</last><affiliation>UNIVERSIDAD INTERNACIONAL DE LA RIOJA</affiliation></author>
      <pages>137-144</pages>
      <abstract>This paper explores social influence in consumer responses to advertising through investment-mediated conversational dynamics. We implement conversational engagement via advertising expenditure patterns, recognizing that marketing spend directly translates into conversational volume and reach across multi-channel ecosystems. Our approach integrates social psychology frameworks with statistical physics analogies as epistemic scaffolding following Ruse’s änalogy as heuristic” idea. The model introduces three parameters—Marketing Sensitivity, Response Sensitivity, and Behavioral Sensitivity—quantifying emergent properties of investment-driven influence networks. Validation against three real-world datasets shows competitive performance compared to conventional approaches of modeling the consumer response curve like Michaelis-Menten and Hill equations, with context-dependent advantages in network-driven scenarios. These findings illustrate how advertising ecosystems operate as complex adaptive systems (CAS) where influence propagates through investment-amplified conversational networks.</abstract>
      <url hash="28a4e2d4">2025.sicon-1.11</url>
      <bibkey>marin-2025-social</bibkey>
      <doi>10.18653/v1/2025.sicon-1.11</doi>
    </paper>
    <paper id="12">
      <title>Extended Abstract: Probing-Guided Parameter-Efficient Fine-Tuning for Balancing Linguistic Adaptation and Safety in <fixed-case>LLM</fixed-case>-based Social Influence Systems</title>
      <author><first>Manyana</first><last>Tiwari</last><affiliation>Indian Institute of Technology, Roorkee</affiliation></author>
      <pages>145-147</pages>
      <abstract>Designing effective LLMs for social influence (SI) tasks demands controlling linguistic output such that it adapts to context (such as user attributes, history etc.) while upholding ethical guardrails. Standard Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA struggle to manage the trade-off between adaptive linguistic expression and safety and they optimize based on overall objectives without differentiating the functional roles of internal model components. Therefore, we introduce Probing-Guided PEFT (PG-PEFT), a novel fine-tuning strategy which utilizes interpretability probes to identify LLM components associated with context-driven linguistic variations versus those linked to safety violations (e.g., toxicity, bias). This functional map then guides LoRA updates, enabling more targeted control over the model’s linguistic output. We evaluate PG-PEFT on SI tasks (persuasion, negotiation) and linguistic adaptability with safety benchmarks against standard PEFT.</abstract>
      <url hash="001437cc">2025.sicon-1.12</url>
      <bibkey>tiwari-2025-extended</bibkey>
      <doi>10.18653/v1/2025.sicon-1.12</doi>
    </paper>
  </volume>
</collection>
