<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.trustnlp">
  <volume id="main" ingest-date="2025-04-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 5th Workshop on Trustworthy NLP (TrustNLP 2025)</booktitle>
      <editor><first>Trista</first><last>Cao</last></editor>
      <editor><first>Anubrata</first><last>Das</last></editor>
      <editor><first>Tharindu</first><last>Kumarage</last></editor>
      <editor><first>Yixin</first><last>Wan</last></editor>
      <editor><first>Satyapriya</first><last>Krishna</last></editor>
      <editor><first>Ninareh</first><last>Mehrabi</last></editor>
      <editor><first>Jwala</first><last>Dhamala</last></editor>
      <editor><first>Anil</first><last>Ramakrishna</last></editor>
      <editor><first>Aram</first><last>Galystan</last></editor>
      <editor><first>Anoop</first><last>Kumar</last></editor>
      <editor><first>Rahul</first><last>Gupta</last></editor>
      <editor><first>Kai-Wei</first><last>Chang</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Albuquerque, New Mexico</address>
      <month>May</month>
      <year>2025</year>
      <url hash="1a002709">2025.trustnlp-main</url>
      <venue>trustnlp</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-233-6</isbn>
    </meta>
    <frontmatter>
      <url hash="d0ba3732">2025.trustnlp-main.0</url>
      <bibkey>trustnlp-ws-2025-main</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Beyond Text-to-<fixed-case>SQL</fixed-case> for <fixed-case>I</fixed-case>o<fixed-case>T</fixed-case> Defense: A Comprehensive Framework for Querying and Classifying <fixed-case>I</fixed-case>o<fixed-case>T</fixed-case> Threats</title>
      <author><first>Ryan</first><last>Pavlich</last><affiliation>NA</affiliation></author>
      <author orcid="0000-0003-3479-2774"><first>Nima</first><last>Ebadi</last></author>
      <author><first>Richard</first><last>Tarbell</last><affiliation>NA</affiliation></author>
      <author><first>Billy</first><last>Linares</last><affiliation>NA</affiliation></author>
      <author><first>Adrian</first><last>Tan</last><affiliation>NA</affiliation></author>
      <author><first>Rachael</first><last>Humphreys</last><affiliation>NA</affiliation></author>
      <author><first>Jayanta</first><last>Das</last></author>
      <author><first>Rambod</first><last>Ghandiparsi</last><affiliation>NA</affiliation></author>
      <author><first>Hannah</first><last>Haley</last><affiliation>NA</affiliation></author>
      <author><first>Jerris</first><last>George</last><affiliation>NA</affiliation></author>
      <author><first>Rocky</first><last>Slavin</last><affiliation>University of Texas at San Antonio</affiliation></author>
      <author orcid="0000-0001-9208-5336"><first>Kim-Kwang Raymond</first><last>Choo</last><affiliation>University of Texas at San Antonio</affiliation></author>
      <author><first>Glenn</first><last>Dietrich</last><affiliation>NA</affiliation></author>
      <author><first>Anthony</first><last>Rios</last><affiliation>University of Texas at San Antonio</affiliation></author>
      <pages>1-12</pages>
      <abstract>Recognizing the promise of natural language interfaces to databases, prior studies have emphasized the development of text-to-SQL systems. Existing research has generally focused on generating SQL statements from text queries, and the broader challenge lies in inferring new information about the returned data. Our research makes two major contributions to address this gap. First, we introduce a novel Internet-of-Things (IoT) text-to-SQL dataset comprising 10,985 text-SQL pairs and 239,398 rows of network traffic activity. The dataset contains additional query types limited in prior text-to-SQL datasets, notably, temporal-related queries. Our dataset is sourced from a smart building’s IoT ecosystem exploring sensor read and network traffic data. Second, our dataset allows two-stage processing, where the returned data (network traffic) from a generated SQL can be categorized as malicious or not. Our results show that joint training to query and infer information about the data improves overall text-to-SQL performance, nearly matching that of substantially larger models. We also show that current large language models (e.g., GPT3.5) struggle to infer new information about returned data (i.e., they are bad at tabular data understanding), thus our dataset provides a novel test bed for integrating complex domain-specific reasoning into LLMs.</abstract>
      <url hash="70b786b1">2025.trustnlp-main.1</url>
      <bibkey>pavlich-etal-2025-beyond</bibkey>
    </paper>
    <paper id="2">
      <title>Gibberish is All You Need for Membership Inference Detection in Contrastive Language-Audio Pretraining</title>
      <author><first>Ruoxi</first><last>Cheng</last></author>
      <author><first>Yizhong</first><last>Ding</last></author>
      <author><first>Shuirong</first><last>Cao</last><affiliation>nanjing university</affiliation></author>
      <author orcid="0000-0002-1789-8414"><first>Zhiqiang</first><last>Wang</last><affiliation>beijing electronic science&amp;technology institute</affiliation></author>
      <author><first>Shitong</first><last>Shao</last></author>
      <pages>13-22</pages>
      <abstract>Audio can disclose PII, particularly when combined with related text data. Therefore, it is essential to develop tools to detect privacy leakage in Contrastive Language-Audio Pretraining(CLAP). Existing MIAs need audio as input, risking exposure of voiceprint and requiring costly shadow models. We first propose PRMID, a membership inference detector based probability ranking given by CLAP, which does not require training shadow models but still requires both audio and text of the individual as input. To address these limitations, we then propose USMID, a textual unimodal speaker-level membership inference detector, querying the target model using only text data. We randomly generate textual gibberish that are clearly not in training dataset. Then we extract feature vectors from these texts using the CLAP model and train a set of anomaly detectors on them. During inference, the feature vector of each test text is input into the anomaly detector to determine if the speaker is in the training set (anomalous) or not (normal). If available, USMID can further enhance detection by integrating real audio of the tested speaker. Extensive experiments on various CLAP model architectures and datasets demonstrate that USMID outperforms baseline methods using only text data.</abstract>
      <url hash="df4343db">2025.trustnlp-main.2</url>
      <bibkey>cheng-etal-2025-gibberish</bibkey>
    </paper>
    <paper id="3">
      <title><fixed-case>PBI</fixed-case>-Attack: Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack for Toxicity Maximization</title>
      <author><first>Ruoxi</first><last>Cheng</last></author>
      <author><first>Yizhong</first><last>Ding</last></author>
      <author><first>Shuirong</first><last>Cao</last><affiliation>nanjing university</affiliation></author>
      <author><first>Ranjie</first><last>Duan</last></author>
      <author><first>Xiaoshuang</first><last>Jia</last></author>
      <author><first>Shaowei</first><last>Yuan</last></author>
      <author orcid="0000-0002-1789-8414"><first>Zhiqiang</first><last>Wang</last><affiliation>beijing electronic science&amp;technology institute</affiliation></author>
      <author><first>Xiaojun</first><last>Jia</last><affiliation>Nanyang Technological University</affiliation></author>
      <pages>23-40</pages>
      <abstract>Understanding the vulnerabilities of Large Vision Language Models (LVLMs) to jailbreak attacks is essential for their responsible real-world deployment. Most previous work requires access to model gradients, or is based on human knowledge (prompt engineering) to complete jailbreak, and they hardly consider the interaction of images and text, resulting in inability to jailbreak in black box scenarios or poor performance. To overcome these limitations, we propose a Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack for toxicity maximization, referred to as PBI-Attack. Our method begins by extracting malicious features from a harmful corpus using an alternative LVLM and embedding these features into a benign image as prior information. Subsequently, we enhance these features through bidirectional cross-modal interaction optimization, which iteratively optimizes the bimodal perturbations in an alternating manner through greedy search, aiming to maximize the toxicity of the generated response. The toxicity level is quantified using a well-trained evaluation model.Experiments demonstrate that PBI-Attack outperforms previous state-of-the-art jailbreak methods, achieving an average attack success rate of 92.5% across three open-source LVLMs and around 67.3% on three closed-source LVLMs.redDisclaimer: This paper contains potentially disturbing and offensive content.</abstract>
      <url hash="eef2f596">2025.trustnlp-main.3</url>
      <bibkey>cheng-etal-2025-pbi</bibkey>
    </paper>
    <paper id="4">
      <title>Ambiguity Detection and Uncertainty Calibration for Question Answering with Large Language Models</title>
      <author orcid="0000-0003-3074-3035"><first>Zhengyan</first><last>Shi</last></author>
      <author><first>Giuseppe</first><last>Castellucci</last><affiliation>Amazon</affiliation></author>
      <author><first>Simone</first><last>Filice</last><affiliation>Technology Innovation Institute</affiliation></author>
      <author orcid="0009-0008-7865-3067"><first>Saar</first><last>Kuzi</last><affiliation>Amazon</affiliation></author>
      <author><first>Elad</first><last>Kravi</last></author>
      <author orcid="0000-0002-3148-5448"><first>Eugene</first><last>Agichtein</last><affiliation>Emory University</affiliation></author>
      <author><first>Oleg</first><last>Rokhlenko</last></author>
      <author><first>Shervin</first><last>Malmasi</last><affiliation>Amazon</affiliation></author>
      <pages>41-55</pages>
      <abstract>Large Language Models (LLMs) have demonstrated excellent capabilities in Question Answering (QA) tasks, yet their ability to identify and address ambiguous questions remains underdeveloped. Ambiguities in user queries often lead to inaccurate or misleading answers, undermining user trust in these systems. Despite prior attempts using prompt-based methods, performance has largely been equivalent to random guessing, leaving a significant gap in effective ambiguity detection. To address this, we propose a novel framework for detecting ambiguous questions within LLM-based QA systems. We first prompt an LLM to generate multiple answers to a question, and then analyze them to infer the ambiguity. We propose to use a lightweight Random Forest model, trained on a bootstrapped and shuffled 6-shot examples dataset. Experimental results on ASQA, PACIFIC, and ABG-COQA datasets demonstrate the effectiveness of our approach, with accuracy up to 70.8%. Furthermore, our framework enhances the confidence calibration of LLM outputs, leading to more trustworthy QA systems able to handle complex questions.</abstract>
      <url hash="07ccc700">2025.trustnlp-main.4</url>
      <bibkey>shi-etal-2025-ambiguity</bibkey>
    </paper>
    <paper id="5">
      <title>Smaller Large Language Models Can Do Moral Self-Correction</title>
      <author><first>Guangliang</first><last>Liu</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Zhiyu</first><last>Xue</last><affiliation>University of California, Santa Barbara</affiliation></author>
      <author><first>Xitong</first><last>Zhang</last><affiliation>Qualcomm Inc, QualComm</affiliation></author>
      <author><first>Rongrong</first><last>Wang</last><affiliation>Michigan State University</affiliation></author>
      <author><first>Kristen</first><last>Johnson</last><affiliation>Michigan State University</affiliation></author>
      <pages>56-65</pages>
      <abstract>Self-correction is one of the most amazing emerging capabilities of Large Language Models (LLMs), enabling LLMs to self-modify an inappropriate output given a natural language feedback which describes the problems of that output. Moral self-correction is a post-hoc approach correcting unethical generations without requiring a gradient update, making it both computationally lightweight and capable of preserving the language modeling ability. Previous works have shown that LLMs can self-debias, and it has been reported that small models, i.e., those with less than 22B parameters, are <i>not</i> capable of moral self-correction.However, there is no direct proof as to why such smaller models fall short of moral self-correction, though previous research hypothesizes that larger models are skilled in following instructions and understanding abstract social norms.In this paper, we empirically validate this hypothesis in the context of social stereotyping, through meticulous prompting.Our experimental results indicate that <b>(i)</b> surprisingly, 3.8B LLMs with proper safety alignment fine-tuning can achieve very good moral self-correction performance, highlighting the significant effects of safety alignment; and <b>(ii)</b> small LLMs are indeed weaker than larger-scale models in terms of comprehending social norms and self-explanation through CoT, but all scales of LLMs show bad self-correction performance given unethical instructions.</abstract>
      <url hash="c1e38d24">2025.trustnlp-main.5</url>
      <bibkey>liu-etal-2025-smaller</bibkey>
    </paper>
    <paper id="6">
      <title>Error Detection for Multimodal Classification</title>
      <author><first>Thomas</first><last>Bonnier</last><affiliation>Centrale Lille Alumni</affiliation></author>
      <pages>66-81</pages>
      <abstract>Machine learning models have proven to be useful in various key applications such as autonomous driving or diagnosis prediction. When a model is implemented under real-world conditions, it is thus essential to detect potential errors with a trustworthy approach. This monitoring practice will render decision-making safer by avoiding catastrophic failures. In this paper, the focus is on multimodal classification. We introduce a method that addresses error detection based on unlabeled data. It leverages fused representations and computes the probability that a model will fail based on detected fault patterns in validation data. To improve transparency, we employ a sampling-based approximation of Shapley values in multimodal settings in order to explain why a prediction is assessed as erroneous in terms of feature values. Further, as explanation methods can sometimes disagree, we suggest evaluating the consistency of explanations produced by different value functions and algorithms. To show the relevance of our method, we measure it against a selection of 9 baselines from various domains on tabular-text and text-image datasets, and 2 multimodal fusion strategies for the classification models. Lastly, we show the usefulness of our explanation algorithm on misclassified samples.</abstract>
      <url hash="ab49632a">2025.trustnlp-main.6</url>
      <bibkey>bonnier-2025-error</bibkey>
    </paper>
    <paper id="7">
      <title>Break the Breakout: Reinventing <fixed-case>LM</fixed-case> Defense Against Jailbreak Attacks with Self-Refine</title>
      <author><first>Heegyu</first><last>Kim</last><affiliation>Ajou University</affiliation></author>
      <author orcid="0000-0002-9134-1921"><first>Hyunsouk</first><last>Cho</last><affiliation>Ajou University</affiliation></author>
      <pages>82-102</pages>
      <abstract>Language models (LMs) are vulnerable to exploitation for adversarial misuse. Training LMs for safety alignment is extensive, making it hard to respond to fast-developing attacks immediately, such as jailbreaks. We propose self-refine with formatting that achieves outstanding safety even in non-safety-aligned LMsand evaluate our method alongside several defense baselines, demonstrating that it is the safest training-free method against jailbreak attacks.Additionally, we proposed a formatting method that improves the efficiency of the self-refine process while reducing attack success rates in fewer iterations. We observed that non-safety-aligned LMs outperform safety-aligned LMs in safety tasks by giving more helpful and safe responses.In conclusion, our findings can achieve less safety risk with fewer computational costs, allowing non-safety LM to be efficiently utilized in real-world service.</abstract>
      <url hash="40fe056b">2025.trustnlp-main.7</url>
      <bibkey>kim-cho-2025-break</bibkey>
    </paper>
    <paper id="8">
      <title>Minimal Evidence Group Identification for Claim Verification</title>
      <author orcid="0000-0002-7493-9534"><first>Xiangci</first><last>Li</last><affiliation>Amazon Web Services</affiliation></author>
      <author><first>Sihao</first><last>Chen</last><affiliation>Microsoft</affiliation></author>
      <author><first>Rajvi</first><last>Kapadia</last><affiliation>Google</affiliation></author>
      <author><first>Jessica</first><last>Ouyang</last><affiliation>University of Texas at Dallas</affiliation></author>
      <author><first>Fan</first><last>Zhang</last><affiliation>Google</affiliation></author>
      <pages>103-111</pages>
      <abstract>When verifying a claim in real-world settings, e.g. against a large collection of candidate evidence text retrieved from the web, a model is typically expected to identify and aggregate a complete set of evidence pieces that collectively provide full support to a claim.The problem becomes particularly challenging as there might exist different sets of evidence that could be used to verify the claim from different perspectives. In this paper, we formally define and study the problem of identifying such minimal evidence groups (MEGs) for fact verification. We show that MEG identification can be reduced to a Set Cover-like problem, based on an entailment model which estimates whether a given evidence group provides full or partial support to a claim. Our proposed approach achieves 18.4% &amp; 34.8% absolute improvements on WiCE and SciFact datasets over LLM prompting. Finally, we demonstrate the downstream benefit of MEGs in applications such as claim generation.</abstract>
      <url hash="3e82442c">2025.trustnlp-main.8</url>
      <bibkey>li-etal-2025-minimal</bibkey>
    </paper>
    <paper id="9">
      <title>Cracking the Code: Enhancing Implicit Hate Speech Detection through Coding Classification</title>
      <author><first>Lu</first><last>Wei</last></author>
      <author><first>Liangzhi</first><last>Li</last><affiliation>Meetyou AI Lab and Qufu Normal University</affiliation></author>
      <author orcid="0000-0002-1559-7757"><first>Tong</first><last>Xiang</last></author>
      <author><first>Liu</first><last>Xiao</last><affiliation>Meetyou AI Lab</affiliation></author>
      <author><first>Noa</first><last>Garcia</last><affiliation>Osaka University</affiliation></author>
      <pages>112-126</pages>
      <abstract>The internet has become a hotspot for hate speech (HS), threatening societal harmony and individual well-being. While automatic detection methods perform well in identifying explicit hate speech (ex-HS), they struggle with more subtle forms, such as implicit hate speech (im-HS). We tackle this problem by introducing a new taxonomy for im-HS detection, defining six encoding strategies named *codetypes*. We present two methods for integrating codetypes into im-HS detection: 1) prompting large language models (LLMs) directly to classify sentences based on generated responses, and 2) using LLMs as encoders with codetypes embedded during the encoding process. Experiments show that the use of codetypes improves im-HS detection in both Chinese and English datasets, validating the effectiveness of our approach across different languages.</abstract>
      <url hash="2db36a1e">2025.trustnlp-main.9</url>
      <bibkey>wei-etal-2025-cracking</bibkey>
    </paper>
    <paper id="10">
      <title>Line of Duty: Evaluating <fixed-case>LLM</fixed-case> Self-Knowledge via Consistency in Feasibility Boundaries</title>
      <author><first>Sahil</first><last>Kale</last></author>
      <author><first>Vijaykant</first><last>Nadadur</last></author>
      <pages>127-140</pages>
      <abstract>As LLMs grow more powerful, their most profound achievement may be recognising when to say “I don’t know”. Existing studies on LLM self-knowledge have been largely constrained by human-defined notions of feasibility, often neglecting the reasons behind unanswerability by LLMs and failing to study deficient types of self-knowledge. This study aims to obtain intrinsic insights into different types of LLM self-knowledge with a novel methodology: allowing them the flexibility to set their own feasibility boundaries and then analysing the consistency of these limits. We find that even frontier models like GPT-4o and Mistral Large are not sure of their own capabilities more than 80% of the time, highlighting a significant lack of trustworthiness in responses. Our analysis of confidence balance in LLMs indicates that models swing between overconfidence and conservatism in feasibility boundaries depending on task categories and that the most significant self-knowledge weaknesses lie in temporal awareness and contextual understanding. These difficulties in contextual comprehension additionally lead models to question their operational boundaries, resulting in considerable confusion within the self-knowledge of LLMs. We make our code and results available publicly.</abstract>
      <url hash="9e2bdc6f">2025.trustnlp-main.10</url>
      <bibkey>kale-vrn-2025-line</bibkey>
    </paper>
    <paper id="11">
      <title>Multi-lingual Multi-turn Automated Red Teaming for <fixed-case>LLM</fixed-case>s</title>
      <author><first>Abhishek</first><last>Singhania</last><affiliation>Amazon</affiliation></author>
      <author><first>Christophe</first><last>Dupuy</last><affiliation>Amazon</affiliation></author>
      <author><first>Shivam Sadashiv</first><last>Mangale</last><affiliation>Amazon</affiliation></author>
      <author><first>Amani</first><last>Namboori</last><affiliation>Amazon</affiliation></author>
      <pages>141-154</pages>
      <abstract>Language Model Models (LLMs) have improved dramatically in the past few years, increasing their adoption and the scope of their capabilities over time. A significant amount of work is dedicated to “model alignment”, i.e., preventing LLMs to generate unsafe responses when deployed into customer-facing applications. One popular method to evaluate safety risks is red-teaming, where agents attempt to bypass alignment by crafting elaborate prompts that trigger unsafe responses from a model. Standard human-driven red-teaming is costly, time-consuming and rarely covers all the recent features (e.g., multi-lingual, multi-modal aspects), while proposed automation methods only cover a small subset of LLMs capabilities (i.e., English or single-turn). We present Multi-lingual Multi-turn Automated Red Teaming (MM-ART), a method to fully automate conversational, multi-lingual red-teaming operations and quickly identify prompts leading to unsafe responses. Through extensive experiments on different languages, we show the studied LLMs are on average 71% more vulnerable after a 5-turn conversation in English than after the initial turn. For conversations in non-English languages, models display up to 195% more safety vulnerabilities than the standard single-turn English approach, confirming the need for automated red-teaming methods matching LLMs capabilities.</abstract>
      <url hash="46b0cdaa">2025.trustnlp-main.11</url>
      <bibkey>singhania-etal-2025-multi</bibkey>
    </paper>
    <paper id="12">
      <title>Rainbow-Teaming for the <fixed-case>P</fixed-case>olish Language: A Reproducibility Study</title>
      <author orcid="0009-0004-1702-0865"><first>Aleksandra</first><last>Krasnodębska</last></author>
      <author orcid="0009-0004-9251-972X"><first>Maciej</first><last>Chrabaszcz</last><affiliation>Warsaw University of Technology</affiliation></author>
      <author orcid="0000-0003-4420-4147"><first>Wojciech</first><last>Kusa</last><affiliation>NASK - National Research Institute</affiliation></author>
      <pages>155-165</pages>
      <abstract>The development of multilingual large language models (LLMs) presents challenges in evaluating their safety across all supported languages. Enhancing safety in one language (e.g., English) may inadvertently introduce vulnerabilities in others. To address this issue, we implement a methodology for the automatic creation of red-teaming datasets for safety evaluation in Polish language. Our approach generates both harmful and non-harmful prompts by sampling different risk categories and attack styles. We test several open-source models, including those trained on Polish data, and evaluate them using metrics such as Attack Success Rate (ASR) and False Reject Rate (FRR). The results reveal clear gaps in safety performance between models and show that better testing across languages is needed.</abstract>
      <url hash="d31631a4">2025.trustnlp-main.12</url>
      <bibkey>krasnodebska-etal-2025-rainbow</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>B</fixed-case>ias<fixed-case>E</fixed-case>dit: Debiasing Stereotyped Language Models via Model Editing</title>
      <author orcid="0000-0001-5238-0955"><first>Xin</first><last>Xu</last></author>
      <author><first>Wei</first><last>Xu</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author orcid="0000-0002-1970-0678"><first>Ningyu</first><last>Zhang</last><affiliation>Zhejiang University</affiliation></author>
      <author orcid="0000-0003-0955-7588"><first>Julian</first><last>McAuley</last><affiliation>University of California, San Diego, University of California, San Diego</affiliation></author>
      <pages>166-184</pages>
      <abstract>Previous studies have established that language models manifest stereotyped biases. Existing debiasing strategies, such as retraining a model with counterfactual data, representation projection, and prompting often fail to efficiently eliminate bias or directly alter the models’ biased internal representations. To address these issues, we propose BiasEdit, an efficient model editing method to remove stereotypical bias from language models through lightweight networks that act as editors to generate parameter updates. BiasEdit employs a *debiasing loss* guiding editor networks to conduct local edits on partial parameters of a language model for debiasing while preserving the language modeling abilities during editing through a *retention loss*. Experiments on StereoSet and Crows-Pairs demonstrate the effectiveness, efficiency, and robustness of BiasEdit in eliminating bias compared to tangental debiasing baselines, and little to no impact on the language models’ general capabilities. In addition, we conduct bias tracing to probe bias in various modules and explore bias editing impacts on different components of language models.</abstract>
      <url hash="afc5690f">2025.trustnlp-main.13</url>
      <bibkey>xu-etal-2025-biasedit</bibkey>
    </paper>
    <paper id="14">
      <title>Do Voters Get the Information They Want? Understanding Authentic Voter <fixed-case>FAQ</fixed-case>s in the <fixed-case>US</fixed-case> and How to Improve for Informed Electoral Participation</title>
      <author orcid="0000-0002-4355-1393"><first>Vipula</first><last>Rawte</last></author>
      <author><first>Deja N</first><last>Scott</last><affiliation>University of South Carolina</affiliation></author>
      <author><first>Gaurav</first><last>Kumar</last></author>
      <author><first>Aishneet</first><last>Juneja</last></author>
      <author orcid="0009-0004-2992-8184"><first>Bharat Sowrya</first><last>Yaddanapalli</last></author>
      <author><first>Biplav</first><last>Srivastava</last><affiliation>University of South Carolina</affiliation></author>
      <pages>185-231</pages>
      <abstract>Accurate information is crucial for democracy as it empowers voters to make informed decisions about their representatives and keeping them accountable. In the US, state election commissions (SECs), often required by law, are the primary providers of Frequently Asked Questions (FAQs) to voters, and secondary sources like non-profits such as League of Women Voters (LWV) try to complement their information shortfall. However, surprisingly, to the best of our knowledge, there is neither a single source with comprehensive FAQs nor a study analyzing the data at national level to identify current practices and ways to improve the status quo. This paper addresses it by providing the <b>first dataset on Voter FAQs covering all the US states</b>. Second, we introduce metrics for FAQ information quality (FIQ) with respect to questions, answers, and answers to corresponding questions. Third, we use FIQs to analyze US FAQs to identify leading, mainstream and lagging content practices and corresponding states. Finally, we identify what states across the spectrum can do to improve FAQ quality and thus, the overall information ecosystem. Across all 50 U.S. states, 12% were identified as leaders and 8% as laggards for FIQSvoter, while 14% were leaders and 12% laggards for FIQSdeveloper. The code and sample data are provided at <url>https://anonymous.4open.science/r/election-qa-analysis-BE4E</url>.</abstract>
      <url hash="a693d02b">2025.trustnlp-main.14</url>
      <bibkey>rawte-etal-2025-voters</bibkey>
    </paper>
    <paper id="15">
      <title><fixed-case>V</fixed-case>i<fixed-case>B</fixed-case>e: A Text-to-Video Benchmark for Evaluating Hallucination in Large Multimodal Models</title>
      <author orcid="0000-0002-4355-1393"><first>Vipula</first><last>Rawte</last></author>
      <author><first>Sarthak</first><last>Jain</last></author>
      <author><first>Aarush</first><last>Sinha</last></author>
      <author><first>Garv</first><last>Kaushik</last></author>
      <author><first>Aman</first><last>Bansal</last></author>
      <author><first>Prathiksha Rumale</first><last>Vishwanath</last></author>
      <author><first>Samyak Rajesh</first><last>Jain</last><affiliation>PowerSchool</affiliation></author>
      <author><first>Aishwarya Naresh</first><last>Reganti</last><affiliation>Amazon</affiliation></author>
      <author><first>Vinija</first><last>Jain</last><affiliation>Facebook</affiliation></author>
      <author orcid="0000-0001-6621-9003"><first>Aman</first><last>Chadha</last><affiliation>Amazon Web Services</affiliation></author>
      <author orcid="0000-0002-0021-5293"><first>Amit</first><last>Sheth</last><affiliation>University of South Carolina</affiliation></author>
      <author><first>Amitava</first><last>Das</last><affiliation>University of South Carolina</affiliation></author>
      <pages>232-246</pages>
      <abstract>Recent advances in Large Multimodal Models (LMMs) have expanded their capabilities to video understanding, with Text-to-Video (T2V) models excelling in generating videos from textual prompts. However, they still frequently produce hallucinated content, revealing AI-generated inconsistencies. We introduce ViBe <url>https://huggingface.co/datasets/ViBe-T2V-Bench/ViBe</url>: a large-scale dataset of hallucinated videos from open-source T2V models. We identify five major hallucination types: Vanishing Subject, Omission Error, Numeric Variability, Subject Dysmorphia, and Visual Incongruity. Using ten T2V models, we generated and manually annotated 3,782 videos from 837 diverse MS COCO captions. Our proposed benchmark includes a dataset of hallucinated videos and a classification framework using video embeddings. ViBe serves as a critical resource for evaluating T2V reliability and advancing hallucination detection. We establish classification as a baseline, with the TimeSFormer + CNN ensemble achieving the best performance (0.345 accuracy, 0.342 F1 score). While initial baselines proposed achieve modest accuracy, this highlights the difficulty of automated hallucination detection and the need for improved methods. Our research aims to drive the development of more robust T2V models and evaluate their outputs based on user preferences. Our code is available at: <url>https://anonymous.4open.science/r/vibe-1840/</url></abstract>
      <url hash="851ee144">2025.trustnlp-main.15</url>
      <bibkey>rawte-etal-2025-vibe</bibkey>
    </paper>
    <paper id="16">
      <title>Know What You do Not Know: Verbalized Uncertainty Estimation Robustness on Corrupted Images in Vision-Language Models</title>
      <author><first>Mirko</first><last>Borszukovszki</last></author>
      <author orcid="0000-0002-1497-8013"><first>Ivo Pascal</first><last>De Jong</last></author>
      <author><first>Matias</first><last>Valdenegro-Toro</last><affiliation>University of Groningen</affiliation></author>
      <pages>247-265</pages>
      <abstract>To leverage the full potential of Large Language Models (LLMs) it is crucial to have some information on their answers’ uncertainty. This means that the model has to be able to quantify how certain it is in the correctness of a given response. Bad uncertainty estimates can lead to overconfident wrong answers undermining trust in these models. Quite a lot of research has been done on language models that work with text inputs and provide text outputs. Still, since the visual capabilities have been added to these models recently, there has not been much progress on the uncertainty of Visual Language Models (VLMs). We tested three state-of-the-art VLMs on corrupted image data. We found that the severity of the corruption negatively impacted the models’ ability to estimate their uncertainty and the models also showed overconfidence in most of the experiments.</abstract>
      <url hash="39cdd9b0">2025.trustnlp-main.16</url>
      <bibkey>borszukovszki-etal-2025-know</bibkey>
    </paper>
    <paper id="17">
      <title>Summary the Savior: Harmful Keyword and Query-based Summarization for <fixed-case>LLM</fixed-case> Jailbreak Defense</title>
      <author orcid="0009-0004-6362-7289"><first>Shagoto</first><last>Rahman</last></author>
      <author><first>Ian</first><last>Harris</last><affiliation>University of California-Irvine</affiliation></author>
      <pages>266-275</pages>
      <abstract>Large Language Models (LLMs) are widely used for their capabilities, but face threats from jailbreak attacks, which exploit LLMs to generate inappropriate information and bypass their defense system. Existing defenses are often specific to jailbreak attacks and as a result, a robust, attack-independent solution is needed to address both Natural Language Processing (NLP) ambiguities and attack variability. In this study, we have introduced, Summary The Savior, a novel jailbreak detection mechanism leveraging harmful keywords and query-based security-aware summary classification. By analyzing the illegal and improper contents of prompts within the summaries, the proposed method remains robust against attack diversity and NLP ambiguities. Two novel datasets for harmful keyword extraction and security aware summaries utilizing GPT-4 and Llama-3.1 70B respectively have been generated in this regard. Moreover, an “ambiguous harmful” class has been introduced to address content and intent ambiguities. Evaluation results demonstrate that, Summary The Savior achieves higher defense performance, outperforming state-of-the-art defense mechanisms namely Perplexity Filtering, SmoothLLM, Erase and Check with lowest attack success rates across various jailbreak attacks namely PAIR, GCG, JBC and Random Search, on Llama-2, Vicuna-13B and GPT-4. Our codes, models, and results are available at: https://github.com/shrestho10/SummaryTheSavior</abstract>
      <url hash="9a4ea672">2025.trustnlp-main.17</url>
      <bibkey>rahman-harris-2025-summary</bibkey>
    </paper>
    <paper id="18">
      <title>Bias A-head? Analyzing Bias in Transformer-Based Language Model Attention Heads</title>
      <author orcid="0000-0001-8863-112X"><first>Yi</first><last>Yang</last><affiliation>Hong Kong University of Science and Technology</affiliation></author>
      <author><first>Hanyu</first><last>Duan</last></author>
      <author orcid="0000-0001-7698-7794"><first>Ahmed</first><last>Abbasi</last><affiliation>University of Notre Dame</affiliation></author>
      <author><first>John P.</first><last>Lalor</last><affiliation>University of Notre Dame</affiliation></author>
      <author orcid="0000-0003-3242-0184"><first>Kar Yan</first><last>Tam</last></author>
      <pages>276-290</pages>
      <abstract>Transformer-based pretrained large language models (PLM) such as BERT and GPT have achieved remarkable success in NLP tasks. However, PLMs are prone to encoding stereotypical biases. Although a burgeoning literature has emerged on stereotypical bias mitigation in PLMs, such as work on debiasing gender and racial stereotyping, how such biases manifest and behave internally within PLMs remains largely unknown. Understanding the internal stereotyping mechanisms may allow better assessment of model fairness and guide the development of effective mitigation strategies. In this work, we focus on attention heads, a major component of the Transformer architecture, and propose a bias analysis framework to explore and identify a small set of biased heads that are found to contribute to a PLM’s stereotypical bias. We conduct extensive experiments to validate the existence of these biased heads and to better understand how they behave. We investigate gender and racial bias in the English language in two types of Transformer-based PLMs: the encoder-based BERT model and the decoder-based autoregressive GPT model, LLaMA-2 (7B), and LLaMA-2-Chat (7B). Overall, the results shed light on understanding the bias behavior in pretrained language models.</abstract>
      <url hash="202d4c7f">2025.trustnlp-main.18</url>
      <bibkey>yang-etal-2025-bias</bibkey>
    </paper>
    <paper id="19">
      <title>Mimicking How Humans Interpret Out-of-Context Sentences Through Controlled Toxicity Decoding</title>
      <author orcid="0000-0002-9204-0389"><first>Maria Mihaela</first><last>Trusca</last></author>
      <author orcid="0000-0002-7776-2156"><first>Liesbeth</first><last>Allein</last></author>
      <pages>291-297</pages>
      <abstract>Interpretations of a single sentence can vary, particularly when its context is lost. This paper aims to simulate how readers perceive content with varying toxicity levels by generating diverse interpretations of out-of-context sentences. By modeling toxicity we can anticipate misunderstandings and reveal hidden toxic meanings. Our proposed decoding strategy explicitly controls toxicity in the set of generated interpretations by (i) aligning interpretation toxicity with the input, (ii) relaxing toxicity constraints for more toxic input sentences, and (iii) promoting diversity in toxicity levels within the set of generated interpretations. Experimental results show that our method improves alignment with human-written interpretations in both syntax and semantics while reducing model prediction uncertainty.</abstract>
      <url hash="28be85e2">2025.trustnlp-main.19</url>
      <bibkey>trusca-allein-2025-mimicking</bibkey>
    </paper>
    <paper id="20">
      <title>On the Robustness of Agentic Function Calling</title>
      <author><first>Ella</first><last>Rabinovich</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Ateret</first><last>Anaby Tavor</last><affiliation>International Business Machines</affiliation></author>
      <pages>298-304</pages>
      <abstract>Large Language Models (LLMs) are increasingly acting as autonomous agents, with function calling (FC) capabilities enabling them to invoke specific tools for tasks. While prior research has primarily focused on improving FC accuracy, little attention has been given to the robustness of these agents to perturbations in their input. We introduce a benchmark assessing FC robustness in two key areas: resilience to naturalistic query variations, and stability in function calling when the toolkit expands with semantically related tools. Evaluating best-performing FC models on a carefully expanded subset of the Berkeley function calling leaderboard (BFCL), we identify critical weaknesses in existing evaluation methodologies, and highlight areas for improvement in real-world agentic deployments.</abstract>
      <url hash="4e70aa13">2025.trustnlp-main.20</url>
      <bibkey>rabinovich-anaby-tavor-2025-robustness</bibkey>
    </paper>
    <paper id="21">
      <title><fixed-case>M</fixed-case>onte <fixed-case>C</fixed-case>arlo Temperature: a robust sampling strategy for <fixed-case>LLM</fixed-case>’s uncertainty quantification methods</title>
      <author><first>Nicola</first><last>Cecere</last><affiliation>Amazon</affiliation></author>
      <author><first>Andrea</first><last>Bacciu</last></author>
      <author><first>Ignacio</first><last>Fernández-Tobías</last><affiliation>Amazon</affiliation></author>
      <author><first>Amin</first><last>Mantrach</last></author>
      <pages>305-320</pages>
      <abstract>Uncertainty quantification (UQ) in Large Language Models (LLMs) is essential for their safe and reliable deployment, particularly in critical applications where incorrect outputs can have serious consequences. Current UQ methods typically rely on querying the model multiple times using non-zero temperature sampling to generate diverse outputs for uncertainty estimation. However, the impact of selecting a given temperature parameter is understudied, and our analysis reveals that temperature plays a fundamental role in the quality of uncertainty estimates. The conventional approach of identifying optimal temperature values requires expensive hyperparameter optimization (HPO) that must be repeated for each new model-dataset combination. We propose Monte Carlo Temperature (MCT), a robust sampling strategy that eliminates the need for temperature calibration. Our analysis reveals that: 1) MCT provides more robust uncertainty estimates across a wide range of temperatures, 2) MCT improves the performance of UQ methods by replacing fixed-temperature strategies that do not rely on HPO, and 3) MCT achieves statistical parity with oracle temperatures, which represent the ideal outcome of a well-tuned but computationally expensive HPO process. These findings demonstrate that effective UQ can be achieved without the computational burden of temperature parameter calibration.</abstract>
      <url hash="5241d15a">2025.trustnlp-main.21</url>
      <bibkey>cecere-etal-2025-monte</bibkey>
    </paper>
    <paper id="22">
      <title>Know Thyself: Validating Knowledge Awareness of <fixed-case>LLM</fixed-case>-based Persona Agents</title>
      <author><first>Savita</first><last>Bhat</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <author><first>Ishaan</first><last>Shukla</last></author>
      <author><first>Shirish</first><last>Karande</last><affiliation>Tata Consultancy Services Limited, India</affiliation></author>
      <pages>321-334</pages>
      <abstract>Large Language Models (LLMs) have demonstrated remarkable capability in simulating human behaviors, personality, and language. Such synthetic agents with personalities are considered as cost-effective proxies for real users to facilitate crowd-sourcing efforts like annotations, surveys, and A/B testing. Accordingly, it is imperative to validate knowledge awareness of these LLM persona agents when they are customized for further usage. Currently, there is no established way for such evaluation and appropriate mitigation. In this work, we propose a generic evaluation approach to validate LLM based persona agents for correctness, relevance, and diversity in the context of self-awareness and domain knowledge.We evaluate the efficacy of this framework using three LLMs ( Llama, GPT-4o, and Gemma) for domains such as air travel, gaming, and fitness. We also experiment with advanced prompting strategies such as ReAct and Reflexion. We find that though GPT-4o and Llama demonstrate comparable performance, they fail some of basic consistency checks under certain perturbations.</abstract>
      <url hash="33c42510">2025.trustnlp-main.22</url>
      <bibkey>bhat-etal-2025-know</bibkey>
    </paper>
    <paper id="23">
      <title>Building Safe <fixed-case>G</fixed-case>en<fixed-case>AI</fixed-case> Applications: An End-to-End Overview of Red Teaming for Large Language Models</title>
      <author orcid="0000-0003-1701-7805"><first>Alberto</first><last>Purpura</last><affiliation>International Business Machines</affiliation></author>
      <author><first>Sahil</first><last>Wadhwa</last><affiliation>CapitalOne</affiliation></author>
      <author><first>Jesse</first><last>Zymet</last><affiliation>CapitalOne</affiliation></author>
      <author><first>Akshay</first><last>Gupta</last><affiliation>CapitalOne</affiliation></author>
      <author><first>Andy</first><last>Luo</last><affiliation>CapitalOne</affiliation></author>
      <author><first>Melissa Kazemi</first><last>Rad</last></author>
      <author><first>Swapnil</first><last>Shinde</last><affiliation>CapitalOne</affiliation></author>
      <author><first>Mohammad Shahed</first><last>Sorower</last><affiliation>CapitalOne</affiliation></author>
      <pages>335-350</pages>
      <abstract>The rapid growth of Large Language Models (LLMs) presents significant privacy, security, and ethical concerns. While much research has proposed methods for defending LLM systems against misuse by malicious actors, researchers have recently complemented these efforts with an offensive approach that involves red teaming, i.e., proactively attacking LLMs with the purpose of identifying their vulnerabilities. This paper provides a concise and practical overview of the LLM red teaming literature, structured so as to describe a multi-component system end-to-end. To motivate red teaming we survey the initial safety needs of some high-profile LLMs, and then dive into the different components of a red teaming system as well as software packages for implementing them. We cover various attack methods, strategies for attack-success evaluation, metrics for assessing experiment outcomes, as well as a host of other considerations. Our survey will be useful for any reader who wants to rapidly obtain a grasp of the major red teaming concepts for their own use in practical applications.</abstract>
      <url hash="0021383f">2025.trustnlp-main.23</url>
      <bibkey>purpura-etal-2025-building</bibkey>
    </paper>
    <paper id="24">
      <title>Difficulty Estimation in Natural Language Tasks with Action Scores</title>
      <author><first>Aleksandar</first><last>Angelov</last></author>
      <author orcid="0000-0002-4498-2486"><first>Tsegaye Misikir</first><last>Tashu</last><affiliation>University of Groningen</affiliation></author>
      <author><first>Matias</first><last>Valdenegro-Toro</last><affiliation>University of Groningen</affiliation></author>
      <pages>351-364</pages>
      <abstract>This study investigates the effectiveness of the action score, a metric originally developed for computer vision tasks, in estimating sample difficulty across various natural language processing (NLP) tasks. Using transformer-based models, the action score is applied to sentiment analysis, natural language inference, and abstractive text summarization. The results demonstrate that the action score can effectively identify challenging samples in sentiment analysis and natural language inference, often capturing difficult instances that are missed by more established metrics like entropy. However, the effectiveness of the action score appears to be task-dependent, as evidenced by its performance in the abstractive text summarization task, where it exhibits a nearly linear relationship with entropy. The findings suggest that the action score can provide valuable insights into the characteristics of challenging samples in NLP tasks, particularly in classification settings. However, its application should be carefully considered in the context of each specific task and in light of emerging research on the potential value of hard samples in machine learning.</abstract>
      <url hash="bebb55a1">2025.trustnlp-main.24</url>
      <bibkey>angelov-etal-2025-difficulty</bibkey>
    </paper>
    <paper id="25">
      <title>Are Small Language Models Ready to Compete with Large Language Models for Practical Applications?</title>
      <author orcid="0000-0002-0945-4696"><first>Neelabh</first><last>Sinha</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <author><first>Vinija</first><last>Jain</last><affiliation>Facebook</affiliation></author>
      <author orcid="0000-0001-6621-9003"><first>Aman</first><last>Chadha</last><affiliation>Amazon Web Services</affiliation></author>
      <pages>365-398</pages>
      <abstract>The rapid rise of Language Models (LMs) has expanded their use in several applications. Yet, due to constraints of model size, associated cost, or proprietary restrictions, utilizing state-of-the-art (SOTA) LLMs is not always feasible. With open, smaller LMs emerging, more applications can leverage their capabilities, but selecting the right LM can be challenging as smaller LMs don’t perform well universally. This work tries to bridge this gap by proposing a framework to experimentally evaluate small, open LMs in practical settings through measuring semantic correctness of outputs across three practical aspects: task types, application domains and reasoning types, using diverse prompt styles. It also conducts an in-depth comparison of 10 small, open LMs to identify best LM and prompt style depending on specific application requirement using the proposed framework. We also show that if selected appropriately, they can outperform SOTA LLMs like DeepSeek-v2, GPT-4o-mini, Gemini-1.5-Pro, and even compete with GPT-4o.</abstract>
      <url hash="17dc654c">2025.trustnlp-main.25</url>
      <bibkey>sinha-etal-2025-small</bibkey>
    </paper>
    <paper id="26">
      <title>A Calibrated Reflection Approach for Enhancing Confidence Estimation in <fixed-case>LLM</fixed-case>s</title>
      <author><first>Umesh</first><last>Bodhwani</last></author>
      <author><first>Yuan</first><last>Ling</last></author>
      <author orcid="0009-0008-0240-681X"><first>Shujing</first><last>Dong</last></author>
      <author><first>Yarong</first><last>Feng</last></author>
      <author><first>Hongfei</first><last>Li</last></author>
      <author><first>Ayush</first><last>Goyal</last></author>
      <pages>399-411</pages>
      <abstract>A critical challenge in deploying Large Language Models (LLMs) is developing reliable mechanisms to estimate their confidence, enabling systems to determine when to trust model outputs and when to seek human intervention. In this paper, we present a Calibrated Reflection Approach for Enhancing Confidence Estimation in LLMs, a framework that combines structured reasoning with distance-aware calibration techniques. Our approach introduces three key innovations: (1) a Maximum Confidence Selection (MCS) method that comprehensively evaluates confidence across all possible labels, (2) a reflection-based prompting mechanism that enhances reasoning reliability, and (3) a distance-aware calibration technique that accounts for ordinal relationships between labels. We evaluate our framework across diverse datasets, including HelpSteer2, Llama T-REx, and an internal conversational dataset, demonstrating its effectiveness across both conversational and fact-based classification tasks. This work contributes to the broader goal of developing reliable and well-calibrated confidence estimation methods for LLMs, enabling informed decisions about when to trust model outputs and when to defer to human judgement.</abstract>
      <url hash="e583ad07">2025.trustnlp-main.26</url>
      <bibkey>bodhwani-etal-2025-calibrated</bibkey>
    </paper>
    <paper id="27">
      <title>Evaluating Design Choices in Verifiable Generation with Open-source Models</title>
      <author><first>Shuyang</first><last>Cao</last></author>
      <author><first>Lu</first><last>Wang</last><affiliation>Northeastern University, Northeastern University and University of Michigan</affiliation></author>
      <pages>412-431</pages>
      <abstract>Verifiable generation is introduced to improve the transparency and trustworthiness of outputs produced by large language models (LLMs). Recent studies observe that open-source models struggle to include accurate citations to supporting documents in their generation with in-context learning, in contrast to the strong performance demonstrated by proprietary models. Our work aims to reveal the critical design choices that can benefit open-source models, including generation pipelines, fine-tuning methods, and inference-time compute techniques.We consider three generation pipelines, producing the outputs directly or decomposing the generation into subtasks.These generation pipelines are fine-tuned using supervised fine-tuning and preference-based optimization including further fine-tuning with rejection sampling data and direct preference optimization (DPO).The construction of preference data with varying content and citation diversity is also investigated.Additionally, we examine the benefit of an additional reranking step. With four open-source models, our experiments show that directly generating the outputs achieves the best performance. Compared to other fine-tuning methods, DPO that computes training signals from contrastive pairs consistently yields better performance, and it reaches the peak performance when the contrastive pairs are constructed with sufficient content diversity.We also find that reranking can further boost the performance of verifiable generation systems, but the marginal improvement might not justify the additional cost.</abstract>
      <url hash="760d19fb">2025.trustnlp-main.27</url>
      <bibkey>cao-wang-2025-evaluating</bibkey>
    </paper>
    <paper id="28">
      <title>Battling Misinformation: An Empirical Study on Adversarial Factuality in Open-Source Large Language Models</title>
      <author><first>Shahnewaz Karim</first><last>Sakib</last><affiliation>University of Tennessee at Chattanooga</affiliation></author>
      <author><first>Anindya Bijoy</first><last>Das</last><affiliation>University of Akron</affiliation></author>
      <author orcid="0000-0003-1183-883X"><first>Shibbir</first><last>Ahmed</last><affiliation>Texas State University</affiliation></author>
      <pages>432-443</pages>
      <abstract>Adversarial factuality refers to the deliberate insertion of misinformation into input prompts by an adversary, characterized by varying levels of expressed confidence. In this study, we systematically evaluate the performance of several open-source large language models (LLMs) when exposed to such adversarial inputs. Three tiers of adversarial confidence are considered: strongly confident, moderately confident, and limited confidence. Our analysis encompasses eight LLMs: LLaMA 3.1 (8B), Phi 3 (3.8B), Qwen 2.5 (7B), Deepseek-v2 (16B), Gemma2 (9B), Falcon (7B), Mistrallite (7B), and LLaVA (7B). Empirical results indicate that LLaMA 3.1 (8B) exhibits a robust capability in detecting adversarial inputs, whereas Falcon (7B) shows comparatively lower performance. Notably, for the majority of the models, detection success improves as the adversary’s confidence decreases; however, this trend is reversed for LLaMA 3.1 (8B) and Phi 3 (3.8B), where a reduction in adversarial confidence corresponds with diminished detection performance. Further analysis of the queries that elicited the highest and lowest rates of successful attacks reveals that adversarial attacks are more effective when targeting less commonly referenced or obscure information.</abstract>
      <url hash="e7734243">2025.trustnlp-main.28</url>
      <bibkey>sakib-etal-2025-battling</bibkey>
    </paper>
    <paper id="29">
      <title>Will the Prince Get True Love’s Kiss? On the Model Sensitivity to Gender Perturbation over Fairytale Texts</title>
      <author><first>Christina A</first><last>Chance</last><affiliation>University of California, Los Angeles</affiliation></author>
      <author><first>Da</first><last>Yin</last></author>
      <author orcid="0000-0001-9371-9441"><first>Dakuo</first><last>Wang</last><affiliation>Northeastern University</affiliation></author>
      <author orcid="0000-0001-5365-0072"><first>Kai-Wei</first><last>Chang</last><affiliation>University of California, Los Angeles and Amazon</affiliation></author>
      <pages>444-460</pages>
      <abstract>In this paper, we study whether language models are affected by learned gender stereotypes during the comprehension of stories. Specifically, we investigate how models respond to gender stereotype perturbations through counterfactual data augmentation. Focusing on Question Answering (QA) tasks in fairytales, we modify the FairytaleQA dataset by swapping gendered character information and introducing counterfactual gender stereotypes during training. This allows us to assess model robustness and examine whether learned biases influence story comprehension. Our results show that models exhibit slight performance drops when faced with gender perturbations in the test set, indicating sensitivity to learned stereotypes. However, when fine-tuned on counterfactual training data, models become more robust to anti-stereotypical narratives. Additionally, we conduct a case study demonstrating how incorporating counterfactual anti-stereotype examples can improve inclusivity in downstream applications.</abstract>
      <url hash="cd852387">2025.trustnlp-main.29</url>
      <bibkey>chance-etal-2025-will</bibkey>
    </paper>
    <paper id="30">
      <title>Disentangling Linguistic Features with Dimension-Wise Analysis of Vector Embeddings</title>
      <author><first>Saniya</first><last>Karwa</last></author>
      <author><first>Navpreet</first><last>Singh</last></author>
      <pages>461-488</pages>
      <abstract>Understanding the inner workings of neural embeddings, particularly in models such as BERT, remains a challenge because of their high-dimensional and opaque nature. This paper proposes a framework for uncovering the specific dimensions of vector embeddings that encode distinct linguistic properties (LPs). We introduce the Linguistically Distinct Sentence Pairs (LDSP-10) dataset, which isolates ten key linguistic features such as synonymy, negation, tense, and quantity. Using this dataset, we analyze BERT embeddings with various methods, including the Wilcoxon signed-rank test, mutual information, and recursive feature elimination, to identify the most influential dimensions for each LP. We introduce a new metric, the Embedding Dimension Impact (EDI) score, which quantifies the relevance of each embedding dimension to a LP. Our findings show that certain properties, such as negation and polarity, are robustly encoded in specific dimensions, while others, like synonymy, exhibit more complex patterns. This study provides insights into the interpretability of embeddings, which can guide the development of more transparent and optimized language models, with implications for model bias mitigation and the responsible deployment of AI systems.</abstract>
      <url hash="f91ecee8">2025.trustnlp-main.30</url>
      <bibkey>karwa-singh-2025-disentangling</bibkey>
    </paper>
    <paper id="31">
      <title>Gender Encoding Patterns in Pretrained Language Model Representations</title>
      <author><first>Mahdi</first><last>Zakizadeh</last></author>
      <author><first>Mohammad Taher</first><last>Pilehvar</last><affiliation>Cardiff University and TeIAS</affiliation></author>
      <pages>489-500</pages>
      <abstract>Gender bias in pretrained language models (PLMs) poses significant social and ethical challenges. Despite growing awareness, there is a lack of comprehensive investigation into how different models internally represent and propagate such biases. This study adopts an information-theoretic approach to analyze how gender biases are encoded within various encoder-based architectures.We focus on three key aspects: identifying how models encode gender information and biases, examining the impact of bias mitigation techniques and fine-tuning on the encoded biases and their effectiveness, and exploring how model design differences influence the encoding of biases.Through rigorous and systematic investigation, our findings reveal a consistent pattern of gender encoding across diverse models. Surprisingly, debiasing techniques often exhibit limited efficacy, sometimes inadvertently increasing the encoded bias in internal representations while reducing bias in model output distributions. This highlights a disconnect between mitigating bias in output distributions and addressing its internal representations. This work provides valuable guidance for advancing bias mitigation strategies and fostering the development of more equitable language models.</abstract>
      <url hash="ddb762ab">2025.trustnlp-main.31</url>
      <bibkey>zakizadeh-pilehvar-2025-gender</bibkey>
    </paper>
    <paper id="32">
      <title>Defining and Quantifying Visual Hallucinations in Vision-Language Models</title>
      <author orcid="0000-0002-4355-1393"><first>Vipula</first><last>Rawte</last></author>
      <author><first>Aryan</first><last>Mishra</last></author>
      <author orcid="0000-0002-0021-5293"><first>Amit</first><last>Sheth</last><affiliation>University of South Carolina</affiliation></author>
      <author><first>Amitava</first><last>Das</last><affiliation>University of South Carolina</affiliation></author>
      <pages>501-510</pages>
      <abstract>The troubling rise of hallucination presents perhaps the most significant impediment to the advancement of responsible AI. In recent times, considerable research has focused on detecting and mitigating hallucination in Large Language Models (LLMs). However, it’s worth noting that hallucination is also quite prevalent in Vision-Language models (VLMs). In this paper, we offer a fine-grained discourse on profiling VLM hallucination based on the image captioning task. We delineate eight fine-grained orientations of visual hallucination: i) Contextual Guessing, ii) Identity Incongruity, iii) Geographical Erratum, iv) Visual Illusion, v) Gender Anomaly, vi) VLM as Classifier, vii) Wrong Reading, and viii) Numeric Discrepancy. We curate Visual HallucInation eLiciTation, a publicly available dataset comprising 2,000 samples generated using eight VLMs across the image captioning task, along with human annotations for the categories as mentioned earlier. To establish a method for quantification and to offer a comparative framework enabling the evaluation and ranking of VLMs according to their vulnerability to producing hallucinations, we propose the Visual Hallucination Vulnerability Index (VHVI). In summary, we introduce the VHILT dataset for image-to-text hallucinations and propose the VHVI metric to quantify hallucinations in VLMs, targeting specific visual hallucination types. A subset sample is available at: <url>https://huggingface.co/datasets/vr25/vhil</url>. The full dataset will be publicly released upon acceptance.</abstract>
      <url hash="e865326d">2025.trustnlp-main.32</url>
      <bibkey>rawte-etal-2025-defining</bibkey>
    </paper>
    <paper id="33">
      <title>Revitalizing Saturated Benchmarks: A Weighted Metric Approach for Differentiating Large Language Model Performance</title>
      <author><first>Bryan</first><last>Etzine</last></author>
      <author orcid="0000-0002-6910-5367"><first>Masoud</first><last>Hashemi</last><affiliation>ServiceNow Inc</affiliation></author>
      <author><first>Nishanth</first><last>Madhusudhan</last><affiliation>ServiceNow Inc</affiliation></author>
      <author><first>Sagar</first><last>Davasam</last><affiliation>ServiceNow Inc</affiliation></author>
      <author><first>Roshnee</first><last>Sharma</last><affiliation>ServiceNow Inc</affiliation></author>
      <author><first>Sathwik Tejaswi</first><last>Madhusudhan</last><affiliation>ServiceNow Inc</affiliation></author>
      <author><first>Vikas</first><last>Yadav</last></author>
      <pages>511-523</pages>
      <abstract>Existing benchmarks are becoming saturated and less effective in evaluating model performance due to factors such as data contamination and the advancing capabilities of the Large Language Models (LLMs). This paper introduces EMDM (Enhanced Model Differentiation Metric), a novel weighted metric designed to revitalize existing benchmarks. EMDM implements a weighting schema for samples based on their complexity and requisite knowledge, utilizing the performance of a baseline LLM in two experimental setups: “Unguided”, where the model has no prior exposure to test samples, and “Guided”, where the model has prior knowledge about the desired answer. This schema is leveraged in an optimization objective to assign weights to test samples, distinguishing instances of varying complexity. EMDM accounts for both answer correctness and the depth and accuracy of reasoning, offering a more nuanced evaluation of model performance. By weighting test examples based on their required reasoning and knowledge, EMDM achieves a distinguishing range of evaluation scores of 46% among various LLMs, compared to just 17% with traditional exact match (EM) metrics, thereby highlighting the saturation of current evaluation methods.</abstract>
      <url hash="a1325500">2025.trustnlp-main.33</url>
      <bibkey>etzine-etal-2025-revitalizing</bibkey>
    </paper>
    <paper id="34">
      <title>Synthetic Lyrics Detection Across Languages and Genres</title>
      <author orcid="0000-0003-1072-3862"><first>Yanis</first><last>Labrak</last></author>
      <author><first>Markus</first><last>Frohmann</last><affiliation>Johannes Kepler Universität Linz</affiliation></author>
      <author><first>Gabriel</first><last>Meseguer-Brocal</last><affiliation>Deezer</affiliation></author>
      <author orcid="0000-0002-6930-9482"><first>Elena V.</first><last>Epure</last><affiliation>Deezer</affiliation></author>
      <pages>524-541</pages>
      <abstract>In recent years, the use of large language models (LLMs) to generate music content, particularly lyrics, has gained in popularity. These advances provide valuable tools for artists and enhance their creative processes, but they also raise concerns about copyright violations, consumer satisfaction, and content spamming. Previous research has explored content detection in various domains. However, no work has focused on the text modality, lyrics, in music. To address this gap, we curated a diverse dataset of real and synthetic lyrics from multiple languages, music genres, and artists. The generation pipeline was validated using both humans and automated methods. We performed a thorough evaluation of existing synthetic text detection approaches on lyrics, a previously unexplored data type. We also investigated methods to adapt the best-performing features to lyrics through unsupervised domain adaptation. Following both music and industrial constraints, we examined how well these approaches generalize across languages, scale with data availability, handle multilingual language content, and perform on novel genres in few-shot settings. Our findings show promising results that could inform policy decisions around AI-generated music and enhance transparency for users.</abstract>
      <url hash="94fc6844">2025.trustnlp-main.34</url>
      <bibkey>labrak-etal-2025-synthetic</bibkey>
    </paper>
    <paper id="35">
      <title>A Lightweight Multi Aspect Controlled Text Generation Solution For Large Language Models</title>
      <author orcid="0009-0003-6390-930X"><first>Chenyang</first><last>Zhang</last></author>
      <author><first>Jiayi</first><last>Lin</last></author>
      <author><first>Haibo</first><last>Tong</last></author>
      <author orcid="0009-0004-8068-9211"><first>Bingxuan</first><last>Hou</last></author>
      <author><first>Dongyu</first><last>Zhang</last></author>
      <author orcid="0009-0006-3796-4331"><first>Jialin</first><last>Li</last></author>
      <author orcid="0000-0002-7185-9731"><first>Junli</first><last>Wang</last><affiliation>Tongji University</affiliation></author>
      <pages>542-551</pages>
      <abstract>Multi-Aspect Controllable Text Generation (MCTG) introduces fine-grained multiple constraints in natural language generation, i.e. control attributes in topics, sentiments, and detoxification.MCTG demonstrates application prospects for trustworthy generation of Large Language Models (LLMs) but is limited by generalization issues.Existing work exploits additional structures and strategies for solutions, requiring LLMs’ modifications.To activate LLMs’ MCTG ability, we propose a lightweight MCTG pipeline based on data augmentation and instruction tuning.We analyze aspect bias and correlations in traditional datasets and address these concerns with augmented control attributes and sentences.Augmented datasets are feasible for instruction tuning.We conduct experiments for various LLMs backbone and parameter sizes, demonstrating general effectiveness on MCTG performance.</abstract>
      <url hash="29b1dd52">2025.trustnlp-main.35</url>
      <bibkey>zhang-etal-2025-lightweight</bibkey>
    </paper>
    <paper id="36">
      <title>Gender Bias in Large Language Models across Multiple Languages: A Case Study of <fixed-case>C</fixed-case>hat<fixed-case>GPT</fixed-case></title>
      <author><first>YiTian</first><last>Ding</last></author>
      <author><first>Jinman</first><last>Zhao</last></author>
      <author orcid="0000-0002-8666-9930"><first>Chen</first><last>Jia</last><affiliation>SI-TECH Information Technology Co., Ltd</affiliation></author>
      <author><first>Yining</first><last>Wang</last></author>
      <author><first>Zifan</first><last>Qian</last></author>
      <author><first>Weizhe</first><last>Chen</last></author>
      <author><first>Xingyu</first><last>Yue</last></author>
      <pages>552-579</pages>
      <abstract>With the growing deployment of large language models (LLMs) across various applications, assessing the influence of gender biases embedded in LLMs becomes crucial. The topic of gender bias within the realm of natural language processing (NLP) has gained considerable focus, particularly in the context of English. Nonetheless, the investigation of gender bias in languages other than English is still relatively under-explored and insufficiently analyzed. In this work, We examine gender bias in LLMs-generated outputs for different languages. We use three measurements: 1) gender bias in selecting descriptive words given the gender-related context. 2) gender bias in selecting gender-related pronouns (she/he) given the descriptive words. 3) gender bias in the topics of LLM-generated dialogues. We investigate the outputs of the GPT series of LLMs in various languages using our three measurement methods. Our findings revealed significant gender biases across all the languages we examined.</abstract>
      <url hash="6ccf7a18">2025.trustnlp-main.36</url>
      <bibkey>ding-etal-2025-gender</bibkey>
    </paper>
    <paper id="37">
      <title>Investigating and Addressing Hallucinations of <fixed-case>LLM</fixed-case>s in Tasks Involving Negation</title>
      <author><first>Neeraj</first><last>Varshney</last><affiliation>Amazon</affiliation></author>
      <author><first>Satyam</first><last>Raj</last></author>
      <author><first>Venkatesh</first><last>Mishra</last></author>
      <author><first>Agneet</first><last>Chatterjee</last><affiliation>Arizona State University</affiliation></author>
      <author><first>Amir</first><last>Saeidi</last><affiliation>Arizona State University</affiliation></author>
      <author orcid="0000-0003-4441-3635"><first>Ritika</first><last>Sarkar</last><affiliation>Arizona State University</affiliation></author>
      <author orcid="0000-0002-7549-723X"><first>Chitta</first><last>Baral</last><affiliation>Arizona State University</affiliation></author>
      <pages>580-598</pages>
      <abstract>Large Language Models (LLMs) have achieved remarkable performance across a wide variety of natural language tasks. However, they have been shown to suffer from a critical limitation pertinent to ‘hallucination’ in their output. Recent research has focused on investigating and addressing this problem for a variety of tasks such as biography generation, question answering, abstractive summarization, and dialogue generation. However, the crucial aspect pertaining to ‘negation’ has remained considerably underexplored. Negation is important because it adds depth and nuance to the understanding of language and is also crucial for logical reasoning and inference. In this work, we address the above limitation and particularly focus on studying the impact of negation in LLM hallucinations. Specifically, we study four tasks with negation: ‘false premise completion’, ‘constrained fact generation’, ‘multiple choice question answering’, and ‘fact generation’. We show that open-source state-of-the-art LLMs such as LLaMA-2-chat, Vicuna, and Orca-2 hallucinate considerably on all these tasks involving negation which underlines a critical shortcoming of these models. Addressing this problem, we further study numerous strategies to mitigate these hallucinations and demonstrate their impact.</abstract>
      <url hash="90d06179">2025.trustnlp-main.37</url>
      <bibkey>varshney-etal-2025-investigating</bibkey>
    </paper>
    <paper id="38">
      <title><fixed-case>FACTOID</fixed-case>: <fixed-case>FAC</fixed-case>tual en<fixed-case>T</fixed-case>ailment f<fixed-case>O</fixed-case>r halluc<fixed-case>I</fixed-case>nation Detection</title>
      <author orcid="0000-0002-4355-1393"><first>Vipula</first><last>Rawte</last></author>
      <author orcid="0009-0000-6076-7068"><first>S.m Towhidul Islam</first><last>Tonmoy</last><affiliation>University of South Carolina</affiliation></author>
      <author><first>Shravani</first><last>Nag</last></author>
      <author orcid="0000-0001-6621-9003"><first>Aman</first><last>Chadha</last><affiliation>Amazon Web Services</affiliation></author>
      <author orcid="0000-0002-0021-5293"><first>Amit</first><last>Sheth</last><affiliation>University of South Carolina</affiliation></author>
      <author><first>Amitava</first><last>Das</last><affiliation>University of South Carolina</affiliation></author>
      <pages>599-617</pages>
      <url hash="95ccd9c3">2025.trustnlp-main.38</url>
      <bibkey>rawte-etal-2025-factoid</bibkey>
    </paper>
  </volume>
</collection>
