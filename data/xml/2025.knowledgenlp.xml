<?xml version='1.0' encoding='UTF-8'?>
<collection id="2025.knowledgenlp">
  <volume id="1" ingest-date="2025-04-26" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 4th International Workshop on Knowledge-Augmented Methods for Natural Language Processing</booktitle>
      <editor><first>Weijia</first><last>Shi</last></editor>
      <editor><first>Wenhao</first><last>Yu</last></editor>
      <editor><first>Akari</first><last>Asai</last></editor>
      <editor><first>Meng</first><last>Jiang</last></editor>
      <editor><first>Greg</first><last>Durrett</last></editor>
      <editor><first>Hannaneh</first><last>Hajishirzi</last></editor>
      <editor><first>Luke</first><last>Zettlemoyer</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Albuquerque, New Mexic, USA</address>
      <month>May</month>
      <year>2025</year>
      <url hash="4389e9f6">2025.knowledgenlp-1</url>
      <venue>knowledgenlp</venue>
      <venue>ws</venue>
      <isbn>979-8-89176-229-9</isbn>
    </meta>
    <frontmatter>
      <url hash="bdff085d">2025.knowledgenlp-1.0</url>
      <bibkey>knowledgenlp-ws-2025-1</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Entity Retrieval for Answering Entity-Centric Questions</title>
      <author><first>Hassan</first><last>Shavarani</last></author>
      <author><first>Anoop</first><last>Sarkar</last><affiliation>Simon Fraser University</affiliation></author>
      <pages>1-17</pages>
      <abstract>The similarity between the question and indexed documents is a key factor in document retrieval for retrieval-augmented question answering. Although this is typically the only method for obtaining the relevant documents, it is not the sole approach when dealing with entity-centric questions. We study Entity Retrieval, an alternative retrieval method, which rather than relying on question-document similarity, depends on the salient entities within the question to identify the retrieval documents. We conduct an in-depth analysis of the performance of both dense and sparse retrieval methods in comparison to Entity Retrieval. Our findings reveal the great potential of entity-driven methods for improving augmentation document retrieval in both accuracy and efficiency.</abstract>
      <url hash="56501342">2025.knowledgenlp-1.1</url>
      <bibkey>shavarani-sarkar-2025-entity</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>ELECTRA</fixed-case> and <fixed-case>GPT</fixed-case>-4o: Cost-Effective Partners for Sentiment Analysis</title>
      <author><first>James P.</first><last>Beno</last><affiliation>Stanford Engineering CGOE</affiliation></author>
      <pages>18-36</pages>
      <abstract>Bidirectional transformers excel at sentiment analysis, and Large LanguageModels (LLM) are effective zero-shot learners. Might they perform better as ateam? This paper explores collaborative approaches between ELECTRA and GPT-4ofor three-way sentiment classification. We fine-tuned (FT) four models (ELECTRABase/Large, GPT-4o/4o-mini) using a mix of reviews from Stanford SentimentTreebank (SST) and DynaSent. We provided input from ELECTRA to GPT as:predicted label, probabilities, and retrieved examples. Sharing ELECTRA Base FTpredictions with GPT-4o-mini significantly improved performance over eithermodel alone (82.50 macro F1 vs. 79.14 ELECTRA Base FT, 79.41 GPT-4o-mini) andyielded the lowest cost/performance ratio (

<tex-math>0.12/F1 point). However, when GPTmodels were fine-tuned, including predictions decreased performance. GPT-4oFT-M was the top performer (86.99), with GPT-4o-mini FT close behind (86.70) atmuch less cost (\\</tex-math>0.38 vs.

$1.59F1 point). Our results show that augmentingprompts with predictions from fine-tuned encoders is an efficient way to boostperformance, and a fine-tuned GPT-4o-mini is nearly as good as GPT-4o FT at 76%less cost. Both are affordable options for projects with limited resources.</abstract>
      <url hash="5d8374c4">2025.knowledgenlp-1.2</url>
      <bibkey>beno-2025-electra</bibkey>
    </paper>
    <paper id="3">
      <title>Retrieval of Temporal Event Sequences from Textual Descriptions</title>
      <author><first>Zefang</first><last>Liu</last><affiliation>J.P. Morgan Chase and Georgia Institute of Technology</affiliation></author>
      <author><first>Yinzhu</first><last>Quan</last><affiliation>Georgia Institute of Technology</affiliation></author>
      <pages>37-49</pages>
      <abstract>Retrieving temporal event sequences from textual descriptions is crucial for applications such as analyzing e-commerce behavior, monitoring social media activities, and tracking criminal incidents. To advance this task, we introduce TESRBench, a comprehensive benchmark for temporal event sequence retrieval (TESR) from textual descriptions. TESRBench includes diverse real-world datasets with synthesized and reviewed textual descriptions, providing a strong foundation for evaluating retrieval performance and addressing challenges in this domain. Building on this benchmark, we propose TPP-Embedding, a novel model for embedding and retrieving event sequences. The model leverages the TPP-LLM framework, integrating large language models (LLMs) with temporal point processes (TPPs) to encode both event texts and times. By pooling representations and applying a contrastive loss, it unifies temporal dynamics and event semantics in a shared embedding space, aligning sequence-level embeddings of event sequences and their descriptions. TPP-Embedding demonstrates superior performance over baseline models across TESRBench datasets, establishing it as a powerful solution for the temporal event sequence retrieval task.</abstract>
      <url hash="182c44b1">2025.knowledgenlp-1.3</url>
      <bibkey>liu-quan-2025-retrieval</bibkey>
    </paper>
    <paper id="4">
      <title>Generating Tables from the Parametric Knowledge of Language Models</title>
      <author><first>Yevgeni</first><last>Berkovitch</last><affiliation>eBay Inc.</affiliation></author>
      <author><first>Oren</first><last>Glickman</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Amit</first><last>Somech</last><affiliation>Bar-Ilan University</affiliation></author>
      <author><first>Tomer</first><last>Wolfson</last><affiliation>University of Pennsylvania, University of Pennsylvania</affiliation></author>
      <pages>50-65</pages>
      <abstract>We explore generating factual tables from the parametric knowledge of large language models (LLMs). While LLMs have demonstrated impressive capabilities in recreating knowledge bases and generating free-form text, their ability to generate structured tabular data has received little attention. To address this gap, we explore the table generation abilities of eight state-of-the-art LLMs, including GPT-4o and Llama3.1-405B, using three prompting methods: full-table, row-by-row, and cell-by-cell. To facilitate evaluation we introduce WikiTabGen, a new benchmark consisting of 119 manually curated Wikipedia tables and their description. Our findings show that table generation remains challenging, with the best performing model (LLaMA3.1-405B) reaching only 25.4% accuracy. We further analyze how properties like table size, popularity, and numerical content impact performance. This study highlights the unique challenges of LLM-based table generation and offers a foundation for future research in this area. All code, data, and prompts are publicly available.</abstract>
      <url hash="2fa7f9e0">2025.knowledgenlp-1.4</url>
      <bibkey>berkovitch-etal-2025-generating</bibkey>
    </paper>
    <paper id="5">
      <title>Investigating Large Language Models for Text-to-<fixed-case>SPARQL</fixed-case> Generation</title>
      <author><first>Jacopo</first><last>D’Abramo</last><affiliation>University of Bologna</affiliation></author>
      <author><first>Andrea</first><last>Zugarini</last><affiliation>Expert.ai Srl</affiliation></author>
      <author><first>Paolo</first><last>Torroni</last><affiliation>University of Bologna</affiliation></author>
      <pages>66-80</pages>
      <abstract>Large Language Models (LLMs) have demonstrated strong capabilities in code generation, such as translating natural language questions into SQL queries. However, state-of-the-art solutions often involve a costly fine-tuning step. In this study, we extensively evaluate In-Context Learning (ICL) solutions for text-to-SPARQL generation with different architectures and configurations, based on methods for retrieving relevant demonstrations for few-shot prompting and working with multiple generated hypotheses. In this way, we demonstrate that LLMs can formulate SPARQL queries achieving state-of-the-art results on several Knowledge Graph Question Answering (KGQA) benchmark datasets without fine-tuning.</abstract>
      <url hash="ad0da72d">2025.knowledgenlp-1.5</url>
      <bibkey>dabramo-etal-2025-investigating</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>GAVEL</fixed-case>: Generative Attribute-Value Extraction Using <fixed-case>LLM</fixed-case>s on <fixed-case>LLM</fixed-case>-Augmented Datasets</title>
      <author><first>Pollawat</first><last>Hongwimol</last><affiliation>Alibaba Group</affiliation></author>
      <author><first>Dong</first><last>Sheng</last></author>
      <author id="li-zhang-aws"><first>Li</first><last>Zhang</last></author>
      <author><first>Kai</first><last>Liu</last></author>
      <author><first>Xiufei</first><last>Wang</last></author>
      <pages>81-90</pages>
      <abstract>In the evolving e-commerce landscape, accurate product attribute-value extraction is crucial for enhancing user experience and increasing sales. This paper introduces GAVEL, a generative approach leveraging large language models (LLMs) to augment training data for attribute extraction from diverse textual sources. Our method extracts over 1,000 unique attributes across 2,000 product categories in multiple Southeast Asian languages, including Thai, Vietnamese, and Indonesian. Rigorous evaluations show significant improvements in accuracy and coverage compared to seller-provided attributes, with enhanced recall and F1 scores. Additionally, GAVEL reduces operational costs by minimizing instruction token usage and improves inference speed. The results of the A/B testing indicate that our model has a positive impact on Gross Merchandise Value (GMV) per page view (PV) across all three operating countries. This research highlights the potential of generative techniques for optimizing attribute extraction in multi-language e-commerce applications.</abstract>
      <url hash="3605d16e">2025.knowledgenlp-1.6</url>
      <bibkey>hongwimol-etal-2025-gavel</bibkey>
    </paper>
    <paper id="7">
      <title>Leveraging Domain Knowledge at Inference Time for <fixed-case>LLM</fixed-case> Translation: Retrieval versus Generation</title>
      <author><first>Bryan</first><last>Li</last></author>
      <author><first>Jiaming</first><last>Luo</last><affiliation>Google</affiliation></author>
      <author><first>Eleftheria</first><last>Briakou</last><affiliation>Google</affiliation></author>
      <author><first>Colin</first><last>Cherry</last><affiliation>Google</affiliation></author>
      <pages>91-106</pages>
      <abstract>While large language models (LLMs) have been increasingly adopted for machine translation (MT), their performance for specialist domains such as medicine and law remains an open challenge. Prior work has shown that LLMs can be domain-adapted at test-time by retrieving targeted few-shot demonstrations or terminologies for inclusion in the prompt. Meanwhile, for general-purpose LLM MT, recent studies have found some success in generating similarly useful domain knowledge from an LLM itself, prior to translation. Our work studies domain-adapted MT with LLMs through a careful prompting setup, finding that demonstrations consistently outperform terminology, and retrieval consistently outperforms generation. We find that generating demonstrations with weaker models can close the gap with larger model’s zero-shot performance. Given the effectiveness of demonstrations, we perform detailed analyses to understand their value. We find that domain-specificity is particularly important, and that the popular multi-domain benchmark is testing adaptation to a particular writing style more so than to a specific domain.</abstract>
      <url hash="1ce95fef">2025.knowledgenlp-1.7</url>
      <bibkey>li-etal-2025-leveraging</bibkey>
    </paper>
    <paper id="8">
      <title>Enhancing Cross-Language Code Translation via Task-Specific Embedding Alignment in Retrieval-Augmented Generation</title>
      <author><first>Manish</first><last>Bhattarai</last><affiliation>Los Alamos National Laboratory</affiliation></author>
      <author><first>Minh N.</first><last>Vu</last><affiliation>Los Alamos National Laboratory</affiliation></author>
      <author><first>Javier</first><last>E. Santos</last><affiliation>Los Alamos National Laboratory</affiliation></author>
      <author><first>Ismael</first><last>Ismael</last><affiliation>Los Alamos National Laboratory</affiliation></author>
      <author><first>Daniel</first><last>O’Malley</last><affiliation>Los Alamos National Laboratory</affiliation></author>
      <pages>107-117</pages>
      <abstract>We introduce a novel method to enhance cross-language code translation from Fortran to C++ by integrating task-specific embedding alignment into a Retrieval-Augmented Generation (RAG) framework. Unlike conventional retrieval approaches that utilize generic embeddings agnostic to the downstream task, our strategy aligns the retrieval model directly with the objective of maximizing translation quality, as quantified by the CodeBLEU metric. This alignment ensures that the embeddings are semantically and syntactically meaningful for the specific code translation task. Our methodology involves constructing a dataset of 25,000 Fortran code snippets sourced from Stack-V2 dataset and generating their corresponding C++ translations using the LLaMA 3.1-8B language model. We compute pairwise CodeBLEU scores between the generated translations and ground truth examples to capture fine-grained similarities. These scores serve as supervision signals in a contrastive learning framework, where we optimize the embedding model to retrieve Fortran-C++ pairs that are most beneficial for improving the language model’s translation performance. By integrating these CodeBLEU-optimized embeddings into the RAG framework, our approach significantly enhances both retrieval accuracy and code generation quality over methods employing generic embeddings. On the HPC Fortran2C++ dataset, our method elevates the average CodeBLEU score from 0.64 to 0.73, achieving a 14% relative improvement. On the Numerical Recipes dataset, we observe an increase from 0.52 to 0.60, marking a 15% relative improvement. Importantly, these gains are realized without any fine-tuning of the language model, underscoring the efficiency and practicality of our approach.</abstract>
      <url hash="e1f0ce76">2025.knowledgenlp-1.8</url>
      <bibkey>bhattarai-etal-2025-enhancing</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>LLM</fixed-case> Reasoning Engine: Specialized Training for Enhanced Mathematical Reasoning</title>
      <author><first>Shuguang</first><last>Chen</last><affiliation>Purdue University</affiliation></author>
      <author><first>Guang</first><last>Lin</last><affiliation>Purdue University</affiliation></author>
      <pages>118-128</pages>
      <abstract>Large Language Models (LLMs) have shown remarkable performance in various natural language processing tasks but face challenges in mathematical reasoning, where complex problem-solving requires both linguistic understanding and mathematical reasoning skills. Existing approaches to address this challenge often rely on ensemble methods and suffer from the problem of data scarcity in target domains. In this work, we present a novel method to enhance the capabilities of LLMs in mathematical reasoning tasks. Motivated by the need to bridge this gap, our approach incorporates a question paraphrase strategy, which aims to diversify the linguistic forms of mathematical questions to improve generalization. Additionally, specialized training objectives are employed to guide the model’s learning process, focusing on enhancing its understanding of mathematical concepts and reasoning processes. We conduct experiments on four datasets using different LLMs, and demonstrate the effectiveness of our approach in improving LLMs’ performance on mathematical reasoning tasks. Our findings underscore the significance of our methodology in advancing large language models and their potential implications for real-world applications that require mathematical reasoning abilities.</abstract>
      <url hash="f29bbedb">2025.knowledgenlp-1.9</url>
      <bibkey>chen-lin-2025-llm</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>R</fixed-case>oute<fixed-case>N</fixed-case>ator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling <fixed-case>LLM</fixed-case>s</title>
      <author><first>Dewang</first><last>Sultania</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Vibha</first><last>Belavadi</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Tushar</first><last>Vatsa</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Suhas</first><last>Suresha</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Ishita</first><last>Verma</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Tracy Holloway</first><last>King</last><affiliation>Adobe Systems</affiliation></author>
      <author><first>Mifriedr</first><last>Mifriedr</last><affiliation>NA</affiliation></author>
      <author><first>Cheng</first><last>Chen</last><affiliation>Adobe Systems</affiliation></author>
      <pages>129-140</pages>
      <abstract>This paper addresses fine-tuning Large Language Models (LLMs) for function calling tasks when real user interaction data is unavailable. In digital content creation tools, where users express their needs through natural language queries that must be mapped to API calls, the lack of real-world task-specific data and privacy constraints for training on it necessitate synthetic data generation. Existing approaches to synthetic data generation fall short in diversity and complexity, failing to replicate real-world data distributions and leading to suboptimal performance after LLM fine-tuning. We present a novel router-based architecture that leverages domain resources like content metadata and structured knowledge graphs, along with text-to-text and vision-to-text language models to generate high-quality synthetic training data. Our architecture’s flexible routing mechanism enables synthetic data generation that matches observed real-world distributions, addressing a fundamental limitation of traditional approaches. Evaluation on a comprehensive set of real user queries demonstrates significant improvements in both function classification accuracy and API parameter selection. Models fine-tuned with our synthetic data consistently outperform traditional approaches, establishing new benchmarks for function calling tasks.</abstract>
      <url hash="30ea48c8">2025.knowledgenlp-1.10</url>
      <bibkey>sultania-etal-2025-routenator</bibkey>
    </paper>
    <paper id="12">
      <title><fixed-case>S</fixed-case>to<fixed-case>C</fixed-case>-<fixed-case>TOT</fixed-case>: Stochastic Tree-of-Thought with Constrained Decoding for Complex Reasoning in Multi-Hop Question Answering</title>
      <author><first>Zhenyu</first><last>Bi</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <author><first>Daniel</first><last>Hajialigol</last></author>
      <author><first>Zhongkai</first><last>Sun</last></author>
      <author><first>Jie</first><last>Hao</last><affiliation>Amazon</affiliation></author>
      <author><first>Xuan</first><last>Wang</last><affiliation>Virginia Polytechnic Institute and State University</affiliation></author>
      <pages>141-151</pages>
      <abstract>Multi-hop question answering (MHQA) requires a model to retrieve and integrate information from multiple passages to answer a complex question. Recent systems leverage the power of large language models and integrate evidence retrieval with reasoning prompts (e.g., chain-of-thought reasoning) for the MHQA task. However, the complexities in the question types (bridge v.s. comparison questions) and the reasoning types (sequential v.s. parallel reasonings) require more novel and fine-grained prompting methods to enhance the performance of MHQA under the zero-shot setting.In this paper, we propose StoC-ToT, a stochastic tree-of-thought reasoning prompting method with constrained decoding for MHQA and conduct a detailed comparison with other reasoning prompts on different question types and reasoning types. Specifically, we construct a tree-like reasoning structure by prompting the model to break down the original question into smaller sub-questions to form different reasoning paths. In addition, we prompt the model to provide a probability estimation for each reasoning path at each reasoning step. At answer time, we conduct constrained decoding on the model to generate more grounded answers and reduce hallucination. Experiments comparing StoC-ToT with on two MHQA datasets and five large language models showed that outperforms other reasoning prompts by a significant margin.</abstract>
      <url hash="0f7bd343">2025.knowledgenlp-1.12</url>
      <bibkey>bi-etal-2025-stoc</bibkey>
    </paper>
    <paper id="13">
      <title><fixed-case>EKRAG</fixed-case>: Benchmark <fixed-case>RAG</fixed-case> for Enterprise Knowledge Question Answering</title>
      <author><first>Tan</first><last>Yu</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Wenfei</first><last>Zhou</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Leiyang</first><last>Leiyang</last><affiliation>NA</affiliation></author>
      <author><first>Aaditya</first><last>Shukla</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Mmadugula</first><last>Mmadugula</last><affiliation>NA</affiliation></author>
      <author><first>Pritam</first><last>Gundecha</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Nicholas</first><last>Burnett</last><affiliation>NVIDIA</affiliation></author>
      <author><first>Anbang</first><last>Xu</last><affiliation>LinkedIn</affiliation></author>
      <author><first>Viseth</first><last>Viseth</last><affiliation>NA</affiliation></author>
      <author><first>Tbar</first><last>Tbar</last><affiliation>NA</affiliation></author>
      <author><first>Rama</first><last>Akkiraju</last></author>
      <author><first>Vivienne</first><last>Zhang</last></author>
      <pages>152-159</pages>
      <abstract>Retrieval-augmented generation (RAG) offers a robust solution for developing enterprise internal virtual assistants by leveraging domain-specific knowledge and utilizing information from frequently updated corporate document repositories. In this work, we introduce the Enterprise-Knowledge RAG (EKRAG) dataset to benchmark RAG for enterprise knowledge question-answering (QA) across a diverse range of corporate documents, such as product releases, technical blogs, and financial reports. Using EKRAG, we systematically evaluate various retrieval models and strategies tailored for corporate content. We propose novel embedding-model (EM)-as-judge and ranking-model (RM)-as-judge approaches to assess answer quality in the context of enterprise information. Combining these with the existing LLM-as-judge method, we then comprehensively evaluate the correctness, relevance, and faithfulness of generated answers to corporate queries. Our extensive experiments shed light on optimizing RAG pipelines for enterprise knowledge QA, providing valuable guidance for practitioners. This work contributes to enhancing information retrieval and question-answering capabilities in corporate environments that demand high degrees of factuality and context-awareness.</abstract>
      <url hash="691be6ec">2025.knowledgenlp-1.13</url>
      <bibkey>yu-etal-2025-ekrag</bibkey>
    </paper>
    <paper id="17">
      <title>Towards Effectively Leveraging Execution Traces for Program Repair with Code <fixed-case>LLM</fixed-case>s</title>
      <author><first>Mirazul</first><last>Haque</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Petr</first><last>Babkin</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Farima</first><last>Farmahinifarahani</last><affiliation>J.P. Morgan Chase</affiliation></author>
      <author><first>Manuela</first><last>Veloso</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>160-179</pages>
      <abstract>Large Language Models (LLMs) show promising performance on various programming tasks, including Automatic Program Repair (APR).However, most approaches to LLM-based APR are limited to the static analysis of the programs, while disregarding their runtime behavior.Inspired by knowledge-augmented NLP, in this work, we aim to remedy this potential blind spot by augmenting standard APR prompts with program execution traces.We evaluate our approach using the GPT family of models on three popular APR datasets. Our findings suggest that simply incorporating execution traces into the prompt provides a limited performance improvement over trace-free baselines, in only 2 out of 6 tested dataset/model configurations. We further find that the effectiveness of execution traces for APR diminishes as their complexity increases. We explore several strategies for leveraging traces in promptsand demonstrate that LLM-optimized prompts help outperform trace-free prompts more consistently.Additionally, we show trace-based prompting to be superior to finetuning a smaller LLM on a small-scale dataset; and conduct probing studies reinforcing the notion that execution traces can complement the reasoning abilities of the LLMs.</abstract>
      <url hash="93f63abb">2025.knowledgenlp-1.17</url>
      <bibkey>haque-etal-2025-towards</bibkey>
    </paper>
    <paper id="18">
      <title>A Novel Multi-Document Retrieval Benchmark: Journalist Source-Selection in Newswriting</title>
      <author><first>Alexander</first><last>Spangher</last><affiliation>University of Southern California</affiliation></author>
      <author><first>Tenghao</first><last>Huang</last></author>
      <author><first>Yiqin</first><last>Huang</last><affiliation>University of California, Berkeley</affiliation></author>
      <author><first>Lucas</first><last>Spangher</last><affiliation>University of California Berkeley</affiliation></author>
      <author><first>Sewon</first><last>Min</last><affiliation>University of California, Berkeley and Allen Institute for Artificial Intelligence</affiliation></author>
      <author><first>Mark</first><last>Dredze</last><affiliation>Department of Computer Science, Whiting School of Engineering and Bloomberg</affiliation></author>
      <pages>180-204</pages>
      <abstract>Multi-document retrieval approaches often overlook the ways different retrievals complement each other when addressing complex queries. In this work, we study journalist source selection in news article writing and examine the discourse roles that different sources serve when paired together, finding that discourse function (not simply informational content) is an important component of source usage. Then, we introduce a novel IR task to benchmark how well language models can reason about this narrative process. We extract a journalist’s initial query and the sources they used from news articles and aim to recover the sources that support this query. We demonstrate that large language models (LLMs) can be employed in multi-step query planning, identifying informational gaps and enhancing retrieval performance, but current approaches to interleave queries fall short. By training auxiliary discourse planners and incorporating this information into LLMs, we enhance query planning, achieving a significant 5% improvement in precision and a 2% increase in F1 score over the previous SOTA, all while maintaining recall.</abstract>
      <url hash="6ebb4427">2025.knowledgenlp-1.18</url>
      <bibkey>spangher-etal-2025-novel</bibkey>
    </paper>
    <paper id="19">
      <title><fixed-case>HEAL</fixed-case>: Hierarchical Embedding Alignment Loss for Improved Retrieval and Representation Learning</title>
      <author><first>Manish</first><last>Bhattarai</last><affiliation>Los Alamos National Laboratory</affiliation></author>
      <author><first>Ryan</first><last>Barron</last><affiliation>, University of Maryland, Baltimore County and Los Alamos National Laboratory</affiliation></author>
      <author><first>Maksim E.</first><last>Eren</last></author>
      <author><first>Minh N.</first><last>Vu</last><affiliation>Los Alamos National Laboratory</affiliation></author>
      <author><first>Vesselin</first><last>Grantcharov</last></author>
      <author><first>Ismael</first><last>Ismael</last><affiliation>Los Alamos National Laboratory</affiliation></author>
      <author><first>Valentin</first><last>Stanev</last></author>
      <author><first>Cynthia</first><last>Matuszek</last><affiliation>University of Maryland, Baltimore County</affiliation></author>
      <author><first>Vladimir I</first><last>Valtchinov</last></author>
      <author><first>Kim</first><last>Rasmussen</last></author>
      <author><first>Boian S.</first><last>Alexandrov</last></author>
      <pages>205-214</pages>
      <abstract>Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating external document retrieval to provide domain-specific or up-to-date knowledge. The effectiveness of RAG depends on the relevance of retrieved documents, which is influenced by the semantic alignment of embeddings with the domain’s specialized content. Although full fine-tuning can align language models to specific domains, it is computationally intensive and demands substantial data. This paper introduces Hierarchical Embedding Alignment Loss (HEAL), a novel method that leverages hierarchical fuzzy clustering with matrix factorization within contrastive learning to efficiently align LLM embeddings with domain-specific content. HEAL computes level/depth-wise contrastive losses and incorporates hierarchical penalties to align embeddings with the underlying relationships in label hierarchies. This approach enhances retrieval relevance and document classification, effectively reducing hallucinations in LLM outputs. In our experiments, we benchmark and evaluate HEAL across diverse domains, including Healthcare, Material Science, Cyber-security, and Applied Maths.</abstract>
      <url hash="343af9b6">2025.knowledgenlp-1.19</url>
      <bibkey>bhattarai-etal-2025-heal</bibkey>
    </paper>
    <paper id="20">
      <title>Hybrid <fixed-case>AI</fixed-case> for Responsive Multi-Turn Online Conversations with Novel Dynamic Routing and Feedback Adaptation</title>
      <author><first>Priyaranjan</first><last>Pattnayak</last><affiliation>Oracle</affiliation></author>
      <author><first>Amit</first><last>Agarwal</last><affiliation>Oracle</affiliation></author>
      <author><first>Hansa</first><last>Meghwani</last></author>
      <author><first>Hitesh Laxmichand</first><last>Patel</last><affiliation>Oracle</affiliation></author>
      <author><first>Srikant</first><last>Panda</last><affiliation>Oracle</affiliation></author>
      <pages>215-229</pages>
      <abstract>Retrieval-Augmented Generation (RAG) systems and large language model (LLM)-powered chatbots have significantly advanced conversational AI by combining generative capabilities with external knowledge retrieval. Despite their success, enterprise-scale deployments face critical challenges, including diverse user queries, high latency, hallucinations, and difficulty integrating frequently updated domain-specific knowledge. This paper introduces a novel hybrid framework that integrates RAG with intent-based canned responses, leveraging predefined high-confidence responses for efficiency while dynamically routing complex or ambiguous queries to the RAG pipeline. Our framework employs a dialogue context manager to ensure coherence in multi-turn interactions and incorporates a feedback loop to refine intents, dynamically adjust confidence thresholds, and expand response coverage over time. Experimental results demonstrate that the proposed framework achieves a balance of high accuracy (95%) and low latency (180ms), outperforming RAG and intent-based systems across diverse query types, positioning it as a scalable and adaptive solution for enterprise conversational AI applications.</abstract>
      <url hash="4db3179a">2025.knowledgenlp-1.20</url>
      <bibkey>pattnayak-etal-2025-hybrid</bibkey>
    </paper>
    <paper id="21">
      <title>Chain of Evidences and Evidence to Generate: Prompting for Context Grounded and Retrieval Augmented Reasoning</title>
      <author><first>Md Rizwan</first><last>Parvez</last><affiliation>Qatar Computing Research Institute</affiliation></author>
      <pages>230-245</pages>
      <abstract>While chain-of-thoughts (CoT) prompting has revolutionized how LLMs perform reasoning tasks, its current methods and variations (e.g, Self-consistency, ReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR) etc.,) suffer from limitations like limited context grounding, hallucination/inconsistent output generation, and iterative sluggishness. To overcome these challenges, we introduce a novel mono/dual-step zero-shot prompting framework built upon two unique strategies <b>Chain of Evidences (CoE)</b> and <b>Evidence to Generate (E2G)</b>. Instead of unverified reasoning claims, our innovative approaches leverage the power of “evidence for decision making” by first focusing exclusively on the thought sequences explicitly mentioned in the context which then serve as extracted evidence, guiding the LLM’s output generation process with greater precision and efficiency. This simple yet potent approach unlocks the full potential of chain-of-thoughts prompting, facilitating faster, more reliable, and contextually aware reasoning in LLMs. Our framework consistently achieves remarkable results across various knowledge-intensive reasoning and generation tasks, surpassing baseline approaches with state-of-the-art LLMs. For instance, (i) on the LogiQA benchmark using GPT-4, CoE achieves a new state-of-the-art accuracy of 53.8%, surpassing CoT by 18%, ToT by 11%, and CR by 9%; (ii) CoE with PaLM-2 outperforms the variable-shot performance of Gemini Ultra by 0.9 F1 points, achieving an F1 score of 83.3 on DROP. We release our prompts and outputs on these benchmarks as a new instruction tuning dataset for future research at <i>Hugging Face</i>.</abstract>
      <url hash="05cd4342">2025.knowledgenlp-1.21</url>
      <bibkey>parvez-2025-chain</bibkey>
    </paper>
    <paper id="23">
      <title>Expertly Informed, Generatively Summarized: A Hybrid <fixed-case>RAG</fixed-case> Approach to Informed Consent Summarization with Auxiliary Expert Knowledge</title>
      <author><first>Autumn</first><last>Toney</last><affiliation>Georgetown University</affiliation></author>
      <author><first>Rsw66</first><last>Rsw66</last><affiliation>NA</affiliation></author>
      <author><first>Calebs</first><last>Calebs</last><affiliation>NA</affiliation></author>
      <pages>246-258</pages>
      <abstract>The utility of retrieval augmented generation (RAG) systems is actively being explored across a wide range of domains. Reliable generative output is increasingly useful in fields where routine tasks can be streamlined and potentially improved by integrating domain-specific data in addition to individual expert knowledge, such as medical care. To that end, we present a hybrid RAG and GraphRAG user interface system to summarize the key information (KI) section in IRB informed consent documents. KI summaries are a unique task, as generative summarization helps the end user (clinical trial expert) but can pose a risk to the affected user (potential study participants) if inaccurately constructed. Thus, the KI summarization task requires reliable, structured output with input from an expert knowledge source outside of the informed consent document. Reviewed by IRB domain experts and clinical trial PIs, our summarization application produces accurate (70% to 100% varied by accuracy type) and useful summaries (63% of PIs stating summaries were as good as or better than their accepted summaries).</abstract>
      <url hash="c6f25512">2025.knowledgenlp-1.23</url>
      <bibkey>toney-etal-2025-expertly</bibkey>
    </paper>
    <paper id="24">
      <title><fixed-case>MSR</fixed-case><tex-math>^2</tex-math>: A Benchmark for Multi-Source Retrieval and Reasoning in Visual Question Answering</title>
      <author><first>Kuo-Han</first><last>Hung</last></author>
      <author><first>Hung-Chieh</first><last>Fang</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Chao-Wei</first><last>Huang</last><affiliation>National Taiwan University</affiliation></author>
      <author><first>Yun-Nung</first><last>Chen</last><affiliation>Department of Computer Science and Informational Engineering, National Taiwan University</affiliation></author>
      <pages>259-271</pages>
      <abstract>This paper introduces MSR<tex-math>^2</tex-math>, a benchmark for multi-source retrieval and reasoning in visual question answering. Unlike previous knowledge-based visual question answering datasets, MSR<tex-math>^2</tex-math> focuses on questions involving multiple fine-grained entities, providing a unique opportunity to assess a model’s spatial reasoning ability and its capacity to retrieve and aggregate information from various sources for different entities. Through comprehensive evaluation using MSR<tex-math>^2</tex-math>, we gain valuable insights into the capabilities and limitations of state-of-the-art large vision-language models (LVLMs).Our findings reveal that even state-of-the-art LVLMs struggle with questions requiring multi-entities and knowledge-intensive reasoning, highlighting important new directions for future research.Additionally, we demonstrate that enhanced visual entity recognition and knowledge retrieval can significantly improve performance on MSR<tex-math>^2</tex-math>, pinpointing key areas for advancement.</abstract>
      <url hash="f58d241a">2025.knowledgenlp-1.24</url>
      <bibkey>hung-etal-2025-msr2</bibkey>
    </paper>
    <paper id="25">
      <title><fixed-case>PROPEL</fixed-case>: Prompt Optimization with Expert Priors for Small and Medium-sized <fixed-case>LLM</fixed-case>s</title>
      <author><first>Kawin</first><last>Mayilvaghanan</last><affiliation>Observe AI</affiliation></author>
      <author><first>Varun</first><last>Nathan</last><affiliation>Indian Institute of Science, Indian institute of science, Bangalore</affiliation></author>
      <author><first>Ayush</first><last>Kumar</last></author>
      <pages>272-302</pages>
      <url hash="89f070ce">2025.knowledgenlp-1.25</url>
      <bibkey>mayilvaghanan-etal-2025-propel</bibkey>
    </paper>
    <paper id="26">
      <title><fixed-case>C</fixed-case>laim<fixed-case>C</fixed-case>heck: Automatic Fact-Checking of Textual Claims using Web Evidence</title>
      <author><first>Akshith Reddy</first><last>Putta</last></author>
      <author><first>Jacob</first><last>Devasier</last><affiliation>University of Texas at Arlington</affiliation></author>
      <author><first>Chengkai</first><last>Li</last><affiliation>University of Texas at Arlington</affiliation></author>
      <pages>303-316</pages>
      <abstract>We introduce ClaimCheck, an efficient fact-checking system that verifies textual claims using smaller, open-source large language models. ClaimCheck integrates two fact-checking strategies, claim-matching and novel claim processing. Claim-matching uses related fact-checks from trusted organizations to fact-check a claim. Novel claim processing breaks down fact-checking into manageable subtasks—generating targeted questions, retrieving Web evidence, extracting answers, and synthesizing verdicts. Evaluation on the AVeriTeC benchmark demonstrates 62.6% verdict prediction accuracy, with claim-matching providing a 2.8% improvement. ClaimCheck approaches the performance of state-of-the-art systems while requiring significantly fewer computational resources, demonstrating the effectiveness of using small language models for fact-checking tasks. Furthermore, our code is publicly available to help make automated fact-checking more accessible.</abstract>
      <url hash="cecbac9d">2025.knowledgenlp-1.26</url>
      <bibkey>putta-etal-2025-claimcheck</bibkey>
    </paper>
    <paper id="27">
      <title>Can dependency parses facilitate generalization in language models? A case study of cross-lingual relation extraction</title>
      <author><first>Ritam</first><last>Dutt</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Shounak</first><last>Sural</last></author>
      <author><first>Carolyn</first><last>Rose</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>317-337</pages>
      <abstract>In this work, we propose DEPGEN, a framework for evaluating the generalization capabilities of language models on the task of relation extraction, with dependency parses as scaffolds. We use a GNN-based framework that takes dependency parses as input and learns embeddings of entities which are augmented to a baseline multilingual encoder. We also investigate the role of dependency parses when they are included as part of the prompt to LLMs in a zero-shot learning setup. We observe that including off-the-shelf dependency parses can aid relation extraction, with the best performing model having a mild relative improvement of 0.91% and 1.5% in the in-domain and zero-shot setting respectively across two datasets. For the in-context learning setup, we observe an average improvement of 1.67%, with significant gains for low-performing LLMs. We also carry out extensive statistical analysis to investigate how different factors such as the choice of the dependency parser or the nature of the prompt impact performance. We make our code and results publicly available for the research community at https://github.com/ShoRit/multilingual-re.git.</abstract>
      <url hash="41ceb8a9">2025.knowledgenlp-1.27</url>
      <bibkey>dutt-etal-2025-dependency</bibkey>
    </paper>
    <paper id="28">
      <title>Can dependency parses facilitate generalization in language models? A case study of cross-lingual relation extraction</title>
      <author><first>Ritam</first><last>Dutt</last><affiliation>Carnegie Mellon University</affiliation></author>
      <author><first>Shounak</first><last>Sural</last></author>
      <author><first>Carolyn</first><last>Rose</last><affiliation>School of Computer Science, Carnegie Mellon University</affiliation></author>
      <pages>338-358</pages>
      <abstract>In this work, we propose DEPGEN, a framework for evaluating the generalization capabilities of language models on the task of relation extraction, with dependency parses as scaffolds. We use a GNN-based framework that takes dependency parses as input and learns embeddings of entities which are augmented to a baseline multilingual encoder. We also investigate the role of dependency parses when they are included as part of the prompt to LLMs in a zero-shot learning setup. We observe that including off-the-shelf dependency parses can aid relation extraction, with the best performing model having a mild relative improvement of 0.91% and 1.5% in the in-domain and zero-shot setting respectively across two datasets. For the in-context learning setup, we observe an average improvement of 1.67%, with significant gains for low-performing LLMs. We also carry out extensive statistical analysis to investigate how different factors such as the choice of the dependency parser or the nature of the prompt impact performance. We make our code and results publicly available for the research community at https://github.com/ShoRit/multilingual-re.git.</abstract>
      <url hash="41ceb8a9">2025.knowledgenlp-1.28</url>
      <bibkey>dutt-etal-2025-dependency-parses</bibkey>
    </paper>
    <paper id="29">
      <title><fixed-case>D</fixed-case>oc<fixed-case>B</fixed-case>ench: A Benchmark for Evaluating <fixed-case>LLM</fixed-case>-based Document Reading Systems</title>
      <author><first>Anni</first><last>Zou</last><affiliation>Shanghai Jiaotong University</affiliation></author>
      <author><first>Wenhao</first><last>Yu</last></author>
      <author><first>Hongming</first><last>Zhang</last></author>
      <author><first>Kaixin</first><last>Ma</last></author>
      <author><first>Deng</first><last>Cai</last></author>
      <author><first>Zhuosheng</first><last>Zhang</last></author>
      <author><first>Hai</first><last>Zhao</last></author>
      <author><first>Dong</first><last>Yu</last></author>
      <pages>359-373</pages>
      <abstract>Recent advancements in proprietary large language models (LLMs), such as those from OpenAI and Anthropic, have led to the development of document reading systems capable of handling raw files with complex layouts, intricate formatting, lengthy content, and multi-modal information. However, the absence of a standardized benchmark hinders objective evaluation of these systems. To address this gap, we introduce DocBench, a benchmark designed to simulate real-world scenarios, where each raw file consists of a document paired with one or more questions. DocBench uniquely evaluates entire document reading systems and adopts a user-centric approach, allowing users to identify the system best suited to their needs.</abstract>
      <url hash="70e5ce37">2025.knowledgenlp-1.29</url>
      <bibkey>zou-etal-2025-docbench</bibkey>
    </paper>
  </volume>
</collection>
