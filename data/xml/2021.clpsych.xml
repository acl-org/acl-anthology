<?xml version='1.0' encoding='UTF-8'?>
<collection id="2021.clpsych">
  <volume id="1" ingest-date="2021-05-24">
    <meta>
      <booktitle>Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access</booktitle>
      <editor><first>Nazli</first><last>Goharian</last></editor>
      <editor><first>Philip</first><last>Resnik</last></editor>
      <editor><first>Andrew</first><last>Yates</last></editor>
      <editor><first>Molly</first><last>Ireland</last></editor>
      <editor><first>Kate</first><last>Niederhoffer</last></editor>
      <editor><first>Rebecca</first><last>Resnik</last></editor>
      <publisher>Association for Computational Linguistics</publisher>
      <address>Online</address>
      <month>June</month>
      <year>2021</year>
      <url hash="85c83d62">2021.clpsych-1</url>
      <venue>clpsych</venue>
    </meta>
    <frontmatter>
      <url hash="a311aa20">2021.clpsych-1.0</url>
      <bibkey>clpsych-2021-linguistics</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Understanding who uses <fixed-case>R</fixed-case>eddit: Profiling individuals with a self-reported bipolar disorder diagnosis</title>
      <author><first>Glorianna</first><last>Jagfeld</last></author>
      <author><first>Fiona</first><last>Lobban</last></author>
      <author><first>Paul</first><last>Rayson</last></author>
      <author><first>Steven</first><last>Jones</last></author>
      <pages>1–14</pages>
      <abstract>Recently, research on mental health conditions using public online data, including Reddit, has surged in NLP and health research but has not reported user characteristics, which are important to judge generalisability of findings. This paper shows how existing NLP methods can yield information on clinical, demographic, and identity characteristics of almost 20K Reddit users who self-report a bipolar disorder diagnosis. This population consists of slightly more feminine- than masculine-gendered mainly young or middle-aged US-based adults who often report additional mental health diagnoses, which is compared with general Reddit statistics and epidemiological studies. Additionally, this paper carefully evaluates all methods and discusses ethical issues.</abstract>
      <url hash="e918a603">2021.clpsych-1.1</url>
      <doi>10.18653/v1/2021.clpsych-1.1</doi>
      <bibkey>jagfeld-etal-2021-understanding</bibkey>
      <pwccode url="https://github.com/glorisonne/reddit_bd_user_characteristics" additional="false">glorisonne/reddit_bd_user_characteristics</pwccode>
      <pwcdataset url="https://paperswithcode.com/dataset/smhd">SMHD</pwcdataset>
    </paper>
    <paper id="2">
      <title>On the State of Social Media Data for Mental Health Research</title>
      <author><first>Keith</first><last>Harrigian</last></author>
      <author><first>Carlos</first><last>Aguirre</last></author>
      <author><first>Mark</first><last>Dredze</last></author>
      <pages>15–24</pages>
      <abstract>Data-driven methods for mental health treatment and surveillance have become a major focus in computational science research in the last decade. However, progress in the domain remains bounded by the availability of adequate data. Prior systematic reviews have not necessarily made it possible to measure the degree to which data-related challenges have affected research progress. In this paper, we offer an analysis specifically on the state of social media data that exists for conducting mental health research. We do so by introducing an open-source directory of mental health datasets, annotated using a standardized schema to facilitate meta-analysis.</abstract>
      <url hash="1724d76c">2021.clpsych-1.2</url>
      <attachment type="OptionalSupplementaryMaterial" hash="40b1cfd9">2021.clpsych-1.2.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2021.clpsych-1.2</doi>
      <bibkey>harrigian-etal-2021-state</bibkey>
      <pwccode url="https://github.com/kharrigian/mental-health-datasets" additional="false">kharrigian/mental-health-datasets</pwccode>
    </paper>
    <paper id="3">
      <title>Individual Differences in the Movement-Mood Relationship in Digital Life Data</title>
      <author><first>Glen</first><last>Coppersmith</last></author>
      <author><first>Alex</first><last>Fine</last></author>
      <author><first>Patrick</first><last>Crutchley</last></author>
      <author><first>Joshua</first><last>Carroll</last></author>
      <pages>25–31</pages>
      <abstract>Our increasingly digitized lives generate troves of data that reflect our behavior, beliefs, mood, and wellbeing. Such “digital life data” provides crucial insight into the lives of patients outside the healthcare setting that has long been lacking, from a better understanding of mundane patterns of exercise and sleep routines to harbingers of emotional crisis. Moreover, information about individual differences and personalities is encoded in digital life data. In this paper we examine the relationship between mood and movement using linguistic and biometric data, respectively. Does increased physical activity (movement) have an effect on a person’s mood (or vice-versa)? We find that weak group-level relationships between movement and mood mask interesting and often strong relationships between the two for individuals within the group. We describe these individual differences, and argue that individual variability in the relationship between movement and mood is one of many such factors that ought be taken into account in wellbeing-focused apps and AI systems.</abstract>
      <url hash="8508a7cd">2021.clpsych-1.3</url>
      <doi>10.18653/v1/2021.clpsych-1.3</doi>
      <bibkey>coppersmith-etal-2021-individual</bibkey>
    </paper>
    <paper id="4">
      <title>Dissociating Semantic and Phonemic Search Strategies in the Phonemic Verbal Fluency Task in early Dementia</title>
      <author><first>Hali</first><last>Lindsay</last></author>
      <author><first>Philipp</first><last>Müller</last></author>
      <author><first>Nicklas</first><last>Linz</last></author>
      <author><first>Radia</first><last>Zeghari</last></author>
      <author><first>Mario</first><last>Magued Mina</last></author>
      <author><first>Alexandra</first><last>Konig</last></author>
      <author><first>Johannes</first><last>Tröger</last></author>
      <pages>32–44</pages>
      <abstract>Effective management of dementia hinges on timely detection and precise diagnosis of the underlying cause of the syndrome at an early mild cognitive impairment (MCI) stage. Verbal fluency tasks are among the most often applied tests for early dementia detection due to their efficiency and ease of use. In these tasks, participants are asked to produce as many words as possible belonging to either a semantic category (SVF task) or a phonemic category (PVF task). Even though both SVF and PVF share neurocognitive function profiles, the PVF is typically believed to be less sensitive to measure MCI-related cognitive impairment and recent research on fine-grained automatic evaluation of VF tasks has mainly focused on the SVF. Contrary to this belief, we show that by applying state-of-the-art semantic and phonemic distance metrics in automatic analysis of PVF word productions, in-depth conclusions about production strategy of MCI patients are possible. Our results reveal a dissociation between semantically- and phonemically-guided search processes in the PVF. Specifically, we show that subjects with MCI rely less on semantic- and more on phonemic processes to guide their word production as compared to healthy controls (HC). We further show that semantic similarity-based features improve automatic MCI versus HC classification by 29% over previous approaches for the PVF. As such, these results point towards the yet underexplored utility of the PVF for in-depth assessment of cognition in MCI.</abstract>
      <url hash="82d67775">2021.clpsych-1.4</url>
      <doi>10.18653/v1/2021.clpsych-1.4</doi>
      <bibkey>lindsay-etal-2021-dissociating</bibkey>
    </paper>
    <paper id="5">
      <title>Demonstrating the Reliability of Self-Annotated Emotion Data</title>
      <author><first>Anton</first><last>Malko</last></author>
      <author><first>Cecile</first><last>Paris</last></author>
      <author><first>Andreas</first><last>Duenser</last></author>
      <author><first>Maria</first><last>Kangas</last></author>
      <author><first>Diego</first><last>Molla</last></author>
      <author><first>Ross</first><last>Sparks</last></author>
      <author><first>Stephen</first><last>Wan</last></author>
      <pages>45–54</pages>
      <abstract>Vent is a specialised iOS/Android social media platform with the stated goal to encourage people to post about their feelings and explicitly label them. In this paper, we study a snapshot of more than 100 million messages obtained from the developers of Vent, together with the labels assigned by the authors of the messages. We establish the quality of the self-annotated data by conducting a qualitative analysis, a vocabulary based analysis, and by training and testing an emotion classifier. We conclude that the self-annotated labels of our corpus are indeed indicative of the emotional contents expressed in the text and thus can support more detailed analyses of emotion expression on social media, such as emotion trajectories and factors influencing them.</abstract>
      <url hash="41992e94">2021.clpsych-1.5</url>
      <doi>10.18653/v1/2021.clpsych-1.5</doi>
      <bibkey>malko-etal-2021-demonstrating</bibkey>
    </paper>
    <paper id="6">
      <title><fixed-case>H</fixed-case>ebrew Psychological Lexicons</title>
      <author><first>Natalie</first><last>Shapira</last></author>
      <author><first>Dana</first><last>Atzil-Slonim</last></author>
      <author><first>Daniel</first><last>Juravski</last></author>
      <author><first>Moran</first><last>Baruch</last></author>
      <author><first>Dana</first><last>Stolowicz-Melman</last></author>
      <author><first>Adar</first><last>Paz</last></author>
      <author><first>Tal</first><last>Alfi-Yogev</last></author>
      <author><first>Roy</first><last>Azoulay</last></author>
      <author><first>Adi</first><last>Singer</last></author>
      <author><first>Maayan</first><last>Revivo</last></author>
      <author><first>Chen</first><last>Dahbash</last></author>
      <author><first>Limor</first><last>Dayan</last></author>
      <author><first>Tamar</first><last>Naim</last></author>
      <author><first>Lidar</first><last>Gez</last></author>
      <author><first>Boaz</first><last>Yanai</last></author>
      <author><first>Adva</first><last>Maman</last></author>
      <author><first>Adam</first><last>Nadaf</last></author>
      <author><first>Elinor</first><last>Sarfati</last></author>
      <author><first>Amna</first><last>Baloum</last></author>
      <author><first>Tal</first><last>Naor</last></author>
      <author><first>Ephraim</first><last>Mosenkis</last></author>
      <author><first>Badreya</first><last>Sarsour</last></author>
      <author><first>Jany</first><last>Gelfand Morgenshteyn</last></author>
      <author><first>Yarden</first><last>Elias</last></author>
      <author><first>Liat</first><last>Braun</last></author>
      <author><first>Moria</first><last>Rubin</last></author>
      <author><first>Matan</first><last>Kenigsbuch</last></author>
      <author><first>Noa</first><last>Bergwerk</last></author>
      <author><first>Noam</first><last>Yosef</last></author>
      <author><first>Sivan</first><last>Peled</last></author>
      <author><first>Coral</first><last>Avigdor</last></author>
      <author><first>Rahav</first><last>Obercyger</last></author>
      <author><first>Rachel</first><last>Mann</last></author>
      <author><first>Tomer</first><last>Alper</last></author>
      <author><first>Inbal</first><last>Beka</last></author>
      <author><first>Ori</first><last>Shapira</last></author>
      <author><first>Yoav</first><last>Goldberg</last></author>
      <pages>55–69</pages>
      <abstract>We introduce a large set of Hebrew lexicons pertaining to psychological aspects. These lexicons are useful for various psychology applications such as detecting emotional state, well being, relationship quality in conversation, identifying topics (e.g., family, work) and many more. We discuss the challenges in creating and validating lexicons in a new language, and highlight our methodological considerations in the data-driven lexicon construction process. Most of the lexicons are publicly available, which will facilitate further research on Hebrew clinical psychology text analysis. The lexicons were developed through data driven means, and verified by domain experts, clinical psychologists and psychology students, in a process of reconciliation with three judges. Development and verification relied on a dataset of a total of 872 psychotherapy session transcripts. We describe the construction process of each collection, the final resource and initial results of research studies employing this resource.</abstract>
      <url hash="db8a127c">2021.clpsych-1.6</url>
      <doi>10.18653/v1/2021.clpsych-1.6</doi>
      <bibkey>shapira-etal-2021-hebrew</bibkey>
      <pwccode url="https://github.com/natalieshapira/hebrewpsychologicallexicons" additional="false">natalieshapira/hebrewpsychologicallexicons</pwccode>
    </paper>
    <paper id="7">
      <title>Community-level Research on Suicidality Prediction in a Secure Environment: Overview of the <fixed-case>CLP</fixed-case>sych 2021 Shared Task</title>
      <author><first>Sean</first><last>MacAvaney</last></author>
      <author><first>Anjali</first><last>Mittu</last></author>
      <author><first>Glen</first><last>Coppersmith</last></author>
      <author><first>Jeff</first><last>Leintz</last></author>
      <author><first>Philip</first><last>Resnik</last></author>
      <pages>70–80</pages>
      <abstract>Progress on NLP for mental health — indeed, for healthcare in general — is hampered by obstacles to shared, community-level access to relevant data. We report on what is, to our knowledge, the first attempt to address this problem in mental health by conducting a shared task using sensitive data in a secure data enclave. Participating teams received access to Twitter posts donated for research, including data from users with and without suicide attempts, and did all work with the dataset entirely within a secure computational environment. We discuss the task, team results, and lessons learned to set the stage for future tasks on sensitive or confidential data.</abstract>
      <url hash="80b90339">2021.clpsych-1.7</url>
      <doi>10.18653/v1/2021.clpsych-1.7</doi>
      <bibkey>macavaney-etal-2021-community</bibkey>
    </paper>
    <paper id="8">
      <title>Determining a Person’s Suicide Risk by Voting on the Short-Term History of Tweets for the <fixed-case>CLP</fixed-case>sych 2021 Shared Task</title>
      <author><first>Ulya</first><last>Bayram</last></author>
      <author><first>Lamia</first><last>Benhiba</last></author>
      <pages>81–86</pages>
      <abstract>In this shared task, we accept the challenge of constructing models to identify Twitter users who attempted suicide based on their tweets 30 and 182 days before the adverse event’s occurrence. We explore multiple machine learning and deep learning methods to identify a person’s suicide risk based on the short-term history of their tweets. Taking the real-life applicability of the model into account, we make the design choice of classifying on the tweet level. By voting the tweet-level suicide risk scores through an ensemble of classifiers, we predict the suicidal users 30-days before the event with an 81.8% true-positives rate. Meanwhile, the tweet-level voting falls short on the six-month-long data as the number of tweets with weak suicidal ideation levels weakens the overall suicidal signals in the long term.</abstract>
      <url hash="325e9793">2021.clpsych-1.8</url>
      <doi>10.18653/v1/2021.clpsych-1.8</doi>
      <bibkey>bayram-benhiba-2021-determining</bibkey>
    </paper>
    <paper id="9">
      <title>Learning Models for Suicide Prediction from Social Media Posts</title>
      <author><first>Ning</first><last>Wang</last></author>
      <author><first>Luo</first><last>Fan</last></author>
      <author><first>Yuvraj</first><last>Shivtare</last></author>
      <author><first>Varsha</first><last>Badal</last></author>
      <author><first>Koduvayur</first><last>Subbalakshmi</last></author>
      <author><first>Rajarathnam</first><last>Chandramouli</last></author>
      <author><first>Ellen</first><last>Lee</last></author>
      <pages>87–92</pages>
      <abstract>We propose a deep learning architecture and test three other machine learning models to automatically detect individuals that will attempt suicide within (1) 30 days and (2) six months, using their social media post data provided in the CL-Psych-Challenge. Additionally, we create and extract three sets of handcrafted features for suicide detection based on the three-stage theory of suicide and prior work on emotions and the use of pronouns among persons exhibiting suicidal ideations. Extensive experimentations show that some of the traditional machine learning methods outperform the baseline with an F1 score of 0.741 and F2 score of 0.833 on subtask 1 (prediction of a suicide attempt 30 days prior). However, the proposed deep learning method outperforms the baseline with F1 score of 0.737 and F2 score of 0.843 on subtask2 (prediction of suicide 6 months prior).</abstract>
      <url hash="be9bbe46">2021.clpsych-1.9</url>
      <doi>10.18653/v1/2021.clpsych-1.9</doi>
      <bibkey>wang-etal-2021-learning</bibkey>
    </paper>
    <paper id="10">
      <title>Suicide Risk Prediction by Tracking Self-Harm Aspects in Tweets: <fixed-case>NUS</fixed-case>-<fixed-case>IDS</fixed-case> at the <fixed-case>CLP</fixed-case>sych 2021 Shared Task</title>
      <author><first>Sujatha Das</first><last>Gollapalli</last></author>
      <author><first>Guilherme Augusto</first><last>Zagatti</last></author>
      <author><first>See-Kiong</first><last>Ng</last></author>
      <pages>93–98</pages>
      <abstract>We describe our system for identifying users at-risk for suicide based on their tweets developed for the CLPsych 2021 Shared Task. Based on research in mental health studies linking self-harm tendencies with suicide, in our system, we attempt to characterize self-harm aspects expressed in user tweets over a period of time. To this end, we design SHTM, a Self-Harm Topic Model that combines Latent Dirichlet Allocation with a self-harm dictionary for modeling daily tweets of users. Next, differences in moods and topics over time are captured as features to train a deep learning model for suicide prediction.</abstract>
      <url hash="90f8d91f">2021.clpsych-1.10</url>
      <attachment type="OptionalSupplementaryCode" hash="a588af6c">2021.clpsych-1.10.OptionalSupplementaryCode.zip</attachment>
      <attachment type="OptionalSupplementaryData" hash="a0bd310f">2021.clpsych-1.10.OptionalSupplementaryData.zip</attachment>
      <doi>10.18653/v1/2021.clpsych-1.10</doi>
      <bibkey>gollapalli-etal-2021-suicide</bibkey>
    </paper>
    <paper id="11">
      <title>Team 9: A Comparison of Simple vs. Complex Models for Suicide Risk Assessment</title>
      <author><first>Michelle</first><last>Morales</last></author>
      <author><first>Prajjalita</first><last>Dey</last></author>
      <author><first>Kriti</first><last>Kohli</last></author>
      <pages>99–102</pages>
      <abstract>This work presents the systems explored as part of the CLPsych 2021 Shared Task. More specifically, this work explores the relative performance of models trained on social me- dia data for suicide risk assessment. For this task, we aim to investigate whether or not simple traditional models can outperform more complex fine-tuned deep learning mod- els. Specifically, we build and compare a range of models including simple baseline models, feature-engineered machine learning models, and lastly, fine-tuned deep learning models. We find that simple more traditional machine learning models are more suited for this task and highlight the challenges faced when trying to leverage more sophisticated deep learning models.</abstract>
      <url hash="951fb645">2021.clpsych-1.11</url>
      <doi>10.18653/v1/2021.clpsych-1.11</doi>
      <bibkey>morales-etal-2021-team</bibkey>
    </paper>
    <paper id="12">
      <title>Using Psychologically-Informed Priors for Suicide Prediction in the <fixed-case>CLP</fixed-case>sych 2021 Shared Task</title>
      <author><first>Avi</first><last>Gamoran</last></author>
      <author><first>Yonatan</first><last>Kaplan</last></author>
      <author><first>Almog</first><last>Simchon</last></author>
      <author><first>Michael</first><last>Gilead</last></author>
      <pages>103–109</pages>
      <abstract>This paper describes our approach to the CLPsych 2021 Shared Task, in which we aimed to predict suicide attempts based on Twitter feed data. We addressed this challenge by emphasizing reliance on prior domain knowledge. We engineered novel theory-driven features, and integrated prior knowledge with empirical evidence in a principled manner using Bayesian modeling. While this theory-guided approach increases bias and lowers accuracy on the training set, it was successful in preventing over-fitting. The models provided reasonable classification accuracy on unseen test data (0.68&lt;=AUC&lt;= 0.84). Our approach may be particularly useful in prediction tasks trained on a relatively small data set.</abstract>
      <url hash="a10c45ed">2021.clpsych-1.12</url>
      <doi>10.18653/v1/2021.clpsych-1.12</doi>
      <bibkey>gamoran-etal-2021-using</bibkey>
    </paper>
    <paper id="13">
      <title>Analysis of Behavior Classification in Motivational Interviewing</title>
      <author><first>Leili</first><last>Tavabi</last></author>
      <author><first>Trang</first><last>Tran</last></author>
      <author><first>Kalin</first><last>Stefanov</last></author>
      <author><first>Brian</first><last>Borsari</last></author>
      <author><first>Joshua</first><last>Woolley</last></author>
      <author><first>Stefan</first><last>Scherer</last></author>
      <author><first>Mohammad</first><last>Soleymani</last></author>
      <pages>110–115</pages>
      <abstract>Analysis of client and therapist behavior in counseling sessions can provide helpful insights for assessing the quality of the session and consequently, the client’s behavioral outcome. In this paper, we study the automatic classification of standardized behavior codes (annotations) used for assessment of psychotherapy sessions in Motivational Interviewing (MI). We develop models and examine the classification of client behaviors throughout MI sessions, comparing the performance by models trained on large pretrained embeddings (RoBERTa) versus interpretable and expert-selected features (LIWC). Our best performing model using the pretrained RoBERTa embeddings beats the baseline model, achieving an F1 score of 0.66 in the subject-independent 3-class classification. Through statistical analysis on the classification results, we identify prominent LIWC features that may not have been captured by the model using pretrained embeddings. Although classification using LIWC features underperforms RoBERTa, our findings motivate the future direction of incorporating auxiliary tasks in the classification of MI codes.</abstract>
      <url hash="b532c9da">2021.clpsych-1.13</url>
      <doi>10.18653/v1/2021.clpsych-1.13</doi>
      <bibkey>tavabi-etal-2021-analysis</bibkey>
    </paper>
    <paper id="14">
      <title>Automatic Detection and Prediction of Psychiatric Hospitalizations From Social Media Posts</title>
      <author><first>Zhengping</first><last>Jiang</last></author>
      <author><first>Jonathan</first><last>Zomick</last></author>
      <author><first>Sarah Ita</first><last>Levitan</last></author>
      <author><first>Mark</first><last>Serper</last></author>
      <author><first>Julia</first><last>Hirschberg</last></author>
      <pages>116–121</pages>
      <abstract>We address the problem of predicting psychiatric hospitalizations using linguistic features drawn from social media posts. We formulate this novel task and develop an approach to automatically extract time spans of self-reported psychiatric hospitalizations. Using this dataset, we build predictive models of psychiatric hospitalization, comparing feature sets, user vs. post classification, and comparing model performance using a varying time window of posts. Our best model achieves an F1 of .718 using 7 days of posts. Our results suggest that this is a useful framework for collecting hospitalization data, and that social media data can be leveraged to predict acute psychiatric crises before they occur, potentially saving lives and improving outcomes for individuals with mental illness.</abstract>
      <url hash="3273c7fe">2021.clpsych-1.14</url>
      <doi>10.18653/v1/2021.clpsych-1.14</doi>
      <bibkey>jiang-etal-2021-automatic</bibkey>
    </paper>
    <paper id="15">
      <title>Automatic Identification of Ruptures in Transcribed Psychotherapy Sessions</title>
      <author><first>Adam</first><last>Tsakalidis</last></author>
      <author><first>Dana</first><last>Atzil-Slonim</last></author>
      <author><first>Asaf</first><last>Polakovski</last></author>
      <author><first>Natalie</first><last>Shapira</last></author>
      <author><first>Rivka</first><last>Tuval-Mashiach</last></author>
      <author><first>Maria</first><last>Liakata</last></author>
      <pages>122–128</pages>
      <abstract>We present the first work on automatically capturing alliance rupture in transcribed therapy sessions, trained on the text and self-reported rupture scores from both therapists and clients. Our NLP baseline outperforms a strong majority baseline by a large margin and captures client reported ruptures unidentified by therapists in 40% of such cases.</abstract>
      <url hash="6dcaf12b">2021.clpsych-1.15</url>
      <attachment type="OptionalSupplementaryMaterial" hash="cb4e089d">2021.clpsych-1.15.OptionalSupplementaryMaterial.zip</attachment>
      <doi>10.18653/v1/2021.clpsych-1.15</doi>
      <bibkey>tsakalidis-etal-2021-automatic</bibkey>
    </paper>
    <paper id="16">
      <title>Automated coherence measures fail to index thought disorder in individuals at risk for psychosis</title>
      <author><first>Kasia</first><last>Hitczenko</last></author>
      <author><first>Henry</first><last>Cowan</last></author>
      <author><first>Vijay</first><last>Mittal</last></author>
      <author><first>Matthew</first><last>Goldrick</last></author>
      <pages>129–150</pages>
      <abstract>Thought disorder – linguistic disturbances including incoherence and derailment of topic – is seen in individuals both with and at risk for psychosis. Methods from computational linguistics have increasingly sought to quantify thought disorder to detect group differences between clinical populations and healthy controls. While previous work has been quite successful at these classification tasks, the lack of interpretability of the computational metrics has made it unclear whether they are in fact measuring thought disorder. In this paper, we dive into these measures to try to better understand what they reflect. While we find group differences between at-risk and healthy control populations, we also find that the measures mostly do not correlate with existing measures of thought disorder symptoms (what they are intended to measure), but rather correlate with surface properties of the speech (e.g., sentence length) and sociodemographic properties of the speaker (e.g., race). These results highlight the importance of considering interpretability and front and center as the field continues to grow. Ethical use of computational measures like those studied here – especially in the high-stakes context of clinical care – requires us to devote substantial attention to potential biases in our measures.</abstract>
      <url hash="fc1f0849">2021.clpsych-1.16</url>
      <doi>10.18653/v1/2021.clpsych-1.16</doi>
      <bibkey>hitczenko-etal-2021-automated</bibkey>
    </paper>
    <paper id="17">
      <title>Detecting Cognitive Distortions from Patient-Therapist Interactions</title>
      <author><first>Sagarika</first><last>Shreevastava</last></author>
      <author><first>Peter</first><last>Foltz</last></author>
      <pages>151–158</pages>
      <abstract>An important part of Cognitive Behavioral Therapy (CBT) is to recognize and restructure certain negative thinking patterns that are also known as cognitive distortions. The aim of this project is to detect these distortions using natural language processing. We compare and contrast different types of linguistic features as well as different classification algorithms and explore the limitations of applying these techniques on a small dataset. We find that pre-trained Sentence-BERT embeddings to train an SVM classifier yields the best results with an F1-score of 0.79. Lastly, we discuss how this work provides insights into the types of linguistic features that are inherent in cognitive distortions.</abstract>
      <url hash="00e3bcee">2021.clpsych-1.17</url>
      <doi>10.18653/v1/2021.clpsych-1.17</doi>
      <bibkey>shreevastava-foltz-2021-detecting</bibkey>
    </paper>
    <paper id="18">
      <title>Evaluating Automatic Speech Recognition Quality and Its Impact on Counselor Utterance Coding</title>
      <author><first>Do June</first><last>Min</last></author>
      <author><first>Verónica</first><last>Pérez-Rosas</last></author>
      <author><first>Rada</first><last>Mihalcea</last></author>
      <pages>159–168</pages>
      <abstract>Automatic speech recognition (ASR) is a crucial step in many natural language processing (NLP) applications, as often available data consists mainly of raw speech. Since the result of the ASR step is considered as a meaningful, informative input to later steps in the NLP pipeline, it is important to understand the behavior and failure mode of this step. In this work, we analyze the quality of ASR in the psychotherapy domain, using motivational interviewing conversations between therapists and clients. We conduct domain agnostic and domain-relevant evaluations using standard evaluation metrics and also identify domain-relevant keywords in the ASR output. Moreover, we empirically study the effect of mixing ASR and manual data during the training of a downstream NLP model, and also demonstrate how additional local context can help alleviate the error introduced by noisy ASR transcripts.</abstract>
      <url hash="70e338c6">2021.clpsych-1.18</url>
      <doi>10.18653/v1/2021.clpsych-1.18</doi>
      <bibkey>min-etal-2021-evaluating</bibkey>
    </paper>
    <paper id="19">
      <title>Qualitative Analysis of Depression Models by Demographics</title>
      <author><first>Carlos</first><last>Aguirre</last></author>
      <author><first>Mark</first><last>Dredze</last></author>
      <pages>169–180</pages>
      <abstract>Models for identifying depression using social media text exhibit biases towards different gender and racial/ethnic groups. Factors like representation and balance of groups within the dataset are contributory factors, but difference in content and social media use may further explain these biases. We present an analysis of the content of social media posts from different demographic groups. Our analysis shows that there are content differences between depression and control subgroups across demographic groups, and that temporal topics and demographic-specific topics are correlated with downstream depression model error. We discuss the implications of our work on creating future datasets, as well as designing and training models for mental health.</abstract>
      <url hash="ce5e857b">2021.clpsych-1.19</url>
      <doi>10.18653/v1/2021.clpsych-1.19</doi>
      <bibkey>aguirre-dredze-2021-qualitative</bibkey>
    </paper>
    <paper id="20">
      <title>Safeguarding against spurious <fixed-case>AI</fixed-case>-based predictions: The case of automated verbal memory assessment</title>
      <author><first>Chelsea</first><last>Chandler</last></author>
      <author><first>Peter</first><last>Foltz</last></author>
      <author><first>Alex</first><last>Cohen</last></author>
      <author><first>Terje</first><last>Holmlund</last></author>
      <author><first>Brita</first><last>Elvevåg</last></author>
      <pages>181–191</pages>
      <abstract>A growing amount of psychiatric research incorporates machine learning and natural language processing methods, however findings have yet to be translated into actual clinical decision support systems. Many of these studies are based on relatively small datasets in homogeneous populations, which has the associated risk that the models may not perform adequately on new data in real clinical practice. The nature of serious mental illness is that it is hard to define, hard to capture, and requires frequent monitoring, which leads to imperfect data where attribute and class noise are common. With the goal of an effective AI-mediated clinical decision support system, there must be computational safeguards placed on the models used in order to avoid spurious predictions and thus allow humans to review data in the settings where models are unstable or bound not to generalize. This paper describes two approaches to implementing safeguards: (1) the determination of cases in which models are unstable by means of attribute and class based outlier detection and (2) finding the extent to which models show inductive bias. These safeguards are illustrated in the automated scoring of a story recall task via natural language processing methods. With the integration of human-in-the-loop machine learning in the clinical implementation process, incorporating safeguards such as these into the models will offer patients increased protection from spurious predictions.</abstract>
      <url hash="57ac577c">2021.clpsych-1.20</url>
      <doi>10.18653/v1/2021.clpsych-1.20</doi>
      <bibkey>chandler-etal-2021-safeguarding</bibkey>
    </paper>
    <paper id="21">
      <title>Towards the Development of Speech-Based Measures of Stress Response in Individuals</title>
      <author><first>Archna</first><last>Bhatia</last></author>
      <author><first>Toshiya</first><last>Miyatsu</last></author>
      <author><first>Peter</first><last>Pirolli</last></author>
      <pages>192–203</pages>
      <abstract>Psychological and physiological stress in the environment can induce a different stress response in different individuals. Given the causal relationship between stress, mental health, and psychopathologies, as well as its impact on individuals’ executive functioning and performance, identifying the extent of stress response in individuals can be useful for providing targeted support to those who are in need. In this paper, we identify and validate features in speech that can be used as indicators of stress response in individuals to develop speech-based measures of stress response. We evaluate effectiveness of two types of tasks used for collecting speech samples in developing stress response measures, namely Read Speech Task and Open-Ended Question Task. Participants completed these tasks, along with the verbal fluency task (an established measure of executive functioning) before and after clinically validated stress induction to see if the changes in the speech-based features are associated with the stress-induced decline in executive functioning. Further, we supplement our analyses with an extensive, external assessment of the individuals’ stress tolerance in the real life to validate the usefulness of the speech-based measures in predicting meaningful outcomes outside of the experimental setting.</abstract>
      <url hash="967edae2">2021.clpsych-1.21</url>
      <doi>10.18653/v1/2021.clpsych-1.21</doi>
      <bibkey>bhatia-etal-2021-towards</bibkey>
    </paper>
    <paper id="22">
      <title>Towards Low-Resource Real-Time Assessment of Empathy in Counselling</title>
      <author><first>Zixiu</first><last>Wu</last></author>
      <author><first>Rim</first><last>Helaoui</last></author>
      <author><first>Diego</first><last>Reforgiato Recupero</last></author>
      <author><first>Daniele</first><last>Riboni</last></author>
      <pages>204–216</pages>
      <abstract>Gauging therapist empathy in counselling is an important component of understanding counselling quality. While session-level empathy assessment based on machine learning has been investigated extensively, it relies on relatively large amounts of well-annotated dialogue data, and real-time evaluation has been overlooked in the past. In this paper, we focus on the task of low-resource utterance-level binary empathy assessment. We train deep learning models on heuristically constructed empathy vs. non-empathy contrast in general conversations, and apply the models directly to therapeutic dialogues, assuming correlation between empathy manifested in those two domains. We show that such training yields poor performance in general, probe its causes, and examine the actual effect of learning from empathy contrast in general conversation.</abstract>
      <url hash="d3e13e36">2021.clpsych-1.22</url>
      <doi>10.18653/v1/2021.clpsych-1.22</doi>
      <bibkey>wu-etal-2021-towards</bibkey>
      <pwcdataset url="https://paperswithcode.com/dataset/pec">PEC</pwcdataset>
    </paper>
    <paper id="23">
      <title>Towards Understanding the Role of Gender in Deploying Social Media-Based Mental Health Surveillance Models</title>
      <author><first>Eli</first><last>Sherman</last></author>
      <author><first>Keith</first><last>Harrigian</last></author>
      <author><first>Carlos</first><last>Aguirre</last></author>
      <author><first>Mark</first><last>Dredze</last></author>
      <pages>217–223</pages>
      <abstract>Spurred by advances in machine learning and natural language processing, developing social media-based mental health surveillance models has received substantial recent attention. For these models to be maximally useful, it is necessary to understand how they perform on various subgroups, especially those defined in terms of protected characteristics. In this paper we study the relationship between user demographics – focusing on gender – and depression. Considering a population of Reddit users with known genders and depression statuses, we analyze the degree to which depression predictions are subject to biases along gender lines using domain-informed classifiers. We then study our models’ parameters to gain qualitative insight into the differences in posting behavior across genders.</abstract>
      <url hash="03e554e8">2021.clpsych-1.23</url>
      <doi>10.18653/v1/2021.clpsych-1.23</doi>
      <bibkey>sherman-etal-2021-towards</bibkey>
    </paper>
    <paper id="24">
      <title>Understanding Patterns of Anorexia Manifestations in Social Media Data with Deep Learning</title>
      <author><first>Ana Sabina</first><last>Uban</last></author>
      <author><first>Berta</first><last>Chulvi</last></author>
      <author><first>Paolo</first><last>Rosso</last></author>
      <pages>224–236</pages>
      <abstract>Eating disorders are a growing problem especially among young people, yet they have been under-studied in computational research compared to other mental health disorders such as depression. Computational methods have a great potential to aid with the automatic detection of mental health problems, but state-of-the-art machine learning methods based on neural networks are notoriously difficult to interpret, which is a crucial problem for applications in the mental health domain. We propose leveraging the power of deep learning models for automatically detecting signs of anorexia based on social media data, while at the same time focusing on interpreting their behavior. We train a hierarchical attention network to detect people with anorexia and use its internal encodings to discover different clusters of anorexia symptoms. We interpret the identified patterns from multiple perspectives, including emotion expression, psycho-linguistic features and personality traits, and we offer novel hypotheses to interpret our findings from a psycho-social perspective. Some interesting findings are patterns of word usage in some users with anorexia which show that they feel less as being part of a group compared to control cases, as well as that they have abandoned explanatory activity as a result of a greater feeling of helplessness and fear.</abstract>
      <url hash="0c502d30">2021.clpsych-1.24</url>
      <attachment type="OptionalSupplementaryData" hash="9bea41b9">2021.clpsych-1.24.OptionalSupplementaryData.zip</attachment>
      <attachment type="OptionalSupplementaryMaterial" hash="cd316339">2021.clpsych-1.24.OptionalSupplementaryMaterial.pdf</attachment>
      <doi>10.18653/v1/2021.clpsych-1.24</doi>
      <bibkey>uban-etal-2021-understanding</bibkey>
    </paper>
  </volume>
</collection>
