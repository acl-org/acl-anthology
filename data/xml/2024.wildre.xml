<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.wildre">
  <volume id="1" ingest-date="2024-05-18" type="proceedings">
    <meta>
      <booktitle>Proceedings of the 7th Workshop on Indian Language Data: Resources and Evaluation</booktitle>
      <editor><first>Girish Nath</first><last>Jha</last></editor>
      <editor><first>Sobha</first><last>L.</last></editor>
      <editor><first>Kalika</first><last>Bali</last></editor>
      <editor><first>Atul Kr.</first><last>Ojha</last></editor>
      <publisher>ELRA and ICCL</publisher>
      <address>Torino, Italia</address>
      <month>May</month>
      <year>2024</year>
      <url hash="6de7b0f2">2024.wildre-1</url>
      <venue>wildre</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="b01788a8">2024.wildre-1.0</url>
      <bibkey>wildre-2024-indian</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Towards Disfluency Annotated Corpora for <fixed-case>I</fixed-case>ndian Languages</title>
      <author><first>Chayan</first><last>Kochar</last></author>
      <author><first>Vandan Vasantlal</first><last>Mujadia</last></author>
      <author><first>Pruthwik</first><last>Mishra</last></author>
      <author><first>Dipti Misra</first><last>Sharma</last></author>
      <pages>1–10</pages>
      <abstract>In the natural course of spoken language, individuals often engage in thinking and self-correction during speech production. These instances of interruption or correction are commonly referred to as disfluencies. When preparing data for subsequent downstream NLP tasks, these linguistic elements can be systematically removed, or handled as required, to enhance data quality. In this study, we present a comprehensive research on disfluencies in Indian languages. Our approach involves not only annotating real-world conversation transcripts but also conducting a detailed analysis of linguistic nuances inherent to Indian languages that are necessary to consider during annotation. Additionally, we introduce a robust algorithm for the synthetic generation of disfluent data. This algorithm aims to facilitate more effective model training for the identification of disfluencies in real-world conversations, thereby contributing to the advancement of disfluency research in Indian languages.</abstract>
      <url hash="f60b9279">2024.wildre-1.1</url>
      <bibkey>kochar-etal-2024-towards</bibkey>
    </paper>
    <paper id="2">
      <title><fixed-case>E</fixed-case>mo<fixed-case>M</fixed-case>ix-3<fixed-case>L</fixed-case>: A Code-Mixed Dataset for <fixed-case>B</fixed-case>angla-<fixed-case>E</fixed-case>nglish-<fixed-case>H</fixed-case>indi for Emotion Detection</title>
      <author><first>Nishat</first><last>Raihan</last></author>
      <author><first>Dhiman</first><last>Goswami</last></author>
      <author><first>Antara</first><last>Mahmud</last></author>
      <author><first>Antonios</first><last>Anastasopoulos</last></author>
      <author><first>Marcos</first><last>Zampieri</last></author>
      <pages>11–16</pages>
      <abstract>Code-mixing is a well-studied linguistic phenomenon that occurs when two or more languages are mixed in text or speech. Several studies have been conducted on building datasets and performing downstream NLP tasks on code-mixed data. Although it is not uncommon to observe code-mixing of three or more languages, most available datasets in this domain contain code-mixed data from only two languages. In this paper, we introduce EmoMix-3L, a novel multi-label emotion detection dataset containing code-mixed data from three different languages. We experiment with several models on EmoMix-3L and we report that MuRIL outperforms other models on this dataset.</abstract>
      <url hash="bcb622ab">2024.wildre-1.2</url>
      <bibkey>raihan-etal-2024-emomix</bibkey>
    </paper>
    <paper id="3">
      <title>Findings of the <fixed-case>WILDRE</fixed-case> Shared Task on Code-mixed Less-resourced Sentiment Analysis for <fixed-case>I</fixed-case>ndo-<fixed-case>A</fixed-case>ryan Languages</title>
      <author><first>Priya</first><last>Rani</last></author>
      <author><first>Gaurav</first><last>Negi</last></author>
      <author><first>Saroj</first><last>Jha</last></author>
      <author><first>Shardul</first><last>Suryawanshi</last></author>
      <author><first>Atul Kr.</first><last>Ojha</last></author>
      <author><first>Paul</first><last>Buitelaar</last></author>
      <author><first>John P.</first><last>McCrae</last></author>
      <pages>17–23</pages>
      <abstract>This paper describes the structure and findings of the WILDRE 2024 shared task on Code-mixed Less-resourced Sentiment Analysis for Indo-Aryan Languages. The participants were asked to submit the test data’s final prediction on CodaLab. A total of fourteen teams registered for the shared task. Only four participants submitted the system for evaluation on CodaLab, with only two teams submitting the system description paper. While all systems show a rather promising performance, they outperform the baseline scores.</abstract>
      <url hash="d5e48df6">2024.wildre-1.3</url>
      <bibkey>rani-etal-2024-findings</bibkey>
    </paper>
    <paper id="4">
      <title>Multilingual Bias Detection and Mitigation for <fixed-case>I</fixed-case>ndian Languages</title>
      <author><first>Ankita</first><last>Maity</last></author>
      <author><first>Anubhav</first><last>Sharma</last></author>
      <author><first>Rudra</first><last>Dhar</last></author>
      <author><first>Tushar</first><last>Abhishek</last></author>
      <author><first>Manish</first><last>Gupta</last></author>
      <author><first>Vasudeva</first><last>Varma</last></author>
      <pages>24–29</pages>
      <abstract>Lack of diverse perspectives causes neutrality bias in Wikipedia content leading to millions of worldwide readers getting exposed by potentially inaccurate information. Hence, neutrality bias detection and mitigation is a critical problem. Although previous studies have proposed effective solutions for English, no work exists for Indian languages. First, we contribute two large datasets, mWIKIBIAS and mWNC, covering 8 languages, for the bias detection and mitigation tasks respectively. Next, we investigate the effectiveness of popular multilingual Transformer-based models for the two tasks by modeling detection as a binary classification problem and mitigation as a style transfer problem. We make the code and data publicly available.</abstract>
      <url hash="ce772215">2024.wildre-1.4</url>
      <bibkey>maity-etal-2024-multilingual</bibkey>
    </paper>
    <paper id="5">
      <title>Dharmaśāstra Informatics: Concept Mining System for Socio-Cultural Facet in <fixed-case>A</fixed-case>ncient <fixed-case>I</fixed-case>ndia</title>
      <author><first>Arooshi</first><last>Nigam</last></author>
      <author><first>Subhash</first><last>Chandra</last></author>
      <pages>30–39</pages>
      <abstract>The heritage of Dharmaśāstra (DS) represents an extensive cultural legacy, spanning diverse fields such as family law, social ethics, culture and economics. In this paper, a new term “Dharmaśāstric Informatics,” is proposed which leverages computational methods for concept mining to unravel the socio-cultural complexities of ancient India as reflected in the DS. Despite its profound significance, the digitization and online information retrieval of DS texts encounter notable challenges. Therefore, the primary aim of this paper is to synergize digital accessibility and information mining techniques to enhance access to DS knowledge traditions. Through the utilization of heritage computing methodologies, it is an endeavour to develop a robust system for digitizing DS texts comprehensively, facilitating instant referencing and efficient retrieval, catering to the needs of researchers and scholars across disciplines worldwide. By leveraging advanced digital technologies and the burgeoning IT landscape, it seeks to create a seamless and user-friendly platform for accessing and exploring DS texts. This experiment not only promotes scholarly engagement but also serves as an invaluable resource for individuals interested in delving into the intricate realms of archaic Indian knowledge traditions. Ultimately, our efforts aim to amplify the visibility and accessibility of DS knowledge, fostering a deeper understanding and appreciation of this profound cultural heritage.</abstract>
      <url hash="8bbf23bb">2024.wildre-1.5</url>
      <bibkey>nigam-chandra-2024-dharmasastra</bibkey>
    </paper>
    <paper id="6">
      <title>Exploring News Summarization and Enrichment in a Highly Resource-Scarce <fixed-case>I</fixed-case>ndian Language: A Case Study of Mizo</title>
      <author><first>Abhinaba</first><last>Bala</last></author>
      <author><first>Ashok</first><last>Urlana</last></author>
      <author><first>Rahul</first><last>Mishra</last></author>
      <author><first>Parameswari</first><last>Krishnamurthy</last></author>
      <pages>40–46</pages>
      <abstract>Obtaining sufficient information in one’s mother tongue is crucial for satisfying the information needs of the users. While high-resource languages have abundant online resources, the situation is less than ideal for very low-resource languages. Moreover, the insufficient reporting of vital national and international events continues to be a worry, especially in languages with scarce resources, like Mizo. In this paper, we conduct a study to investigate the effectiveness of a simple methodology designed to generate a holistic summary for Mizo news articles, which leverages English-language news to supplement and enhance the information related to the corresponding news events. Furthermore, we make available 500 Mizo news articles and corresponding enriched holistic summaries. Human evaluation confirms that our approach significantly enhances the information coverage of Mizo news articles.</abstract>
      <url hash="41d25f98">2024.wildre-1.6</url>
      <bibkey>bala-etal-2024-exploring</bibkey>
    </paper>
    <paper id="7">
      <title>Finding the Causality of an Event in News Articles</title>
      <author><first>Sobha</first><last>Lalitha Devi</last></author>
      <author><first>Pattabhi</first><last>RK Rao</last></author>
      <pages>47–53</pages>
      <abstract>This paper discusses about the finding of causality of an event in newspaper articles. The analysis of causality , otherwise known as cause and effect is crucial for building efficient Natural Language Understanding (NLU) supported AI systems such as Event tracking and it is considered as a complex semantic relation under discourse theory. A cause-effect relation consists of a linguistic marker and its two arguments. The arguments are semantic arguments where the cause is the first argument (Arg1) and the effect is the second argument(Arg2). In this work we have considered the causal relations in Tamil Newspaper articles. The analysis of causal constructions, the causal markers and their syntactic relation lead to the identification of different features for developing the language model using RBMs (Restricted Boltzmann Machine). The experiments we performed have given encouraging results. The Cause-Effect system developed is used in a mobile App for Event profiling called “Nigalazhvi” where the cause and effect of an event is identified and given to the user.</abstract>
      <url hash="a2e15d38">2024.wildre-1.7</url>
      <bibkey>lalitha-devi-rk-rao-2024-finding</bibkey>
    </paper>
    <paper id="8">
      <title>Creating Corpus of Low Resource <fixed-case>I</fixed-case>ndian Languages for Natural Language Processing: Challenges and Opportunities</title>
      <author><first>Pratibha</first><last>Dongare</last></author>
      <pages>54–58</pages>
      <abstract>Addressing tasks in Natural Language Processing requires access to sufficient and high-quality data. However, working with languages that have limited resources poses a significant challenge due to the absence of established methodologies, frameworks, and collaborative efforts. This paper intends to briefly outline the challenges associated with standardization in data creation, focusing on Indian languages, which are often categorized as low resource languages. Additionally, potential solutions and the importance of standardized procedures for low-resource language data are proposed. Furthermore, the critical role of standardized protocols in corpus creation and their impact on research is highlighted. Lastly, this paper concludes by defining what constitutes a corpus.</abstract>
      <url hash="a969c905">2024.wildre-1.8</url>
      <bibkey>dongare-2024-creating</bibkey>
    </paper>
    <paper id="9">
      <title><fixed-case>FZZG</fixed-case> at <fixed-case>WILDRE</fixed-case>-7: Fine-tuning Pre-trained Models for Code-mixed, Less-resourced Sentiment Analysis</title>
      <author><first>Gaurish</first><last>Thakkar</last></author>
      <author><first>Marko</first><last>Tadić</last></author>
      <author><first>Nives</first><last>Mikelic Preradovic</last></author>
      <pages>59–65</pages>
      <abstract>This paper describes our system used for a shared task on code-mixed, less-resourced sentiment analysis for Indo-Aryan languages. We are using the large language models (LLMs) since they have demonstrated excellent performance on classification tasks. In our participation in all tracks, we use <i>unsloth/mistral-7b-bnb-4bit</i> LLM for the task of code-mixed sentiment analysis. For track 1, we used a simple fine-tuning strategy on PLMs by combining data from multiple phases. Our trained systems secured first place in four phases out of five. In addition, we present the results achieved using several PLMs for each language.</abstract>
      <url hash="ff689e00">2024.wildre-1.9</url>
      <bibkey>thakkar-etal-2024-fzzg</bibkey>
    </paper>
    <paper id="10">
      <title><fixed-case>MLI</fixed-case>nitiative@<fixed-case>WILDRE</fixed-case>7: Hybrid Approaches with Large Language Models for Enhanced Sentiment Analysis in Code-Switched and Code-Mixed Texts</title>
      <author><first>Hariram</first><last>Veeramani</last></author>
      <author><first>Surendrabikram</first><last>Thapa</last></author>
      <author><first>Usman</first><last>Naseem</last></author>
      <pages>66–72</pages>
      <abstract>Code-switched and code-mixed languages are prevalent in multilingual societies, reflecting the complex interplay of cultures and languages in daily communication. Understanding the sentiment embedded in such texts is crucial for a range of applications, from improving social media analytics to enhancing customer feedback systems. Despite their significance, research in code-mixed and code-switched languages remains limited, particularly in less-resourced languages. This scarcity of research creates a gap in natural language processing (NLP) technologies, hindering their ability to accurately interpret the rich linguistic diversity of global communications. To bridge this gap, this paper presents a novel methodology for sentiment analysis in code-mixed and code-switched texts. Our approach combines the power of large language models (LLMs) and the versatility of the multilingual BERT (mBERT) framework to effectively process and analyze sentiments in multilingual data. By decomposing code-mixed texts into their constituent languages, employing mBERT for named entity recognition (NER) and sentiment label prediction, and integrating these insights into a decision-making LLM, we provide a comprehensive framework for understanding sentiment in complex linguistic contexts. Our system achieves competitive rank on all subtasks in the Code-mixed Less-Resourced Sentiment analysis (Code-mixed) shared task at WILDRE-7 (LREC-COLING).</abstract>
      <url hash="8f9d556d">2024.wildre-1.10</url>
      <bibkey>veeramani-etal-2024-mlinitiative</bibkey>
    </paper>
    <paper id="11">
      <title>Aalamaram: A Large-Scale Linguistically Annotated Treebank for the <fixed-case>T</fixed-case>amil Language</title>
      <author><first>A M</first><last>Abirami</last></author>
      <author><first>Wei Qi</first><last>Leong</last></author>
      <author><first>Hamsawardhini</first><last>Rengarajan</last></author>
      <author><first>D</first><last>Anitha</last></author>
      <author><first>R</first><last>Suganya</last></author>
      <author><first>Himanshu</first><last>Singh</last></author>
      <author><first>Kengatharaiyer</first><last>Sarveswaran</last></author>
      <author><first>William Chandra</first><last>Tjhi</last></author>
      <author><first>Rajiv Ratn</first><last>Shah</last></author>
      <pages>73–83</pages>
      <abstract>Tamil is a relatively low-resource language in the field of Natural Language Processing (NLP). Recent years have seen a growth in Tamil NLP datasets in Natural Language Understanding (NLU) or Natural Language Generation (NLG) tasks, but high-quality linguistic resources remain scarce. In order to alleviate this gap in resources, this paper introduces Aalamaram, a treebank with rich linguistic annotations for the Tamil language. It is hitherto the largest publicly available Tamil treebank with almost 10,000 sentences from diverse sources and is annotated for the tasks of Part-of-speech (POS) tagging, Named Entity Recognition (NER), Morphological Parsing and Dependency Parsing. Close attention has also been paid to multi-word segmentation, especially in the context of Tamil clitics. Although the treebank is based largely on the Universal Dependencies (UD) specifications, significant effort has been made to adjust the annotation rules according to the idiosyncrasies and complexities of the Tamil language, thereby providing a valuable resource for linguistic research and NLP developments.</abstract>
      <url hash="84bc38f4">2024.wildre-1.11</url>
      <bibkey>abirami-etal-2024-aalamaram</bibkey>
    </paper>
  </volume>
</collection>
