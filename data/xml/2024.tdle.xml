<?xml version='1.0' encoding='UTF-8'?>
<collection id="2024.tdle">
  <volume id="1" ingest-date="2024-05-16" type="proceedings">
    <meta>
      <booktitle>Proceedings of the Second International Workshop Towards Digital Language Equality (TDLE): Focusing on Sustainability @ LREC-COLING 2024</booktitle>
      <editor><first>Federico</first><last>Gaspari</last></editor>
      <editor><first>Joss</first><last>Moorkens</last></editor>
      <editor><first>Itziar</first><last>Aldabe</last></editor>
      <editor><first>Aritz</first><last>Farwell</last></editor>
      <editor><first>Begona</first><last>Altuna</last></editor>
      <editor id="stelios-piperidis"><first>Stelios</first><last>Piperidis</last></editor>
      <editor><first>Georg</first><last>Rehm</last></editor>
      <editor id="german-rigau"><first>German</first><last>Rigau</last></editor>
      <publisher>ELRA and ICCL</publisher>
      <address>Torino, Italia</address>
      <month>May</month>
      <year>2024</year>
      <url hash="157c7db1">2024.tdle-1</url>
      <venue>tdle</venue>
      <venue>ws</venue>
    </meta>
    <frontmatter>
      <url hash="cd05324c">2024.tdle-1.0</url>
      <bibkey>tdle-2024-international</bibkey>
    </frontmatter>
    <paper id="1">
      <title>Surveying the Technology Support of Languages</title>
      <author><first>Annika</first><last>Grützner-Zahn</last></author>
      <author><first>Federico</first><last>Gaspari</last></author>
      <author><first>Maria</first><last>Giagkou</last></author>
      <author><first>Stefanie</first><last>Hegele</last></author>
      <author><first>Andy</first><last>Way</last></author>
      <author><first>Georg</first><last>Rehm</last></author>
      <pages>1–17</pages>
      <abstract>Many of the world’s languages are left behind when it comes to Language Technology applications, since most of these are available only in a limited number of languages, creating a digital divide that affects millions of users worldwide. It is crucial, therefore, to monitor and quantify the progress of technology support for individual languages, which also enables comparisons across language communities. In this way, efforts can be directed towards reducing language barriers, promoting economic and social inclusion, and ensuring that all citizens can use their preferred language in the digital age. This paper critically reviews and compares recent quantitative approaches to measuring technology support for languages. Despite using different approaches and methodologies, the findings of all analysed papers demonstrate the unequal distribution of technology support and emphasise the existence of a digital divide among languages.</abstract>
      <url hash="313f0512">2024.tdle-1.1</url>
      <bibkey>grutzner-zahn-etal-2024-surveying</bibkey>
    </paper>
    <paper id="2">
      <title>Which Domains, Tasks and Languages are in the Focus of <fixed-case>NLP</fixed-case> Research on the Languages of <fixed-case>E</fixed-case>urope?</title>
      <author><first>Diego</first><last>Alves</last></author>
      <author><first>Marko</first><last>Tadić</last></author>
      <author><first>Georg</first><last>Rehm</last></author>
      <pages>18–32</pages>
      <abstract>This article provides a thorough mapping of NLP and Language Technology research on 39 European languages onto 46 domains. Our analysis is based on almost 50,000 papers published between 2010 and October 2022 in the ACL Anthology. We use a dictionary-based approach to identify 1) languages, 2) domains, and 3) NLP tasks in these papers; the dictionary-based method using exact terms has a precision value of 0.81. Moreover, we identify common mistakes which can be useful to fine-tune the methodology for future work. While we are only able to highlight selected results in this submitted version, the final paper will contain detailed analyses and charts on a per-language basis. We hope that this study can contribute to digital language equality in Europe by providing information to the academic and industrial research community about the opportunities for novel LT/NLP research.</abstract>
      <url hash="74dff7ff">2024.tdle-1.2</url>
      <bibkey>alves-etal-2024-domains</bibkey>
    </paper>
    <paper id="3">
      <title>Fine-Tuning Open Access <fixed-case>LLM</fixed-case>s for High-Precision <fixed-case>NLU</fixed-case> in Goal-Driven Dialog Systems</title>
      <author id="lluis-padro"><first>Lluís</first><last>Padró</last></author>
      <author id="roser-sauri"><first>Roser</first><last>Saurí</last></author>
      <pages>33–42</pages>
      <abstract>This paper presents a set of experiments on fine-tuning LLMs to produce high-precision semantic representations for the NLU component of a dialog system front-end. The aim of this research is threefold: First, we want to explore the capabilities of LLMs on real, industry-based use cases that involve complex data and strict requirements on results. Since the LLM output should usable by the application back-end, the produced semantic representation must satisfy strict format and consistency requirements. Second, we want to evaluate the cost-benefit of open-source LLMs, that is, the feasibility of running this kind of models in machines affordable to small-medium enterprises (SMEs), in order to assess how far this organizations can go without depending on the large players controlling the market, and with a moderate use of computation resources. Finally, we also want to assess the language scalability of the LLMs in this kind of applications; specifically, whether a multilingual model is able to cast patterns learnt from one language to other ones –with special attention to underresourced languages–, thus reducing required training data and computation costs. This work was carried out within an R&amp;D context of assisting a real company in defining its NLU model strategy, and thus the results have a practical, industry-level focus.</abstract>
      <url hash="a77510d5">2024.tdle-1.3</url>
      <bibkey>padro-sauri-2024-fine</bibkey>
    </paper>
    <paper id="4">
      <title>Could We Have Had Better Multilingual <fixed-case>LLM</fixed-case>s if <fixed-case>E</fixed-case>nglish Was Not the Central Language?</title>
      <author><first>Ryandito</first><last>Diandaru</last></author>
      <author><first>Lucky</first><last>Susanto</last></author>
      <author><first>Zilu</first><last>Tang</last></author>
      <author><first>Ayu</first><last>Purwarianti</last></author>
      <author id="derry-tanti-wijaya"><first>Derry Tanti</first><last>Wijaya</last></author>
      <pages>43–52</pages>
      <abstract>Large Language Models (LLMs) demonstrate strong machine translation capabilities on languages they are trained on. However, the impact of factors beyond training data size on translation performance remains a topic of debate, especially concerning languages not directly encountered during training. Our study delves into Llama2’s translation capabilities. By modeling a linear relationship between linguistic feature distances and machine translation scores, we ask ourselves if there are potentially better central languages for LLMs other than English. Our experiments show that the 7B Llama2 model yields above 10 BLEU when translating into all languages it has seen, which rarely happens for languages it has not seen. Most translation improvements into unseen languages come from scaling up the model size rather than instruction tuning or increasing shot count. Furthermore, our correlation analysis reveals that syntactic similarity is not the only linguistic factor that strongly correlates with machine translation scores. Interestingly, we discovered that under specific circumstances, some languages (e.g. Swedish, Catalan), despite having significantly less training data, exhibit comparable correlation levels to English. These insights challenge the prevailing landscape of LLMs, suggesting that models centered around languages other than English could provide a more efficient foundation for multilingual applications.</abstract>
      <url hash="ea226ddb">2024.tdle-1.4</url>
      <bibkey>diandaru-etal-2024-better</bibkey>
    </paper>
    <paper id="5">
      <title>A Language Model Trained on Uruguayan <fixed-case>S</fixed-case>panish News Text</title>
      <author><first>Juan Pablo</first><last>Filevich</last></author>
      <author><first>Gonzalo</first><last>Marco</last></author>
      <author><first>Santiago</first><last>Castro</last></author>
      <author><first>Luis</first><last>Chiruzzo</last></author>
      <author><first>Aiala</first><last>Rosá</last></author>
      <pages>53–60</pages>
      <abstract>This paper presents a language model trained from scratch exclusively on a brand new corpus consisting of about 6 GiB of Uruguayan newspaper text. We trained the model for 30 days on a single Nvidia P100 using the RoBERTa-base architecture but with considerably fewer parameters than other standard RoBERTa models. We evaluated the model on two NLP tasks and found that it outperforms BETO, the widely used Spanish BERT pre-trained model. We also compared our model on the masked-word prediction task with two popular multilingual BERT-based models, Multilingual BERT and XLM-RoBERTa, obtaining outstanding results on sentences from the Uruguayan press domain. Our experiments show that training a language model on a domain-specific corpus can significantly improve performance even when the model is smaller and was trained with significantly less data than more standard pre-trained models.</abstract>
      <url hash="b13c75fa">2024.tdle-1.5</url>
      <bibkey>filevich-etal-2024-language</bibkey>
    </paper>
    <paper id="6">
      <title>Environmental Impact Measurement in the <fixed-case>M</fixed-case>ental<fixed-case>R</fixed-case>isk<fixed-case>ES</fixed-case> Evaluation Campaign</title>
      <author><first>Alba María</first><last>Mármol-Romero</last></author>
      <author><first>Adrián</first><last>Moreno-Muñoz</last></author>
      <author><first>Flor Miriam</first><last>Plaza-Del-Arco</last></author>
      <author><first>M. Dolores</first><last>Molina-González</last></author>
      <author><first>Arturo</first><last>Montejo-Ráez</last></author>
      <pages>61–72</pages>
      <abstract>With the rise of Large Language Models (LLMs), the NLP community is increasingly aware of the environmental consequences of model development due to the energy consumed for training and running these models. This study investigates the energy consumption and environmental impact of systems participating in the MentalRiskES shared task, at the Iberian Language Evaluation Forum (IberLEF) in the year 2023, which focuses on early risk identification of mental disorders in Spanish comments. Participants were asked to submit, for each prediction, a set of efficiency metrics, being carbon dioxide emissions among them. We conduct an empirical analysis of the data submitted considering model architecture, task complexity, and dataset characteristics, covering a spectrum from traditional Machine Learning (ML) models to advanced LLMs. Our findings contribute to understanding the ecological footprint of NLP systems and advocate for prioritizing environmental impact assessment in shared tasks to foster sustainability across diverse model types and approaches, being evaluation campaigns an adequate framework for this kind of analysis.</abstract>
      <url hash="7df1a74e">2024.tdle-1.6</url>
      <bibkey>marmol-romero-etal-2024-environmental</bibkey>
    </paper>
  </volume>
</collection>
