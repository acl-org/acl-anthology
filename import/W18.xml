<volume id="W18">
  <paper id="0100">
    <title>
      Proceedings of the 8th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2018)
    </title>
    <editor>
      <first>Asad</first>
      <last>Sayeed</last>
    </editor>
    <editor>
      <first>Cassandra</first>
      <last>Jacobs</last>
    </editor>
    <editor>
      <first>Tal</first>
      <last>Linzen</last>
    </editor>
    <editor>
      <first>Marten</first>
      <last>van Schijndel</last>
    </editor>
    <month>January</month>
    <year>2018</year>
    <address>Salt Lake City, Utah</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-01</url>
    <doi>10.18653/v1/W18-01</doi>
    <bibtype>book</bibtype>
    <bibkey>CMCL:2018</bibkey>
  </paper>
  <paper id="0101">
    <title>Coreference and Focus in Reading Times</title>
    <author>
      <first>Evan</first>
      <last>Jaffe</last>
    </author>
    <author>
      <first>Cory</first>
      <last>Shain</last>
    </author>
    <author>
      <first>William</first>
      <last>Schuler</last>
    </author>
    <booktitle>
      Proceedings of the 8th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2018)
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Salt Lake City, Utah</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–9</pages>
    <url>http://www.aclweb.org/anthology/W18-0101</url>
    <doi>10.18653/v1/W18-0101</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>jaffe-shain-schuler:2018:CMCL</bibkey>
  </paper>
  <paper id="0102">
    <title>
      Predictive power of word surprisal for reading times is a linear function
      of language model quality
    </title>
    <author>
      <first>Adam</first>
      <last>Goodkind</last>
    </author>
    <author>
      <first>Klinton</first>
      <last>Bicknell</last>
    </author>
    <booktitle>
      Proceedings of the 8th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2018)
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Salt Lake City, Utah</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>10–18</pages>
    <url>http://www.aclweb.org/anthology/W18-0102</url>
    <doi>10.18653/v1/W18-0102</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>goodkind-bicknell:2018:CMCL</bibkey>
  </paper>
  <paper id="0103">
    <title>Dynamic encoding of structural uncertainty in gradient symbols</title>
    <author>
      <first>Pyeong Whan</first>
      <last>Cho</last>
    </author>
    <author>
      <first>Matthew</first>
      <last>Goldrick</last>
    </author>
    <author>
      <first>Richard L.</first>
      <last>Lewis</last>
    </author>
    <author>
      <first>Paul</first>
      <last>Smolensky</last>
    </author>
    <booktitle>
      Proceedings of the 8th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2018)
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Salt Lake City, Utah</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>19–28</pages>
    <url>http://www.aclweb.org/anthology/W18-0103</url>
    <doi>10.18653/v1/W18-0103</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>cho-EtAl:2018:CMCL</bibkey>
  </paper>
  <paper id="0104">
    <title>Phonological (un)certainty weights lexical activation</title>
    <author>
      <first>Laura</first>
      <last>Gwilliams</last>
    </author>
    <author>
      <first>David</first>
      <last>Poeppel</last>
    </author>
    <author>
      <first>Alec</first>
      <last>Marantz</last>
    </author>
    <author>
      <first>Tal</first>
      <last>Linzen</last>
    </author>
    <booktitle>
      Proceedings of the 8th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2018)
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Salt Lake City, Utah</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>29–34</pages>
    <url>http://www.aclweb.org/anthology/W18-0104</url>
    <doi>10.18653/v1/W18-0104</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>gwilliams-EtAl:2018:CMCL</bibkey>
  </paper>
  <paper id="0105">
    <title>Predicting and Explaining Human Semantic Search in a Cognitive Model</title>
    <author>
      <first>Filip</first>
      <last>Miscevic</last>
    </author>
    <author>
      <first>Aida</first>
      <last>Nematzadeh</last>
    </author>
    <author>
      <first>Suzanne</first>
      <last>Stevenson</last>
    </author>
    <booktitle>
      Proceedings of the 8th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2018)
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Salt Lake City, Utah</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>35–45</pages>
    <url>http://www.aclweb.org/anthology/W18-0105</url>
    <doi>10.18653/v1/W18-0105</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>miscevic-nematzadeh-stevenson:2018:CMCL</bibkey>
  </paper>
  <paper id="0106">
    <title>Modeling bilingual word associations as connected monolingual networks</title>
    <author>
      <first>Yevgen</first>
      <last>Matusevych</last>
    </author>
    <author>
      <first>Amir Ardalan</first>
      <last>Kalantari Dehaghi</last>
    </author>
    <author>
      <first>Suzanne</first>
      <last>Stevenson</last>
    </author>
    <booktitle>
      Proceedings of the 8th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2018)
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Salt Lake City, Utah</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>46–56</pages>
    <url>http://www.aclweb.org/anthology/W18-0106</url>
    <doi>10.18653/v1/W18-0106</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>matusevych-kalantaridehaghi-stevenson:2018:CMCL</bibkey>
  </paper>
  <paper id="0107">
    <title>
      Experiential, Distributional and Dependency-based Word Embeddings have
      Complementary Roles in Decoding Brain Activity
    </title>
    <author>
      <first>Samira</first>
      <last>Abnar</last>
    </author>
    <author>
      <first>Rasyan</first>
      <last>Ahmed</last>
    </author>
    <author>
      <first>Max</first>
      <last>Mijnheer</last>
    </author>
    <author>
      <first>Willem</first>
      <last>Zuidema</last>
    </author>
    <booktitle>
      Proceedings of the 8th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2018)
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Salt Lake City, Utah</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>57–66</pages>
    <url>http://www.aclweb.org/anthology/W18-0107</url>
    <doi>10.18653/v1/W18-0107</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>abnar-EtAl:2018:CMCL</bibkey>
  </paper>
  <paper id="0108">
    <title>
      Exactly two things to learn from modeling scope ambiguity resolution:
      Developmental continuity and numeral semantics
    </title>
    <author>
      <first>K.J.</first>
      <last>Savinelli</last>
    </author>
    <author>
      <first>Greg</first>
      <last>Scontras</last>
    </author>
    <author>
      <first>Lisa</first>
      <last>Pearl</last>
    </author>
    <booktitle>
      Proceedings of the 8th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2018)
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Salt Lake City, Utah</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>67–75</pages>
    <url>http://www.aclweb.org/anthology/W18-0108</url>
    <doi>10.18653/v1/W18-0108</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>savinelli-scontras-pearl:2018:CMCL</bibkey>
  </paper>
  <paper id="0200">
    <title>
      Proceedings of the Fourth International Workshop on Computational
      Linguistics of Uralic Languages
    </title>
    <editor><first>Tommi A.</first><last>Pirinen</last></editor>
    <editor><first>Michael</first><last>Rießler</last></editor>
    <editor><first>Jack</first><last>Rueter</last></editor>
    <editor><first>Trond</first><last>Trosterud</last></editor>
    <editor><first>Francis M.</first><last>Tyers</last></editor>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-02</url>
    <doi>10.18653/v1/W18-02</doi>
    <bibtype>book</bibtype>
    <bibkey>IWCLUL2018:2018</bibkey>
  </paper>
  <paper id="0201">
    <title>
      Dependency Parsing of Code-Switching Data with Cross-Lingual Feature
      Representations
    </title>
    <author>
      <first>Niko</first>
      <last>Partanen</last>
    </author>
    <author>
      <first>Kyungtae</first>
      <last>Lim</last>
    </author>
    <author>
      <first>Michael</first>
      <last>Rießler</last>
    </author>
    <author>
      <first>Thierry</first>
      <last>Poibeau</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–17</pages>
    <url>http://www.aclweb.org/anthology/W18-0201</url>
    <doi>10.18653/v1/W18-0201</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>partanen-EtAl:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id="0202">
    <title>Building a Finnish SOM-based ontology concept tagger and harvester</title>
    <author>
      <first>Seppo</first>
      <last>Nyrkkö</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>18–25</pages>
    <url>http://www.aclweb.org/anthology/W18-0202</url>
    <doi>10.18653/v1/W18-0202</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>nyrkko:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id="0203">
    <title>Sound-aligned corpus of Udmurt dialectal texts</title>
    <author>
      <first>Timofey</first>
      <last>Arkhangelskiy</last>
    </author>
    <author>
      <first>Ekaterina</first>
      <last>Georgieva</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>26–38</pages>
    <url>http://www.aclweb.org/anthology/W18-0203</url>
    <doi>10.18653/v1/W18-0203</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>arkhangelskiy-georgieva:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id="0204">
    <title>Automatic Generation of Wiktionary Entries for Finno-Ugric Minority Languages</title>
    <author>
      <first>Zsanett</first>
      <last>Ferenczi</last>
    </author>
    <author>
      <first>Iván</first>
      <last>Mittelholcz</last>
    </author>
    <author>
      <first>Eszter</first>
      <last>Simon</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>39–50</pages>
    <url>http://www.aclweb.org/anthology/W18-0204</url>
    <doi>10.18653/v1/W18-0204</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>ferenczi-mittelholcz-simon:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id="0205">
    <title>Development of an Open Source Natural Language Generation Tool for Finnish</title>
    <author>
      <first>Mika</first>
      <last>Hämäläinen</last>
    </author>
    <author>
      <first>Jack</first>
      <last>Rueter</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>51–58</pages>
    <url>http://www.aclweb.org/anthology/W18-0205</url>
    <doi>10.18653/v1/W18-0205</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>hamalainen-rueter:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id="0206">
    <title>Guessing lexicon entries using finite-state methods</title>
    <author>
      <first>Kimmo</first>
      <last>Koskenniemi</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>59–75</pages>
    <url>http://www.aclweb.org/anthology/W18-0206</url>
    <doi>10.18653/v1/W18-0206</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>koskenniemi:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id="0207">
    <title>
      Tracking Typological Traits of Uralic Languages in Distributed Language
      Representations
    </title>
    <author>
      <first>Johannes</first>
      <last>Bjerva</last>
    </author>
    <author>
      <first>Isabelle</first>
      <last>Augenstein</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>76–86</pages>
    <url>http://www.aclweb.org/anthology/W18-0207</url>
    <doi>10.18653/v1/W18-0207</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>bjerva-augenstein:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id="0208">
    <title>New Baseline in Automatic Speech Recognition for Northern Sámi</title>
    <author>
      <first>Juho</first>
      <last>Leinonen</last>
    </author>
    <author>
      <first>Peter</first>
      <last>Smit</last>
    </author>
    <author>
      <first>Sami</first>
      <last>Virpioja</last>
    </author>
    <author>
      <first>Mikko</first>
      <last>Kurimo</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>87–97</pages>
    <url>http://www.aclweb.org/anthology/W18-0208</url>
    <doi>10.18653/v1/W18-0208</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>leinonen-EtAl:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id="0209">
    <title>Initial Experiments in Data-Driven Morphological Analysis for Finnish</title>
    <author>
      <first>Miikka</first>
      <last>Silfverberg</last>
    </author>
    <author>
      <first>Mans</first>
      <last>Hulden</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>98–105</pages>
    <url>http://www.aclweb.org/anthology/W18-0209</url>
    <doi>10.18653/v1/W18-0209</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>silfverberg-hulden:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id="0210">
    <title>Towards an open-source universal-dependency treebank for Erzya</title>
    <author>
      <first>Jack</first>
      <last>Rueter</last>
    </author>
    <author>
      <first>Francis</first>
      <last>Tyers</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>106–118</pages>
    <url>http://www.aclweb.org/anthology/W18-0210</url>
    <doi>10.18653/v1/W18-0210</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>rueter-tyers:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id="0211">
    <title>
      Utilization of Nganasan digital resources: a statistical approach to vowel
      harmony
    </title>
    <author>
      <first>László</first>
      <last>Fejes</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>119–138</pages>
    <url>http://www.aclweb.org/anthology/W18-0211</url>
    <doi>10.18653/v1/W18-0211</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>fejes:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id="0212">
    <title>Parallel Forms in Estonian Finite State Morphology</title>
    <author>
      <first>Heiki-Jaan</first>
      <last>Kaalep</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>139–153</pages>
    <url>http://www.aclweb.org/anthology/W18-0212</url>
    <doi>10.18653/v1/W18-0212</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>kaalep:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id="0213">
    <title>
      Extracting inflectional class assignment in Pite Saami: Nouns, verbs and
      those pesky adjectives
    </title>
    <author>
      <first>Joshua</first>
      <last>Wilbur</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>154–168</pages>
    <url>http://www.aclweb.org/anthology/W18-0213</url>
    <doi>10.18653/v1/W18-0213</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>wilbur:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id="0214">
    <title>Analysing Finnish with word lists: the DDI approach to morphology revisited</title>
    <author>
      <first>Atro</first>
      <last>Voutilainen</last>
    </author>
    <author>
      <first>Maria</first>
      <last>Palolahti</last>
    </author>
    <booktitle>
      Proceedings of the Fourth International Workshop on Computatinal
      Linguistics of Uralic Languages
    </booktitle>
    <month>January</month>
    <year>2018</year>
    <address>Helsinki, Finland</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>169–179</pages>
    <url>http://www.aclweb.org/anthology/W18-0214</url>
    <doi>10.18653/v1/W18-0214</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>voutilainen-palolahti:2018:IWCLUL2018</bibkey>
  </paper>
  <paper id="0300">
    <title>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</title>
    <editor><first>Gaja</first><last>Jarosz</last></editor>
    <editor><first>Brendan</first><last>O’Connor</last></editor>
    <editor><first>Joe</first><last>Pater</last></editor>
    <doi>10.7275/R5GF0RQW</doi>
  </paper>
  <paper id="0301">
    <title> 

Statistical Learning Theory and Linguistic Typology: a Learnability Perspective on OT’s Strict Domination </title>
    <author><first>Émile</first><last>Enguehard</last></author>
    <author><first>Edward</first><last>Flemming</last></author>
    <author><first>Giorgio</first><last>Magri</last></author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>1-11</pages>
    <attachment type="note">W18-0301.Notes.pdf</attachment>
    <dataset>W18-0301.Datasets.zip</dataset>
    <doi>10.7275/R5BP010Z</doi>
  </paper>
  <paper id="0302">
    <title>
      Detecting Language Impairments in Autism: A Computational Analysis of
      Semi-structured Conversations with Vector Semantics
    </title>
    <author><first>Adam</first><last>Goodkind</last></author>
    <author><first>Michelle</first><last>Lee</last></author>
    <author><first>Gary E.</first><last>Martin</last></author>
    <author><first>Molly</first><last>Losh</last></author>
    <author><first>Klinton</first><last>Bicknell</last></author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>12-22</pages>
    <doi>10.7275/R56W988P</doi>
  </paper>
  <paper id="0303">
    <title>Grammar Size and Quantitative Restrictions on Movement</title>
    <author><first>Thomas</first><last>Graf</last></author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>23-33</pages>
    <doi>10.7275/R5348HJ8</doi>
  </paper>
  <paper id="0304">
    <title>Modeling the Decline in English Passivization</title>
    <author><first>Liwen</first><last>Hou</last></author>
    <author><first>David</first><last>Smith</last></author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>34-43</pages>
    <doi>10.7275/R5ZC812C</doi>
  </paper>
  <paper id="0305">
    <title>Syntactic Category Learning as Iterative Prototype-Driven Clustering</title>
    <author><first>Jordan</first><last>Kodner</last></author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>44-54</pages>
    <doi>10.7275/R5TQ5ZQ4</doi>
  </paper>
  <paper id="0306">
    <title>A bidirectional mapping between English and CNF-based reasoners</title>
    <author><first>Steven</first><last>Abney</last></author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>55-63</pages>
    <doi>10.7275/R5PZ571N</doi>
  </paper>
  <paper id="0307">
    <title>Formal Restrictions On Multiple Tiers</title>
    <author><first>Alena</first><last>Aksenova</last></author>
    <author><first>Sanket</first><last>Deshmukh</last></author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>64-73</pages>
    <doi>10.7275/R5K64G8S</doi>
  </paper>
  <paper id="0308">
    <title>Differentiating Phrase Structure Parsing and Memory Retrieval in the Brain</title>
    <author><first>Shohini</first><last>Bhattasali</last></author>
    <author><first>John</first><last>Hale</last></author>
    <author><first>Christophe</first><last>Pallier</last></author>
    <author><first>Jonathan</first><last>Brennan</last></author>
    <author><first>Wen-Ming</first><last>Luh</last></author>
    <author><first>R. Nathan</first><last>Spreng</last></author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>74-80</pages>
    <doi>10.7275/R5FF3QJ2</doi>
  </paper>
  <paper id="0309">
    <title>Modeling the Complexity and Descriptive Adequacy of Construction Grammars</title>
    <author><first>Jonathan</first><last>Dunn</last></author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>81-90</pages>
    <doi>10.7275/R59P2ZTB</doi>
  </paper>
  <paper id="0310">
    <title>Decomposing phonological transformations in serial derivations</title>
    <author><first>Andrew</first><last>Lamont</last></author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>91-101</pages>
    <doi>10.7275/R55X273D</doi>
  </paper>
  <paper id="0311">
    <title>
      Phonologically Informed Edit Distance Algorithms for Word Alignment with
      Low-Resource Languages
    </title>
    <author><first>Richard T.</first><last>McCoy</last></author>
    <author><first>Robert</first><last>Frank</last></author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>102-112</pages>
    <doi>10.7275/R5251GC0</doi>
  </paper>
  <paper id="0312">
    <title>Conditions on abruptness in a gradient-ascent Maximum Entropy learner</title>
    <author><first>Elliott</first><last>Moreton</last></author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>113-124</pages>
    <doi>10.7275/R5XG9PBX</doi>
  </paper>
  <paper id="0313">
    <title>Using Rhetorical Topics for Automatic Summarization</title>
    <author><first>Natalie M.</first><last>Schrimpf</last></author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>125-135</pages>
    <doi>10.7275/R5SQ8XM6</doi>
  </paper>
  <paper id="0314">
    <title>Sound Analogies with Phoneme Embeddings</title>
    <author><first>Miikka P.</first><last>Silfverberg</last></author>
    <author><first>Lingshuang</first><last>Mao</last></author>
    <author><first>Mans</first><last>Hulden</last></author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>136-144</pages>
    <doi>10.7275/R5NZ85VD</doi>
  </paper>
  <paper id="0315">
    <title>Imdlawn Tashlhiyt Berber Syllabification is Quantifier-Free</title>
    <author><first>Kristina</first><last>Strother-Garcia</last></author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>145-153</pages>
    <doi>10.7275/R5J67F4D</doi>
  </paper>
  <paper id="0316">
    <title>Towards a Formal Description of NPI-licensing Patterns</title>
    <author><first>Mai Ha</first><last>Vu</last></author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>154-163</pages>
    <doi>10.7275/R5DF6PDP</doi>
  </paper>
  <paper id="0317">
    <title>The Organization of Lexicons: a Cross-Linguistic Analysis of Monosyllabic Words</title>
    <author><first>Shiying</first><last>Yang</last></author>
    <author><first>Chelsea</first><last>Sanker</last></author>
    <author><first>Uriel Cohen</first><last>Priva</last></author>
    <booktitle>Proceedings of the Society for Computation in Linguistics (SCiL) 2018</booktitle>
    <pages>164-173</pages>
    <doi>10.7275/R58P5XPZ</doi>
  </paper>
  <paper id="0500">
    <title>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </title>
    <editor>
      <first>Joel</first>
      <last>Tetreault</last>
    </editor>
    <editor>
      <first>Jill</first>
      <last>Burstein</last>
    </editor>
    <editor>
      <first>Ekaterina</first>
      <last>Kochmar</last>
    </editor>
    <editor>
      <first>Claudia</first>
      <last>Leacock</last>
    </editor>
    <editor>
      <first>Helen</first>
      <last>Yannakoudakis</last>
    </editor>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-05</url>
    <doi>10.18653/v1/W18-05</doi>
    <bibtype>book</bibtype>
    <bibkey>W18-05:2018</bibkey>
  </paper>
  <paper id="0501">
    <title>
      Using exemplar responses for training and evaluating automated speech
      scoring systems
    </title>
    <author>
      <first>Anastassia</first>
      <last>Loukina</last>
    </author>
    <author>
      <first>Klaus</first>
      <last>Zechner</last>
    </author>
    <author>
      <first>James</first>
      <last>Bruno</last>
    </author>
    <author>
      <first>Beata</first>
      <last>Beigman Klebanov</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–12</pages>
    <abstract>
      Automated scoring engines are usually trained and evaluated against human
      scores and compared to the benchmark of human-human agreement. In this
      paper we compare the performance of an automated speech scoring engine
      using two corpora: a corpus of almost 700,000 randomly sampled spoken
      responses with scores assigned by one or two raters during operational
      scoring, and a corpus of 16,500 exemplar responses with scores reviewed by
      multiple expert raters. We show that the choice of corpus used for model
      evaluation has a major effect on estimates of system performance with r
      varying between 0.64 and 0.80. Surprisingly, this is not the case for the
      choice of corpus for model training: when the training corpus is
      sufficiently large, the systems trained on different corpora showed almost
      identical performance when evaluated on the same corpus. We show that this
      effect is consistent across several learning algorithms. We conclude that
      evaluating the model on a corpus of exemplar responses if one is available
      provides additional evidence about system validity; at the same time,
      investing effort into creating a corpus of exemplar responses for model
      training is unlikely to lead to a substantial gain in model performance.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0501</url>
    <doi>10.18653/v1/W18-0501</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>loukina-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id="0502">
    <title>
      Using Paraphrasing and Memory-Augmented Models to Combat Data Sparsity in
      Question Interpretation with a Virtual Patient Dialogue System
    </title>
    <author>
      <first>Lifeng</first>
      <last>Jin</last>
    </author>
    <author>
      <first>David</first>
      <last>King</last>
    </author>
    <author>
      <first>Amad</first>
      <last>Hussein</last>
    </author>
    <author>
      <first>Michael</first>
      <last>White</last>
    </author>
    <author>
      <first>Douglas</first>
      <last>Danforth</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>13–23</pages>
    <abstract> 

When interpreting questions in a virtual patient dialogue system one must inevitably tackle the challenge of a long tail of relatively infrequently asked questions. To make progress on this challenge, we investigate the use of paraphrasing for data augmentation and neural memory-based classification, finding that the two methods work best in combination. In particular, we find that the neural memory-based approach not only outperforms a straight CNN classifier on low frequency questions, but also takes better advantage of the augmented data created by paraphrasing, together yielding a nearly 10% absolute improvement in accuracy on the least frequently asked questions. </abstract>
    <url>http://www.aclweb.org/anthology/W18-0502</url>
    <doi>10.18653/v1/W18-0502</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>jin-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id="0503">
    <title>Predicting misreadings from gaze in children with reading difficulties</title>
    <author>
      <first>Joachim</first>
      <last>Bingel</last>
    </author>
    <author>
      <first>Maria</first>
      <last>Barrett</last>
    </author>
    <author>
      <first>Sigrid</first>
      <last>Klerke</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>24–34</pages>
    <abstract>
      We present the first work on predicting reading mistakes in children with
      reading difficulties based on eye-tracking data from real-world reading
      teaching. Our approach employs several linguistic and gaze-based features
      to inform an ensemble of different classifiers, including multi-task
      learning models that let us transfer knowledge about individual readers to
      attain better predictions. Notably, the data we use in this work stems
      from noisy readings in the wild, outside of controlled lab conditions. Our
      experiments show that despite the noise and despite the small fraction of
      misreadings, gaze data improves the performance more than any other
      feature group and our models achieve good performance. We further show
      that gaze patterns for misread words do not fully generalize across
      readers, but that we can transfer some knowledge between readers using
      multitask learning at least in some cases. Applications of our models
      include partial automation of reading assessment as well as personalized
      text simplification.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0503</url>
    <doi>10.18653/v1/W18-0503</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>bingel-barrett-klerke:2018:W18-05</bibkey>
  </paper>
  <paper id="0504">
    <title>
      Automatic Input Enrichment for Selecting Reading Material: An Online Study
      with English Teachers
    </title>
    <author>
      <first>Maria</first>
      <last>Chinkina</last>
    </author>
    <author>
      <first>Ankita</first>
      <last>Oswal</last>
    </author>
    <author>
      <first>Detmar</first>
      <last>Meurers</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>35–44</pages>
    <abstract>
      Input material at the appropriate level is crucial for language
      acquisition. Automating the search for such material can systematically
      and efficiently support teachers in their pedagogical practice. This is
      the goal of the computational linguistic task of automatic input
      enrichment (Chinkina &amp; Meurers, 2016): It analyzes and re-ranks a
      collection of texts in order to prioritize those containing target
      linguistic forms. In the online study described in the paper, we collected
      240 responses from English teachers in order to investigate whether they
      preferred automatic input enrichment over web search when selecting
      reading material for class. Participants demonstrated a general preference
      for the material provided by an automatic input enrichment system. It was
      also rated significantly higher than the texts retrieved by a standard web
      search engine with regard to the representation of linguistic forms and
      equivalent with regard to the relevance of the content to the topic. We
      discuss the implications of the results for language teaching and consider
      the potential strands of future research.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0504</url>
    <doi>10.18653/v1/W18-0504</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>chinkina-oswal-meurers:2018:W18-05</bibkey>
  </paper>
  <paper id="0505">
    <title>Estimating Linguistic Complexity for Science Texts</title>
    <author>
      <first>Farah</first>
      <last>Nadeem</last>
    </author>
    <author>
      <first>Mari</first>
      <last>Ostendorf</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>45–55</pages>
    <abstract>
      Evaluation of text difficulty is important both for downstream tasks like
      text simplification, and for supporting educators in classrooms. Existing
      work on automated text complexity analysis uses linear models with
      engineered knowledge-driven features as inputs. While this offers
      interpretability, these models have lower accuracy for shorter texts.
      Traditional readability metrics have the additional drawback of not
      generalizing to informational texts such as science. We propose a neural
      approach, training on science and other informational texts, to mitigate
      both problems. Our results show that neural methods outperform
      knowledge-based linear models for short texts, and have the capacity to
      generalize to genres not present in the training data.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0505</url>
    <doi>10.18653/v1/W18-0505</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>nadeem-ostendorf:2018:W18-05</bibkey>
  </paper>
  <paper id="0506">
    <title>Second Language Acquisition Modeling</title>
    <author>
      <first>Burr</first>
      <last>Settles</last>
    </author>
    <author>
      <first>Chris</first>
      <last>Brust</last>
    </author>
    <author>
      <first>Erin</first>
      <last>Gustafson</last>
    </author>
    <author>
      <first>Masato</first>
      <last>Hagiwara</last>
    </author>
    <author>
      <first>Nitin</first>
      <last>Madnani</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>56–65</pages>
    <abstract>
      We present the task of <i>second language acquisition (SLA)
      modeling</i>. Given a history of errors made by learners of a
      second language, the task is to predict errors that they are
      likely to make at arbitrary points in the future. We describe a
      large corpus of more than 7M words produced by more than 6k
      learners of English, Spanish, and French using Duolingo, a
      popular online language-learning app. Then we report on the
      results of a shared task challenge aimed studying the SLA task
      via this corpus, which attracted 15 teams and synthesized work
      from various fields including cognitive science, linguistics,
      and machine learning.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0506</url>
    <doi>10.18653/v1/W18-0506</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>settles-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id="0507">
    <title>A Report on the Complex Word Identification Shared Task 2018</title>
    <author>
      <first>Seid Muhie</first>
      <last>Yimam</last>
    </author>
    <author>
      <first>Chris</first>
      <last>Biemann</last>
    </author>
    <author>
      <first>Shervin</first>
      <last>Malmasi</last>
    </author>
    <author>
      <first>Gustavo</first>
      <last>Paetzold</last>
    </author>
    <author>
      <first>Lucia</first>
      <last>Specia</last>
    </author>
    <author>
      <first>Sanja</first>
      <last>Štajner</last>
    </author>
    <author>
      <first>Anaïs</first>
      <last>Tack</last>
    </author>
    <author>
      <first>Marcos</first>
      <last>Zampieri</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>66–78</pages>
    <abstract>
      We report the findings of the second Complex Word Identification (CWI)
      shared task organized as part of the BEA workshop co-located with
      NAACL-HLT’2018. The second CWI shared task featured multilingual and
      multi-genre datasets divided into four tracks: English monolingual, German
      monolingual, Spanish monolingual, and a multilingual track with a French
      test set, and two tasks: binary classification and probabilistic
      classification. A total of 12 teams submitted their results in different
      task/track combinations and 11 of them wrote system description papers
      that are referred to in this report and appear in the BEA workshop
      proceedings.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0507</url>
    <doi>10.18653/v1/W18-0507</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>yimam-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id="0508">
    <title>Towards Single Word Lexical Complexity Prediction</title>
    <author>
      <first>David</first>
      <last>Alfter</last>
    </author>
    <author>
      <first>Elena</first>
      <last>Volodina</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>79–88</pages>
    <abstract>
      In this paper we present work-in-progress where we investigate the
      usefulness of previously created word lists to the task of single-word
      lexical complexity analysis and prediction of the complexity level for
      learners of Swedish as a second language. The word lists used map each
      word to a single CEFR level, and the task consists of predicting CEFR
      levels for unseen words. In contrast to previous work on word-level
      lexical complexity, we experiment with topics as additional features and
      show that linking words to topics significantly increases accuracy of
      classification.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0508</url>
    <doi>10.18653/v1/W18-0508</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>alfter-volodina:2018:W18-05</bibkey>
  </paper>
  <paper id="0509">
    <title>
      COAST - Customizable Online Syllable Enhancement in Texts. A flexible
      framework for automatically enhancing reading materials
    </title>
    <author>
      <first>Heiko</first>
      <last>Holz</last>
    </author>
    <author>
      <first>Zarah</first>
      <last>Weiss</last>
    </author>
    <author>
      <first>Oliver</first>
      <last>Brehm</last>
    </author>
    <author>
      <first>Detmar</first>
      <last>Meurers</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>89–100</pages>
    <abstract>
      This paper presents COAST, a web-based application to easily and
      automatically enhance syllable structure, word stress, and spacing in
      texts, that was designed in close collaboration with learning therapists
      to ensure its practical relevance. Such syllable-enhanced texts are
      commonly used in learning therapy or private tuition to promote the
      recognition of syllables in order to improve reading and writing skills.
      In a state of the art solutions for automatic syllable enhancement, we put
      special emphasis on syllable stress and support specific marking of the
      primary syllable stress in words. Core features of our tool are i) a
      highly customizable text enhancement and template functionality, and ii) a
      novel crowd-sourcing mechanism that we employ to address the issue of data
      sparsity in language resources. We successfully tested COAST with
      real-life practitioners in a series of user tests validating the concept
      of our framework.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0509</url>
    <doi>10.18653/v1/W18-0509</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>holz-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id="0510">
    <title>Annotating picture description task responses for content analysis</title>
    <author>
      <first>Levi</first>
      <last>King</last>
    </author>
    <author>
      <first>Markus</first>
      <last>Dickinson</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>101–109</pages>
    <abstract>
      Given that all users of a language can be creative in their language
      usage, the overarching goal of this work is to investigate issues of
      variability and acceptability in written text, for both non-native
      speakers (NNSs) and native speakers (NSs). We control for meaning by
      collecting a dataset of picture description task (PDT) responses from a
      number of NSs and NNSs, and we define and annotate a handful of features
      pertaining to form and meaning, to capture the multi-dimensional ways in
      which responses can vary and can be acceptable. By examining the decisions
      made in this corpus development, we highlight the questions facing anyone
      working with learner language properties like variability, acceptability
      and native-likeness. We find reliable inter-annotator agreement, though
      disagreements point to difficult areas for establishing a link between
      form and meaning.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0510</url>
    <doi>10.18653/v1/W18-0510</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>king-dickinson:2018:W18-05</bibkey>
  </paper>
  <paper id="0511">
    <title>Annotating Student Talk in Text-based Classroom Discussions</title>
    <author>
      <first>Luca</first>
      <last>Lugini</last>
    </author>
    <author>
      <first>Diane</first>
      <last>Litman</last>
    </author>
    <author>
      <first>Amanda</first>
      <last>Godley</last>
    </author>
    <author>
      <first>Christopher</first>
      <last>Olshefski</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>110–116</pages>
    <abstract>
      Classroom discussions in English Language Arts have a positive effect on
      students’ reading, writing and reasoning skills. Although prior work has
      largely focused on teacher talk and student-teacher interactions, we focus
      on three theoretically-motivated aspects of high-quality student talk:
      argumentation, specificity, and knowledge domain. We introduce an
      annotation scheme, then show that the scheme can be used to produce
      reliable annotations and that the annotations are predictive of discussion
      quality. We also highlight opportunities provided by our scheme for
      education and natural language processing research.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0511</url>
    <doi>10.18653/v1/W18-0511</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>lugini-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id="0512">
    <title>
      Toward Automatically Measuring Learner Ability from Human-Machine Dialog
      Interactions using Novel Psychometric Models
    </title>
    <author>
      <first>Vikram</first>
      <last>Ramanarayanan</last>
    </author>
    <author>
      <first>Michelle</first>
      <last>LaMar</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>117–126</pages>
    <abstract>
      While dialog systems have been widely deployed for computer-assisted
      language learning (CALL) and formative assessment systems in recent years,
      relatively limited work has been done with respect to the psychometrics
      and validity of these technologies in evaluating and providing feedback
      regarding student learning and conversational ability. This paper
      formulates a Markov decision process based measurement model, and applies
      it to text chat data collected from crowdsourced native and non-native
      English language speakers interacting with an automated dialog agent. We
      investigate how well the model measures speaker conversational ability,
      and find that it effectively captures the differences in how native and
      non-native speakers of English accomplish the dialog task. Such models
      could have important implications for CALL systems of the future that
      effectively combine dialog management with measurement of learner
      conversational ability in real-time.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0512</url>
    <doi>10.18653/v1/W18-0512</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>ramanarayanan-lamar:2018:W18-05</bibkey>
  </paper>
  <paper id="0513">
    <title>Generating Feedback for English Foreign Language Exercises</title>
    <author>
      <first>Björn</first>
      <last>Rudzewitz</last>
    </author>
    <author>
      <first>Ramon</first>
      <last>Ziai</last>
    </author>
    <author>
      <first>Kordula</first>
      <last>De Kuthy</last>
    </author>
    <author>
      <first>Verena</first>
      <last>Möller</last>
    </author>
    <author>
      <first>Florian</first>
      <last>Nuxoll</last>
    </author>
    <author>
      <first>Detmar</first>
      <last>Meurers</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>127–136</pages>
    <abstract>
      While immediate feedback on learner language is often discussed in the
      Second Language Acquisition literature (e.g., Mackey 2006), few systems
      used in real-life educational settings provide helpful, metalinguistic
      feedback to learners. In this paper, we present a novel approach
      leveraging task information to generate the expected range of well-formed
      and ill-formed variability in learner answers along with the required
      diagnosis and feedback. We combine this offline generation approach with
      an online component that matches the actual student answers against the
      pre-computed hypotheses. The results obtained for a set of 33 thousand
      answers of 7th grade German high school students learning English show
      that the approach successfully covers frequent answer patterns. At the
      same time, paraphrases and content errors require a more flexible
      alignment approach, for which we are planning to complement the method
      with the CoMiC approach successfully used for the analysis of reading
      comprehension answers (Meurers et al., 2011).
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0513</url>
    <doi>10.18653/v1/W18-0513</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>rudzewitz-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id="0514">
    <title>
      NT2Lex: A CEFR-Graded Lexical Resource for Dutch as a Foreign Language
      Linked to Open Dutch WordNet
    </title>
    <author>
      <first>Anaïs</first>
      <last>Tack</last>
    </author>
    <author>
      <first>Thomas</first>
      <last>François</last>
    </author>
    <author>
      <first>Piet</first>
      <last>Desmet</last>
    </author>
    <author>
      <first>Cédrick</first>
      <last>Fairon</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>137–146</pages>
    <abstract>
      In this paper, we introduce NT2Lex, a novel lexical resource for Dutch as
      a foreign language (NT2) which includes frequency distributions of 17,743
      words and expressions attested in expert-written textbook texts and
      readers graded along the scale of the Common European Framework of
      Reference (CEFR). In essence, the lexicon informs us about what kind of
      vocabulary should be understood when reading Dutch as a non-native reader
      at a particular proficiency level. The main novelty of the resource with
      respect to the previously developed CEFR-graded lexicons concerns the
      introduction of corpus-based evidence for L2 word sense complexity through
      the linkage to Open Dutch WordNet (Postma et al., 2016). The resource thus
      contains, on top of the lemmatised and part-of-speech tagged lexical
      entries, a total of 11,999 unique word senses and 8,934 distinct synsets.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0514</url>
    <doi>10.18653/v1/W18-0514</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>tack-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id="0515">
    <title>Experiments with Universal CEFR Classification</title>
    <author>
      <first>Sowmya</first>
      <last>Vajjala</last>
    </author>
    <author>
      <first>Taraka</first>
      <last>Rama</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>147–153</pages>
    <abstract>
      The Common European Framework of Reference (CEFR) guidelines describe
      language proficiency of learners on a scale of 6 levels. While the
      description of CEFR guidelines is generic across languages, the
      development of automated proficiency classification systems for different
      languages follow different approaches. In this paper, we explore universal
      CEFR classification using domain-specific and domain-agnostic,
      theory-guided as well as data-driven features. We report the results of
      our preliminary experiments in monolingual, cross-lingual, and
      multilingual classification with three languages: German, Czech, and
      Italian. Our results show that both monolingual and multilingual models
      achieve similar performance, and cross-lingual classification yields
      lower, but comparable results to monolingual classification.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0515</url>
    <doi>10.18653/v1/W18-0515</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>vajjala-rama:2018:W18-05</bibkey>
  </paper>
  <paper id="0516">
    <title>Chengyu Cloze Test</title>
    <author>
      <first>Zhiying</first>
      <last>Jiang</last>
    </author>
    <author>
      <first>Boliang</first>
      <last>Zhang</last>
    </author>
    <author>
      <first>Lifu</first>
      <last>Huang</last>
    </author>
    <author>
      <first>Heng</first>
      <last>Ji</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>154–158</pages>
    <abstract>
      We present a neural recommendation model for Chengyu, which is a special
      type of Chinese idiom. Given a query, which is a sentence with an empty
      slot where the Chengyu is taken out, our model will recommend the best
      Chengyu candidate that best fits the slot context. The main challenge lies
      in that the literal meaning of a Chengyu is usually very different from
      it’s figurative meaning. We propose a new neural approach to leverage the
      definition of each Chengyu and incorporate it as background knowledge.
      Experiments on both Chengyu cloze test and coherence checking in college
      entrance exams show that our system achieves 89.5% accuracy on cloze test
      and outperforms human subjects who attended competitive universities in
      China. We will make all of our data sets and resources publicly available
      as a new benchmark for research purposes.
    </abstract>
    <dataset>W18-0516.Datasets.zip</dataset>
    <url>http://www.aclweb.org/anthology/W18-0516</url>
    <doi>10.18653/v1/W18-0516</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>jiang-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id="0517">
    <title>LaSTUS/TALN at Complex Word Identification (CWI) 2018 Shared Task</title>
    <author>
      <first>Ahmed</first>
      <last>AbuRa’ed</last>
    </author>
    <author>
      <first>Horacio</first>
      <last>Saggion</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>159–165</pages>
    <abstract>
      This paper presents the participation of the LaSTUS/TALN team in the
      Complex Word Identification (CWI) Shared Task 2018 in the English
      monolingual track . The purpose of the task was to determine if a word in
      a given sentence can be judged as complex or not by a certain target
      audience. For the English track, task organizers provided a training and a
      development datasets of 27,299 and 3,328 words respectively together with
      the sentence in which each word occurs. The words were judged as complex
      or not by 20 human evaluators; ten of whom are natives. We submitted two
      systems: one system modeled each word to evaluate as a numeric vector
      populated with a set of lexical, semantic and contextual features while
      the other system relies on a word embedding representation and a distance
      metric. We trained two separate classifiers to automatically decide if
      each word is complex or not. We submitted six runs, two for each of the
      three subsets of the English monolingual CWI track.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0517</url>
    <doi>10.18653/v1/W18-0517</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>aburaed-saggion:2018:W18-05</bibkey>
  </paper>
  <paper id="0518">
    <title>Cross-lingual complex word identification with multitask learning</title>
    <author>
      <first>Joachim</first>
      <last>Bingel</last>
    </author>
    <author>
      <first>Johannes</first>
      <last>Bjerva</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>166–174</pages>
    <abstract>
      We approach the 2018 Shared Task on Complex Word Identification by
      leveraging a cross-lingual multitask learning approach. Our method is
      highly language agnostic, as evidenced by the ability of our system to
      generalize across languages, including languages for which we have no
      training data. In the shared task, this is the case for French, for which
      our system achieves the best performance. We further provide a qualitative
      and quantitative analysis of which words pose problems for our system.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0518</url>
    <doi>10.18653/v1/W18-0518</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>bingel-bjerva:2018:W18-05</bibkey>
  </paper>
  <paper id="0519">
    <title>UnibucKernel: A kernel-based learning method for complex word identification</title>
    <author>
      <first>Andrei</first>
      <last>Butnaru</last>
    </author>
    <author>
      <first>Radu Tudor</first>
      <last>Ionescu</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>175–183</pages>
    <abstract>
      In this paper, we present a kernel-based learning approach for the 2018
      Complex Word Identification (CWI) Shared Task. Our approach is based on
      combining multiple low-level features, such as character n-grams, with
      high-level semantic features that are either automatically learned using
      word embeddings or extracted from a lexical knowledge base, namely
      WordNet. After feature extraction, we employ a kernel method for the
      learning phase. The feature matrix is first transformed into a normalized
      kernel matrix. For the binary classification task (simple versus complex),
      we employ Support Vector Machines. For the regression task, in which we
      have to predict the complexity level of a word (a word is more complex if
      it is labeled as complex by more annotators), we employ v-Support Vector
      Regression. We applied our approach only on the three English data sets
      containing documents from Wikipedia, WikiNews and News domains. Our best
      result during the competition was the third place on the English Wikipedia
      data set. However, in this paper, we also report better post-competition
      results.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0519</url>
    <doi>10.18653/v1/W18-0519</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>butnaru-ionescu:2018:W18-05</bibkey>
  </paper>
  <paper id="0520">
    <title>
      CAMB at CWI Shared Task 2018: Complex Word Identification with
      Ensemble-Based Voting
    </title>
    <author>
      <first>Sian</first>
      <last>Gooding</last>
    </author>
    <author>
      <first>Ekaterina</first>
      <last>Kochmar</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>184–194</pages>
    <abstract>
      This paper presents the winning systems we submitted to the Complex Word
      Identifica- tion Shared Task 2018. We describe our best performing
      systems’ implementations and discuss our key findings from this research.
      Our best-performing systems achieve an F1 score of 0.8792 on the NEWS,
      0.8430 on the WIKINEWS and 0.8115 on the WIKIPEDIA test sets in the
      monolingual English binary classification track, and a mean absolute error
      of 0.0558 on the NEWS, 0.0674 on the WIKINEWS and 0.0739 on the WIKIPEDIA
      test sets in the probabilistic track.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0520</url>
    <doi>10.18653/v1/W18-0520</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>gooding-kochmar:2018:W18-05</bibkey>
  </paper>
  <paper id="0521">
    <title>Complex Word Identification Based on Frequency in a Learner Corpus</title>
    <author>
      <first>Tomoyuki</first>
      <last>Kajiwara</last>
    </author>
    <author>
      <first>Mamoru</first>
      <last>Komachi</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>195–199</pages>
    <abstract>
      We introduce the TMU systems for the Complex Word Identification (CWI)
      Shared Task 2018. TMU systems use random forest classifiers and regressors
      whose features are the number of characters, the number of words, and the
      frequency of target words in various corpora. Our simple systems performed
      best on 5 tracks out of 12 tracks. Our ablation analysis revealed the
      usefulness of a learner corpus for CWI task.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0521</url>
    <doi>10.18653/v1/W18-0521</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>kajiwara-komachi:2018:W18-05</bibkey>
  </paper>
  <paper id="0522">
    <title>
      The Whole is Greater than the Sum of its Parts: Towards the Effectiveness
      of Voting Ensemble Classifiers for Complex Word Identification
    </title>
    <author>
      <first>Nikhil</first>
      <last>Wani</last>
    </author>
    <author>
      <first>Sandeep</first>
      <last>Mathias</last>
    </author>
    <author>
      <first>Jayashree Aanand</first>
      <last>Gajjam</last>
    </author>
    <author>
      <first>Pushpak</first>
      <last>Bhattacharyya</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>200–205</pages>
    <abstract>
      In this paper, we present an effective system using voting ensemble
      classifiers to detect contextually complex words for non-native English
      speakers. To make the final decision, we channel a set of eight calibrated
      classifiers based on lexical, size and vocabulary features and train our
      model with annotated datasets collected from a mixture of native and
      non-native speakers. Thereafter, we test our system on three datasets
      namely News, WikiNews, and Wikipedia and report competitive results with
      an F1-Score ranging between 0.777 to 0.855 for each of the datasets. Our
      system outperforms multiple other models and falls within 0.042 to 0.026
      percent of the best-performing model’s score in the shared task.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0522</url>
    <doi>10.18653/v1/W18-0522</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>wani-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id="0523">
    <title>
      Grotoco@SLAM: Second Language Acquisition Modeling with Simple Features,
      Learners and Task-wise Models
    </title>
    <author>
      <first>Sigrid</first>
      <last>Klerke</last>
    </author>
    <author>
      <first>Héctor</first>
      <last>Martínez Alonso</last>
    </author>
    <author>
      <first>Barbara</first>
      <last>Plank</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>206–211</pages>
    <abstract>
      We present our submission to the 2018 Duolingo Shared Task on Second
      Language Acquisition Modeling (SLAM). We focus on evaluating a range of
      features for the task, including user-derived measures, while examining
      how far we can get with a simple linear classifier. Our analysis reveals
      that errors differ per exercise format, which motivates our final and
      best-performing system: a task-wise (per exercise-format) model.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0523</url>
    <doi>10.18653/v1/W18-0523</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>klerke-martnezalonso-plank:2018:W18-05</bibkey>
  </paper>
  <paper id="0524">
    <title>Context Based Approach for Second Language Acquisition</title>
    <author>
      <first>Nihal V.</first>
      <last>Nayak</last>
    </author>
    <author>
      <first>Arjun R.</first>
      <last>Rao</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>212–216</pages>
    <abstract>
      SLAM 2018 focuses on predicting a student’s mistake while using the
      Duolingo application. In this paper, we describe the system we developed
      for this shared task. Our system uses a logistic regression model to
      predict the likelihood of a student making a mistake while answering an
      exercise on Duolingo in all three language tracks - English/Spanish
      (en/es), Spanish/English (es/en) and French/English (fr/en). We conduct an
      ablation study with several features during the development of this system
      and discover that context based features plays a major role in language
      acquisition modeling. Our model beats Duolingo’s baseline scores in all
      three language tracks (AUROC scores for en/es = 0.821, es/en = 0.790 and
      fr/en = 0.812). Our work makes a case for providing favourable textual
      context for students while learning second language.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0524</url>
    <doi>10.18653/v1/W18-0524</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>nayak-rao:2018:W18-05</bibkey>
  </paper>
  <paper id="0525">
    <title>Second Language Acquisition Modeling: An Ensemble Approach</title>
    <author>
      <first>Anton</first>
      <last>Osika</last>
    </author>
    <author>
      <first>Susanna</first>
      <last>Nilsson</last>
    </author>
    <author>
      <first>Andrii</first>
      <last>Sydorchuk</last>
    </author>
    <author>
      <first>Faruk</first>
      <last>Sahin</last>
    </author>
    <author>
      <first>Anders</first>
      <last>Huss</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>217–222</pages>
    <abstract>
      Accurate prediction of students’ knowledge is a fundamental building block
      of personalized learning systems. Here, we propose an ensemble model to
      predict student knowledge gaps. Applying our approach to student trace
      data from the online educational platform Duolingo we achieved highest
      score on all three datasets in the 2018 Shared Task on Second Language
      Acquisition Modeling. We describe our model and discuss relevance of the
      task compared to how it would be setup in a production environment for
      personalized education.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0525</url>
    <doi>10.18653/v1/W18-0525</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>osika-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id="0526">
    <title>Modeling Second-Language Learning from a Psychological Perspective</title>
    <author>
      <first>Alexander</first>
      <last>Rich</last>
    </author>
    <author>
      <first>Pamela</first>
      <last>Osborn Popp</last>
    </author>
    <author>
      <first>David</first>
      <last>Halpern</last>
    </author>
    <author>
      <first>Anselm</first>
      <last>Rothe</last>
    </author>
    <author>
      <first>Todd</first>
      <last>Gureckis</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>223–230</pages>
    <abstract>
      Psychological research on learning and memory has tended to emphasize
      small-scale laboratory studies. However, large datasets of people using
      educational software provide opportunities to explore these issues from a
      new perspective. In this paper we describe our approach to the Duolingo
      Second Language Acquisition Modeling (SLAM) competition which was run in
      early 2018. We used a well-known class of algorithms (gradient boosted
      decision trees), with features partially informed by theories from the
      psychological literature. After detailing our modeling approach and a
      number of supplementary simulations, we reflect on the degree to which
      psychological theory aided the model, and the potential for cognitive
      science and predictive modeling competitions to gain from each other.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0526</url>
    <doi>10.18653/v1/W18-0526</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>rich-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id="0527">
    <title>
      A Memory-Sensitive Classification Model of Errors in Early Second Language
      Learning
    </title>
    <author>
      <first>Brendan</first>
      <last>Tomoschuk</last>
    </author>
    <author>
      <first>Jarrett</first>
      <last>Lovelett</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>231–239</pages>
    <abstract>
      In this paper, we explore a variety of lin- guistic and cognitive features
      to better un- derstand second language acquisition in early users of the
      language learning app Duolingo. With these features, we trained a random
      forest classifier to predict errors in early learners of French, Spanish,
      and English. Of particular note was our finding that mean and variance in
      error for each user and token can be a memory efficient replacement for
      their respective dummy- encoded categorical variables. At test, the- se
      models improved over the baseline model with AUROC values of 0.803 for
      English, 0.823 for French, and 0.829 for Spanish.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0527</url>
    <doi>10.18653/v1/W18-0527</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>tomoschuk-lovelett:2018:W18-05</bibkey>
  </paper>
  <paper id="0528">
    <title>Annotation and Classification of Sentence-level Revision Improvement</title>
    <author>
      <first>Tazin</first>
      <last>Afrin</last>
    </author>
    <author>
      <first>Diane</first>
      <last>Litman</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>240–246</pages>
    <abstract>
      Studies of writing revisions rarely focus on revision quality. To address
      this issue, we introduce a corpus of between-draft revisions of student
      argumentative essays, annotated as to whether each revision improves essay
      quality. We demonstrate a potential usage of our annotations by developing
      a machine learning model to predict revision improvement. With the goal of
      expanding training data, we also extract revisions from a dataset edited
      by expert proofreaders. Our results indicate that blending expert and
      non-expert revisions increases model performance, with expert data
      particularly important for predicting low-quality revisions.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0528</url>
    <doi>10.18653/v1/W18-0528</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>afrin-litman:2018:W18-05</bibkey>
  </paper>
  <paper id="0529">
    <title>
      Language Model Based Grammatical Error Correction without Annotated
      Training Data
    </title>
    <author>
      <first>Christopher</first>
      <last>Bryant</last>
    </author>
    <author>
      <first>Ted</first>
      <last>Briscoe</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>247–253</pages>
    <abstract>
      Since the end of the CoNLL-2014 shared task on grammatical error
      correction (GEC), research into language model (LM) based approaches to
      GEC has largely stagnated. In this paper, we re-examine LMs in GEC and
      show that it is entirely possible to build a simple system that not only
      requires minimal annotated data (∼1000 sentences), but is also fairly
      competitive with several state-of-the-art systems. This approach should be
      of particular interest for languages where very little annotated training
      data exists, although we also hope to use it as a baseline to motivate
      future research.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0529</url>
    <doi>10.18653/v1/W18-0529</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>bryant-briscoe:2018:W18-05</bibkey>
  </paper>
  <paper id="0530">
    <title>A Semantic Role-based Approach to Open-Domain Automatic Question Generation</title>
    <author>
      <first>Michael</first>
      <last>Flor</last>
    </author>
    <author>
      <first>Brian</first>
      <last>Riordan</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>254–263</pages>
    <abstract>
      We present a novel rule-based system for automatic generation of factual
      questions from sentences, using semantic role labeling (SRL) as the main
      form of text analysis. The system is capable of generating both
      wh-questions and yes/no questions from the same semantic analysis. We
      present an extensive evaluation of the system and compare it to a recent
      neural network architecture for question generation. The SRL-based system
      outperforms the neural system in both average quality and variety of
      generated questions.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0530</url>
    <doi>10.18653/v1/W18-0530</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>flor-riordan:2018:W18-05</bibkey>
  </paper>
  <paper id="0531">
    <title>Automated Content Analysis: A Case Study of Computer Science Student Summaries</title>
    <author>
      <first>Yanjun</first>
      <last>Gao</last>
    </author>
    <author>
      <first>Patricia</first>
      <last>M.Davies</last>
    </author>
    <author>
      <first>Rebecca J.</first>
      <last>Passonneau</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>264–272</pages>
    <abstract>
      Technology is transforming Higher Education learning and teaching. This
      paper reports on a project to examine how and why automated content
      analysis could be used to assess precis writing by university students. We
      examine the case of one hundred and twenty-two summaries written by
      computer science freshmen. The texts, which had been hand scored using a
      teacher-designed rubric, were autoscored using the Natural Language
      Processing software, PyrEval. Pearson’s correlation coefficient and
      Spearman rank correlation were used to analyze the relationship between
      the teacher score and the PyrEval score for each summary. Three content
      models automatically constructed by PyrEval from different sets of human
      reference summaries led to consistent correlations, showing that the
      approach is reliable. Also observed was that, in cases where the focus of
      student assessment centers on formative feedback, categorizing the PyrEval
      scores by examining the average and standard deviations could lead to
      novel interpretations of their relationships. It is suggested that this
      project has implications for the ways in which automated content analysis
      could be used to help university students improve their summarization
      skills.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0531</url>
    <doi>10.18653/v1/W18-0531</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>gao-mdavies-passonneau:2018:W18-05</bibkey>
  </paper>
  <paper id="0532">
    <title>
      Toward Data-Driven Tutorial Question Answering with Deep Learning
      Conversational Models
    </title>
    <author>
      <first>Mayank</first>
      <last>Kulkarni</last>
    </author>
    <author>
      <first>Kristy</first>
      <last>Boyer</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>273–283</pages>
    <abstract>
      There has been an increase in popularity of data-driven question answering
      systems given their recent success. This pa-per explores the possibility
      of building a tutorial question answering system for Java programming from
      data sampled from a community-based question answering forum. This paper
      reports on the creation of a dataset that could support building such a
      tutorial question answering system and discusses the methodology to create
      the 106,386 question strong dataset. We investigate how retrieval-based
      and generative models perform on the given dataset. The work also
      investigates the usefulness of using hybrid approaches such as combining
      retrieval-based and generative models. The results indicate that building
      data-driven tutorial systems using community-based question answering
      forums holds significant promise.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0532</url>
    <doi>10.18653/v1/W18-0532</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>kulkarni-boyer:2018:W18-05</bibkey>
  </paper>
  <paper id="0533">
    <title>Distractor Generation for Multiple Choice Questions Using Learning to Rank</title>
    <author>
      <first>Chen</first>
      <last>Liang</last>
    </author>
    <author>
      <first>Xiao</first>
      <last>Yang</last>
    </author>
    <author>
      <first>Neisarg</first>
      <last>Dave</last>
    </author>
    <author>
      <first>Drew</first>
      <last>Wham</last>
    </author>
    <author>
      <first>Bart</first>
      <last>Pursel</last>
    </author>
    <author>
      <first>C Lee</first>
      <last>Giles</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>284–290</pages>
    <abstract>
      We investigate how machine learning models, specifically ranking models,
      can be used to select useful distractors for multiple choice questions.
      Our proposed models can learn to select distractors that resemble those in
      actual exam questions, which is different from most existing unsupervised
      ontology-based and similarity-based methods. We empirically study
      feature-based and neural net (NN) based ranking models with experiments on
      the recently released SciQ dataset and our MCQL dataset. Experimental
      results show that feature-based ensemble learning methods (random forest
      and LambdaMART) outperform both the NN-based method and unsupervised
      baselines. These two datasets can also be used as benchmarks for
      distractor generation.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0533</url>
    <doi>10.18653/v1/W18-0533</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>liang-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id="0534">
    <title>A Portuguese Native Language Identification Dataset</title>
    <author>
      <first>Iria</first>
      <last>del Río Gayo</last>
    </author>
    <author>
      <first>Marcos</first>
      <last>Zampieri</last>
    </author>
    <author>
      <first>Shervin</first>
      <last>Malmasi</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>291–296</pages>
    <abstract>
      In this paper we present NLI-PT, the first Portuguese dataset compiled for
      Native Language Identification (NLI), the task of identifying an author’s
      first language based on their second language writing. The dataset
      includes 1,868 student essays written by learners of European Portuguese,
      native speakers of the following L1s: Chinese, English, Spanish, German,
      Russian, French, Japanese, Italian, Dutch, Tetum, Arabic, Polish, Korean,
      Romanian, and Swedish. NLI-PT includes the original student text and four
      different types of annotation: POS, fine-grained POS, constituency parses,
      and dependency parses. NLI-PT can be used not only in NLI but also in
      research on several topics in the field of Second Language Acquisition and
      educational NLP. We discuss possible applications of this dataset and
      present the results obtained for the first lexical baseline system for
      Portuguese NLI.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0534</url>
    <doi>10.18653/v1/W18-0534</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>delrogayo-zampieri-malmasi:2018:W18-05</bibkey>
  </paper>
  <paper id="0535">
    <title>
      OneStopEnglish corpus: A new corpus for automatic readability assessment
      and text simplification
    </title>
    <author>
      <first>Sowmya</first>
      <last>Vajjala</last>
    </author>
    <author>
      <first>Ivana</first>
      <last>Lucic</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>297–304</pages>
    <abstract>
      This paper describes the collection and compilation of the OneStopEnglish
      corpus of texts written at three reading levels, and demonstrates its
      usefulness for through two applications - automatic readability assessment
      and automatic text simplification. The corpus consists of 189 texts, each
      in three versions (567 in total). The corpus is now freely available under
      a CC by-SA 4.0 license and we hope that it would foster further research
      on the topics of readability assessment and text simplification.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0535</url>
    <doi>10.18653/v1/W18-0535</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>vajjala-lucic:2018:W18-05</bibkey>
  </paper>
  <paper id="0536">
    <title>The Effect of Adding Authorship Knowledge in Automated Text Scoring</title>
    <author>
      <first>Meng</first>
      <last>Zhang</last>
    </author>
    <author>
      <first>Xie</first>
      <last>Chen</last>
    </author>
    <author>
      <first>Ronan</first>
      <last>Cummins</last>
    </author>
    <author>
      <first>Øistein E.</first>
      <last>Andersen</last>
    </author>
    <author>
      <first>Ted</first>
      <last>Briscoe</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>305–314</pages>
    <abstract>
      Some language exams have multiple writing tasks. When a learner writes
      multiple texts in a language exam, it is not surprising that the quality
      of these texts tends to be similar, and the existing automated text
      scoring (ATS) systems do not explicitly model this similarity. In this
      paper, we suggest that it could be useful to include the other texts
      written by this learner in the same exam as extra references in an ATS
      system. We propose various approaches of fusing information from multiple
      tasks and pass this authorship knowledge into our ATS model on six
      different datasets. We show that this can positively affect the model
      performance at a global level.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0536</url>
    <doi>10.18653/v1/W18-0536</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>zhang-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id="0537">
    <title>SB@GU at the Complex Word Identification 2018 Shared Task</title>
    <author>
      <first>David</first>
      <last>Alfter</last>
    </author>
    <author>
      <first>Ildikó</first>
      <last>Pilán</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>315–321</pages>
    <abstract>
      In this paper, we describe our experiments for the Shared Task on Complex
      Word Identification (CWI) 2018 (Yimam et al., 2018), hosted by the 13th
      Workshop on Innovative Use of NLP for Building Educational Applications
      (BEA) at NAACL 2018. Our system for English builds on previous work for
      Swedish concerning the classification of words into proficiency levels. We
      investigate different features for English and compare their usefulness
      using feature selection methods. For the German, Spanish and French data
      we use simple systems based on character n-gram models and show that
      sometimes simple models achieve comparable results to fully
      feature-engineered systems.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0537</url>
    <doi>10.18653/v1/W18-0537</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>alfter-piln:2018:W18-05</bibkey>
  </paper>
  <paper id="0538">
    <title>
      Complex Word Identification: Convolutional Neural Network vs. Feature
      Engineering
    </title>
    <author>
      <first>Segun Taofeek</first>
      <last>Aroyehun</last>
    </author>
    <author>
      <first>Jason</first>
      <last>Angel</last>
    </author>
    <author>
      <first>Daniel Alejandro</first>
      <last>Pérez Alvarez</last>
    </author>
    <author>
      <first>Alexander</first>
      <last>Gelbukh</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>322–327</pages>
    <abstract>
      We describe the systems of NLP-CIC team that participated in the Complex
      Word Identification (CWI) 2018 shared task. The shared task aimed to
      benchmark approaches for identifying complex words in English and other
      languages from the perspective of non-native speakers. Our goal is to
      compare two approaches: feature engineering and a deep neural network.
      Both approaches achieved comparable performance on the English test set.
      We demonstrated the flexibility of the deep-learning approach by using the
      same deep neural network setup in the Spanish track. Our systems achieved
      competitive results: all our systems were within 0.01 of the system with
      the best macro-F1 score on the test sets except on Wikipedia test set, on
      which our best system is 0.04 below the best macro-F1 score.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0538</url>
    <doi>10.18653/v1/W18-0538</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>aroyehun-EtAl:2018:W18-05</bibkey>
  </paper>
  <paper id="0539">
    <title>Deep Learning Architecture for Complex Word Identification</title>
    <author>
      <first>Dirk</first>
      <last>De Hertog</last>
    </author>
    <author>
      <first>Anaïs</first>
      <last>Tack</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>328–334</pages>
    <abstract>
      We describe a system for the CWI-task that includes information on 5
      aspects of the (complex) lexical item, namely distributional information
      of the item itself, morphological structure, psychological measures,
      corpus-counts and topical information. We constructed a deep learning
      architecture that combines those features and apply it to the
      probabilistic and binary classification task for all English sets and
      Spanish. We achieved reasonable performance on all sets with best
      performances seen on the probabilistic task, particularly on the English
      news set (MAE 0.054 and F1-score of 0.872). An analysis of the results
      shows that reasonable performance can be achieved with a single
      architecture without any domain-specific tweaking of the parameter
      settings and that distributional features capture almost all of the
      information also found in hand-crafted features.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0539</url>
    <doi>10.18653/v1/W18-0539</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>dehertog-tack:2018:W18-05</bibkey>
  </paper>
  <paper id="0540">
    <title>NILC at CWI 2018: Exploring Feature Engineering and Feature Learning</title>
    <author>
      <first>Nathan</first>
      <last>Hartmann</last>
    </author>
    <author>
      <first>Leandro Borges</first>
      <last>dos Santos</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>335–340</pages>
    <abstract>
      This paper describes the results of NILC team at CWI 2018. We developed
      solutions following three approaches: (i) a feature engineering method
      using lexical, n-gram and psycholinguistic features, (ii) a shallow neural
      network method using only word embeddings, and (iii) a Long Short-Term
      Memory (LSTM) language model, which is pre-trained on a large text corpus
      to produce a contextualized word vector. The feature engineering method
      obtained our best results for the classification task and the LSTM model
      achieved the best results for the probabilistic classification task. Our
      results show that deep neural networks are able to perform as well as
      traditional machine learning methods using manually engineered features
      for the task of complex word identification in English.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0540</url>
    <doi>10.18653/v1/W18-0540</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>hartmann-dossantos:2018:W18-05</bibkey>
  </paper>
  <paper id="0541">
    <title>Complex Word Identification Using Character n-grams</title>
    <author>
      <first>Maja</first>
      <last>Popović</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>341–348</pages>
    <abstract>
      This paper investigates the use of character n-gram frequencies for
      identifying complex words in English, German and Spanish texts. The
      approach is based on the assumption that complex words are likely to
      contain different character sequences than simple words. The multinomial
      Naive Bayes classifier was used with n-grams of different lengths as
      features, and the best results were obtained for the combination of
      2-grams and 4-grams. This variant was submitted to the Complex Word
      Identification Shared Task 2018 for all texts and achieved F-scores
      between 70% and 83%. The system was ranked in the middle range for all
      English texts, as third of fourteen submissions for German, and as tenth
      of seventeen submissions for Spanish. The method is not very convenient
      for the cross-language task, achieving only 59% on the French text.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0541</url>
    <doi>10.18653/v1/W18-0541</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>popovi:2018:W18-05</bibkey>
  </paper>
  <paper id="0542">
    <title>
      Predicting Second Language Learner Successes and Mistakes by Means of
      Conjunctive Features
    </title>
    <author>
      <first>Yves</first>
      <last>Bestgen</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>349–355</pages>
    <abstract>
      This paper describes the system developed by the Centre for English Corpus
      Linguistics for the 2018 Duolingo SLAM challenge. It aimed at predicting
      the successes and mistakes of second language learners on each of the
      words that compose the exercises they answered. Its main characteristic is
      to include conjunctive features, built by combining word ngrams with
      metadata about the user and the exercise. It achieved a relatively good
      performance, ranking fifth out of 15 systems. Complementary analyses
      carried out to gauge the contribution of the different sets of features to
      the performance confirmed the usefulness of the conjunctive features for
      the SLAM task.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0542</url>
    <doi>10.18653/v1/W18-0542</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>bestgen:2018:W18-05</bibkey>
  </paper>
  <paper id="0543">
    <title>Feature Engineering for Second Language Acquisition Modeling</title>
    <author>
      <first>Guanliang</first>
      <last>Chen</last>
    </author>
    <author>
      <first>Claudia</first>
      <last>Hauff</last>
    </author>
    <author>
      <first>Geert-Jan</first>
      <last>Houben</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>356–364</pages>
    <abstract>
      Knowledge tracing serves as a keystone in delivering personalized
      education. However, few works attempted to model students’ knowledge state
      in the setting of Second Language Acquisition. The Duolingo Shared Task on
      Second Language Acquisition Modeling provides students’ trace data that we
      extensively analyze and engineer features from for the task of predicting
      whether a student will correctly solve a vocabulary exercise. Our analyses
      of students’ learning traces reveal that factors like exercise format and
      engagement impact their exercise performance to a large extent. Overall,
      we extracted 23 different features as input to a Gradient Tree Boosting
      framework, which resulted in an AUC score of between 0.80 and 0.82 on the
      official test set.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0543</url>
    <doi>10.18653/v1/W18-0543</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>chen-hauff-houben:2018:W18-05</bibkey>
  </paper>
  <paper id="0544">
    <title>TMU System for SLAM-2018</title>
    <author>
      <first>Masahiro</first>
      <last>Kaneko</last>
    </author>
    <author>
      <first>Tomoyuki</first>
      <last>Kajiwara</last>
    </author>
    <author>
      <first>Mamoru</first>
      <last>Komachi</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>365–369</pages>
    <abstract>
      We introduce the TMU systems for the second language acquisition modeling
      shared task 2018 (Settles et al., 2018). To model learner error patterns,
      it is necessary to maintain a considerable amount of information regarding
      the type of exercises learners have been learning in the past and the
      manner in which they answered them. Tracking an enormous learner’s
      learning history and their correct and mistaken answers is essential to
      predict the learner’s future mistakes. Therefore, we propose a model which
      tracks the learner’s learning history efficiently. Our systems ranked
      fourth in the English and Spanish subtasks, and fifth in the French
      subtask.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0544</url>
    <doi>10.18653/v1/W18-0544</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>kaneko-kajiwara-komachi:2018:W18-05</bibkey>
  </paper>
  <paper id="0545">
    <title>Deep Factorization Machines for Knowledge Tracing</title>
    <author>
      <first>Jill-Jênn</first>
      <last>Vie</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>370–373</pages>
    <abstract>
      This paper introduces our solution to the 2018 Duolingo Shared Task on
      Second Language Acquisition Modeling (SLAM). We used deep factorization
      machines, a wide and deep learning model of pairwise relationships between
      users, items, skills, and other entities considered. Our solution (AUC
      0.815) hopefully managed to beat the logistic regression baseline (AUC
      0.774) but not the top performing model (AUC 0.861) and reveals
      interesting strategies to build upon item response theory models.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0545</url>
    <doi>10.18653/v1/W18-0545</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>vie:2018:W18-05</bibkey>
  </paper>
  <paper id="0546">
    <title>CLUF: a Neural Model for Second Language Acquisition Modeling</title>
    <author>
      <first>Shuyao</first>
      <last>Xu</last>
    </author>
    <author>
      <first>Jin</first>
      <last>Chen</last>
    </author>
    <author>
      <first>Long</first>
      <last>Qin</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>374–380</pages>
    <abstract>
      Second Language Acquisition Modeling is the task to predict whether a
      second language learner would respond correctly in future exercises based
      on their learning history. In this paper, we propose a neural network
      based system to utilize rich contextual, linguistic and user information.
      Our neural model consists of a Context encoder, a Linguistic feature
      encoder, a User information encoder and a Format information encoder
      (CLUF). Furthermore, a decoder is introduced to combine such encoded
      features and make final predictions. Our system ranked in first place in
      the English track and second place in the Spanish and French track with an
      AUROC score of 0.861, 0.835 and 0.854 respectively.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0546</url>
    <doi>10.18653/v1/W18-0546</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>xu-chen-qin:2018:W18-05</bibkey>
  </paper>
  <paper id="0547">
    <title>Neural sequence modelling for learner error prediction</title>
    <author>
      <first>Zheng</first>
      <last>Yuan</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>381–388</pages>
    <abstract>
      This paper describes our use of two recurrent neural network sequence
      models: sequence labelling and sequence-to-sequence models, for the
      prediction of future learner errors in our submission to the 2018 Duolingo
      Shared Task on Second Language Acquisition Modeling (SLAM). We show that
      these two models capture complementary information as combining them
      improves performance. Furthermore, the same network architecture and group
      of features can be used directly to build competitive prediction models in
      all three language tracks, demonstrating that our approach generalises
      well across languages.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0547</url>
    <doi>10.18653/v1/W18-0547</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>yuan:2018:W18-05</bibkey>
  </paper>
  <paper id="0548">
    <title>
      Automatic Distractor Suggestion for Multiple-Choice Tests Using Concept
      Embeddings and Information Retrieval
    </title>
    <author>
      <first>Le An</first>
      <last>Ha</last>
    </author>
    <author>
      <first>Victoria</first>
      <last>Yaneva</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>389–398</pages>
    <abstract>
      Developing plausible distractors (wrong answer options) when writing
      multiple-choice questions has been described as one of the most
      challenging and time-consuming parts of the item-writing process. In this
      paper we propose a fully automatic method for generating distractor
      suggestions for multiple-choice questions used in high-stakes medical
      exams. The system uses a question stem and the correct answer as an input
      and produces a list of suggested distractors ranked based on their
      similarity to the stem and the correct answer. To do this we use a novel
      approach of combining concept embeddings with information retrieval
      methods. We frame the evaluation as a prediction task where we aim to
      “predict” the human-produced distractors used in large sets of medical
      questions, i.e. if a distractor generated by our system is good enough it
      is likely to feature among the list of distractors produced by the human
      item-writers. The results reveal that combining concept embeddings with
      information retrieval approaches significantly improves the generation of
      plausible distractors and enables us to match around 1 in 5 of the
      human-produced distractors. The approach proposed in this paper is
      generalisable to all scenarios where the distractors refer to concepts.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0548</url>
    <doi>10.18653/v1/W18-0548</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>ha-yaneva:2018:W18-05</bibkey>
  </paper>
  <paper id="0549">
    <title>Co-Attention Based Neural Network for Source-Dependent Essay Scoring</title>
    <author>
      <first>Haoran</first>
      <last>Zhang</last>
    </author>
    <author>
      <first>Diane</first>
      <last>Litman</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>399–409</pages>
    <abstract>
      This paper presents an investigation of using a co-attention based neural
      network for source-dependent essay scoring. We use a co-attention
      mechanism to help the model learn the importance of each part of the essay
      more accurately. Also, this paper shows that the co-attention based neural
      network model provides reliable score prediction of source-dependent
      responses. We evaluate our model on two source-dependent response corpora.
      Results show that our model outperforms the baseline on both corpora. We
      also show that the attention of the model is similar to the expert
      opinions with examples.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0549</url>
    <revision id="2">W18-0549v2</revision>
    <doi>10.18653/v1/W18-0549</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>zhang-litman:2018:W18-05</bibkey>
  </paper>
  <paper id="0550">
    <title>Cross-Lingual Content Scoring</title>
    <author>
      <first>Andrea</first>
      <last>Horbach</last>
    </author>
    <author>
      <first>Sebastian</first>
      <last>Stennmanns</last>
    </author>
    <author>
      <first>Torsten</first>
      <last>Zesch</last>
    </author>
    <booktitle>
      Proceedings of the Thirteenth Workshop on Innovative Use of NLP for
      Building Educational Applications
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, Louisiana</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>410–419</pages>
    <abstract>
      We investigate the feasibility of cross-lingual content scoring, a
      scenario where training and test data in an automatic scoring task are
      from two different languages. Cross-lingual scoring can contribute to
      educational equality by allowing answers in multiple languages. Training a
      model in one language and applying it to another language might also help
      to overcome data sparsity issues by re-using trained models from other
      languages. As there is no suitable dataset available for this new task, we
      create a comparable bi-lingual corpus by extending the English ASAP
      dataset with German answers. Our experiments with cross-lingual scoring
      based on machine-translating either training or test data show a
      considerable drop in scoring quality.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0550</url>
    <doi>10.18653/v1/W18-0550</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>horbach-stennmanns-zesch:2018:W18-05</bibkey>
  </paper>
  <paper id="0600">
    <title>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </title>
    <editor>
      <first>Kate</first>
      <last>Loveys</last>
    </editor>
    <editor>
      <first>Kate</first>
      <last>Niederhoffer</last>
    </editor>
    <editor>
      <first>Emily</first>
      <last>Prud’hommeaux</last>
    </editor>
    <editor>
      <first>Rebecca</first>
      <last>Resnik</last>
    </editor>
    <editor>
      <first>Philip</first>
      <last>Resnik</last>
    </editor>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-06</url>
    <doi>10.18653/v1/W18-06</doi>
    <bibtype>book</bibtype>
    <bibkey>W18-06:2018</bibkey>
  </paper>
  <paper id="0601">
    <title>
      What type of happiness are you looking for? - A closer look at detecting
      mental health from language
    </title>
    <author>
      <first>Alina</first>
      <last>Arseniev-Koehler</last>
    </author>
    <author>
      <first>Sharon</first>
      <last>Mozgai</last>
    </author>
    <author>
      <first>Stefan</first>
      <last>Scherer</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–12</pages>
    <abstract>
      Computational models to detect mental illnesses from text and speech could
      enhance our understanding of mental health while offering opportunities
      for early detection and intervention. However, these models are often
      disconnected from the lived experience of depression and the larger
      diagnostic debates in mental health. This article investigates these
      disconnects, primarily focusing on the labels used to diagnose depression,
      how these labels are computationally represented, and the performance
      metrics used to evaluate computational models. We also consider how
      medical instruments used to measure depression, such as the Patient Health
      Questionnaire (PHQ), contribute to these disconnects. To illustrate our
      points, we incorporate mixed-methods analyses of 698 interviews on
      emotional health, which are coupled with self-report PHQ screens for
      depression. We propose possible strategies to bridge these gaps between
      modern psychiatric understandings of depression, lay experience of
      depression, and computational representation.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0601</url>
    <doi>10.18653/v1/W18-0601</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>arsenievkoehler-mozgai-scherer:2018:W18-06</bibkey>
  </paper>
  <paper id="0602">
    <title>A Linguistically-Informed Fusion Approach for Multimodal Depression Detection</title>
    <author>
      <first>Michelle</first>
      <last>Morales</last>
    </author>
    <author>
      <first>Stefan</first>
      <last>Scherer</last>
    </author>
    <author>
      <first>Rivka</first>
      <last>Levitan</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>13–24</pages>
    <abstract>
      Automated depression detection is inherently a multimodal problem.
      Therefore, it is critical that researchers investigate fusion techniques
      for multimodal design. This paper presents the first-ever comprehensive
      study of fusion techniques for depression detection. In addition, we
      present novel linguistically-motivated fusion techniques, which we find
      outperform existing approaches.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0602</url>
    <doi>10.18653/v1/W18-0602</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>morales-scherer-levitan:2018:W18-06</bibkey>
  </paper>
  <paper id="0603">
    <title>
      Expert, Crowdsourced, and Machine Assessment of Suicide Risk via Online
      Postings
    </title>
    <author>
      <first>Han-Chin</first>
      <last>Shing</last>
    </author>
    <author>
      <first>Suraj</first>
      <last>Nair</last>
    </author>
    <author>
      <first>Ayah</first>
      <last>Zirikly</last>
    </author>
    <author>
      <first>Meir</first>
      <last>Friedenberg</last>
    </author>
    <author>
      <first>Hal</first>
      <last>Daumé III</last>
    </author>
    <author>
      <first>Philip</first>
      <last>Resnik</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>25–36</pages>
    <abstract>
      We report on the creation of a dataset for studying assessment of suicide
      risk via online postings in Reddit. Evaluation of risk-level annotations
      by experts yields what is, to our knowledge, the first demonstration of
      reliability in risk assessment by clinicians based on social media
      postings. We also introduce and demonstrate the value of a new, detailed
      rubric for assessing suicide risk, compare crowdsourced with expert
      performance, and present baseline predictive modeling experiments using
      the new dataset, which will be made available to researchers through the
      American Association of Suicidology.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0603</url>
    <doi>10.18653/v1/W18-0603</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>shing-EtAl:2018:W18-06</bibkey>
  </paper>
  <paper id="0604">
    <title>
      CLPsych 2018 Shared Task: Predicting Current and Future Psychological
      Health from Childhood Essays
    </title>
    <author>
      <first>Veronica</first>
      <last>Lynn</last>
    </author>
    <author>
      <first>Alissa</first>
      <last>Goodman</last>
    </author>
    <author>
      <first>Kate</first>
      <last>Niederhoffer</last>
    </author>
    <author>
      <first>Kate</first>
      <last>Loveys</last>
    </author>
    <author>
      <first>Philip</first>
      <last>Resnik</last>
    </author>
    <author>
      <first>H. Andrew</first>
      <last>Schwartz</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>37–46</pages>
    <abstract>
      We describe the shared task for the CLPsych 2018 workshop, which focused
      on predicting current and future psychological health from an essay
      authored in childhood. Language-based predictions of a person’s current
      health have the potential to supplement traditional psychological
      assessment such as questionnaires, improving intake risk measurement and
      monitoring. Predictions of future psychological health can aid with both
      early detection and the development of preventative care. Research into
      the mental health trajectory of people, beginning from their childhood,
      has thus far been an area of little work within the NLP community. This
      shared task represents one of the first attempts to evaluate the use of
      early language to predict future health; this has the potential to support
      a wide variety of clinical health care tasks, from early assessment of
      lifetime risk for mental health problems, to optimal timing for targeted
      interventions aimed at both prevention and treatment.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0604</url>
    <doi>10.18653/v1/W18-0604</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>lynn-EtAl:2018:W18-06</bibkey>
  </paper>
  <paper id="0605">
    <title>
      An Approach to the CLPsych 2018 Shared Task Using Top-Down Text
      Representation and Simple Bottom-Up Model Selection
    </title>
    <author>
      <first>Micah</first>
      <last>Iserman</last>
    </author>
    <author>
      <first>Molly</first>
      <last>Ireland</last>
    </author>
    <author>
      <first>Andrew</first>
      <last>Littlefield</last>
    </author>
    <author>
      <first>Tyler</first>
      <last>Davis</last>
    </author>
    <author>
      <first>Sage</first>
      <last>Maliepaard</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>47–56</pages>
    <abstract>
      The Computational Linguistics and Clinical Psychology (CLPsych) 2018
      Shared Task asked teams to predict cross-sectional indices of anxiety and
      distress, and longitudinal indices of psychological distress from a
      subsample of the National Child Development Study, started in the United
      Kingdom in 1958. Teams aimed to predict mental health outcomes from essays
      written by 11-year-olds about what they believed their lives would be like
      at age 25. In the hopes of producing results that could be easily
      disseminated and applied, we used largely theory-based dictionaries to
      process the texts, and a simple data-driven approach to model selection.
      This approach yielded only modest results in terms of out-of-sample
      accuracy, but most of the category-level findings are interpretable and
      consistent with existing literature on psychological distress, anxiety,
      and depression.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0605</url>
    <doi>10.18653/v1/W18-0605</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>iserman-EtAl:2018:W18-06</bibkey>
  </paper>
  <paper id="0606">
    <title>
      Using contextual information for automatic triage of posts in a
      peer-support forum
    </title>
    <author>
      <first>Edgar</first>
      <last>Altszyler</last>
    </author>
    <author>
      <first>Ariel J.</first>
      <last>Berenstein</last>
    </author>
    <author>
      <first>David</first>
      <last>Milne</last>
    </author>
    <author>
      <first>Rafael A.</first>
      <last>Calvo</last>
    </author>
    <author>
      <first>Diego</first>
      <last>Fernandez Slezak</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>57–68</pages>
    <abstract>
      Mental health forums are online spaces where people can share their
      experiences anonymously and get peer support. These forums, require the
      supervision of moderators to provide support in delicate cases, such as
      posts expressing suicide ideation. The large increase in the number of
      forum users makes the task of the moderators unmanageable without the help
      of automatic triage systems. In the present paper, we present a Machine
      Learning approach for the triage of posts. Most approaches in the
      literature focus on the content of the posts, but only a few authors take
      advantage of features extracted from the context in which they appear. Our
      approach consists of the development and implementation of a large variety
      of new features from both, the content and the context of posts, such as
      previous messages, interaction with other users and author’s history. Our
      method has competed in the CLPsych 2017 Shared Task, obtaining the first
      place for several of the subtasks. Moreover, we also found that models
      that take advantage of post context improve significantly its performance
      in the detection of flagged posts (posts that require moderators
      attention), as well as those that focus on post content outperforms in the
      detection of most urgent events.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0606</url>
    <doi>10.18653/v1/W18-0606</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>altszyler-EtAl:2018:W18-06</bibkey>
  </paper>
  <paper id="0607">
    <title>
      Hierarchical neural model with attention mechanisms for the classification
      of social media text related to mental health
    </title>
    <author>
      <first>Julia</first>
      <last>Ive</last>
    </author>
    <author>
      <first>George</first>
      <last>Gkotsis</last>
    </author>
    <author>
      <first>Rina</first>
      <last>Dutta</last>
    </author>
    <author>
      <first>Robert</first>
      <last>Stewart</last>
    </author>
    <author>
      <first>Sumithra</first>
      <last>Velupillai</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>69–77</pages>
    <abstract>
      Mental health problems represent a major public health challenge.
      Automated analysis of text related to mental health is aimed to help
      medical decision-making, public health policies and to improve health
      care. Such analysis may involve text classification. Traditionally,
      automated classification has been performed mainly using machine learning
      methods involving costly feature engineering. Recently, the performance of
      those methods has been dramatically improved by neural methods. However,
      mainly Convolutional neural networks (CNNs) have been explored. In this
      paper, we apply a hierarchical Recurrent neural network (RNN) architecture
      with an attention mechanism on social media data related to mental health.
      We show that this architecture improves overall classification results as
      compared to previously reported results on the same data. Benefitting from
      the attention mechanism, it can also efficiently select text elements
      crucial for classification decisions, which can also be used for in-depth
      analysis.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0607</url>
    <doi>10.18653/v1/W18-0607</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>ive-EtAl:2018:W18-06</bibkey>
  </paper>
  <paper id="0608">
    <title>Cross-cultural differences in language markers of depression online</title>
    <author>
      <first>Kate</first>
      <last>Loveys</last>
    </author>
    <author>
      <first>Jonathan</first>
      <last>Torrez</last>
    </author>
    <author>
      <first>Alex</first>
      <last>Fine</last>
    </author>
    <author>
      <first>Glen</first>
      <last>Moriarty</last>
    </author>
    <author>
      <first>Glen</first>
      <last>Coppersmith</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>78–87</pages>
    <abstract>
      Depression is a global mental health condition that affects all cultures.
      Despite this, the way depression is expressed varies by culture. Uptake of
      machine learning technology for diagnosing mental health conditions means
      that increasingly more depression classifiers are created from online
      language data. Yet, culture is rarely considered as a factor affecting
      online language in this literature. This study explores cultural
      differences in online language data of users with depression. Written
      language data from 1,593 users with self-reported depression from the
      online peer support community 7 Cups of Tea was analyzed using the
      Linguistic Inquiry and Word Count (LIWC), topic modeling, data
      visualization, and other techniques. We compared the language of users
      identifying as White, Black or African American, Hispanic or Latino, and
      Asian or Pacific Islander. Exploratory analyses revealed cross-cultural
      differences in depression expression in online language data, particularly
      in relation to emotion expression, cognition, and functioning. The results
      have important implications for avoiding depression misclassification from
      machine-driven assessments when used in a clinical setting, and for
      avoiding inadvertent cultural biases in this line of research more
      broadly.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0608</url>
    <doi>10.18653/v1/W18-0608</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>loveys-EtAl:2018:W18-06</bibkey>
  </paper>
  <paper id="0609">
    <title>Deep Learning for Depression Detection of Twitter Users</title>
    <author>
      <first>Ahmed</first>
      <last>Husseini Orabi</last>
    </author>
    <author>
      <first>Prasadith</first>
      <last>Buddhitha</last>
    </author>
    <author>
      <first>Mahmoud</first>
      <last>Husseini Orabi</last>
    </author>
    <author>
      <first>Diana</first>
      <last>Inkpen</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>88–97</pages>
    <abstract>
      Mental illness detection in social media can be considered a complex task,
      mainly due to the complicated nature of mental disorders. In recent years,
      this research area has started to evolve with the continuous increase in
      popularity of social media platforms that became an integral part of
      people’s life. This close relationship between social media platforms and
      their users has made these platforms to reflect the users’ personal life
      with different limitations. In such an environment, researchers are
      presented with a wealth of information regarding one’s life. In addition
      to the level of complexity in identifying mental illnesses through social
      media platforms, adopting supervised machine learning approaches such as
      deep neural networks have not been widely accepted due to the difficulties
      in obtaining sufficient amounts of annotated training data. Due to these
      reasons, we try to identify the most effective deep neural network
      architecture among a few of selected architectures that were successfully
      used in natural language processing tasks. The chosen architectures are
      used to detect users with signs of mental illnesses (depression in our
      case) given limited unstructured text data extracted from the Twitter
      social media platform.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0609</url>
    <doi>10.18653/v1/W18-0609</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>husseiniorabi-EtAl:2018:W18-06</bibkey>
  </paper>
  <paper id="0610">
    <title>
      Current and Future Psychological Health Prediction using Language and
      Socio-Demographics of Children for the CLPysch 2018 Shared Task
    </title>
    <author>
      <first>Sharath Chandra</first>
      <last>Guntuku</last>
    </author>
    <author>
      <first>Salvatore</first>
      <last>Giorgi</last>
    </author>
    <author>
      <first>Lyle</first>
      <last>Ungar</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>98–106</pages>
    <abstract>
      This article is a system description and report on the submission of a
      team from the University of Pennsylvania in the ’CLPsych 2018’ shared
      task. The goal of the shared task was to use childhood language as a
      marker for both current and future psychological health over individual
      lifetimes. Our system employs multiple textual features derived from the
      essays written and individuals’ socio-demographic variables at the age of
      11. We considered several word clustering approaches, and explore the use
      of linear regression based on different feature sets. Our approach showed
      best results for predicting distress at the age of 42 and for predicting
      current anxiety on Disattenuated Pearson Correlation, and ranked fourth in
      the future health prediction task. In addition to the subtasks presented,
      we attempted to provide insight into mental health aspects at different
      ages. Our findings indicate that misspellings, words with illegible
      letters and increased use of personal pronouns are correlated with poor
      mental health at age 11, while descriptions about future physical
      activity, family and friends are correlated with good mental health.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0610</url>
    <doi>10.18653/v1/W18-0610</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>guntuku-giorgi-ungar:2018:W18-06</bibkey>
  </paper>
  <paper id="0611">
    <title>
      Predicting Psychological Health from Childhood Essays with Convolutional
      Neural Networks for the CLPsych 2018 Shared Task (Team UKNLP)
    </title>
    <author>
      <first>Anthony</first>
      <last>Rios</last>
    </author>
    <author>
      <first>Tung</first>
      <last>Tran</last>
    </author>
    <author>
      <first>Ramakanth</first>
      <last>Kavuluru</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>107–112</pages>
    <abstract>
      This paper describes the systems we developed for tasks A and B of the
      2018 CLPsych shared task. The first task (task A) focuses on predicting
      behavioral health scores at age 11 using childhood essays. The second task
      (task B) asks participants to predict future psychological distress at
      ages 23, 33, 42, and 50 using the age 11 essays. We propose two
      convolutional neural network based methods that map each task to a
      regression problem. Among seven teams we ranked third on task A with
      disattenuated Pearson correlation (DPC) score of 0.5587. Likewise, we
      ranked third on task B with an average DPC score of 0.3062.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0611</url>
    <doi>10.18653/v1/W18-0611</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>rios-tran-kavuluru:2018:W18-06</bibkey>
  </paper>
  <paper id="0612">
    <title>A Psychologically Informed Approach to CLPsych Shared Task 2018</title>
    <author>
      <first>Almog</first>
      <last>Simchon</last>
    </author>
    <author>
      <first>Michael</first>
      <last>Gilead</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>113–118</pages>
    <abstract>
      This paper describes our approach to the CLPsych 2018 Shared Task, in
      which we attempted to predict cross-sectional psychological health at age
      11 and future psychological distress based on childhood essays. We
      attempted several modeling approaches and observed best cross-validated
      prediction accuracy with relatively simple models based on psychological
      theory. The models provided reasonable predictions in most outcomes.
      Notably, our model was especially successful in predicting out-of-sample
      psychological distress (across people and across time) at age 50.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0612</url>
    <doi>10.18653/v1/W18-0612</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>simchon-gilead:2018:W18-06</bibkey>
  </paper>
  <paper id="0613">
    <title>
      Predicting Psychological Health from Childhood Essays. The UGent-IDLab
      CLPsych 2018 Shared Task System.
    </title>
    <author>
      <first>Klim</first>
      <last>Zaporojets</last>
    </author>
    <author>
      <first>Lucas</first>
      <last>Sterckx</last>
    </author>
    <author>
      <first>Johannes</first>
      <last>Deleu</last>
    </author>
    <author>
      <first>Thomas</first>
      <last>Demeester</last>
    </author>
    <author>
      <first>Chris</first>
      <last>Develder</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>119–125</pages>
    <abstract>
      This paper describes the IDLab system submitted to Task A of the CLPsych
      2018 shared task. The goal of this task is predicting psychological health
      of children based on language used in hand-written essays and
      socio-demographic control variables. Our entry uses word- and
      character-based features as well as lexicon-based features and features
      derived from the essays such as the quality of the language. We apply
      linear models, gradient boosting as well as neural-network based
      regressors (feed-forward, CNNs and RNNs) to predict scores. We then make
      ensembles of our best performing models using a weighted average.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0613</url>
    <doi>10.18653/v1/W18-0613</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>zaporojets-EtAl:2018:W18-06</bibkey>
  </paper>
  <paper id="0614">
    <title>
      Can adult mental health be predicted by childhood future-self narratives?
      Insights from the CLPsych 2018 Shared Task
    </title>
    <author>
      <first>Kylie</first>
      <last>Radford</last>
    </author>
    <author>
      <first>Louise</first>
      <last>Lavrencic</last>
    </author>
    <author>
      <first>Ruth</first>
      <last>Peters</last>
    </author>
    <author>
      <first>Kim</first>
      <last>Kiely</last>
    </author>
    <author>
      <first>Ben</first>
      <last>Hachey</last>
    </author>
    <author>
      <first>Scott</first>
      <last>Nowson</last>
    </author>
    <author>
      <first>Will</first>
      <last>Radford</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>126–135</pages>
    <abstract>
      The CLPsych 2018 Shared Task B explores how childhood essays can predict
      psychological distress throughout the author’s life. Our main aim was to
      build tools to help our psychologists understand the data, propose
      features and interpret predictions. We submitted two linear regression
      models: ModelA uses simple demographic and word- count features, while
      ModelB uses linguistic, entity, typographic, expert-gazetteer, and
      readability features. Our models perform best at younger prediction ages,
      with our best unofficial score at 23 of 0.426 disattenuated Pearson
      correlation. This task is challenging and although predictive performance
      is limited, we propose that tight integration of expertise across
      computational linguistics and clinical psychology is a productive
      direction.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0614</url>
    <doi>10.18653/v1/W18-0614</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>radford-EtAl:2018:W18-06</bibkey>
  </paper>
  <paper id="0615">
    <title>Automatic Detection of Incoherent Speech for Diagnosing Schizophrenia</title>
    <author>
      <first>Dan</first>
      <last>Iter</last>
    </author>
    <author>
      <first>Jong</first>
      <last>Yoon</last>
    </author>
    <author>
      <first>Dan</first>
      <last>Jurafsky</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>136–146</pages>
    <abstract> 

Schizophrenia is a mental disorder which afflicts an estimated 0.7% of adults world wide. It affects many areas of mental function, often evident from incoherent speech. Diagnosing schizophrenia relies on subjective judgments resulting in disagreements even among trained clinicians. Recent studies have proposed the use of natural language processing for diagnosis by drawing on automatically-extracted linguistic features like discourse coherence and lexicon. Here, we present the first benchmark comparison of previously proposed coherence models for detecting symptoms of schizophrenia and evaluate their performance on a new dataset of recorded interviews between subjects and clinicians. We also present two alternative coherence metrics based on modern sentence embedding techniques that outperform the previous methods on our dataset. Lastly, we propose a novel computational model for reference incoherence based on ambiguous pronoun usage and show that it is a highly predictive feature on our data. While the number of subjects is limited in this pilot study, our results suggest new directions for diagnosing common symptoms of schizophrenia. </abstract>
    <url>http://www.aclweb.org/anthology/W18-0615</url>
    <doi>10.18653/v1/W18-0615</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>iter-yoon-jurafsky:2018:W18-06</bibkey>
  </paper>
  <paper id="0616">
    <title>
      Oral-Motor and Lexical Diversity During Naturalistic Conversations in
      Adults with Autism Spectrum Disorder
    </title>
    <author>
      <first>Julia</first>
      <last>Parish-Morris</last>
    </author>
    <author>
      <first>Evangelos</first>
      <last>Sariyanidi</last>
    </author>
    <author>
      <first>Casey</first>
      <last>Zampella</last>
    </author>
    <author>
      <first>G. Keith</first>
      <last>Bartley</last>
    </author>
    <author>
      <first>Emily</first>
      <last>Ferguson</last>
    </author>
    <author>
      <first>Ashley A.</first>
      <last>Pallathra</last>
    </author>
    <author>
      <first>Leila</first>
      <last>Bateman</last>
    </author>
    <author>
      <first>Samantha</first>
      <last>Plate</last>
    </author>
    <author>
      <first>Meredith</first>
      <last>Cola</last>
    </author>
    <author>
      <first>Juhi</first>
      <last>Pandey</last>
    </author>
    <author>
      <first>Edward S.</first>
      <last>Brodkin</last>
    </author>
    <author>
      <first>Robert T.</first>
      <last>Schultz</last>
    </author>
    <author>
      <first>Birkan</first>
      <last>Tunc</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>147–157</pages>
    <abstract>
      Autism spectrum disorder (ASD) is a neurodevelopmental condition
      characterized by impaired social communication and the presence of
      restricted, repetitive patterns of behaviors and interests. Prior research
      suggests that restricted patterns of behavior in ASD may be cross-domain
      phenomena that are evident in a variety of modalities. Computational
      studies of language in ASD provide support for the existence of an
      underlying dimension of restriction that emerges during a conversation.
      Similar evidence exists for restricted patterns of facial movement. Using
      tools from computational linguistics, computer vision, and information
      theory, this study tests whether cognitive-motor restriction can be
      detected across multiple behavioral domains in adults with ASD during a
      naturalistic conversation. Our methods identify restricted behavioral
      patterns, as measured by entropy in word use and mouth movement. Results
      suggest that adults with ASD produce significantly less diverse mouth
      movements and words than neurotypical adults, with an increased reliance
      on repeated patterns in both domains. The diversity values of the two
      domains are not significantly correlated, suggesting that they provide
      complementary information.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0616</url>
    <doi>10.18653/v1/W18-0616</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>parishmorris-EtAl:2018:W18-06</bibkey>
  </paper>
  <paper id="0617">
    <title>Dynamics of an idiostyle of a Russian suicidal blogger</title>
    <author>
      <first>Tatiana</first>
      <last>Litvinova</last>
    </author>
    <author>
      <first>Olga</first>
      <last>Litvinova</last>
    </author>
    <author>
      <first>Pavel</first>
      <last>Seredin</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>158–167</pages>
    <abstract>
      Over 800000 people die of suicide each year. It is es-timated that by the
      year 2020, this figure will have in-creased to 1.5 million. It is
      considered to be one of the major causes of mortality during adolescence.
      Thus there is a growing need for methods of identifying su-icidal
      individuals. Language analysis is known to be a valuable psychodiagnostic
      tool, however the material for such an analysis is not easy to obtain.
      Currently as the Internet communications are developing, there is an
      opportunity to study texts of suicidal individuals. Such an analysis can
      provide a useful insight into the peculiarities of suicidal thinking,
      which can be used to further develop methods for diagnosing the risk of
      suicidal behavior. The paper analyzes the dynamics of a number of
      linguistic parameters of an idiostyle of a Russian-language blogger who
      died by suicide. For the first time such an analysis has been conducted
      using the material of Russian online texts. For text processing, the LIWC
      program is used. A correlation analysis was performed to identify the
      relationship between LIWC variables and number of days prior to suicide.
      Data visualization, as well as comparison with the results of related
      studies was performed.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0617</url>
    <doi>10.18653/v1/W18-0617</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>litvinova-litvinova-seredin:2018:W18-06</bibkey>
  </paper>
  <paper id="0618">
    <title>RSDD-Time: Temporal Annotation of Self-Reported Mental Health Diagnoses</title>
    <author>
      <first>Sean</first>
      <last>MacAvaney</last>
    </author>
    <author>
      <first>Bart</first>
      <last>Desmet</last>
    </author>
    <author>
      <first>Arman</first>
      <last>Cohan</last>
    </author>
    <author>
      <first>Luca</first>
      <last>Soldaini</last>
    </author>
    <author>
      <first>Andrew</first>
      <last>Yates</last>
    </author>
    <author>
      <first>Ayah</first>
      <last>Zirikly</last>
    </author>
    <author>
      <first>Nazli</first>
      <last>Goharian</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>168–173</pages>
    <abstract>
      Self-reported diagnosis statements have been widely employed in studying
      language related to mental health in social media. However, existing
      research has largely ignored the temporality of mental health diagnoses.
      In this work, we introduce RSDD-Time: a new dataset of 598 manually
      annotated self-reported depression diagnosis posts from Reddit that
      include temporal information about the diagnosis. Annotations include
      whether a mental health con- dition is present and how recently the
      diagnosis happened. Furthermore, we include exact temporal spans that
      relate to the date of diagnosis. This information is valuable for various
      computational methods to examine mental health through social media
      because one’s mental health state is not static. We also test several
      baseline classification and extraction approaches, which suggest that
      extracting temporal information from self-reported diagnosis statements is
      challenging.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0618</url>
    <doi>10.18653/v1/W18-0618</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>macavaney-EtAl:2018:W18-06</bibkey>
  </paper>
  <paper id="0619">
    <title>Predicting Human Trustfulness from Facebook Language</title>
    <author>
      <first>Mohammadzaman</first>
      <last>Zamani</last>
    </author>
    <author>
      <first>Anneke</first>
      <last>Buffone</last>
    </author>
    <author>
      <first>H. Andrew</first>
      <last>Schwartz</last>
    </author>
    <booktitle>
      Proceedings of the Fifth Workshop on Computational Linguistics and
      Clinical Psychology: From Keyboard to Clinic
    </booktitle>
    <month>June</month>
    <year>2018</year>
    <address>New Orleans, LA</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>174–181</pages>
    <abstract>
      Trustfulness — one’s general tendency to have confidence in unknown people
      or situations — predicts many important real-world outcomes such as mental
      health and likelihood to cooperate with others such as clinicians. While
      data-driven measures of interpersonal trust have previously been
      introduced, here, we develop the first language-based assessment of the
      personality trait of trustfulness by fitting one’s language to an accepted
      questionnaire-based trust score. Further, using trustfulness as a type of
      case study, we explore the role of questionnaire size as well as word
      count in developing language-based predictive models of users’
      psychological traits. We find that leveraging a longer questionnaire can
      yield greater test set accuracy, while, for training, we find it
      beneficial to include users who took smaller questionnaires which offers
      more observations for training. Similarly, after noting a decrease in
      individual prediction error as word count increased, we found a word
      count-weighted training scheme was helpful when there were very few users
      in the first place.
    </abstract>
    <url>http://www.aclweb.org/anthology/W18-0619</url>
    <doi>10.18653/v1/W18-0619</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>zamani-buffone-schwartz:2018:W18-06</bibkey>
  </paper>
   <paper id="0620">
     <title>
       Within and Between-Person Differences in Language Used Across Anxiety
       Support and Neutral Reddit Communities
     </title>
     <author>
       <first>Molly</first>
       <last>Ireland</last>
     </author>
     <author>
       <first>Micah</first>
       <last>Iserman</last>
     </author>
     <booktitle>
       Proceedings of the Fifth Workshop on Computational Linguistics and
       Clinical Psychology: From Keyboard to Clinic
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, LA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>182–193</pages>
     <abstract>
       Although many studies have distinguished between the social media language
       use of people who do and do not have a mental health condition,
       within-person context-sensitive comparisons (for example, analyzing
       individuals’ language use when seeking support or discussing neutral
       topics) are less common. Two dictionary-based analyses of Reddit
       communities compared (1) anxious individuals’ comments in anxiety support
       communities (e.g., /r/PanicParty) with the same users’ comments in neutral
       communities (e.g., /r/todayilearned), and, (2) within popular neutral
       communities, comments by members of anxiety subreddits with comments by
       other users. Each comparison yielded theory-consistent effects as well as
       unexpected results that suggest novel hypotheses to be tested in the
       future. Results have relevance for improving researchers’ and
       practitioners’ ability to unobtrusively assess anxiety symptoms in
       conversations that are not explicitly about mental health.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0620</url>
     <doi>10.18653/v1/W18-0620</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>ireland-iserman:2018:W18-06</bibkey>
   </paper>
   <paper id="0621">
     <title>
       Helping or Hurting? Predicting Changes in Users’ Risk of Self-Harm Through
       Online Community Interactions
     </title>
     <author>
       <first>Luca</first>
       <last>Soldaini</last>
     </author>
     <author>
       <first>Timothy</first>
       <last>Walsh</last>
     </author>
     <author>
       <first>Arman</first>
       <last>Cohan</last>
     </author>
     <author>
       <first>Julien</first>
       <last>Han</last>
     </author>
     <author>
       <first>Nazli</first>
       <last>Goharian</last>
     </author>
     <booktitle>
       Proceedings of the Fifth Workshop on Computational Linguistics and
       Clinical Psychology: From Keyboard to Clinic
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, LA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>194–203</pages>
     <abstract>
       In recent years, online communities have formed around suicide and
       self-harm prevention. While these communities offer support in moment of
       crisis, they can also normalize harmful behavior, discourage professional
       treatment, and instigate suicidal ideation. In this work, we focus on how
       interaction with others in such a community affects the mental state of
       users who are seeking support. We first build a dataset of conversation
       threads between users in a distressed state and community members offering
       support. We then show how to construct a classifier to predict whether
       distressed users are helped or harmed by the interactions in the thread,
       and we achieve a macro-F1 score of up to 0.69.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0621</url>
     <doi>10.18653/v1/W18-0621</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>soldaini-EtAl:2018:W18-06</bibkey>
   </paper>
   <paper id="0700">
     <title>
       Proceedings of the First Workshop on Computational Models of Reference,
       Anaphora and Coreference
     </title>
     <editor>
       <first>Massimo</first>
       <last>Poesio</last>
     </editor>
     <editor>
       <first>Vincent</first>
       <last>Ng</last>
     </editor>
     <editor>
       <first>Maciej</first>
       <last>Ogrodniczuk</last>
     </editor>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-07</url>
     <doi>10.18653/v1/W18-07</doi>
     <bibtype>book</bibtype>
     <bibkey>W18-07:2018</bibkey>
   </paper>
   <paper id="0701">
     <title>Anaphora Resolution for Twitter Conversations: An Exploratory Study</title>
     <author>
       <first>Berfin</first>
       <last>Aktaş</last>
     </author>
     <author>
       <first>Tatjana</first>
       <last>Scheffler</last>
     </author>
     <author>
       <first>Manfred</first>
       <last>Stede</last>
     </author>
     <booktitle>
       Proceedings of the First Workshop on Computational Models of Reference,
       Anaphora and Coreference
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–10</pages>
     <abstract>
       We present a corpus study of pronominal anaphora on Twitter conversations.
       After outlining the specific features of this genre, with respect to
       reference resolution, we explain the construction of our corpus and the
       annotation steps. From this we derive a list of phenomena that need to be
       considered when performing anaphora resolution on this type of data.
       Finally, we test the performance of an off-the-shelf resolution system,
       and provide some qualitative error analysis.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0701</url>
     <doi>10.18653/v1/W18-0701</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>akta-scheffler-stede:2018:W18-07</bibkey>
   </paper>
   <paper id="0702">
     <title>Anaphora Resolution with the ARRAU Corpus</title>
     <author>
       <first>Massimo</first>
       <last>Poesio</last>
     </author>
     <author>
       <first>Yulia</first>
       <last>Grishina</last>
     </author>
     <author>
       <first>Varada</first>
       <last>Kolhatkar</last>
     </author>
     <author>
       <first>Nafise</first>
       <last>Moosavi</last>
     </author>
     <author>
       <first>Ina</first>
       <last>Roesiger</last>
     </author>
     <author>
       <first>Adam</first>
       <last>Roussel</last>
     </author>
     <author>
       <first>Fabian</first>
       <last>Simonjetz</last>
     </author>
     <author>
       <first>Alexandra</first>
       <last>Uma</last>
     </author>
     <author>
       <first>Olga</first>
       <last>Uryupina</last>
     </author>
     <author>
       <first>Juntao</first>
       <last>Yu</last>
     </author>
     <author>
       <first>Heike</first>
       <last>Zinsmeister</last>
     </author>
     <booktitle>
       Proceedings of the First Workshop on Computational Models of Reference,
       Anaphora and Coreference
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>11–22</pages>
     <abstract>
       The ARRAU corpus is an anaphorically annotated corpus of English providing
       rich linguistic information about anaphora resolution. The most
       distinctive feature of the corpus is the annotation of a wide range of
       anaphoric relations, including bridging references and discourse deixis in
       addition to identity (coreference). Other distinctive features include
       treating all NPs as markables, including non-referring NPs; and the
       annotation of a variety of morphosyntactic and semantic mention and entity
       attributes, including the genericity status of the entities referred to by
       markables. The corpus however has not been extensively used for anaphora
       resolution research so far. In this paper, we discuss three datasets
       extracted from the ARRAU corpus to support the three subtasks of the CRAC
       2018 Shared Task–identity anaphora resolution over ARRAU-style markables,
       bridging references resolution, and discourse deixis; the evaluation
       scripts assessing system performance on those datasets; and preliminary
       results on these three tasks that may serve as baseline for subsequent
       research in these phenomena.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0702</url>
     <doi>10.18653/v1/W18-0702</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>poesio-EtAl:2018:W18-07</bibkey>
   </paper>
   <paper id="0703">
     <title>Rule- and Learning-based Methods for Bridging Resolution in the ARRAU Corpus</title>
     <author>
       <first>Ina</first>
       <last>Roesiger</last>
     </author>
     <booktitle>
       Proceedings of the First Workshop on Computational Models of Reference,
       Anaphora and Coreference
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>23–33</pages>
     <abstract>
       We present two systems for bridging resolution, which we submitted to the
       CRAC shared task on bridging anaphora resolution in the ARRAU corpus
       (track 2): a rule-based approach following Hou et al. 2014 and a
       learning-based approach. The re-implementation of Hou et al. 2014 achieves
       very poor performance when being applied to ARRAU. We found that the
       reasons for this lie in the different bridging annotations: whereas the
       rule-based system suggests many referential bridging pairs, ARRAU contains
       mostly lexical bridging. We describe the differences between these two
       types of bridging and adapt the rule-based approach to be able to handle
       lexical bridging. The modified rule-based approach achieves reasonable
       performance on all (sub)-tasks and outperforms a simple learning-based
       approach.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0703</url>
     <doi>10.18653/v1/W18-0703</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>roesiger:2018:W18-07</bibkey>
   </paper>
   <paper id="0704">
     <title>A Predictive Model for Notional Anaphora in English</title>
     <author>
       <first>Amir</first>
       <last>Zeldes</last>
     </author>
     <booktitle>
       Proceedings of the First Workshop on Computational Models of Reference,
       Anaphora and Coreference
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>34–43</pages>
     <abstract> 

Notional anaphors are pronouns which disagree with their antecedents’ grammatical categories for notional reasons, such as plural to singular agreement in: “the government ... they”. Since such cases are rare and conflict with evidence from strictly agreeing cases (“the government ... it”), they present a substantial challenge to both coreference resolution and referring expression generation. Using the OntoNotes corpus, this paper takes an ensemble approach to predicting English notional anaphora in context on the basis of the largest empirical data to date. In addition to state of the art prediction accuracy, the results suggest that theoretical approaches positing a plural construal at the antecedent’s utterance are insufficient, and that circumstances at the anaphor’s utterance location, as well as global factors such as genre, have a strong effect on the choice of referring expression. </abstract>
     <url>http://www.aclweb.org/anthology/W18-0704</url>
     <doi>10.18653/v1/W18-0704</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>zeldes:2018:W18-07</bibkey>
   </paper>
   <paper id="0705">
     <title>
       Integrating Predictions from Neural-Network Relation Classifiers into
       Coreference and Bridging Resolution
     </title>
     <author>
       <first>Ina</first>
       <last>Roesiger</last>
     </author>
     <author>
       <first>Maximilian</first>
       <last>Köper</last>
     </author>
     <author>
       <first>Kim Anh</first>
       <last>Nguyen</last>
     </author>
     <author>
       <first>Sabine</first>
       <last>Schulte im Walde</last>
     </author>
     <booktitle>
       Proceedings of the First Workshop on Computational Models of Reference,
       Anaphora and Coreference
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>44–49</pages>
     <abstract>
       Cases of coreference and bridging resolution often require knowledge about
       semantic relations between anaphors and antecedents. We suggest
       state-of-the-art neural-network classifiers trained on relation benchmarks
       to predict and integrate likelihoods for relations. Two experiments with
       representations differing in noise and complexity improve our bridging but
       not our coreference resolver.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0705</url>
     <doi>10.18653/v1/W18-0705</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>roesiger-EtAl:2018:W18-07</bibkey>
   </paper>
   <paper id="0706">
     <title>Towards Bridging Resolution in German: Data Analysis and Rule-based Experiments</title>
     <author>
       <first>Janis</first>
       <last>Pagel</last>
     </author>
     <author>
       <first>Ina</first>
       <last>Roesiger</last>
     </author>
     <booktitle>
       Proceedings of the First Workshop on Computational Models of Reference,
       Anaphora and Coreference
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>50–60</pages>
     <abstract>
       Bridging resolution is the task of recognising bridging anaphors and
       linking them to their antecedents. While there is some work on bridging
       resolution for English, there is only little work for German. We present
       two datasets which contain bridging annotations, namely DIRNDL and GRAIN,
       and compare the performance of a rule-based system with a simple baseline
       approach on these two corpora. The performance for full bridging
       resolution ranges between an F1 score of 13.6% for DIRNDL and 11.8% for
       GRAIN. An analysis using oracle lists suggests that the system could, to a
       certain extent, benefit from ranking and re-ranking antecedent candidates.
       Furthermore, we investigate the importance of single features and show
       that the features used in our work seem promising for future bridging
       resolution approaches.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0706</url>
     <doi>10.18653/v1/W18-0706</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>pagel-roesiger:2018:W18-07</bibkey>
   </paper>
   <paper id="0707">
     <title>Detecting and Resolving Shell Nouns in German</title>
     <author>
       <first>Adam</first>
       <last>Roussel</last>
     </author>
     <booktitle>
       Proceedings of the First Workshop on Computational Models of Reference,
       Anaphora and Coreference
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>61–67</pages>
     <abstract> 

This paper describes the design and evaluation of a system for the automatic detection and resolution of shell nouns in German. Shell nouns are general nouns, such as fact, question, or problem, whose full interpretation relies on a content phrase located elsewhere in a text, which these nouns simultaneously serve to characterize and encapsulate. To accomplish this, the system uses a series of lexico-syntactic patterns in order to extract shell noun candidates and their content in parallel. Each pattern has its own classifier, which makes the final decision as to whether or not a link is to be established and the shell noun resolved. Overall, about 26.2% of the annotated shell noun instances were correctly identified by the system, and of these cases, about 72.5% are assigned the correct content phrase. Though it remains difficult to identify shell noun instances reliably (recall is accordingly low in this regard), this system usually assigns the right content to correctly classified cases. cases. </abstract>
     <url>http://www.aclweb.org/anthology/W18-0707</url>
     <doi>10.18653/v1/W18-0707</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>roussel:2018:W18-07</bibkey>
   </paper>
   <paper id="0708">
     <title>PAWS: A Multi-lingual Parallel Treebank with Anaphoric Relations</title>
     <author>
       <first>Anna</first>
       <last>Nedoluzhko</last>
     </author>
     <author>
       <first>Michal</first>
       <last>Novák</last>
     </author>
     <author>
       <first>Maciej</first>
       <last>Ogrodniczuk</last>
     </author>
     <booktitle>
       Proceedings of the First Workshop on Computational Models of Reference,
       Anaphora and Coreference
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>68–76</pages>
     <abstract>
       We present PAWS, a multi-lingual parallel treebank with coreference
       annotation. It consists of English texts from the Wall Street Journal
       translated into Czech, Russian and Polish. In addition, the texts are
       syntactically parsed and word-aligned. PAWS is based on PCEDT 2.0 and
       continues the tradition of multilingual treebanks with coreference
       annotation. The paper focuses on the coreference annotation in PAWS and
       its language-specific differences. PAWS offers linguistic material that
       can be further leveraged in cross-lingual studies, especially on
       coreference.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0708</url>
     <doi>10.18653/v1/W18-0708</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>nedoluzhko-novk-ogrodniczuk:2018:W18-07</bibkey>
   </paper>
   <paper id="0709">
     <title>A Fine-grained Large-scale Analysis of Coreference Projection</title>
     <author>
       <first>Michal</first>
       <last>Novák</last>
     </author>
     <booktitle>
       Proceedings of the First Workshop on Computational Models of Reference,
       Anaphora and Coreference
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>77–86</pages>
     <abstract>
       We perform a fine-grained large-scale analysis of coreference projection.
       By projecting gold coreference from Czech to English and vice versa on
       Prague Czech-English Dependency Treebank 2.0 Coref, we set an upper bound
       of a proposed projection approach for these two languages. We undertake a
       detailed thorough analysis that combines the analysis of projection’s
       subtasks with analysis of performance on individual mention types. The
       findings are accompanied with examples from the corpus.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0709</url>
     <doi>10.18653/v1/W18-0709</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>novk:2018:W18-07</bibkey>
   </paper>
   <paper id="0710">
     <title>
       Modeling Brain Activity Associated with Pronoun Resolution in English and
       Chinese
     </title>
     <author>
       <first>Jixing</first>
       <last>Li</last>
     </author>
     <author>
       <first>Murielle</first>
       <last>Fabre</last>
     </author>
     <author>
       <first>Wen-Ming</first>
       <last>Luh</last>
     </author>
     <author>
       <first>John</first>
       <last>Hale</last>
     </author>
     <booktitle>
       Proceedings of the First Workshop on Computational Models of Reference,
       Anaphora and Coreference
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>87–96</pages>
     <abstract>
       Typological differences between English and Chinese suggest stronger
       reliance on salience of the antecedent during pronoun resolution in
       Chinese. We examined this hypothesis by correlating a difficulty measure
       of pronoun resolution derived by the activation-based ACT-R model with the
       brain activity of English and Chinese participants listening to a same
       audiobook during fMRI recording. The ACT-R model predicts higher overall
       difficulty for English speakers, which is supported at the brain level in
       left Broca’s area. More generally, it confirms that computational modeling
       approach is able to dissociate different dimensions that are involved in
       the complex process of pronoun resolution in the brain.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0710</url>
     <doi>10.18653/v1/W18-0710</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>li-EtAl:2018:W18-07</bibkey>
   </paper>
   <paper id="0711">
     <title>
       Event versus entity co-reference: Effects of context and form of referring
       expression
     </title>
     <author>
       <first>Sharid</first>
       <last>Loáiciga</last>
     </author>
     <author>
       <first>Luca</first>
       <last>Bevacqua</last>
     </author>
     <author>
       <first>Hannah</first>
       <last>Rohde</last>
     </author>
     <author>
       <first>Christian</first>
       <last>Hardmeier</last>
     </author>
     <booktitle>
       Proceedings of the First Workshop on Computational Models of Reference,
       Anaphora and Coreference
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>97–103</pages>
     <abstract>
       Anaphora resolution systems require both an enumeration of
       possible candidate antecedents and an identification process of
       the antecedent.  This paper focuses on (i) the impact of the
       form of referring expression on entity-vs-event preferences and
       (ii) how properties of the passage interact with referential
       form. Two crowd-sourced story-continuation experiments were
       conducted, using constructed and naturally-occurring passages,
       to see how participants interpret <i>It</i> and <i>This</i>
       pronouns following a context sentence that makes available
       event and entity referents. Our participants show a strong, but
       not categorical, bias to use <i>This</i> to refer to events and
       <i>It</i> to refer to entities. However, these preferences vary
       with passage characteristics such as verb class (a proxy in our
       constructed examples for the number of explicit and implicit
       entities) and more subtle author intentions regarding
       subsequent re-mention (the original event-vs-entity re-mention
       of our corpus items).
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0711</url>
     <doi>10.18653/v1/W18-0711</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>loiciga-EtAl:2018:W18-07</bibkey>
   </paper>
   <paper id="0800">
     <title>Proceedings of the Second ACL Workshop on Ethics in Natural Language Processing</title>
     <editor>
       <first>Mark</first>
       <last>Alfano</last>
     </editor>
     <editor>
       <first>Dirk</first>
       <last>Hovy</last>
     </editor>
     <editor>
       <first>Margaret</first>
       <last>Mitchell</last>
     </editor>
     <editor>
       <first>Michael</first>
       <last>Strube</last>
     </editor>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-08</url>
     <doi>10.18653/v1/W18-08</doi>
     <bibtype>book</bibtype>
     <bibkey>W18-08:2018</bibkey>
   </paper>
   <paper id="0801">
     <title>
       On the Utility of Lay Summaries and AI Safety Disclosures: Toward Robust,
       Open Research Oversight
     </title>
     <author>
       <first>Allen</first>
       <last>Schmaltz</last>
     </author>
     <booktitle>Proceedings of the Second ACL Workshop on Ethics in Natural Language Processing</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–6</pages>
     <abstract>
       In this position paper, we propose that the community consider encouraging
       researchers to include two riders, a ”Lay Summary” and an ”AI Safety
       Disclosure”, as part of future NLP papers published in ACL forums that
       present user-facing systems. The goal is to encourage researchers–via a
       relatively non-intrusive mechanism–to consider the societal implications
       of technologies carrying (un)known and/or (un)knowable long-term risks, to
       highlight failure cases, and to provide a mechanism by which the general
       public (and scientists in other disciplines) can more readily engage in
       the discussion in an informed manner. This simple proposal requires
       minimal additional up-front costs for researchers; the lay summary, at
       least, has significant precedence in the medical literature and other
       areas of science; and the proposal is aimed to supplement, rather than
       replace, existing approaches for encouraging researchers to consider the
       ethical implications of their work, such as those of the Collaborative
       Institutional Training Initiative (CITI) Program and institutional review
       boards (IRBs).
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0801</url>
     <doi>10.18653/v1/W18-0801</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>schmaltz:2018:W18-08</bibkey>
   </paper>
   <paper id="0802">
     <title>#MeToo Alexa: How Conversational Systems Respond to Sexual Harassment</title>
     <author>
       <first>Amanda</first>
       <last>Cercas Curry</last>
     </author>
     <author>
       <first>Verena</first>
       <last>Rieser</last>
     </author>
     <booktitle>Proceedings of the Second ACL Workshop on Ethics in Natural Language Processing</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>7–14</pages>
     <abstract>
       Conversational AI systems, such as Amazon’s Alexa, are rapidly developing
       from purely transactional systems to social chatbots, which can respond to
       a wide variety of user requests. In this article, we establish how current
       state-of-the-art conversational systems react to inappropriate requests,
       such as bullying and sexual harassment on the part of the user, by
       collecting and analysing the novel #MeTooAlexa corpus. Our results show
       that commercial systems mainly avoid answering, while rule-based chatbots
       show a variety of behaviours and often deflect. Data-driven systems, on
       the other hand, are often non-coherent, but also run the risk of being
       interpreted as flirtatious and sometimes react with counter-aggression.
       This includes our own system, trained on “clean” data, which suggests that
       inappropriate system behaviour is not caused by data bias.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0802</url>
     <revision id="2">W18-0802v2</revision>
     <doi>10.18653/v1/W18-0802</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>cercascurry-rieser:2018:W18-08</bibkey>
   </paper>
   <paper id="0900">
     <title>Proceedings of the Workshop on Figurative Language Processing</title>
     <editor>
       <first>Beata Beigman</first>
       <last>Klebanov</last>
     </editor>
     <editor>
       <first>Ekaterina</first>
       <last>Shutova</last>
     </editor>
     <editor>
       <first>Patricia</first>
       <last>Lichtenstein</last>
     </editor>
     <editor>
       <first>Smaranda</first>
       <last>Muresan</last>
     </editor>
     <editor>
       <first>Chee</first>
       <last>Wee</last>
     </editor>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-09</url>
     <doi>10.18653/v1/W18-09</doi>
     <bibtype>book</bibtype>
     <bibkey>W18-09:2018</bibkey>
   </paper>
   <paper id="0901">
     <title>Challenges in Finding Metaphorical Connections</title>
     <author>
       <first>Katy</first>
       <last>Gero</last>
     </author>
     <author>
       <first>Lydia</first>
       <last>Chilton</last>
     </author>
     <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–6</pages>
     <abstract> 

Poetry is known for its novel expression using figurative language. We introduce a writing task that contains the essential challenges of generating meaningful figurative language and can be evaluated. We investigate how to find metaphorical connections between abstract themes and concrete domains by asking people to write four-line poems on a given metaphor, such as “death is a rose” or “anger is wood”. We find that only 21% of poems successfully make a metaphorical connection. We present five alternate ways people respond to the prompt and release our dataset of 100 categorized poems. We suggest opportunities for computational approaches. </abstract>
     <url>http://www.aclweb.org/anthology/W18-0901</url>
     <doi>10.18653/v1/W18-0901</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>gero-chilton:2018:W18-09</bibkey>
   </paper>
   <paper id="0902">
     <title>Linguistic Features of Sarcasm and Metaphor Production Quality</title>
     <author>
       <first>Stephen</first>
       <last>Skalicky</last>
     </author>
     <author>
       <first>Scott</first>
       <last>Crossley</last>
     </author>
     <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>7–16</pages>
     <abstract>
       Using linguistic features to detect figurative language has provided a
       deeper in-sight into figurative language. The purpose of this study is to
       assess whether linguistic features can help explain differences in quality
       of figurative language. In this study a large corpus of metaphors and
       sarcastic responses are collected from human subjects and rated for
       figurative language quality based on theoretical components of metaphor,
       sarcasm, and creativity. Using natural language processing tools, specific
       linguistic features related to lexical sophistication and semantic
       cohesion were used to predict the human ratings of figurative language
       quality. Results demonstrate linguistic features were able to predict
       small amounts of variance in metaphor and sarcasm production quality.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0902</url>
     <doi>10.18653/v1/W18-0902</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>skalicky-crossley:2018:W18-09</bibkey>
   </paper>
   <paper id="0903">
     <title>Leveraging Syntactic Constructions for Metaphor Identification</title>
     <author>
       <first>Kevin</first>
       <last>Stowe</last>
     </author>
     <author>
       <first>Martha</first>
       <last>Palmer</last>
     </author>
     <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>17–26</pages>
     <abstract>
       Identification of metaphoric language in text is critical for generating
       effective semantic representations for natural language understanding.
       Computational approaches to metaphor identification have largely relied on
       heuristic based models or feature-based machine learning, using
       hand-crafted lexical resources coupled with basic syntactic information.
       However, recent work has shown the predictive power of syntactic
       constructions in determining metaphoric source and target domains
       (Sullivan 2013). Our work intends to explore syntactic constructions and
       their relation to metaphoric language. We undertake a corpus-based
       analysis of predicate-argument constructions and their metaphoric
       properties, and attempt to effectively represent syntactic constructions
       as features for metaphor processing, both in identifying source and target
       domains and in distinguishing metaphoric words from non-metaphoric.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0903</url>
     <doi>10.18653/v1/W18-0903</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>stowe-palmer:2018:W18-09</bibkey>
   </paper>
   <paper id="0904">
     <title>
       Literal, Metphorical or Both? Detecting Metaphoricity in Isolated
       Adjective-Noun Phrases
     </title>
     <author>
       <first>Agnieszka</first>
       <last>Mykowiecka</last>
     </author>
     <author>
       <first>Malgorzata</first>
       <last>Marciniak</last>
     </author>
     <author>
       <first>Aleksander</first>
       <last>Wawer</last>
     </author>
     <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>27–33</pages>
     <abstract>
       The paper addresses the classification of isolated Polish adjective-noun
       phrases according to their metaphoricity. We tested neural networks to
       predict if a phrase has a literal or metaphorical sense or can have both
       senses depending on usage. The input to the neural network consists of
       word embeddings, but we also tested the impact of information about the
       domain of the adjective and about the abstractness of the noun. We applied
       our solution to English data available on the Internet and compared it to
       results published in papers. We found that the solution based on word
       embeddings only can achieve results comparable with complex solutions
       requiring additional information.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0904</url>
     <doi>10.18653/v1/W18-0904</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>mykowiecka-marciniak-wawer:2018:W18-09</bibkey>
   </paper>
   <paper id="0905">
     <title>Catching Idiomatic Expressions in EFL Essays</title>
     <author>
       <first>Michael</first>
       <last>Flor</last>
     </author>
     <author>
       <first>Beata</first>
       <last>Beigman Klebanov</last>
     </author>
     <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>34–44</pages>
     <abstract>
       This paper presents an exploratory study on large-scale detection of
       idiomatic expressions in essays written by non-native speakers of English.
       We describe a computational search procedure for automatic detection of
       idiom-candidate phrases in essay texts. The study used a corpus of essays
       written during a standardized examination of English language proficiency.
       Automatically-flagged candidate expressions were manually annotated for
       idiomaticity. The study found that idioms are widely used in EFL essays.
       The study also showed that a search algorithm that accommodates the
       syntactic and lexical exibility of idioms can increase the recall of idiom
       instances by 30%, but it also increases the amount of false positives.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0905</url>
     <doi>10.18653/v1/W18-0905</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>flor-beigmanklebanov:2018:W18-09</bibkey>
   </paper>
   <paper id="0906">
     <title>Predicting Human Metaphor Paraphrase Judgments with Deep Neural Networks</title>
     <author>
       <first>Yuri</first>
       <last>Bizzoni</last>
     </author>
     <author>
       <first>Shalom</first>
       <last>Lappin</last>
     </author>
     <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>45–55</pages>
     <abstract> 

We propose a new annotated corpus for metaphor interpretation by paraphrase, and a novel DNN model for performing this task. Our corpus consists of 200 sets of 5 sentences, with each set containing one reference metaphorical sentence, and four ranked candidate paraphrases. Our model is trained for a binary classification of paraphrase candidates, and then used to predict graded paraphrase acceptability. It reaches an encouraging 75% accuracy on the binary classification task, and high Pearson (.75) and Spearman (.68) correlations on the gradient judgment prediction task. </abstract>
     <url>http://www.aclweb.org/anthology/W18-0906</url>
     <doi>10.18653/v1/W18-0906</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>bizzoni-lappin:2018:W18-09</bibkey>
   </paper>
   <paper id="0907">
     <title>A Report on the 2018 VUA Metaphor Detection Shared Task</title>
     <author>
       <first>Chee Wee (Ben)</first>
       <last>Leong</last>
     </author>
     <author>
       <first>Beata</first>
       <last>Beigman Klebanov</last>
     </author>
     <author>
       <first>Ekaterina</first>
       <last>Shutova</last>
     </author>
     <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>56–66</pages>
     <abstract>
       As the community working on computational approaches to figurative
       language is growing and as methods and data become increasingly diverse,
       it is important to create widely shared empirical knowledge of the level
       of system performance in a range of contexts, thus facilitating progress
       in this area. One way of creating such shared knowledge is through
       benchmarking multiple systems on a common dataset. We report on the shared
       task on metaphor identification on the VU Amsterdam Metaphor Corpus
       conducted at the NAACL 2018 Workshop on Figurative Language Processing.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0907</url>
     <doi>10.18653/v1/W18-0907</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>leong-beigmanklebanov-shutova:2018:W18-09</bibkey>
   </paper>
   <paper id="0908">
     <title>An LSTM-CRF Based Approach to Token-Level Metaphor Detection</title>
     <author>
       <first>Malay</first>
       <last>Pramanick</last>
     </author>
     <author>
       <first>Ashim</first>
       <last>Gupta</last>
     </author>
     <author>
       <first>Pabitra</first>
       <last>Mitra</last>
     </author>
     <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>67–75</pages>
     <abstract>
       Automatic processing of figurative languages is gaining popularity in NLP
       community for their ubiquitous nature and increasing volume. In this era
       of web 2.0, automatic analysis of sarcasm and metaphors is important for
       their extensive usage. Metaphors are a part of figurative language that
       compares different concepts, often on a cognitive level. Many approaches
       have been proposed for automatic detection of metaphors, even using
       sequential models or neural networks. In this paper, we propose a method
       for detection of metaphors at the token level using a hybrid model of
       Bidirectional-LSTM and CRF. We used fewer features, as compared to the
       previous state-of-the-art sequential model. On experimentation with VUAMC,
       our method obtained an F-score of 0.674.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0908</url>
     <doi>10.18653/v1/W18-0908</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>pramanick-gupta-mitra:2018:W18-09</bibkey>
   </paper>
   <paper id="0909">
     <title>Unsupervised Detection of Metaphorical Adjective-Noun Pairs</title>
     <author>
       <first>Malay</first>
       <last>Pramanick</last>
     </author>
     <author>
       <first>Pabitra</first>
       <last>Mitra</last>
     </author>
     <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>76–80</pages>
     <abstract>
       Metaphor is a popular figure of speech. Popularity of metaphors calls for
       their automatic identification and interpretation. Most of the
       unsupervised methods directed at detection of metaphors use some
       hand-coded knowledge. We propose an unsupervised framework for metaphor
       detection that does not require any hand-coded knowledge. We applied
       clustering on features derived from Adjective-Noun pairs for classifying
       them into two disjoint classes. We experimented with adjective-noun pairs
       of a popular dataset annotated for metaphors and obtained an accuracy of
       72.87% with k-means clustering algorithm.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0909</url>
     <doi>10.18653/v1/W18-0909</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>pramanick-mitra:2018:W18-09</bibkey>
   </paper>
   <paper id="0910">
     <title>
       Phrase-Level Metaphor Identification Using Distributed Representations of
       Word Meaning
     </title>
     <author>
       <first>Omnia</first>
       <last>Zayed</last>
     </author>
     <author>
       <first>John Philip</first>
       <last>McCrae</last>
     </author>
     <author>
       <first>Paul</first>
       <last>Buitelaar</last>
     </author>
     <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>81–90</pages>
     <abstract>
       Metaphor is an essential element of human cognition which is often used to
       express ideas and emotions that might be difficult to express using
       literal language. Processing metaphoric language is a challenging task for
       a wide range of applications ranging from text simplification to
       psychotherapy. Despite the variety of approaches that are trying to
       process metaphor, there is still a need for better models that mimic the
       human cognition while exploiting fewer resources. In this paper, we
       present an approach based on distributional semantics to identify
       metaphors on the phrase-level. We investigated the use of different word
       embeddings models to identify verb-noun pairs where the verb is used
       metaphorically. Several experiments are conducted to show the performance
       of the proposed approach on benchmark datasets.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0910</url>
     <doi>10.18653/v1/W18-0910</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>zayed-mccrae-buitelaar:2018:W18-09</bibkey>
   </paper>
   <paper id="0911">
     <title>Bigrams and BiLSTMs Two Neural Networks for Sequential Metaphor Detection</title>
     <author>
       <first>Yuri</first>
       <last>Bizzoni</last>
     </author>
     <author>
       <first>Mehdi</first>
       <last>Ghanimifard</last>
     </author>
     <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>91–101</pages>
     <abstract>
       We present and compare two alternative deep neural architectures to
       perform word-level metaphor detection on text: a bi-LSTM model and a new
       structure based on recursive feed-forward concatenation of the input. We
       discuss different versions of such models and the effect that input
       manipulation - specifically, reducing the length of sentences and
       introducing concreteness scores for words - have on their performance.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0911</url>
     <doi>10.18653/v1/W18-0911</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>bizzoni-ghanimifard:2018:W18-09</bibkey>
   </paper>
   <paper id="0912">
     <title>
       Computationally Constructed Concepts: A Machine Learning Approach to
       Metaphor Interpretation Using Usage-Based Construction Grammatical Cues
     </title>
     <author>
       <first>Zachary</first>
       <last>Rosen</last>
     </author>
     <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>102–109</pages>
     <abstract>
       The current study seeks to implement a deep learning classification
       algorithm using argument-structure level representation of metaphoric
       constructions, for the identification of source domain mappings in
       metaphoric utterances. It thus builds on previous work in computational
       metaphor interpretation (Mohler et al. 2014; Shutova 2010; Bollegala
       &amp; Shutova 2013; Hong 2016; Su et al. 2017) while implementing a
       theoretical framework based off of work in the interface of metaphor and
       construction grammar (Sullivan 2006, 2007, 2013). The results indicate
       that it is possible to achieve an accuracy of approximately 80.4% using
       the proposed method, combining construction grammatical features with a
       simple deep learning NN. I attribute this increase in accuracy to the use
       of constructional cues, extracted from the raw text of metaphoric
       instances.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0912</url>
     <doi>10.18653/v1/W18-0912</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>rosen:2018:W18-09</bibkey>
   </paper>
   <paper id="0913">
     <title>Neural Metaphor Detecting with CNN-LSTM Model</title>
     <author>
       <first>Chuhan</first>
       <last>Wu</last>
     </author>
     <author>
       <first>Fangzhao</first>
       <last>Wu</last>
     </author>
     <author>
       <first>Yubo</first>
       <last>Chen</last>
     </author>
     <author>
       <first>Sixing</first>
       <last>Wu</last>
     </author>
     <author>
       <first>Zhigang</first>
       <last>Yuan</last>
     </author>
     <author>
       <first>Yongfeng</first>
       <last>Huang</last>
     </author>
     <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>110–114</pages>
     <abstract>
       Metaphors are figurative languages widely used in daily life and
       literatures. It’s an important task to detect the metaphors evoked by
       texts. Thus, the metaphor shared task is aimed to extract metaphors from
       plain texts at word level. We propose to use a CNN-LSTM model for this
       task. Our model combines CNN and LSTM layers to utilize both local and
       long-range contextual information for identifying metaphorical
       information. In addition, we compare the performance of the softmax
       classifier and conditional random field (CRF) for sequential labeling in
       this task. We also incorporated some additional features such as part of
       speech (POS) tags and word cluster to improve the performance of model.
       Our best model achieved 65.06% F-score in the all POS testing subtask and
       67.15% in the verbs testing subtask.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0913</url>
     <doi>10.18653/v1/W18-0913</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>wu-EtAl:2018:W18-09</bibkey>
   </paper>
   <paper id="0914">
     <title>Di-LSTM Contrast : A Deep Neural Network for Metaphor Detection</title>
     <author>
       <first>Krishnkant</first>
       <last>Swarnkar</last>
     </author>
     <author>
       <first>Anil Kumar</first>
       <last>Singh</last>
     </author>
     <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>115–120</pages>
     <abstract>
       The contrast between the contextual and general meaning of a word serves
       as an important clue for detecting its metaphoricity. In this paper, we
       present a deep neural architecture for metaphor detection which exploits
       this contrast. Additionally, we also use cost-sensitive learning by
       re-weighting examples, and baseline features like concreteness ratings,
       POS and WordNet-based features. The best performing system of ours
       achieves an overall F1 score of 0.570 on All POS category and 0.605 on the
       Verbs category at the Metaphor Shared Task 2018.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0914</url>
     <doi>10.18653/v1/W18-0914</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>swarnkar-singh:2018:W18-09</bibkey>
   </paper>
   <paper id="0915">
     <title>Conditional Random Fields for Metaphor Detection</title>
     <author>
       <first>Anna</first>
       <last>Mosolova</last>
     </author>
     <author>
       <first>Ivan</first>
       <last>Bondarenko</last>
     </author>
     <author>
       <first>Vadim</first>
       <last>Fomin</last>
     </author>
     <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>121–123</pages>
     <abstract>
       We present an algorithm for detecting metaphor in sentences which was used
       in Shared Task on Metaphor Detection by First Workshop on Figurative
       Language Processing. The algorithm is based on different features and
       Conditional Random Fields.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0915</url>
     <doi>10.18653/v1/W18-0915</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>mosolova-bondarenko-fomin:2018:W18-09</bibkey>
   </paper>
   <paper id="0916">
     <title>Detecting Figurative Word Occurrences Using Recurrent Neural Networks</title>
     <author>
       <first>Agnieszka</first>
       <last>Mykowiecka</last>
     </author>
     <author>
       <first>Aleksander</first>
       <last>Wawer</last>
     </author>
     <author>
       <first>Malgorzata</first>
       <last>Marciniak</last>
     </author>
     <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>124–127</pages>
     <abstract>
       The paper addresses detection of figurative usage of words in English
       text. The chosen method was to use neural nets fed by pretrained word
       embeddings. The obtained results show that simple solutions, based on
       words embeddings only, are comparable to complex solutions, using many
       sources of information which are not available for languages less-studied
       than English.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0916</url>
     <doi>10.18653/v1/W18-0916</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>mykowiecka-wawer-marciniak:2018:W18-09</bibkey>
   </paper>
   <paper id="0917">
     <title>Multi-Module Recurrent Neural Networks with Transfer Learning</title>
     <author>
       <first>Filip</first>
       <last>Skurniak</last>
     </author>
     <author>
       <first>Maria</first>
       <last>Janicka</last>
     </author>
     <author>
       <first>Aleksander</first>
       <last>Wawer</last>
     </author>
     <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>128–132</pages>
     <abstract>
       This paper describes multiple solutions designed and tested for the
       problem of word-level metaphor detection. The proposed systems are all
       based on variants of recurrent neural network architectures. Specifically,
       we explore multiple sources of information: pre-trained word embeddings
       (Glove), a dictionary of language concreteness and a transfer learning
       scenario based on the states of an encoder network from neural network
       machine translation system. One of the architectures is based on combining
       all three systems: (1) Neural CRF (Conditional Random Fields), trained
       directly on the metaphor data set; (2) Neural Machine Translation encoder
       of a transfer learning scenario; (3) a neural network used to predict
       final labels, trained directly on the metaphor data set. Our results vary
       between test sets: Neural CRF standalone is the best one on submission
       data, while combined system scores the highest on a test subset randomly
       selected from training data.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0917</url>
     <doi>10.18653/v1/W18-0917</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>skurniak-janicka-wawer:2018:W18-09</bibkey>
   </paper>
   <paper id="0918">
     <title>Using Language Learner Data for Metaphor Detection</title>
     <author>
       <first>Egon</first>
       <last>Stemle</last>
     </author>
     <author>
       <first>Alexander</first>
       <last>Onysko</last>
     </author>
     <booktitle>Proceedings of the Workshop on Figurative Language Processing</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>133–138</pages>
     <abstract>
       This article describes the system that participated in the shared task on
       metaphor detection on the Vrije University Amsterdam Metaphor Corpus
       (VUA). The ST was part of the workshop on processing figurative language
       at the 16th annual conference of the North American Chapter of the
       Association for Computational Linguistics (NAACL2018). The system combines
       a small assertion of trending techniques, which implement matured methods
       from NLP and ML; in particular, the system uses word embeddings from
       standard corpora and from corpora representing different proficiency
       levels of language learners in a LSTM BiRNN architecture. The system is
       available under the APLv2 open-source license.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-0918</url>
     <doi>10.18653/v1/W18-0918</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>stemle-onysko:2018:W18-09</bibkey>
   </paper>
   <paper id="1000">
     <title>Proceedings of the Workshop on Generalization in the Age of Deep Learning</title>
     <editor>
       <first>Yonatan</first>
       <last>Bisk</last>
     </editor>
     <editor>
       <first>Omer</first>
       <last>Levy</last>
     </editor>
     <editor>
       <first>Mark</first>
       <last>Yatskar</last>
     </editor>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-10</url>
     <doi>10.18653/v1/W18-10</doi>
     <bibtype>book</bibtype>
     <bibkey>W18-10:2018</bibkey>
   </paper>
   <paper id="1001">
     <title>Towards Inference-Oriented Reading Comprehension: ParallelQA</title>
     <author>
       <first>Soumya</first>
       <last>Wadhwa</last>
     </author>
     <author>
       <first>Varsha</first>
       <last>Embar</last>
     </author>
     <author>
       <first>Matthias</first>
       <last>Grabmair</last>
     </author>
     <author>
       <first>Eric</first>
       <last>Nyberg</last>
     </author>
     <booktitle>Proceedings of the Workshop on Generalization in the Age of Deep Learning</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–7</pages>
     <abstract>
       In this paper, we investigate the tendency of end-to-end neural Machine
       Reading Comprehension (MRC) models to match shallow patterns rather than
       perform inference-oriented reasoning on RC benchmarks. We aim to test the
       ability of these systems to answer questions which focus on referential
       inference. We propose ParallelQA, a strategy to formulate such questions
       using parallel passages. We also demonstrate that existing neural models
       fail to generalize well to this setting.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1001</url>
     <doi>10.18653/v1/W18-1001</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>wadhwa-EtAl:2018:W18-10</bibkey>
   </paper>
   <paper id="1002">
     <title>
       Commonsense mining as knowledge base completion? A study on the impact of
       novelty
     </title>
     <author>
       <first>Stanislaw</first>
       <last>Jastrzebski</last>
     </author>
     <author>
       <first>Dzmitry</first>
       <last>Bahdanau</last>
     </author>
     <author>
       <first>Seyedarian</first>
       <last>Hosseini</last>
     </author>
     <author>
       <first>Michael</first>
       <last>Noukhovitch</last>
     </author>
     <author>
       <first>Yoshua</first>
       <last>Bengio</last>
     </author>
     <author>
       <first>Jackie</first>
       <last>Cheung</last>
     </author>
     <booktitle>Proceedings of the Workshop on Generalization in the Age of Deep Learning</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>8–16</pages>
     <abstract>
       Commonsense knowledge bases such as ConceptNet represent knowledge in the
       form of relational triples. Inspired by recent work by Li et al., we
       analyse if knowledge base completion models can be used to mine
       commonsense knowledge from raw text. We propose novelty of predicted
       triples with respect to the training set as an important factor in
       interpreting results. We critically analyse the difficulty of mining novel
       commonsense knowledge, and show that a simple baseline method that
       outperforms the previous state of the art on predicting more novel
       triples.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1002</url>
     <doi>10.18653/v1/W18-1002</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>jastrzebski-EtAl:2018:W18-10</bibkey>
   </paper>
   <paper id="1003">
     <title>Deep learning evaluation using deep linguistic processing</title>
     <author>
       <first>Alexander</first>
       <last>Kuhnle</last>
     </author>
     <author>
       <first>Ann</first>
       <last>Copestake</last>
     </author>
     <booktitle>Proceedings of the Workshop on Generalization in the Age of Deep Learning</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>17–23</pages>
     <abstract>
       We discuss problems with the standard approaches to evaluation for tasks
       like visual question answering, and argue that artificial data can be used
       to address these as a complement to current practice. We demonstrate that
       with the help of existing ‘deep’ linguistic processing technology we are
       able to create challenging abstract datasets, which enable us to
       investigate the language understanding abilities of multimodal deep
       learning models in detail, as compared to a single performance value on a
       static and monolithic dataset.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1003</url>
     <doi>10.18653/v1/W18-1003</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>kuhnle-copestake:2018:W18-10</bibkey>
   </paper>
   <paper id="1004">
     <title>
       The Fine Line between Linguistic Generalization and Failure in
       Seq2Seq-Attention Models
     </title>
     <author>
       <first>Noah</first>
       <last>Weber</last>
     </author>
     <author>
       <first>Leena</first>
       <last>Shekhar</last>
     </author>
     <author>
       <first>Niranjan</first>
       <last>Balasubramanian</last>
     </author>
     <booktitle>Proceedings of the Workshop on Generalization in the Age of Deep Learning</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>24–27</pages>
     <abstract>
       Seq2Seq based neural architectures have become the go-to architecture to
       apply to sequence to sequence language tasks. Despite their excellent
       performance on these tasks, recent work has noted that these models
       typically do not fully capture the linguistic structure required to
       generalize beyond the dense sections of the data distribution
       (Ettinger et al., 2017), and as such, are likely to fail on examples from the
       tail end of the distribution (such as inputs that are noisy
       (Belinkov and Bisk, 2018), or of different length (Bentivogli et al., 2016)). In this
       paper we look at a model’s ability to generalize on a simple symbol
       rewriting task with a clearly defined structure. We find that the model’s
       ability to generalize this structure beyond the training distribution
       depends greatly on the chosen random seed, even when performance on the
       test set remains the same. This finding suggests that model’s ability to
       capture generalizable structure is highly sensitive, and more so, this
       sensitivity may not be apparent when evaluating the model on standard test
       sets.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1004</url>
     <doi>10.18653/v1/W18-1004</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>weber-shekhar-balasubramanian:2018:W18-10</bibkey>
   </paper>
   <paper id="1005">
     <title>Extrapolation in NLP</title>
     <author>
       <first>Jeff</first>
       <last>Mitchell</last>
     </author>
     <author>
       <first>Pontus</first>
       <last>Stenetorp</last>
     </author>
     <author>
       <first>Pasquale</first>
       <last>Minervini</last>
     </author>
     <author>
       <first>Sebastian</first>
       <last>Riedel</last>
     </author>
     <booktitle>Proceedings of the Workshop on Generalization in the Age of Deep Learning</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>28–33</pages>
     <abstract>
       We argue that extrapolation to unseen data will often be easier for models
       that capture global structures, rather than just maximise their local fit
       to the training data. We show that this is true for two popular models:
       the Decomposable Attention Model and word2vec.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1005</url>
     <doi>10.18653/v1/W18-1005</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>mitchell-EtAl:2018:W18-10</bibkey>
   </paper>
   <paper id="1100">
     <title>
       Proceedings of the Second Workshop on Computational Modeling of People’s
       Opinions, Personality, and Emotions in Social Media
     </title>
     <editor>
       <first>Malvina</first>
       <last>Nissim</last>
     </editor>
     <editor>
       <first>Viviana</first>
       <last>Patti</last>
     </editor>
     <editor>
       <first>Barbara</first>
       <last>Plank</last>
     </editor>
     <editor>
       <first>Claudia</first>
       <last>Wagner</last>
     </editor>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-11</url>
     <doi>10.18653/v1/W18-11</doi>
     <bibtype>book</bibtype>
     <bibkey>W18-11:2018</bibkey>
   </paper>
   <paper id="1101">
     <title>What makes us laugh? Investigations into Automatic Humor Classification</title>
     <author>
       <first>Vikram</first>
       <last>Ahuja</last>
     </author>
     <author>
       <first>Taradheesh</first>
       <last>Bali</last>
     </author>
     <author>
       <first>Navjyoti</first>
       <last>Singh</last>
     </author>
     <booktitle>
       Proceedings of the Second Workshop on Computational Modeling of People’s
       Opinions, Personality, and Emotions in Social Media
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–9</pages>
     <abstract>
       Most scholarly works in the field of computational detection of humour
       derive their inspiration from the incongruity theory. Incongruity is an
       indispensable facet in drawing a line between humorous and non-humorous
       occurrences but is immensely inadequate in shedding light on what actually
       made the particular occurrence a funny one. Classical theories like
       Script-based Semantic Theory of Humour and General Verbal Theory of Humour
       try and achieve this feat to an adequate extent. In this paper we adhere
       to a more holistic approach towards classification of humour based on
       these classical theories with a few improvements and revisions. Through
       experiments based on our linear approach and performed on large data-sets
       of jokes, we are able to demonstrate the adaptability and show
       componentizability of our model, and that a host of classification
       techniques can be used to overcome the challenging problem of
       distinguishing between various categories and sub-categories of jokes.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1101</url>
     <doi>10.18653/v1/W18-1101</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>ahuja-bali-singh:2018:W18-11</bibkey>
   </paper>
   <paper id="1102">
     <title>Social and Emotional Correlates of Capitalization on Twitter</title>
     <author>
       <first>Sophia</first>
       <last>Chan</last>
     </author>
     <author>
       <first>Alona</first>
       <last>Fyshe</last>
     </author>
     <booktitle>
       Proceedings of the Second Workshop on Computational Modeling of People’s
       Opinions, Personality, and Emotions in Social Media
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>10–15</pages>
     <abstract>
       Social media text is replete with unusual capitalization patterns. We
       posit that capitalizing a token like THIS performs two expressive
       functions: it marks a person socially, and marks certain parts of an
       utterance as more salient than others. Focusing on gender and sentiment,
       we illustrate using a corpus of tweets that capitalization appears in more
       negative than positive contexts, and is used more by females compared to
       males. Yet we find that both genders use capitalization in a similar way
       when expressing sentiment.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1102</url>
     <doi>10.18653/v1/W18-1102</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>chan-fyshe:2018:W18-11</bibkey>
   </paper>
   <paper id="1103">
     <title>
       Building an annotated dataset of app store reviews with Appraisal features
       in English and Spanish
     </title>
     <author>
       <first>Natalia</first>
       <last>Mora</last>
     </author>
     <author>
       <first>Julia</first>
       <last>Lavid-López</last>
     </author>
     <booktitle>
       Proceedings of the Second Workshop on Computational Modeling of People’s
       Opinions, Personality, and Emotions in Social Media
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>16–24</pages>
     <abstract>
       This paper describes the creation and annotation of a dataset consisting
       of 250 English and Spanish app store reviews from Google’s Play Store with
       Appraisal features. This is one of the most influential linguistic
       frameworks for the analysis of evaluation and opinion in discourse due to
       its insightful descriptive features. However, it has not been extensively
       applied in NLP in spite of its potential for the classification of the
       subjective content of these reviews. We describe the dataset, the
       annotation scheme and guidelines, the agreement studies, the annotation
       results and their impact on the characterisation of this genre.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1103</url>
     <doi>10.18653/v1/W18-1103</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>mora-lavidlpez:2018:W18-11</bibkey>
   </paper>
   <paper id="1104">
     <title>Enabling Deep Learning of Emotion With First-Person Seed Expressions</title>
     <author>
       <first>Hassan</first>
       <last>Alhuzali</last>
     </author>
     <author>
       <first>Muhammad</first>
       <last>Abdul-Mageed</last>
     </author>
     <author>
       <first>Lyle</first>
       <last>Ungar</last>
     </author>
     <booktitle>
       Proceedings of the Second Workshop on Computational Modeling of People’s
       Opinions, Personality, and Emotions in Social Media
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>25–35</pages>
     <abstract> 

The computational treatment of emotion in natural language text remains relatively limited, and Arabic is no exception. This is partly due to lack of labeled data. In this work, we describe and manually validate a method for the automatic acquisition of emotion labeled data and introduce a newly developed data set for Modern Standard and Dialectal Arabic emotion detection focused at Robert Plutchik’s 8 basic emotion types. Using a hybrid supervision method that exploits first person emotion seeds, we show how we can acquire promising results with a deep gated recurrent neural network. Our best model reaches 70% <i>F</i>-score, significantly (i.e., 11%, <tex-math>p &lt; 0.05</tex-math>) outperforming a competitive baseline. Applying our method and data on an external dataset of 4 emotions released around the same time we finalized our work, we acquire 7% absolute gain in <tex-math>F</tex-math>-score over a linear SVM classifier trained on gold data, thus validating our approach. </abstract>
     <url>http://www.aclweb.org/anthology/W18-1104</url>
     <doi>10.18653/v1/W18-1104</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>alhuzali-abdulmageed-ungar:2018:W18-11</bibkey>
   </paper>
   <paper id="1105">
     <title>
       A Dataset of Hindi-English Code-Mixed Social Media Text for Hate Speech
       Detection
     </title>
     <author>
       <first>Aditya</first>
       <last>Bohra</last>
     </author>
     <author>
       <first>Deepanshu</first>
       <last>Vijay</last>
     </author>
     <author>
       <first>Vinay</first>
       <last>Singh</last>
     </author>
     <author>
       <first>Syed Sarfaraz</first>
       <last>Akhtar</last>
     </author>
     <author>
       <first>Manish</first>
       <last>Shrivastava</last>
     </author>
     <booktitle>
       Proceedings of the Second Workshop on Computational Modeling of People’s
       Opinions, Personality, and Emotions in Social Media
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>36–41</pages>
     <abstract>
       Hate speech detection in social media texts is an important Natural
       language Processing task, which has several crucial applications like
       sentiment analysis, investigating cyberbullying and examining
       socio-political controversies. While relevant research has been done
       independently on code-mixed social media texts and hate speech detection,
       our work is the first attempt in detecting hate speech in Hindi-English
       code-mixed social media text. In this paper, we analyze the problem of
       hate speech detection in code-mixed texts and present a Hindi-English
       code-mixed dataset consisting of tweets posted online on Twitter. The
       tweets are annotated with the language at word level and the class they
       belong to (Hate Speech or Normal Speech). We also propose a supervised
       classification system for detecting hate speech in the text using various
       character level, word level, and lexicon based features.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1105</url>
     <doi>10.18653/v1/W18-1105</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>bohra-EtAl:2018:W18-11</bibkey>
   </paper>
   <paper id="1106">
     <title>
       The Social and the Neural Network: How to Make Natural Language Processing
       about People again
     </title>
     <author>
       <first>Dirk</first>
       <last>Hovy</last>
     </author>
     <booktitle>
       Proceedings of the Second Workshop on Computational Modeling of People’s
       Opinions, Personality, and Emotions in Social Media
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>42–49</pages>
     <abstract>
       Over the years, natural language processing has increasingly focused on
       tasks that can be solved by statistical models, but ignored the social
       aspects of language. These limitations are in large part due to
       historically available data and the limitations of the models, but have
       narrowed our focus and biased the tools demographically. However, with the
       increased availability of data sets including socio-demographic
       information and more expressive (neural) models, we have the opportunity
       to address both issues. I argue that this combination can broaden the
       focus of NLP to solve a whole new range of tasks, enable us to generate
       novel linguistic insights, and provide fairer tools for everyone.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1106</url>
     <doi>10.18653/v1/W18-1106</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>hovy:2018:W18-11</bibkey>
   </paper>
   <paper id="1107">
     <title>Observational Comparison of Geo-tagged and Randomly-drawn Tweets</title>
     <author>
       <first>Tom</first>
       <last>Lippincott</last>
     </author>
     <author>
       <first>Annabelle</first>
       <last>Carrell</last>
     </author>
     <booktitle>
       Proceedings of the Second Workshop on Computational Modeling of People’s
       Opinions, Personality, and Emotions in Social Media
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>50–55</pages>
     <abstract> 

Twitter is a ubiquitous source of micro-blog social media data, providing the academic, industrial, and public sectors real-time access to actionable information. A particularly attractive property of some tweets is *geo-tagging*, where a user account has opted-in to attaching their current location to each message. Unfortunately (from a researcher’s perspective) only a fraction of Twitter accounts agree to this, and these accounts are likely to have systematic diffences with the general population. This work is an exploratory study of these differences across the full range of Twitter content, and complements previous studies that focus on the English-language subset. Additionally, we compare methods for querying users by self-identified properties, finding that the constrained semantics of the “description” field provides cleaner, higher-volume results than more complex regular expressions. </abstract>
     <url>http://www.aclweb.org/anthology/W18-1107</url>
     <doi>10.18653/v1/W18-1107</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>lippincott-carrell:2018:W18-11</bibkey>
   </paper>
   <paper id="1108">
     <title>
       Johns Hopkins or johnny-hopkins: Classifying Individuals versus
       Organizations on Twitter
     </title>
     <author>
       <first>Zach</first>
       <last>Wood-Doughty</last>
     </author>
     <author>
       <first>Praateek</first>
       <last>Mahajan</last>
     </author>
     <author>
       <first>Mark</first>
       <last>Dredze</last>
     </author>
     <booktitle>
       Proceedings of the Second Workshop on Computational Modeling of People’s
       Opinions, Personality, and Emotions in Social Media
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>56–61</pages>
     <abstract>
       Twitter user accounts include a range of different user types. While many
       individuals use Twitter, organizations also have Twitter accounts.
       Identifying opinions and trends from Twitter requires the accurate
       differentiation of these two groups. Previous work (McCorriston et al.,
       2015) presented a method for determining if an account was an individual
       or organization based on account profile and a collection of tweets. We
       present a method that relies solely on the account profile, allowing for
       the classification of individuals versus organizations based on a single
       tweet. Our method obtains accuracies comparable to methods that rely on
       much more information by leveraging two improvements: a character-based
       Convolutional Neural Network, and an automatically derived labeled corpus
       an order of magnitude larger than the previously available dataset. We
       make both the dataset and the resulting tool available.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1108</url>
     <doi>10.18653/v1/W18-1108</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>wooddoughty-mahajan-dredze:2018:W18-11</bibkey>
   </paper>
   <paper id="1109">
     <title>
       The Potential of the Computational Linguistic Analysis of Social Media for
       Population Studies
     </title>
     <author>
       <first>Letizia</first>
       <last>Mencarini</last>
     </author>
     <booktitle>
       Proceedings of the Second Workshop on Computational Modeling of People’s
       Opinions, Personality, and Emotions in Social Media
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>62–68</pages>
     <abstract>
       The paper provides an outline of the scope for synergy between
       computational linguistic analysis and population stud-ies. It first
       reviews where population studies stand in terms of using social media
       data. Demographers are entering the realm of big data in force. But, this
       paper argues, population studies have much to gain from computational
       linguis-tic analysis, especially in terms of ex-plaining the drivers
       behind population processes. The paper gives two examples of how the
       method can be applied, and concludes with a fundamental caveat. Yes,
       computational linguistic analysis provides a possible key for integrating
       micro theory into any demographic analysis of social media data. But
       results may be of little value in as much as knowledge about fundamental
       sample characteristics are unknown.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1109</url>
     <doi>10.18653/v1/W18-1109</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>mencarini:2018:W18-11</bibkey>
   </paper>
   <paper id="1110">
     <title> 

Understanding the Effect of Gender and Stance in Opinion Expression in Debates on “Abortion” </title>
     <author>
       <first>Esin</first>
       <last>Durmus</last>
     </author>
     <author>
       <first>Claire</first>
       <last>Cardie</last>
     </author>
     <booktitle>
       Proceedings of the Second Workshop on Computational Modeling of People’s
       Opinions, Personality, and Emotions in Social Media
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>69–75</pages>
     <abstract>
       In this paper, we focus on understanding linguistic differences across
       groups with different self-identified gender and stance in expressing
       opinions about ABORTION. We provide a new dataset consisting of users’
       gender, stance on ABORTION as well as the debates in ABOR- TION drawn from
       debate.org. We use the gender and stance information to identify
       significant linguistic differences across individuals with different
       gender and stance. We show the importance of considering the stance
       information along with the gender since we observe significant linguistic
       differences across individuals with different stance even within the same
       gender group.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1110</url>
     <doi>10.18653/v1/W18-1110</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>durmus-cardie:2018:W18-11</bibkey>
   </paper>
   <paper id="1111">
     <title>Frustrated, Polite, or Formal: Quantifying Feelings and Tone in Email</title>
     <author>
       <first>Niyati</first>
       <last>Chhaya</last>
     </author>
     <author>
       <first>Kushal</first>
       <last>Chawla</last>
     </author>
     <author>
       <first>Tanya</first>
       <last>Goyal</last>
     </author>
     <author>
       <first>Projjal</first>
       <last>Chanda</last>
     </author>
     <author>
       <first>Jaya</first>
       <last>Singh</last>
     </author>
     <booktitle>
       Proceedings of the Second Workshop on Computational Modeling of People’s
       Opinions, Personality, and Emotions in Social Media
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>76–86</pages>
     <abstract>
       Email conversations are the primary mode of communication in enterprises.
       The email content expresses an individual’s needs, requirements and
       intentions. Affective information in the email text can be used to get an
       insight into the sender’s mood or emotion. We present a novel approach to
       model human frustration in text. We identify linguistic features that
       influence human perception of frustration and model it as a supervised
       learning task. The paper provides a detailed comparison across traditional
       regression and word distribution-based models. We report a mean-squared
       error (MSE) of 0.018 against human-annotated frustration for the best
       performing model. The approach establishes the importance of affect
       features in frustration prediction for email data. We further evaluate the
       efficacy of the proposed feature set and model in predicting other tone or
       affects in text, namely formality and politeness; results demonstrate a
       comparable performance against the state-of-the-art baselines.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1111</url>
     <doi>10.18653/v1/W18-1111</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>chhaya-EtAl:2018:W18-11</bibkey>
   </paper>
   <paper id="1112">
     <title>Reddit: A Gold Mine for Personality Prediction</title>
     <author>
       <first>Matej</first>
       <last>Gjurković</last>
     </author>
     <author>
       <first>Jan</first>
       <last>Šnajder</last>
     </author>
     <booktitle>
       Proceedings of the Second Workshop on Computational Modeling of People’s
       Opinions, Personality, and Emotions in Social Media
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>87–97</pages>
     <abstract>
       Automated personality prediction from social media is gaining increasing
       attention in natural language processing and social sciences communities.
       However, due to high labeling costs and privacy issues, the few publicly
       available datasets are of limited size and low topic diversity. We address
       this problem by introducing a large-scale dataset derived from Reddit, a
       source so far overlooked for personality prediction. The dataset is
       labeled with Myers-Briggs Type Indicators (MBTI) and comes with a rich set
       of features for more than 9k users. We carry out a preliminary feature
       analysis, revealing marked differences between the MBTI dimensions and
       poles. Furthermore, we use the dataset to train and evaluate benchmark
       personality prediction models, achieving macro F1-scores between 67% and
       82% on the individual dimensions and 82% accuracy for exact or one-off
       accurate type prediction. These results are encouraging and comparable
       with the reliability of standardized tests.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1112</url>
     <doi>10.18653/v1/W18-1112</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>gjurkovi-najder:2018:W18-11</bibkey>
   </paper>
   <paper id="1113">
     <title>Predicting Authorship and Author Traits from Keystroke Dynamics</title>
     <author>
       <first>Barbara</first>
       <last>Plank</last>
     </author>
     <booktitle>
       Proceedings of the Second Workshop on Computational Modeling of People’s
       Opinions, Personality, and Emotions in Social Media
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>98–104</pages>
     <abstract>
       Written text transmits a good deal of nonverbal information related to the
       author’s identity and social factors, such as age, gender and personality.
       However, it is less known to what extent behavioral biometric traces
       transmit such information. We use typist data to study the predictiveness
       of authorship, and present first experiments on predicting both age and
       gender from keystroke dynamics. Our results show that the model based on
       keystroke features, while being two orders of magnitude smaller, leads to
       significantly higher accuracies for authorship than the text-based system.
       For user attribute prediction, the best approach is to combine the two,
       suggesting that extralinguistic factors are disclosed to a larger degree
       in written text, while author identity is better transmitted in typing
       behavior.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1113</url>
     <doi>10.18653/v1/W18-1113</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>plank:2018:W18-11</bibkey>
   </paper>
   <paper id="1114">
     <title>Predicting Twitter User Demographics from Names Alone</title>
     <author>
       <first>Zach</first>
       <last>Wood-Doughty</last>
     </author>
     <author>
       <first>Nicholas</first>
       <last>Andrews</last>
     </author>
     <author>
       <first>Rebecca</first>
       <last>Marvin</last>
     </author>
     <author>
       <first>Mark</first>
       <last>Dredze</last>
     </author>
     <booktitle>
       Proceedings of the Second Workshop on Computational Modeling of People’s
       Opinions, Personality, and Emotions in Social Media
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>105–111</pages>
     <abstract>
       Social media analysis frequently requires tools that can automatically
       infer demographics to contextualize trends. These tools often require
       hundreds of user-authored messages for each user, which may be prohibitive
       to obtain when analyzing millions of users. We explore character-level
       neural models that learn a representation of a user’s name and screen name
       to predict gender and ethnicity, allowing for demographic inference with
       minimal data. We release trained models1 which may enable new demographic
       analyses that would otherwise require enormous amounts of data collection
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1114</url>
     <doi>10.18653/v1/W18-1114</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>wooddoughty-EtAl:2018:W18-11</bibkey>
   </paper>
   <paper id="1115">
     <title>Modeling Personality Traits of Filipino Twitter Users</title>
     <author>
       <first>Edward</first>
       <last>Tighe</last>
     </author>
     <author>
       <first>Charibeth</first>
       <last>Cheng</last>
     </author>
     <booktitle>
       Proceedings of the Second Workshop on Computational Modeling of People’s
       Opinions, Personality, and Emotions in Social Media
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>112–122</pages>
     <abstract>
       Recent studies in the field of text-based personality recognition
       experiment with different languages, feature extraction techniques, and
       machine learning algorithms to create better and more accurate models;
       however, little focus is placed on exploring the language use of a group
       of individuals defined by nationality. Individuals of the same nationality
       share certain practices and communicate certain ideas that can become
       embedded into their natural language. Many nationals are also not limited
       to speaking just one language, such as how Filipinos speak Filipino and
       English, the two national languages of the Philippines. The addition of
       several regional/indigenous languages, along with the commonness of
       code-switching, allow for a Filipino to have a rich vocabulary. This
       presents an opportunity to create a text-based personality model based on
       how Filipinos speak, regardless of the language they use. To do so, data
       was collected from 250 Filipino Twitter users. Different combinations of
       data processing techniques were experimented upon to create personality
       models for each of the Big Five. The results for both regression and
       classification show that Conscientiousness is consistently the easiest
       trait to model, followed by Extraversion. Classification models for
       Agreeableness and Neuroticism had subpar performances, but performed
       better than those of Openness. An analysis on personality trait score
       representation showed that classifying extreme outliers generally produce
       better results for all traits except for Neuroticism and Openness.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1115</url>
     <doi>10.18653/v1/W18-1115</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>tighe-cheng:2018:W18-11</bibkey>
   </paper>
   <paper id="1116">
     <title>Grounding the Semantics of Part-of-Day Nouns Worldwide using Twitter</title>
     <author>
       <first>David</first>
       <last>Vilares</last>
     </author>
     <author>
       <first>Carlos</first>
       <last>Gómez-Rodríguez</last>
     </author>
     <booktitle>
       Proceedings of the Second Workshop on Computational Modeling of People’s
       Opinions, Personality, and Emotions in Social Media
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>123–128</pages>
     <abstract>
       The usage of part-of-day nouns, such as ’night’, and their time-specific
       greetings (’good night’), varies across languages and cultures. We show
       the possibilities that Twitter offers for studying the semantics of these
       terms and its variability between countries. We mine a worldwide sample of
       multilingual tweets with temporal greetings, and study how their
       frequencies vary in relation with local time. The results provide insights
       into the semantics of these temporal expressions and the cultural and
       sociological factors influencing their usage.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1116</url>
     <doi>10.18653/v1/W18-1116</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>vilares-gmezrodrguez:2018:W18-11</bibkey>
   </paper>
   <paper id="1200">
     <title>Proceedings of the Second Workshop on Subword/Character LEvel Models</title>
     <editor>
       <first>Manaal</first>
       <last>Faruqui</last>
     </editor>
     <editor>
       <first>Hinrich</first>
       <last>Schütze</last>
     </editor>
     <editor>
       <first>Isabel</first>
       <last>Trancoso</last>
     </editor>
     <editor>
       <first>Yulia</first>
       <last>Tsvetkov</last>
     </editor>
     <editor>
       <first>Yadollah</first>
       <last>Yaghoobzadeh</last>
     </editor>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-12</url>
     <doi>10.18653/v1/W18-12</doi>
     <bibtype>book</bibtype>
     <bibkey>W18-12:2018</bibkey>
   </paper>
   <paper id="1201">
     <title>
       Morphological Word Embeddings for Arabic Neural Machine Translation in
       Low-Resource Settings
     </title>
     <author>
       <first>Pamela</first>
       <last>Shapiro</last>
     </author>
     <author>
       <first>Kevin</first>
       <last>Duh</last>
     </author>
     <booktitle>Proceedings of the Second Workshop on Subword/Character LEvel Models</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–11</pages>
     <abstract>
       Neural machine translation has achieved impressive results in the last few
       years, but its success has been limited to settings with large amounts of
       parallel data. One way to improve NMT for lower-resource settings is to
       initialize a word-based NMT model with pretrained word embeddings.
       However, rare words still suffer from lower quality word embeddings when
       trained with standard word-level objectives. We introduce word embeddings
       that utilize morphological resources, and compare to purely unsupervised
       alternatives. We work with Arabic, a morphologically rich language with
       available linguistic resources, and perform Ar-to-En MT experiments on a
       small corpus of TED subtitles. We find that word embeddings utilizing
       subword information consistently outperform standard word embeddings on a
       word similarity task and as initialization of the source word embeddings
       in a low-resource NMT system.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1201</url>
     <doi>10.18653/v1/W18-1201</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>shapiro-duh:2018:W18-12</bibkey>
   </paper>
   <paper id="1202">
     <title>Entropy-Based Subword Mining with an Application to Word Embeddings</title>
     <author>
       <first>Ahmed</first>
       <last>El-Kishky</last>
     </author>
     <author>
       <first>Frank</first>
       <last>Xu</last>
     </author>
     <author>
       <first>Aston</first>
       <last>Zhang</last>
     </author>
     <author>
       <first>Stephen</first>
       <last>Macke</last>
     </author>
     <author>
       <first>Jiawei</first>
       <last>Han</last>
     </author>
     <booktitle>Proceedings of the Second Workshop on Subword/Character LEvel Models</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>12–21</pages>
     <abstract>
       Recent literature has shown a wide variety of benefits to mapping
       traditional one-hot representations of words and phrases to
       lower-dimensional real-valued vectors known as word embeddings.
       Traditionally, most word embedding algorithms treat each word as the
       finest meaningful semantic granularity and perform embedding by learning
       distinct embedding vectors for each word. Contrary to this line of
       thought, technical domains such as scientific and medical literature
       compose words from subword structures such as prefixes, suffixes, and
       root-words as well as compound words. Treating individual words as the
       finest-granularity unit discards meaningful shared semantic structure
       between words sharing substructures. This not only leads to poor
       embeddings for text corpora that have long-tail distributions, but also
       heuristic methods for handling out-of-vocabulary words. In this paper we
       propose SubwordMine, an entropy-based subword mining algorithm that is
       fast, unsupervised, and fully data-driven. We show that this allows for
       great cross-domain performance in identifying semantically meaningful
       subwords. We then investigate utilizing the mined subwords within the
       FastText embedding model and compare performance of the learned
       representations in a downstream language modeling task.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1202</url>
     <doi>10.18653/v1/W18-1202</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>elkishky-EtAl:2018:W18-12</bibkey>
   </paper>
   <paper id="1203">
     <title>
       A Comparison of Character Neural Language Model and Bootstrapping for
       Language Identification in Multilingual Noisy Texts
     </title>
     <author>
       <first>Wafia</first>
       <last>Adouane</last>
     </author>
     <author>
       <first>Simon</first>
       <last>Dobnik</last>
     </author>
     <author>
       <first>Jean-Philippe</first>
       <last>Bernardy</last>
     </author>
     <author>
       <first>Nasredine</first>
       <last>Semmar</last>
     </author>
     <booktitle>Proceedings of the Second Workshop on Subword/Character LEvel Models</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>22–31</pages>
     <abstract>
       This paper seeks to examine the effect of including background knowledge
       in the form of character pre-trained neural language model (LM), and data
       bootstrapping to overcome the problem of unbalanced limited resources. As
       a test, we explore the task of language identification in mixed-language
       short non-edited texts with an under-resourced language, namely the case
       of Algerian Arabic for which both labelled and unlabelled data are
       limited. We compare the performance of two traditional machine learning
       methods and a deep neural networks (DNNs) model. The results show that
       overall DNNs perform better on labelled data for the majority categories
       and struggle with the minority ones. While the effect of the untokenised
       and unlabelled data encoded as LM differs for each category,
       bootstrapping, however, improves the performance of all systems and all
       categories. These methods are language independent and could be
       generalised to other under-resourced languages for which a small labelled
       data and a larger unlabelled data are available.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1203</url>
     <doi>10.18653/v1/W18-1203</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>adouane-EtAl:2018:W18-12</bibkey>
   </paper>
   <paper id="1204">
     <title>Addressing Low-Resource Scenarios with Character-aware Embeddings</title>
     <author>
       <first>Sean</first>
       <last>Papay</last>
     </author>
     <author>
       <first>Sebastian</first>
       <last>Padó</last>
     </author>
     <author>
       <first>Ngoc Thang</first>
       <last>Vu</last>
     </author>
     <booktitle>Proceedings of the Second Workshop on Subword/Character LEvel Models</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>32–37</pages>
     <abstract>
       Most modern approaches to computing word embeddings assume the
       availability of text corpora with billions of words. In this paper, we
       explore a setup where only corpora with millions of words are available,
       and many words in any new text are out of vocabulary. This setup is both
       of practical interests – modeling the situation for specific domains and
       low-resource languages – and of psycholinguistic interest, since it
       corresponds much more closely to the actual experiences and challenges of
       human language learning and use. We compare standard skip-gram word
       embeddings with character-based embeddings on word relatedness prediction.
       Skip-grams excel on large corpora, while character-based embeddings do
       well on small corpora generally and rare and complex words specifically.
       The models can be combined easily.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1204</url>
     <doi>10.18653/v1/W18-1204</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>papay-pad-vu:2018:W18-12</bibkey>
   </paper>
   <paper id="1205">
     <title>Subword-level Composition Functions for Learning Word Embeddings</title>
     <author>
       <first>Bofang</first>
       <last>Li</last>
     </author>
     <author>
       <first>Aleksandr</first>
       <last>Drozd</last>
     </author>
     <author>
       <first>Tao</first>
       <last>Liu</last>
     </author>
     <author>
       <first>Xiaoyong</first>
       <last>Du</last>
     </author>
     <booktitle>Proceedings of the Second Workshop on Subword/Character LEvel Models</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>38–48</pages>
     <abstract>
       Subword-level information is crucial for capturing the meaning and
       morphology of words, especially for out-of-vocabulary entries. We propose
       CNN- and RNN-based subword-level composition functions for learning word
       embeddings, and systematically compare them with popular word-level and
       subword-level models (Skip-Gram and FastText). Additionally, we propose a
       hybrid training scheme in which a pure subword-level model is trained
       jointly with a conventional word-level embedding model based on
       lookup-tables. This increases the fitness of all types of subword-level
       word embeddings; the word-level embeddings can be discarded after
       training, leaving only compact subword-level representation with much
       smaller data volume. We evaluate these embeddings on a set of intrinsic
       and extrinsic tasks, showing that subword-level models have advantage on
       tasks related to morphology and datasets with high OOV rate, and can be
       combined with other types of embeddings.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1205</url>
     <doi>10.18653/v1/W18-1205</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>li-EtAl:2018:W18-12</bibkey>
   </paper>
   <paper id="1206">
     <title>Discovering Phonesthemes with Sparse Regularization</title>
     <author>
       <first>Nelson F.</first>
       <last>Liu</last>
     </author>
     <author>
       <first>Gina-Anne</first>
       <last>Levow</last>
     </author>
     <author>
       <first>Noah A.</first>
       <last>Smith</last>
     </author>
     <booktitle>Proceedings of the Second Workshop on Subword/Character LEvel Models</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>49–54</pages>
     <abstract>
       We introduce a simple method for extracting non-arbitrary form-meaning
       representations from a collection of semantic vectors. We treat the
       problem as one of feature selection for a model trained to predict word
       vectors from subword features. We apply this model to the problem of
       automatically discovering phonesthemes, which are submorphemic sound
       clusters that appear in words with similar meaning. Many of our
       model-predicted phonesthemes overlap with those proposed in the
       linguistics literature, and we validate our approach with human judgments.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1206</url>
     <doi>10.18653/v1/W18-1206</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>liu-levow-smith:2018:W18-12</bibkey>
   </paper>
   <paper id="1207">
     <title>Meaningless yet meaningful: Morphology grounded subword-level NMT</title>
     <author>
       <first>Tamali</first>
       <last>Banerjee</last>
     </author>
     <author>
       <first>Pushpak</first>
       <last>Bhattacharyya</last>
     </author>
     <booktitle>Proceedings of the Second Workshop on Subword/Character LEvel Models</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>55–60</pages>
     <abstract>
       We explore the use of two independent subsystems Byte Pair Encoding (BPE)
       and Morfessor as basic units for subword-level neural machine translation
       (NMT). We show that, for linguistically distant language-pairs
       Morfessor-based segmentation algorithm produces significantly better
       quality translation than BPE. However, for close language-pairs BPE-based
       subword-NMT may translate better than Morfessor-based subword-NMT. We
       propose a combined approach of these two segmentation algorithms
       Morfessor-BPE (M-BPE) which outperforms these two baseline systems in
       terms of BLEU score. Our results are supported by experiments on three
       language-pairs: English-Hindi, Bengali-Hindi and English-Bengali.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1207</url>
     <doi>10.18653/v1/W18-1207</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>banerjee-bhattacharyya:2018:W18-12</bibkey>
   </paper>
   <paper id="1208">
     <title>Fast Query Expansion on an Accounting Corpus using Sub-Word Embeddings</title>
     <author>
       <first>Hrishikesh</first>
       <last>Ganu</last>
     </author>
     <author>
       <first>Viswa Datha</first>
       <last>P</last>
     </author>
     <booktitle>Proceedings of the Second Workshop on Subword/Character LEvel Models</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>61–65</pages>
     <abstract> 

We present early results from a system under development which uses sub-word embeddings for query expansion in presence of mis-spelled words and other aberrations. We work for a company which creates accounting software and the end goal is to improve customer experience when they search for help on our “Customer Care” portal. Our customers use colloquial language, non-standard acronyms and sometimes mis-spell words when they use our Search portal or interact over other channels. However, our Knowledge Base has curated content which leverages technical terms and is in language which is quite formal. This results in the answer not being retrieved even though the answer might actually be present in the documentation (as assessed by a human). We address this problem by creating equivalence classes of words with similar meanings (with the additional property that the mappings to these equivalence classes are robust to mis-spellings) using sub-word embeddings and then use them to fine tune an Elasticsearch index to improve recall. We demonstrate through an end-end system that using sub-word embeddings leads to a significant lift in correct answers retrieved for an accounting corpus available in the public domain. </abstract>
     <url>http://www.aclweb.org/anthology/W18-1208</url>
     <doi>10.18653/v1/W18-1208</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>ganu-p:2018:W18-12</bibkey>
   </paper>
   <paper id="1209">
     <title>Incorporating Subword Information into Matrix Factorization Word Embeddings</title>
     <author>
       <first>Alexandre</first>
       <last>Salle</last>
     </author>
     <author>
       <first>Aline</first>
       <last>Villavicencio</last>
     </author>
     <booktitle>Proceedings of the Second Workshop on Subword/Character LEvel Models</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>66–71</pages>
     <abstract>
       The positive effect of adding subword information to word embeddings has
       been demonstrated for predictive models. In this paper we investigate
       whether similar benefits can also be derived from incorporating subwords
       into counting models. We evaluate the impact of different types of
       subwords (n-grams and unsupervised morphemes), with results confirming the
       importance of subword information in learning representations of rare and
       out-of-vocabulary words.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1209</url>
     <doi>10.18653/v1/W18-1209</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>salle-villavicencio:2018:W18-12</bibkey>
   </paper>
   <paper id="1210">
     <title>A Multi-Context Character Prediction Model for a Brain-Computer Interface</title>
     <author>
       <first>Shiran</first>
       <last>Dudy</last>
     </author>
     <author>
       <first>Shaobin</first>
       <last>Xu</last>
     </author>
     <author>
       <first>Steven</first>
       <last>Bedrick</last>
     </author>
     <author>
       <first>David</first>
       <last>Smith</last>
     </author>
     <booktitle>Proceedings of the Second Workshop on Subword/Character LEvel Models</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>72–77</pages>
     <abstract>
       Brain-computer interfaces and other augmentative and alternative
       communication devices introduce language-modeing challenges distinct from
       other character-entry methods. In particular, the acquired signal of the
       EEG (electroencephalogram) signal is noisier, which, in turn, makes the
       user intent harder to decipher. In order to adapt to this condition, we
       propose to maintain ambiguous history for every time step, and to employ,
       apart from the character language model, word information to produce a
       more robust prediction system. We present preliminary results that compare
       this proposed Online-Context Language Model (OCLM) to current algorithms
       that are used in this type of setting. Evaluation on both perplexity and
       predictive accuracy demonstrates promising results when dealing with
       ambiguous histories in order to provide to the front end a distribution of
       the next character the user might type.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1210</url>
     <doi>10.18653/v1/W18-1210</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>dudy-EtAl:2018:W18-12</bibkey>
   </paper>
   <paper id="1300">
     <title>Proceedings of the Workshop on Computational Semantics beyond Events and Roles</title>
     <editor>
       <first>Eduardo</first>
       <last>Blanco</last>
     </editor>
     <editor>
       <first>Roser</first>
       <last>Morante</last>
     </editor>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-13</url>
     <doi>10.18653/v1/W18-13</doi>
     <bibtype>book</bibtype>
     <bibkey>W18-13:2018</bibkey>
   </paper>
   <paper id="1301">
     <title>Using Hedge Detection to Improve Committed Belief Tagging</title>
     <author>
       <first>Morgan</first>
       <last>Ulinski</last>
     </author>
     <author>
       <first>Seth</first>
       <last>Benjamin</last>
     </author>
     <author>
       <first>Julia</first>
       <last>Hirschberg</last>
     </author>
     <booktitle>Proceedings of the Workshop on Computational Semantics beyond Events and Roles</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–5</pages>
     <abstract>
       We describe a novel method for identifying hedge terms using a set of
       manually constructed rules. We present experiments adding hedge features
       to a committed belief system to improve classification. We compare
       performance of this system (a) without hedging features, (b) with
       dictionary-based features, and (c) with rule-based features. We find that
       using hedge features improves performance of the committed belief system,
       particularly in identifying instances of non-committed belief and reported
       belief.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1301</url>
     <doi>10.18653/v1/W18-1301</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>ulinski-benjamin-hirschberg:2018:W18-13</bibkey>
   </paper>
   <paper id="1302">
     <title>
       Paths for uncertainty: Exploring the intricacies of uncertainty
       identification for news
     </title>
     <author>
       <first>Chrysoula</first>
       <last>Zerva</last>
     </author>
     <author>
       <first>Sophia</first>
       <last>Ananiadou</last>
     </author>
     <booktitle>Proceedings of the Workshop on Computational Semantics beyond Events and Roles</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>6–20</pages>
     <abstract>
       Currently, news articles are produced, shared and consumed at an extremely
       rapid rate. Although their quantity is increasing, at the same time, their
       quality and trustworthiness is becoming fuzzier. Hence, it is important
       not only to automate information extraction but also to quantify the
       certainty of this information. Automated identification of certainty has
       been studied both in the scientific and newswire domains, but performance
       is considerably higher in tasks focusing on scientific text. We compare
       the differences in the definition and expression of uncertainty between a
       scientific domain, i.e., biomedicine, and newswire. We delve into the
       different aspects that affect the certainty of an extracted event in a
       news article and examine whether they can be easily identified by
       techniques already validated in the biomedical domain. Finally, we present
       a comparison of the syntactic and lexical differences between the the
       expression of certainty in the biomedical and newswire domains, using two
       annotated corpora.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1302</url>
     <doi>10.18653/v1/W18-1302</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>zerva-ananiadou:2018:W18-13</bibkey>
   </paper>
   <paper id="1303">
     <title>Detecting Sarcasm is Extremely Easy ;-)</title>
     <author>
       <first>Natalie</first>
       <last>Parde</last>
     </author>
     <author>
       <first>Rodney</first>
       <last>Nielsen</last>
     </author>
     <booktitle>Proceedings of the Workshop on Computational Semantics beyond Events and Roles</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>21–26</pages>
     <abstract>
       Detecting sarcasm in text is a particularly challenging problem in
       computational semantics, and its solution may vary across different types
       of text. We analyze the performance of a domain-general sarcasm detection
       system on datasets from two very different domains: Twitter, and Amazon
       product reviews. We categorize the errors that we identify with each, and
       make recommendations for addressing these issues in NLP systems in the
       future.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1303</url>
     <doi>10.18653/v1/W18-1303</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>parde-nielsen:2018:W18-13</bibkey>
   </paper>
   <paper id="1304">
     <title>GKR: the Graphical Knowledge Representation for semantic parsing</title>
     <author>
       <first>Aikaterini-Lida</first>
       <last>Kalouli</last>
     </author>
     <author>
       <first>Richard</first>
       <last>Crouch</last>
     </author>
     <booktitle>Proceedings of the Workshop on Computational Semantics beyond Events and Roles</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>27–37</pages>
     <abstract>
       This paper describes the first version of an open-source semantic parser
       that creates graphical representations of sentences to be used for further
       semantic processing, e.g. for natural language inference, reasoning and
       semantic similarity. The Graphical Knowledge Representation which is
       output by the parser is inspired by the Abstract Knowledge Representation,
       which separates out conceptual and contextual levels of representation
       that deal respectively with the subject matter of a sentence and its
       existential commitments. Our representation is a layered graph with each
       sub-graph holding different kinds of information, including one sub-graph
       for concepts and one for contexts. Our first evaluation of the system
       shows an F-score of 85% in accurately representing sentences as semantic
       graphs.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1304</url>
     <doi>10.18653/v1/W18-1304</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>kalouli-crouch:2018:W18-13</bibkey>
   </paper>
   <paper id="1305">
     <title>
       Computational Argumentation: A Journey Beyond Semantics, Logic, Opinions,
       and Easy Tasks
     </title>
     <author>
       <first>Ivan</first>
       <last>Habernal</last>
     </author>
     <booktitle>Proceedings of the Workshop on Computational Semantics beyond Events and Roles</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>38</pages>
     <abstract>
       The classical view on argumentation, such that arguments are logical
       structures consisting of different distinguishable parts and that parties
       exchange arguments in a rational way, is prevalent in textbooks but
       nonexistent in the real world. Instead, argumentation is a multifaceted
       communication tool built upon humans’ capabilities to easily use common
       sense, emotions, and social context. As humans, we are pretty good at it.
       Computational Argumentation tries to tackle these phenomena but has a long
       and not so easy way to go. In this talk, I would like to shed a light on
       several recent attempts to deal with argumentation computationally, such
       as addressing argument quality, understanding argument reasoning, dealing
       with fallacies, and how should we never ever argue online.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1305</url>
     <doi>10.18653/v1/W18-1305</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>habernal:2018:W18-13</bibkey>
   </paper>
   <paper id="1400">
     <title>
       Proceedings of the First International Workshop on Spatial Language
       Understanding
     </title>
     <editor>
       <first>Parisa</first>
       <last>Kordjamshidi</last>
     </editor>
     <editor>
       <first>Archna</first>
       <last>Bhatia</last>
     </editor>
     <editor>
       <first>James</first>
       <last>Pustejovsky</last>
     </editor>
     <editor>
       <first>Marie-Francine</first>
       <last>Moens</last>
     </editor>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-14</url>
     <doi>10.18653/v1/W18-14</doi>
     <bibtype>book</bibtype>
     <bibkey>W18-14:2018</bibkey>
   </paper>
   <paper id="1401">
     <title>
       Exploring the Functional and Geometric Bias of Spatial Relations Using
       Neural Language Models
     </title>
     <author>
       <first>Simon</first>
       <last>Dobnik</last>
     </author>
     <author>
       <first>Mehdi</first>
       <last>Ghanimifard</last>
     </author>
     <author>
       <first>John</first>
       <last>Kelleher</last>
     </author>
     <booktitle>
       Proceedings of the First International Workshop on Spatial Language
       Understanding
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–11</pages>
     <abstract>
       The challenge for computational models of spatial descriptions for
       situated dialogue systems is the integration of information from different
       modalities. The semantics of spatial descriptions are grounded in at least
       two sources of information: (i) a geometric representation of space and
       (ii) the functional interaction of related objects that. We train several
       neural language models on descriptions of scenes from a dataset of image
       captions and examine whether the functional or geometric bias of spatial
       descriptions reported in the literature is reflected in the estimated
       perplexity of these models. The results of these experiments have
       implications for the creation of models of spatial lexical semantics for
       human-robot dialogue systems. Furthermore, they also provide an insight
       into the kinds of the semantic knowledge captured by neural language
       models trained on spatial descriptions, which has implications for image
       captioning systems.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1401</url>
     <doi>10.18653/v1/W18-1401</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>dobnik-ghanimifard-kelleher:2018:W18-14</bibkey>
   </paper>
   <paper id="1402">
     <title>
       Building and Learning Structures in a Situated Blocks World Through Deep
       Language Understanding
     </title>
     <author>
       <first>Ian</first>
       <last>Perera</last>
     </author>
     <author>
       <first>James</first>
       <last>Allen</last>
     </author>
     <author>
       <first>Choh Man</first>
       <last>Teng</last>
     </author>
     <author>
       <first>Lucian</first>
       <last>Galescu</last>
     </author>
     <booktitle>
       Proceedings of the First International Workshop on Spatial Language
       Understanding
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>12–20</pages>
     <abstract>
       We demonstrate a system for understanding natural language utterances for
       structure description and placement in a situated blocks world context. By
       relying on a rich, domain-specific adaptation of a generic ontology and a
       logical form structure produced by a semantic parser, we obviate the need
       for an intermediate, domain-specific representation and can produce a
       reasoner that grounds and reasons over concepts and constraints with
       real-valued data. This linguistic base enables more flexibility in
       interpreting natural language expressions invoking intrinsic concepts and
       features of structures and space. We demonstrate some of the capabilities
       of a system grounded in deep language understanding and present initial
       results in a structure learning task.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1402</url>
     <doi>10.18653/v1/W18-1402</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>perera-EtAl:2018:W18-14</bibkey>
   </paper>
   <paper id="1403">
     <title>Computational Models for Spatial Prepositions</title>
     <author>
       <first>Georgiy</first>
       <last>Platonov</last>
     </author>
     <author>
       <first>Lenhart</first>
       <last>Schubert</last>
     </author>
     <booktitle>
       Proceedings of the First International Workshop on Spatial Language
       Understanding
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>21–30</pages>
     <abstract> 

Developing computational models of spatial prepositions (such as on, in, above, etc.) is crucial for such tasks as human-machine collaboration, story understanding, and 3D model generation from descriptions. However, these prepositions are notoriously vague and ambiguous, with meanings depending on the types, shapes and sizes of entities in the argument positions, the physical and task context, and other factors. As a result truth value judgments for prepositional relations are often uncertain and variable. In this paper we treat the modeling task as calling for assignment of probabilities to such relations as a function of multiple factors, where such probabilities can be viewed as estimates of whether humans would judge the relations to hold in given circumstances. We implemented our models in a 3D blocks world and a room world in a computer graphics setting, and found that true/false judgments based on these models do not differ much more from human judgments that the latter differ from one another. However, what really matters pragmatically is not the accuracy of truth value judgments but whether, for instance, the computer models suffice for identifying objects described in terms of prepositional relations, (e.g., “the box to the left of the table”, where there are multiple boxes). For such tasks, our models achieved accuracies above 90% for most relations. </abstract>
     <url>http://www.aclweb.org/anthology/W18-1403</url>
     <doi>10.18653/v1/W18-1403</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>platonov-schubert:2018:W18-14</bibkey>
   </paper>
   <paper id="1404">
     <title> 

Lexical Conceptual Structure of Literal and Metaphorical Spatial Language: A Case Study of “Push” </title>
     <author>
       <first>Bonnie</first>
       <last>Dorr</last>
     </author>
     <author>
       <first>Mari</first>
       <last>Olsen</last>
     </author>
     <booktitle>
       Proceedings of the First International Workshop on Spatial Language
       Understanding
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>31–40</pages>
     <abstract> 

Prior methodologies for understanding spatial language have treated literal expressions such as “Mary pushed the car over the edge” differently from metaphorical extensions such as “Mary’s job pushed her over the edge”. We demonstrate a methodology for standardizing literal and metaphorical meanings, by building on work in Lexical Conceptual Structure (LCS), a general-purpose representational component used in machine translation. We argue that spatial predicates naturally extend into other fields (e.g., circumstantial or temporal), and that LCS provides both a framework for distinguishing spatial from non-spatial, and a system for finding metaphorical meaning extensions. We start with MetaNet (MN), a large repository of conceptual metaphors, condensing 197 spatial entries into sixteen top-level categories of motion frames. Using naturally occurring instances of English push , and expansions of MN frames, we demonstrate that literal and metaphorical extensions exhibit patterns predicted and represented by the LCS model. </abstract>
     <url>http://www.aclweb.org/anthology/W18-1404</url>
     <doi>10.18653/v1/W18-1404</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>dorr-olsen:2018:W18-14</bibkey>
   </paper>
   <paper id="1405">
     <title>Representing Spatial Relations in FrameNet</title>
     <author>
       <first>Miriam R L</first>
       <last>Petruck</last>
     </author>
     <author>
       <first>Michael J</first>
       <last>Ellsworth</last>
     </author>
     <booktitle>
       Proceedings of the First International Workshop on Spatial Language
       Understanding
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>41–45</pages>
     <abstract>
       While humans use natural language to express spatial relations between and
       across entities in the world with great facility, natural language systems
       have a facility that depends on that human facility. This position paper
       presents approach to representing spatial relations in language, and
       advocates its adoption for representing the meaning of spatial language.
       This work shows the importance of axis-orientation systems for capturing
       the complexity of spatial relations, which FrameNet encodes with semantic
       types.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1405</url>
     <doi>10.18653/v1/W18-1405</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>petruck-ellsworth:2018:W18-14</bibkey>
   </paper>
   <paper id="1406">
     <title>
       Points, Paths, and Playscapes: Large-scale Spatial Language Understanding
       Tasks Set in the Real World
     </title>
     <author>
       <first>Jason</first>
       <last>Baldridge</last>
     </author>
     <author>
       <first>Tania</first>
       <last>Bedrax-Weiss</last>
     </author>
     <author>
       <first>Daphne</first>
       <last>Luong</last>
     </author>
     <author>
       <first>Srini</first>
       <last>Narayanan</last>
     </author>
     <author>
       <first>Bo</first>
       <last>Pang</last>
     </author>
     <author>
       <first>Fernando</first>
       <last>Pereira</last>
     </author>
     <author>
       <first>Radu</first>
       <last>Soricut</last>
     </author>
     <author>
       <first>Michael</first>
       <last>Tseng</last>
     </author>
     <author>
       <first>Yuan</first>
       <last>Zhang</last>
     </author>
     <booktitle>
       Proceedings of the First International Workshop on Spatial Language
       Understanding
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>46–52</pages>
     <abstract>
       Spatial language understanding is important for practical applications and
       as a building block for better abstract language understanding. Much
       progress has been made through work on understanding spatial relations and
       values in images and texts as well as on giving and following navigation
       instructions in restricted domains. We argue that the next big advances in
       spatial language understanding can be best supported by creating
       large-scale datasets that focus on points and paths based in the real
       world, and then extending these to create online, persistent playscapes
       that mix human and bot players, where the bot players must learn, evolve,
       and survive according to their depth of understanding of scenes,
       navigation, and interactions.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1406</url>
     <revision id="2">W18-1406v2</revision>
     <doi>10.18653/v1/W18-1406</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>baldridge-EtAl:2018:W18-14</bibkey>
   </paper>
   <paper id="1407">
     <title>Anaphora Resolution for Improving Spatial Relation Extraction from Text</title>
     <author>
       <first>Umar</first>
       <last>Manzoor</last>
     </author>
     <author>
       <first>Parisa</first>
       <last>Kordjamshidi</last>
     </author>
     <booktitle>
       Proceedings of the First International Workshop on Spatial Language
       Understanding
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>53–62</pages>
     <abstract>
       Spatial relation extraction from generic text is a challenging problem due
       to the ambiguity of the prepositions spatial meaning as well as the
       nesting structure of the spatial descriptions. In this work, we highlight
       the difficulties that the anaphora can make in the extraction of spatial
       relations. We use external multi-modal (here visual) resources to find the
       most probable candidates for resolving the anaphoras that refer to the
       landmarks of the spatial relations. We then use global inference to decide
       jointly on resolving the anaphora and extraction of the spatial relations.
       Our preliminary results show that resolving anaphora improves the
       state-of-the-art results on spatial relation extraction.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1407</url>
     <doi>10.18653/v1/W18-1407</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>manzoor-kordjamshidi:2018:W18-14</bibkey>
   </paper>
   <paper id="1408">
     <title>The Case for Systematically Derived Spatial Language Usage</title>
     <author>
       <first>Bonnie</first>
       <last>Dorr</last>
     </author>
     <author>
       <first>Clare</first>
       <last>Voss</last>
     </author>
     <booktitle>
       Proceedings of the First International Workshop on Spatial Language
       Understanding
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>63–70</pages>
     <abstract>
       This position paper argues that, while prior work in spatial language
       understanding for tasks such as robot navigation focuses on mapping
       natural language into deep conceptual or non-linguistic representations,
       it is possible to systematically derive regular patterns of spatial
       language usage from existing lexical-semantic resources. Furthermore, even
       with access to such resources, effective solutions to many application
       areas such as robot navigation and narrative generation also require
       additional knowledge at the syntax-semantics interface to cover the wide
       range of spatial expressions observed and available to natural language
       speakers. We ground our insights in, and present our extensions to, an
       existing lexico-semantic resource, covering 500 semantic classes of verbs,
       of which 219 fall within a spatial subset. We demonstrate that these
       extensions enable systematic derivation of regular patterns of spatial
       language without requiring manual annotation.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1408</url>
     <doi>10.18653/v1/W18-1408</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>dorr-voss:2018:W18-14</bibkey>
   </paper>
   <paper id="1500">
     <title>Proceedings of the First Workshop on Storytelling</title>
     <editor>
       <first>Margaret</first>
       <last>Mitchell</last>
     </editor>
     <editor>
       <first>Ting-Hao ‘Kenneth’</first>
       <last>Huang</last>
     </editor>
     <editor>
       <first>Francis</first>
       <last>Ferraro</last>
     </editor>
     <editor>
       <first>Ishan</first>
       <last>Misra</last>
     </editor>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-15</url>
     <revision id="2">W18-1500v2</revision>
     <doi>10.18653/v1/W18-15</doi>
     <bibtype>book</bibtype>
     <bibkey>W18-15:2018</bibkey>
   </paper>
   <paper id="1501">
     <title>
       Learning to Listen: Critically Considering the Role of AI in Human
       Storytelling and Character Creation
     </title>
     <author>
       <first>Anna</first>
       <last>Kasunic</last>
     </author>
     <author>
       <first>Geoff</first>
       <last>Kaufman</last>
     </author>
     <booktitle>Proceedings of the First Workshop on Storytelling</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–13</pages>
     <abstract>
       In this opinion piece, we argue that there is a need for alternative
       design directions to complement existing AI efforts in narrative and
       character generation and algorithm development. To make our argument, we
       a) outline the predominant roles and goals of AI research in storytelling;
       b) present existing discourse on the benefits and harms of narratives; and
       c) highlight the pain points in character creation revealed by
       semi-structured interviews we conducted with 14 individuals deeply
       involved in some form of character creation. We conclude by proffering
       several specific design avenues that we believe can seed fruitful research
       collaborations. In our vision, AI collaborates with humans during creative
       processes and narrative generation, helps amplify voices and perspectives
       that are currently marginalized or misrepresented, and engenders
       experiences of narrative that support spectatorship and listening roles.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1501</url>
     <doi>10.18653/v1/W18-1501</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>kasunic-kaufman:2018:W18-15</bibkey>
   </paper>
   <paper id="1502">
     <title>Linguistic Features of Helpfulness in Automated Support for Creative Writing</title>
     <author>
       <first>Melissa</first>
       <last>Roemmele</last>
     </author>
     <author>
       <first>Andrew</first>
       <last>Gordon</last>
     </author>
     <booktitle>Proceedings of the First Workshop on Storytelling</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>14–19</pages>
     <abstract> 

We examine an emerging NLP application that supports creative writing by automatically suggesting continuing sentences in a story. The application tracks users’ modifications to generated sentences, which can be used to quantify their “helpfulness” in advancing the story. We explore the task of predicting helpfulness based on automatically detected linguistic features of the suggestions. We illustrate this analysis on a set of user interactions with the application using an initial selection of features relevant to story generation. </abstract>
     <url>http://www.aclweb.org/anthology/W18-1502</url>
     <doi>10.18653/v1/W18-1502</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>roemmele-gordon:2018:W18-151</bibkey>
   </paper>
   <paper id="1503">
     <title>A Pipeline for Creative Visual Storytelling</title>
     <author>
       <first>Stephanie</first>
       <last>Lukin</last>
     </author>
     <author>
       <first>Reginald</first>
       <last>Hobbs</last>
     </author>
     <author>
       <first>Clare</first>
       <last>Voss</last>
     </author>
     <booktitle>Proceedings of the First Workshop on Storytelling</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>20–32</pages>
     <abstract>
       Computational visual storytelling produces a textual description of events
       and interpretations depicted in a sequence of images. These texts are made
       possible by advances and cross-disciplinary approaches in natural language
       processing, generation, and computer vision. We define a computational
       creative visual storytelling as one with the ability to alter the telling
       of a story along three aspects: to speak about different environments, to
       produce variations based on narrative goals, and to adapt the narrative to
       the audience. These aspects of creative storytelling and their effect on
       the narrative have yet to be explored in visual storytelling. This paper
       presents a pipeline of task-modules, Object Identification, Single-Image
       Inferencing, and Multi-Image Narration, that serve as a preliminary design
       for building a creative visual storyteller. We have piloted this design
       for a sequence of images in an annotation task. We present and analyze the
       collected corpus and describe plans towards automation.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1503</url>
     <doi>10.18653/v1/W18-1503</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>lukin-hobbs-voss:2018:W18-15</bibkey>
   </paper>
   <paper id="1504">
     <title>Telling Stories with Soundtracks: An Empirical Analysis of Music in Film</title>
     <author>
       <first>Jon</first>
       <last>Gillick</last>
     </author>
     <author>
       <first>David</first>
       <last>Bamman</last>
     </author>
     <booktitle>Proceedings of the First Workshop on Storytelling</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>33–42</pages>
     <abstract>
       Soundtracks play an important role in carrying the story of a film. In
       this work, we collect a corpus of movies and television shows matched with
       subtitles and soundtracks and analyze the relationship between story,
       song, and audience reception. We look at the content of a film through the
       lens of its latent topics and at the content of a song through descriptors
       of its musical attributes. In two experiments, we find first that
       individual topics are strongly associated with musical attributes, and
       second, that musical attributes of soundtracks are predictive of film
       ratings, even after controlling for topic and genre.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1504</url>
     <doi>10.18653/v1/W18-1504</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>gillick-bamman:2018:W18-15</bibkey>
   </paper>
   <paper id="1505">
     <title>Towards Controllable Story Generation</title>
     <author>
       <first>Nanyun</first>
       <last>Peng</last>
     </author>
     <author>
       <first>Marjan</first>
       <last>Ghazvininejad</last>
     </author>
     <author>
       <first>Jonathan</first>
       <last>May</last>
     </author>
     <author>
       <first>Kevin</first>
       <last>Knight</last>
     </author>
     <booktitle>Proceedings of the First Workshop on Storytelling</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>43–49</pages>
     <abstract>
       We present a general framework of analyzing existing story corpora to
       generate controllable and creative new stories. The proposed framework
       needs little manual annotation to achieve controllable story generation.
       It creates a new interface for humans to interact with computers to
       generate personalized stories. We apply the framework to build recurrent
       neural network (RNN)-based generation models to control story ending
       valence and storyline. Experiments show that our methods successfully
       achieve the control and enhance the coherence of stories through
       introducing storylines. with additional control factors, the generation
       model gets lower perplexity, and yields more coherent stories that are
       faithful to the control factors according to human evaluation.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1505</url>
     <doi>10.18653/v1/W18-1505</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>peng-EtAl:2018:W18-15</bibkey>
   </paper>
   <paper id="1506">
     <title>An Encoder-decoder Approach to Predicting Causal Relations in Stories</title>
     <author>
       <first>Melissa</first>
       <last>Roemmele</last>
     </author>
     <author>
       <first>Andrew</first>
       <last>Gordon</last>
     </author>
     <booktitle>Proceedings of the First Workshop on Storytelling</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>50–59</pages>
     <abstract>
       We address the task of predicting causally related events in stories
       according to a standard evaluation framework, the Choice of Plausible
       Alternatives (COPA). We present a neural encoder-decoder model that learns
       to predict relations between adjacent sequences in stories as a means of
       modeling causality. We explore this approach using different methods for
       extracting and representing sequence pairs as well as different model
       architectures. We also compare the impact of different training datasets
       on our model. In particular, we demonstrate the usefulness of a corpus not
       previously applied to COPA, the ROCStories corpus. While not
       state-of-the-art, our results establish a new reference point for systems
       evaluated on COPA, and one that is particularly informative for future
       neural-based approaches.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1506</url>
     <doi>10.18653/v1/W18-1506</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>roemmele-gordon:2018:W18-152</bibkey>
   </paper>
   <paper id="1507">
     <title>Neural Event Extraction from Movies Description</title>
     <author>
       <first>Alex</first>
       <last>Tozzo</last>
     </author>
     <author>
       <first>Dejan</first>
       <last>Jovanovic</last>
     </author>
     <author>
       <first>Mohamed</first>
       <last>Amer</last>
     </author>
     <booktitle>Proceedings of the First Workshop on Storytelling</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>60–66</pages>
     <abstract>
       We present a novel approach for event extraction and abstraction from
       movie descriptions. Our event frame consists of ‘who”, “did what” “to
       whom”, “where”, and “when”. We formulate our problem using a recurrent
       neural network, enhanced with structural features extracted from syntactic
       parser, and trained using curriculum learning by progressively increasing
       the difficulty of the sentences. Our model serves as an intermediate step
       towards question answering systems, visual storytelling, and story
       completion tasks. We evaluate our approach on MovieQA dataset.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1507</url>
     <doi>10.18653/v1/W18-1507</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>tozzo-jovanovic-amer:2018:W18-15</bibkey>
   </paper>
   <paper id="1600">
     <title>Proceedings of the Second Workshop on Stylistic Variation</title>
     <editor>
       <first>Julian</first>
       <last>Brooke</last>
     </editor>
     <editor>
       <first>Lucie</first>
       <last>Flekova</last>
     </editor>
     <editor>
       <first>Moshe</first>
       <last>Koppel</last>
     </editor>
     <editor>
       <first>Thamar</first>
       <last>Solorio</last>
     </editor>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-16</url>
     <doi>10.18653/v1/W18-16</doi>
     <bibtype>book</bibtype>
     <bibkey>W18-16:2018</bibkey>
   </paper>
   <paper id="1601">
     <title>
       Stylistic variation over 200 years of court proceedings according to
       gender and social class
     </title>
     <author>
       <first>Stefania</first>
       <last>Degaetano-Ortlieb</last>
     </author>
     <booktitle>Proceedings of the Second Workshop on Stylistic Variation</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–10</pages>
     <abstract>
       We present an approach to detect stylistic variation across social
       variables (here: gender and social class), considering also diachronic
       change in language use. For detection of stylistic variation, we use
       relative entropy, measuring the difference between probability
       distributions at different linguistic levels (here: lexis and grammar). In
       addition, by relative entropy, we can determine which linguistic units are
       related to stylistic variation.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1601</url>
     <doi>10.18653/v1/W18-1601</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>degaetanoortlieb:2018:W18-16</bibkey>
   </paper>
   <paper id="1602">
     <title>Stylistic Variation in Social Media Part-of-Speech Tagging</title>
     <author>
       <first>Murali Raghu Babu</first>
       <last>Balusu</last>
     </author>
     <author>
       <first>Taha</first>
       <last>Merghani</last>
     </author>
     <author>
       <first>Jacob</first>
       <last>Eisenstein</last>
     </author>
     <booktitle>Proceedings of the Second Workshop on Stylistic Variation</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>11–19</pages>
     <abstract>
       Social media features substantial stylistic variation, raising new
       challenges for syntactic analysis of online writing. However, this
       variation is often aligned with author attributes such as age, gender, and
       geography, as well as more readily-available social network metadata. In
       this paper, we report new evidence on the link between language and social
       networks in the task of part-of-speech tagging. We find that tagger error
       rates are correlated with network structure, with high accuracy in some
       parts of the network, and lower accuracy elsewhere. As a result, tagger
       accuracy depends on training from a balanced sample of the network, rather
       than training on texts from a narrow subcommunity. We also describe our
       attempts to add robustness to stylistic variation, by building a
       mixture-of-experts model in which each expert is associated with a region
       of the social network. While prior work found that similar approaches
       yield performance improvements in sentiment analysis and entity linking,
       we were unable to obtain performance improvements in part-of-speech
       tagging, despite strong evidence for the link between part-of-speech error
       rates and social network structure.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1602</url>
     <doi>10.18653/v1/W18-1602</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>balusu-merghani-eisenstein:2018:W18-16</bibkey>
   </paper>
   <paper id="1603">
     <title>Detecting Syntactic Features of Translated Chinese</title>
     <author>
       <first>Hai</first>
       <last>Hu</last>
     </author>
     <author>
       <first>Wen</first>
       <last>Li</last>
     </author>
     <author>
       <first>Sandra</first>
       <last>Kübler</last>
     </author>
     <booktitle>Proceedings of the Second Workshop on Stylistic Variation</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>20–28</pages>
     <abstract> 

We present a machine learning approach to distinguish texts translated to Chinese (by humans) from texts originally written in Chinese, with a focus on a wide range of syntactic features. Using Support Vector Machines (SVMs) as classifier on a genre-balanced corpus in translation studies of Chinese, we find that constituent parse trees and dependency triples as features without lexical information perform very well on the task, with an F-measure above 90%, close to the results of lexical n-gram features, without the risk of learning topic information rather than translation features. Thus, we claim syntactic features alone can accurately distinguish translated from original Chinese. Translated Chinese exhibits an increased use of determiners, subject position pronouns, NP + “的” as NP modifiers, multiple NPs or VPs conjoined by "、", among other structures. We also interpret the syntactic features with reference to previous translation studies in Chinese, particularly the usage of pronouns. </abstract>
     <url>http://www.aclweb.org/anthology/W18-1603</url>
     <doi>10.18653/v1/W18-1603</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>hu-li-kbler:2018:W18-16</bibkey>
   </paper>
   <paper id="1604">
     <title>Evaluating Creative Language Generation: The Case of Rap Lyric Ghostwriting</title>
     <author>
       <first>Peter</first>
       <last>Potash</last>
     </author>
     <author>
       <first>Alexey</first>
       <last>Romanov</last>
     </author>
     <author>
       <first>Anna</first>
       <last>Rumshisky</last>
     </author>
     <booktitle>Proceedings of the Second Workshop on Stylistic Variation</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>29–38</pages>
     <abstract>
       Language generation tasks that seek to mimic human ability to use language
       creatively are difficult to evaluate, since one must consider creativity,
       style, and other non-trivial aspects of the generated text. The goal of
       this paper is to develop evaluations methods for one such task,
       ghostwriting of rap lyrics, and to provide an explicit, quantifiable
       foundation for the goals and future directions for this task. Ghostwriting
       must produce text that is similar in style to the emulated artist, yet
       distinct in content. We develop a novel evaluation methodology that
       addresses several complementary aspects of this task, and illustrate how
       such evaluation can be used to meaning fully analyze system performance.
       We provide a corpus of lyrics for 13 rap artists, annotated for stylistic
       similarity, which allows us to assess the feasibility of manual evaluation
       for generated verse.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1604</url>
     <doi>10.18653/v1/W18-1604</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>potash-romanov-rumshisky:2018:W18-16</bibkey>
   </paper>
   <paper id="1605">
     <title>Cross-corpus Native Language Identification via Statistical Embedding</title>
     <author>
       <first>Francisco</first>
       <last>Rangel</last>
     </author>
     <author>
       <first>Paolo</first>
       <last>Rosso</last>
     </author>
     <author>
       <first>Julian</first>
       <last>Brooke</last>
     </author>
     <author>
       <first>Alexandra</first>
       <last>Uitdenbogerd</last>
     </author>
     <booktitle>Proceedings of the Second Workshop on Stylistic Variation</booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>39–43</pages>
     <abstract>
       In this paper, we approach the task of native language identification in a
       realistic cross-corpus scenario where a model is trained with available
       data and has to predict the native language from data of a different
       corpus. The motivation behind this study is to investigate native language
       identification in the Australian academic scenario where a majority of
       students come from China, Indonesia, and Arabic-speaking nations. We have
       proposed a statistical embedding representation reporting a significant
       improvement over common single-layer approaches of the state of the art,
       identifying Chinese, Arabic, and Indonesian in a cross-corpus scenario.
       The proposed approach was shown to be competitive even when the data is
       scarce and imbalanced.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1605</url>
     <doi>10.18653/v1/W18-1605</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>rangel-EtAl:2018:W18-16</bibkey>
   </paper>
   <paper id="1700">
     <title>
       Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural
       Language Processing (TextGraphs-12)
     </title>
     <editor>
       <first>Goran</first>
       <last>Glavaš</last>
     </editor>
     <editor>
       <first>Swapna</first>
       <last>Somasundaran</last>
     </editor>
     <editor>
       <first>Martin</first>
       <last>Riedl</last>
     </editor>
     <editor>
       <first>Eduard</first>
       <last>Hovy</last>
     </editor>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-17</url>
     <doi>10.18653/v1/W18-17</doi>
     <bibtype>book</bibtype>
     <bibkey>W18-17:2018</bibkey>
   </paper>
   <paper id="1701">
     <title>Scientific Discovery as Link Prediction in Influence and Citation Graphs</title>
     <author>
       <first>Fan</first>
       <last>Luo</last>
     </author>
     <author>
       <first>Marco A.</first>
       <last>Valenzuela-Escárcega</last>
     </author>
     <author>
       <first>Gus</first>
       <last>Hahn-Powell</last>
     </author>
     <author>
       <first>Mihai</first>
       <last>Surdeanu</last>
     </author>
     <booktitle>
       Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural
       Language Processing (TextGraphs-12)
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–6</pages>
     <abstract>
       We introduce a machine learning approach for the identification of “white
       spaces” in scientific knowledge. Our approach addresses this task as link
       prediction over a graph that contains over 2M influence statements such as
       “CTCF activates FOXA1”, which were automatically extracted using
       open-domain machine reading. We model this prediction task using
       graph-based features extracted from the above influence graph, as well as
       from a citation graph that captures scientific communities. We evaluated
       the proposed approach through backtesting. Although the data is heavily
       unbalanced (50 times more negative examples than positives), our approach
       predicts which influence links will be discovered in the “near future”
       with a F1 score of 27 points, and a mean average precision of 68%.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1701</url>
     <doi>10.18653/v1/W18-1701</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>luo-EtAl:2018:W18-17</bibkey>
   </paper>
   <paper id="1702">
     <title>
       Efficient Generation and Processing of Word Co-occurrence Networks Using
       corpus2graph
     </title>
     <author>
       <first>Zheng</first>
       <last>Zhang</last>
     </author>
     <author>
       <first>Pierre</first>
       <last>Zweigenbaum</last>
     </author>
     <author>
       <first>Ruiqing</first>
       <last>Yin</last>
     </author>
     <booktitle>
       Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural
       Language Processing (TextGraphs-12)
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>7–11</pages>
     <abstract>
       Corpus2graph is an open-source NLP-application-oriented tool that
       generates a word co-occurrence network from a large corpus. It not only
       contains different built-in methods to preprocess words, analyze
       sentences, extract word pairs and define edge weights, but also supports
       user-customized functions. By using parallelization techniques, it can
       generate a large word co-occurrence network of the whole English Wikipedia
       data within hours. And thanks to its nodes-edges-weight three-level
       progressive calculation design, rebuilding networks with different
       configurations is even faster as it does not need to start all over again.
       This tool also works with other graph libraries such as igraph, NetworkX
       and graph-tool as a front end providing data to boost network generation
       speed.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1702</url>
     <doi>10.18653/v1/W18-1702</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>zhang-zweigenbaum-yin:2018:W18-17</bibkey>
   </paper>
   <paper id="1703">
     <title>
       Multi-hop Inference for Sentence-level TextGraphs: How Challenging is
       Meaningfully Combining Information for Science Question Answering?
     </title>
     <author>
       <first>Peter</first>
       <last>Jansen</last>
     </author>
     <booktitle>
       Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural
       Language Processing (TextGraphs-12)
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>12–17</pages>
     <abstract> 

Question Answering for complex questions is often modelled as a graph construction or traversal task, where a solver must build or traverse a graph of facts that answer and explain a given question. This “multi-hop” inference has been shown to be extremely challenging, with few models able to aggregate more than two facts before being overwhelmed by “semantic drift”, or the tendency for long chains of facts to quickly drift off topic. This is a major barrier to current inference models, as even elementary science questions require an average of 4 to 6 facts to answer and explain. In this work we empirically characterize the difficulty of building or traversing a graph of sentences connected by lexical overlap, by evaluating chance sentence aggregation quality through 9,784 manually-annotated judgements across knowledge graphs built from three free-text corpora (including study guides and Simple Wikipedia). We demonstrate semantic drift tends to be high and aggregation quality low, at between 0.04 and 3, and highlight scenarios that maximize the likelihood of meaningfully combining information. </abstract>
     <url>http://www.aclweb.org/anthology/W18-1703</url>
     <doi>10.18653/v1/W18-1703</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>jansen:2018:W18-17</bibkey>
   </paper>
   <paper id="1704">
     <title>
       Multi-Sentence Compression with Word Vertex-Labeled Graphs and Integer
       Linear Programming
     </title>
     <author>
       <first>Elvys</first>
       <last>Linhares Pontes</last>
     </author>
     <author>
       <first>Stéphane</first>
       <last>Huet</last>
     </author>
     <author>
       <first>Thiago</first>
       <last>Gouveia da Silva</last>
     </author>
     <author>
       <first>Andréa carneiro</first>
       <last>Linhares</last>
     </author>
     <author>
       <first>Juan-Manuel</first>
       <last>Torres-Moreno</last>
     </author>
     <booktitle>
       Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural
       Language Processing (TextGraphs-12)
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>18–27</pages>
     <abstract>
       Multi-Sentence Compression (MSC) aims to generate a short sentence with
       key information from a cluster of closely related sentences. MSC enables
       summarization and question-answering systems to generate outputs combining
       fully formed sentences from one or several documents. This paper describes
       a new Integer Linear Programming method for MSC using a vertex-labeled
       graph to select different keywords, and novel 3-gram scores to generate
       more informative sentences while maintaining their grammaticality. Our
       system is of good quality and outperforms the state-of-the-art for
       evaluations led on news dataset. We led both automatic and manual
       evaluations to determine the informativeness and the grammaticality of
       compressions for each dataset. Additional tests, which take advantage of
       the fact that the length of compressions can be modulated, still improve
       ROUGE scores with shorter output sentences.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1704</url>
     <doi>10.18653/v1/W18-1704</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>linharespontes-EtAl:2018:W18-17</bibkey>
   </paper>
   <paper id="1705">
     <title>
       Large-scale spectral clustering using diffusion coordinates on
       landmark-based bipartite graphs
     </title>
     <author>
       <first>Khiem</first>
       <last>Pham</last>
     </author>
     <author>
       <first>Guangliang</first>
       <last>Chen</last>
     </author>
     <booktitle>
       Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural
       Language Processing (TextGraphs-12)
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>28–37</pages>
     <abstract>
       Spectral clustering has received a lot of attention due to its ability to
       separate nonconvex, non-intersecting manifolds, but its high computational
       complexity has significantly limited its applicability. Motivated by the
       document-term co-clustering framework by Dhillon (2001), we propose a
       landmark-based scalable spectral clustering approach in which we first use
       the selected landmark set and the given data to form a bipartite graph and
       then run a diffusion process on it to obtain a family of diffusion
       coordinates for clustering. We show that our proposed algorithm can be
       implemented based on very efficient operations on the affinity matrix
       between the given data and selected landmarks, thus capable of handling
       large data. Finally, we demonstrate the excellent performance of our
       method by comparing with the state-of-the-art scalable algorithms on
       several benchmark data sets.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1705</url>
     <doi>10.18653/v1/W18-1705</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>pham-chen:2018:W18-17</bibkey>
   </paper>
   <paper id="1706">
     <title>
       Efficient Graph-based Word Sense Induction by Distributional Inclusion
       Vector Embeddings
     </title>
     <author>
       <first>Haw-Shiuan</first>
       <last>Chang</last>
     </author>
     <author>
       <first>Amol</first>
       <last>Agrawal</last>
     </author>
     <author>
       <first>Ananya</first>
       <last>Ganesh</last>
     </author>
     <author>
       <first>Anirudha</first>
       <last>Desai</last>
     </author>
     <author>
       <first>Vinayak</first>
       <last>Mathur</last>
     </author>
     <author>
       <first>Alfred</first>
       <last>Hough</last>
     </author>
     <author>
       <first>Andrew</first>
       <last>McCallum</last>
     </author>
     <booktitle>
       Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural
       Language Processing (TextGraphs-12)
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>38–48</pages>
     <abstract>
       Word sense induction (WSI), which addresses polysemy by unsupervised
       discovery of multiple word senses, resolves ambiguities for downstream NLP
       tasks and also makes word representations more interpretable. This paper
       proposes an accurate and efficient graph-based method for WSI that builds
       a global non-negative vector embedding basis (which are interpretable like
       topics) and clusters the basis indexes in the ego network of each
       polysemous word. By adopting distributional inclusion vector embeddings as
       our basis formation model, we avoid the expensive step of nearest neighbor
       search that plagues other graph-based methods without sacrificing the
       quality of sense clusters. Experiments on three datasets show that our
       proposed method produces similar or better sense clusters and embeddings
       compared with previous state-of-the-art methods while being significantly
       more efficient.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1706</url>
     <doi>10.18653/v1/W18-1706</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>chang-EtAl:2018:W18-17</bibkey>
   </paper>
   <paper id="1707">
     <title>
       Fusing Document, Collection and Label Graph-based Representations with
       Word Embeddings for Text Classification
     </title>
     <author>
       <first>Konstantinos</first>
       <last>Skianis</last>
     </author>
     <author>
       <first>Fragkiskos</first>
       <last>Malliaros</last>
     </author>
     <author>
       <first>Michalis</first>
       <last>Vazirgiannis</last>
     </author>
     <booktitle>
       Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural
       Language Processing (TextGraphs-12)
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>49–58</pages>
     <abstract>
       Contrary to the traditional Bag-of-Words approach, we consider the
       Graph-of-Words(GoW) model in which each document is represented by a graph
       that encodes relationships between the different terms. Based on this
       formulation, the importance of a term is determined by weighting the
       corresponding node in the document, collection and label graphs, using
       node centrality criteria. We also introduce novel graph-based weighting
       schemes by enriching graphs with word-embedding similarities, in order to
       reward or penalize semantic relationships. Our methods produce more
       discriminative feature weights for text categorization, outperforming
       existing frequency-based criteria.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1707</url>
     <doi>10.18653/v1/W18-1707</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>skianis-malliaros-vazirgiannis:2018:W18-17</bibkey>
   </paper>
   <paper id="1708">
     <title>Embedding Text in Hyperbolic Spaces</title>
     <author>
       <first>Bhuwan</first>
       <last>Dhingra</last>
     </author>
     <author>
       <first>Christopher</first>
       <last>Shallue</last>
     </author>
     <author>
       <first>Mohammad</first>
       <last>Norouzi</last>
     </author>
     <author>
       <first>Andrew</first>
       <last>Dai</last>
     </author>
     <author>
       <first>George</first>
       <last>Dahl</last>
     </author>
     <booktitle>
       Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural
       Language Processing (TextGraphs-12)
     </booktitle>
     <month>June</month>
     <year>2018</year>
     <address>New Orleans, Louisiana, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>59–69</pages>
     <abstract>
       Natural language text exhibits hierarchical structure in a variety of
       respects. Ideally, we could incorporate our prior knowledge of this
       hierarchical structure into unsupervised learning algorithms that work on
       text data. Recent work by Nickel and Kiela (2017) proposed using hyperbolic
       instead of Euclidean embedding spaces to represent hierarchical data and
       demonstrated encouraging results when embedding graphs. In this work, we
       extend their method with a re-parameterization technique that allows us to
       learn hyperbolic embeddings of arbitrarily parameterized objects. We apply
       this framework to learn word and sentence embeddings in hyperbolic space
       in an unsupervised manner from text corpora. The resulting embeddings seem
       to encode certain intuitive notions of hierarchy, such as word-context
       frequency and phrase constituency. However, the implicit continuous
       hierarchy in the learned hyperbolic space makes interrogating the model’s
       learned hierarchies more difficult than for models that learn explicit
       edges between items. The learned hyperbolic embeddings show improvements
       over Euclidean embeddings in some – but not all – downstream tasks,
       suggesting that hierarchical organization is more useful for some tasks
       than others.
     </abstract>
     <url>http://www.aclweb.org/anthology/W18-1708</url>
     <doi>10.18653/v1/W18-1708</doi>
     <bibtype>inproceedings</bibtype>
     <bibkey>dhingra-EtAl:2018:W18-17</bibkey>
   </paper>

	 <paper id="1800">
		 <title>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)</title>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <url>http://www.aclweb.org/anthology/W18-1800</url>
		 <bibtype>book</bibtype>
		 <bibkey>2018:AMTA</bibkey>
		 <editor><first>Colin</first><last>Cherry</last></editor>
		 <editor><first>Graham</first><last>Neubig</last></editor>
	 </paper>
	 <paper id="1801">
		 <title>Keynote: Unveiling the Linguistic Weaknesses of Neural MT</title>
		 <author>
			 <first>Arianna</first>
			 <last>Bisazza</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <url>http://www.aclweb.org/anthology/W18-1801</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Bisazza:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1802">
		 <title>Document-Level Information as Side Constraints for Improved Neural Patent Translation</title>
		 <author>
			 <first>Laura</first>
			 <last>Jehl</last>
		 </author>
		 <author>
			 <first>Stefan</first>
			 <last>Riezler</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>1–12</pages>
		 <url>http://www.aclweb.org/anthology/W18-1802</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Jehl-Riezler:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1803">
		 <title>Fluency Over Adequacy: A Pilot Study in Measuring User Trust in Imperfect MT</title>
		 <author>
			 <first>Marianna</first>
			 <last>Martindale</last>
		 </author>
		 <author>
			 <first>Marine</first>
			 <last>Carpuat</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>13–25</pages>
		 <url>http://www.aclweb.org/anthology/W18-1803</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Martindale-Carpuat:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1804">
		 <title>Combining Quality Estimation and Automatic Post-editing to Enhance Machine Translation output</title>
		 <author>
			 <first>Rajen</first>
			 <last>Chatterjee</last>
		 </author>
		 <author>
			 <first>Matteo</first>
			 <last>Negri</last>
		 </author>
		 <author>
			 <first>Marco</first>
			 <last>Turchi</last>
		 </author>
		 <author>
			 <first>Frédéric</first>
			 <last>Blain</last>
		 </author>
		 <author>
			 <first>Lucia</first>
			 <last>Specia</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>26–38</pages>
		 <url>http://www.aclweb.org/anthology/W18-1804</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Chatterjee-Negri-Turchi-Blain-Specia:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1805">
		 <title>Neural Morphological Tagging of Lemma Sequences for Machine Translation</title>
		 <author>
			 <first>Costanza</first>
			 <last>Conforti</last>
		 </author>
		 <author>
			 <first>Matthias</first>
			 <last>Huck</last>
		 </author>
		 <author>
			 <first>Alexander</first>
			 <last>Fraser</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>39–53</pages>
		 <url>http://www.aclweb.org/anthology/W18-1805</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Conforti-Huck-Fraser:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1806">
		 <title>Context Models for OOV Word Translation in Low-Resource Languages</title>
		 <author>
			 <first>Angli</first>
			 <last>Liu</last>
		 </author>
		 <author>
			 <first>Katrin</first>
			 <last>Kirchhoff</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>54–67</pages>
		 <url>http://www.aclweb.org/anthology/W18-1806</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Liu-Kirchhoff:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1807">
		 <title>How Robust Are Character-Based Word Embeddings in Tagging and MT Against Wrod Scramlbing or Randdm Nouse?</title>
		 <author>
			 <first>Georg</first>
			 <last>Heigold</last>
		 </author>
		 <author>
			 <first>Stalin</first>
			 <last>Varanasi</last>
		 </author>
		 <author>
			 <first>Günter</first>
			 <last>Neumann</last>
		 </author>
		 <author>
			 <first>Josef</first>
			 <last>Genabith</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>68–80</pages>
		 <url>http://www.aclweb.org/anthology/W18-1807</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Heigold-Varanasi-Neumann-Genabith:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1808">
		 <title>Balancing Translation Quality and Sentiment Preservation (Non-archival Extended Abstract)</title>
		 <author>
			 <first>Pintu</first>
			 <last>Lohar</last>
		 </author>
		 <author>
			 <first>Haithem</first>
			 <last>Afli</last>
		 </author>
		 <author>
			 <first>Andy</first>
			 <last>Way</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>81–88</pages>
		 <url>http://www.aclweb.org/anthology/W18-1808</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Lohar-Afli-Way:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1809">
		 <title>Register-sensitive Translation: a Case Study of Mandarin and Cantonese (Non-archival Extended Abstract)</title>
		 <author>
			 <first>Tak-sum</first>
			 <last>Wong</last>
		 </author>
		 <author>
			 <first>John</first>
			 <last>Lee</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>89–96</pages>
		 <url>http://www.aclweb.org/anthology/W18-1809</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Wong-Lee:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1810">
		 <title>An Evaluation of Two Vocabulary Reduction Methods for Neural Machine Translation</title>
		 <author>
			 <first>Duygu</first>
			 <last>Ataman</last>
		 </author>
		 <author>
			 <first>Marcello</first>
			 <last>Federico</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>97–110</pages>
		 <url>http://www.aclweb.org/anthology/W18-1810</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Ataman-Federico:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1811">
		 <title>A Smorgasbord of Features to Combine Phrase-Based and Neural Machine Translation</title>
		 <author>
			 <first>Benjamin</first>
			 <last>Marie</last>
		 </author>
		 <author>
			 <first>Atsushi</first>
			 <last>Fujita</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>111–124</pages>
		 <url>http://www.aclweb.org/anthology/W18-1811</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Marie-Fujita:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1812">
		 <title>Exploring Word Sense Disambiguation Abilities of Neural Machine Translation Systems (Non-archival Extended Abstract)</title>
		 <author>
			 <first>Rebecca</first>
			 <last>Marvin</last>
		 </author>
		 <author>
			 <first>Philipp</first>
			 <last>Koehn</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>125–131</pages>
		 <url>http://www.aclweb.org/anthology/W18-1812</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Marvin-Koehn:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1813">
		 <title>Improving Low Resource Machine Translation using Morphological Glosses (Non-archival Extended Abstract)</title>
		 <author>
			 <first>Steven</first>
			 <last>Shearing</last>
		 </author>
		 <author>
			 <first>Christo</first>
			 <last>Kirov</last>
		 </author>
		 <author>
			 <first>Huda</first>
			 <last>Khayrallah</last>
		 </author>
		 <author>
			 <first>David</first>
			 <last>Yarowsky</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>132–139</pages>
		 <url>http://www.aclweb.org/anthology/W18-1813</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Shearing-Kirov-Khayrallah-Yarowsky:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1814">
		 <title>A Dataset and Reranking Method for Multimodal MT of User-Generated Image Captions</title>
		 <author>
			 <first>Shigehiko</first>
			 <last>Schamoni</last>
		 </author>
		 <author>
			 <first>Julian</first>
			 <last>Hitschler</last>
		 </author>
		 <author>
			 <first>Stefan</first>
			 <last>Riezler</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>140–153</pages>
		 <url>http://www.aclweb.org/anthology/W18-1814</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Schamoni-Hitschler-Riezler:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1815">
		 <title>Simultaneous Translation using Optimized Segmentation</title>
		 <author>
			 <first>Maryam</first>
			 <last>Siahbani</last>
		 </author>
		 <author>
			 <first>Hassan</first>
			 <last>Shavarani</last>
		 </author>
		 <author>
			 <first>Ashkan</first>
			 <last>Alinejad</last>
		 </author>
		 <author>
			 <first>Anoop</first>
			 <last>Sarkar</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>154–167</pages>
		 <url>http://www.aclweb.org/anthology/W18-1815</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Siahbani-Shavarani-Alinejad-Sarkar:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1816">
		 <title>Neural Monkey: The Current State and Beyond</title>
		 <author>
			 <first>Jindřich</first>
			 <last>Helcl</last>
		 </author>
		 <author>
			 <first>Jindřich</first>
			 <last>Libovický</last>
		 </author>
		 <author>
			 <first>Tom</first>
			 <last>Kocmi</last>
		 </author>
		 <author>
			 <first>Tomáš</first>
			 <last>Musil</last>
		 </author>
		 <author>
			 <first>Ondřej</first>
			 <last>Cífka</last>
		 </author>
		 <author>
			 <first>Dusan</first>
			 <last>Varis</last>
		 </author>
		 <author>
			 <first>Ondřej</first>
			 <last>Bojar</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>168–176</pages>
		 <url>http://www.aclweb.org/anthology/W18-1816</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Helcl-Libovický-Kocmi-Musil-Cífka-Varis-Bojar:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1817">
		 <title>OpenNMT: Neural Machine Translation Toolkit</title>
		 <author>
			 <first>Guillaume</first>
			 <last>Klein</last>
		 </author>
		 <author>
			 <first>Yoon</first>
			 <last>Kim</last>
		 </author>
		 <author>
			 <first>Yuntian</first>
			 <last>Deng</last>
		 </author>
		 <author>
			 <first>Vincent</first>
			 <last>Nguyen</last>
		 </author>
		 <author>
			 <first>Jean</first>
			 <last>Senellart</last>
		 </author>
		 <author>
			 <first>Alexander</first>
			 <last>Rush</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>177–184</pages>
		 <url>http://www.aclweb.org/anthology/W18-1817</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Klein-Kim-Deng-Nguyen-Senellart-Rush:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1818">
		 <title>XNMT: The eXtensible Neural Machine Translation Toolkit</title>
		 <author>
			 <first>Graham</first>
			 <last>Neubig</last>
		 </author>
		 <author>
			 <first>Matthias</first>
			 <last>Sperber</last>
		 </author>
		 <author>
			 <first>Xinyi</first>
			 <last>Wang</last>
		 </author>
		 <author>
			 <first>Matthieu</first>
			 <last>Felix</last>
		 </author>
		 <author>
			 <first>Austin</first>
			 <last>Matthews</last>
		 </author>
		 <author>
			 <first>Sarguna</first>
			 <last>Padmanabhan</last>
		 </author>
		 <author>
			 <first>Ye</first>
			 <last>Qi</last>
		 </author>
		 <author>
			 <first>Devendra</first>
			 <last>Sachan</last>
		 </author>
		 <author>
			 <first>Philip</first>
			 <last>Arthur</last>
		 </author>
		 <author>
			 <first>Pierre</first>
			 <last>Godard</last>
		 </author>
		 <author>
			 <first>John</first>
			 <last>Hewitt</last>
		 </author>
		 <author>
			 <first>Rachid</first>
			 <last>Riad</last>
		 </author>
		 <author>
			 <first>Liming</first>
			 <last>Wang</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>185–192</pages>
		 <url>http://www.aclweb.org/anthology/W18-1818</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Neubig-Sperber-Wang-Felix-Matthews-Padmanabhan-Qi-Sachan-Arthur-Godard-Hewitt-Riad-Wang:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1819">
		 <title>Tensor2Tensor for Neural Machine Translation</title>
		 <author>
			 <first>Ashish</first>
			 <last>Vaswani</last>
		 </author>
		 <author>
			 <first>Samy</first>
			 <last>Bengio</last>
		 </author>
		 <author>
			 <first>Eugene</first>
			 <last>Brevdo</last>
		 </author>
		 <author>
			 <first>Francois</first>
			 <last>Chollet</last>
		 </author>
		 <author>
			 <first>Aidan</first>
			 <last>Gomez</last>
		 </author>
		 <author>
			 <first>Stephan</first>
			 <last>Gouws</last>
		 </author>
		 <author>
			 <first>Llion</first>
			 <last>Jones</last>
		 </author>
		 <author>
			 <first>Łukasz</first>
			 <last>Kaiser</last>
		 </author>
		 <author>
			 <first>Nal</first>
			 <last>Kalchbrenner</last>
		 </author>
		 <author>
			 <first>Niki</first>
			 <last>Parmar</last>
		 </author>
		 <author>
			 <first>Ryan</first>
			 <last>Sepassi</last>
		 </author>
		 <author>
			 <first>Noam</first>
			 <last>Shazeer</last>
		 </author>
		 <author>
			 <first>Jakob</first>
			 <last>Uszkoreit</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>193–199</pages>
		 <url>http://www.aclweb.org/anthology/W18-1819</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Vaswani-Bengio-Brevdo-Chollet-Gomez-Gouws-Jones-Kaiser-Kalchbrenner-Parmar-Sepassi-Shazeer-Uszkoreit:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1820">
		 <title>The Sockeye Neural Machine Translation Toolkit at AMTA 2018</title>
		 <author>
			 <first>Felix</first>
			 <last>Hieber</last>
		 </author>
		 <author>
			 <first>Tobias</first>
			 <last>Domhan</last>
		 </author>
		 <author>
			 <first>Michael</first>
			 <last>Denkowski</last>
		 </author>
		 <author>
			 <first>David</first>
			 <last>Vilar</last>
		 </author>
		 <author>
			 <first>Artem</first>
			 <last>Sokolov</last>
		 </author>
		 <author>
			 <first>Ann</first>
			 <last>Clifton</last>
		 </author>
		 <author>
			 <first>Matt</first>
			 <last>Post</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>200–207</pages>
		 <url>http://www.aclweb.org/anthology/W18-1820</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Hieber-Domhan-Denkowski-Vilar-Sokolov-Clifton-Post:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1821">
		 <title>Why not be Versatile? Applications of the SGNMT Decoder for Machine Translation</title>
		 <author>
			 <first>Felix</first>
			 <last>Stahlberg</last>
		 </author>
		 <author>
			 <first>Danielle</first>
			 <last>Saunders</last>
		 </author>
		 <author>
			 <first>Gonzalo</first>
			 <last>Iglesias</last>
		 </author>
		 <author>
			 <first>Bill</first>
			 <last>Byrne</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>208–216</pages>
		 <url>http://www.aclweb.org/anthology/W18-1821</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Stahlberg-Saunders-Iglesias-Byrne:2018:AMTA</bibkey>
	 </paper>

	 <paper id="1900">
		 <title>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</title>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1900</url>
		 <bibtype>book</bibtype>
		 <bibkey>2018:AMTA</bibkey>
		 <editor><first>Janice</first><last>Campbell</last></editor>
		 <editor><first>Alex</first><last>Yanishevsky</last></editor>
		 <editor><first>Jennifer</first><last>Doyon</last></editor>
		 <editor><first>Doug</first><last>Jones</last></editor>
	 </paper>
	 <paper id="1901">
		 <title>Keynote: Machine Translation Beyond the Sentence</title>
		 <author>
			 <first>Macduff</first>
			 <last>Hughes</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1901</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Hughes:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1902">
		 <title>Keynote: Setting up a Machine Translation Program for IARPA</title>
		 <author>
			 <first>Carl</first>
			 <last>Rubino</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1902</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Rubino:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1903">
		 <title>Keynote: Use more Machine Translation and Keep Your Customers Happy</title>
		 <author>
			 <first>Glen</first>
			 <last>Poor</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1903</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Poor:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1904">
		 <title>Technology Showcase and Presentations</title>
		 <author>
			 <first>Jennifer</first>
			 <last>DeCamp</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>1–4</pages>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1904</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>DeCamp:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1905">
		 <title>Augmented Translation: A New Approach to Combining Human and Machine Capabilities</title>
		 <author>
			 <first>Arle</first>
			 <last>Lommel</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>5–12</pages>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1905</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Lommel:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1906">
		 <title>Training, feedback and productivity measurement with NMT and Adaptive MT</title>
		 <author>
			 <first>Jean-Luc</first>
			 <last>Saillard</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>13–27</pages>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1906</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Saillard:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1907">
		 <title>The Collision of Quality and Technology with Reality</title>
		 <author>
			 <first>Don</first>
			 <last>DePalma</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>28–34</pages>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1907</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>DePalma:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1908">
		 <title>Same-language machine translation for local flavours/flavors</title>
		 <author>
			 <first>Gema</first>
			 <last>Ramírez-Sánchez</last>
		 </author>
		 <author>
			 <first>Janice</first>
			 <last>Campbell</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>35–53</pages>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1908</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Ramírez-Sánchez-Campbell:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1909">
		 <title>Thinking of Going Neural? Factors Honda R&amp;D Americas is Considering before Making the Switch</title>
		 <author>
			 <first>Phil</first>
			 <last>Soldini</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>54–71</pages>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1909</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Soldini:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1910">
		 <title>Developing a Neural Machine Translation Service for the 2017-2018 European Union Presidency</title>
		 <author>
			 <first>Mārcis</first>
			 <last>Pinnis</last>
		 </author>
		 <author>
			 <first>Rihards</first>
			 <last>Kalnins</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>72–83</pages>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1910</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Pinnis-Kalnins:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1911">
		 <title>Neural Won! Now What?</title>
		 <author>
			 <first>Alex</first>
			 <last>Yanishevsky</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>84–112</pages>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1911</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Yanishevsky:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1912">
		 <title>MT for L10n: How we build and evaluate MT systems at eBay</title>
		 <author>
			 <first>José</first>
			 <last>Sánchez</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>113–149</pages>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1912</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Sánchez:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1913">
		 <title>VMware MT Tiered Model</title>
		 <author>
			 <first>Lynn</first>
			 <last>Ma</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>150–165</pages>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1913</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Ma:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1914">
		 <title>Turning NMT Research into Commercial Products</title>
		 <author>
			 <first>Dragos</first>
			 <last>Munteanu</last>
		 </author>
		 <author>
			 <first>Adrià</first>
			 <last>Gispert</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>166–193</pages>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1914</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Munteanu-Gispert:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1915">
		 <title>Beyond Quality, Considerations for an MT solution</title>
		 <author>
			 <first>Quinn</first>
			 <last>Lam</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>194–199</pages>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1915</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Lam:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1916">
		 <title>Towards Less Post-Editing</title>
		 <author>
			 <first>Bill</first>
			 <last>Lafferty</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>200–222</pages>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1916</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Lafferty:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1917">
		 <title>Leveraging Data Resources for Cross-Linguistic Information Retrieval Using Statistical Machine Translation</title>
		 <author>
			 <first>Steve</first>
			 <last>Sloto</last>
		 </author>
		 <author>
			 <first>Ann</first>
			 <last>Clifton</last>
		 </author>
		 <author>
			 <first>Greg</first>
			 <last>Hanneman</last>
		 </author>
		 <author>
			 <first>Patrick</first>
			 <last>Porter</last>
		 </author>
		 <author>
			 <first>Donna</first>
			 <last>Gates</last>
		 </author>
		 <author>
			 <first>Almut</first>
			 <last>Hildebrand</last>
		 </author>
		 <author>
			 <first>Anish</first>
			 <last>Kumar</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>223–233</pages>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1917</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Sloto-Clifton-Hanneman-Porter-Gates-Hildebrand-Kumar:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1918">
		 <title>The Impact of Advances in Neural and Statistical MT on the Translation Workforce</title>
		 <author>
			 <first>Jennifer</first>
			 <last>DeCamp</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>234–243</pages>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1918</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>DeCamp:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1919">
		 <title>PEMT for the Public Sector - Evolution of a Solution</title>
		 <author>
			 <first>Konstantine</first>
			 <last>Boukhvalov</last>
		 </author>
		 <author>
			 <first>Sandy</first>
			 <last>Hogg</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>244–274</pages>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1919</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Boukhvalov-Hogg:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1920">
		 <title>Embedding Register-Aware MT into the CAT Workflow</title>
		 <author>
			 <first>Corey</first>
			 <last>Miller</last>
		 </author>
		 <author>
			 <first>Danielle</first>
			 <last>Silverman</last>
		 </author>
		 <author>
			 <first>Vanesa</first>
			 <last>Jurica</last>
		 </author>
		 <author>
			 <first>Elizabeth</first>
			 <last>Richerson</last>
		 </author>
		 <author>
			 <first>Rodney</first>
			 <last>Morris</last>
		 </author>
		 <author>
			 <first>Elisabeth</first>
			 <last>Mallard</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>275–282</pages>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1920</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Miller-Silverman-Jurica-Richerson-Morris-Mallard:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1921">
		 <title>Challenges in Speech Recognition and Translation of High-Value Low-Density Polysynthetic Languages</title>
		 <author>
			 <first>Judith</first>
			 <last>Klavans</last>
		 </author>
		 <author>
			 <first>John</first>
			 <last>Morgan</last>
		 </author>
		 <author>
			 <first>Stephen</first>
			 <last>LaRocca</last>
		 </author>
		 <author>
			 <first>Jeffrey</first>
			 <last>Micher</last>
		 </author>
		 <author>
			 <first>Clare</first>
			 <last>Voss</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>283–293</pages>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1921</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Klavans-Morgan-LaRocca-Micher-Voss:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1922">
		 <title>Evaluating Automatic Speech Recognition in Translation</title>
		 <author>
			 <first>Evelyne</first>
			 <last>Tzoukermann</last>
		 </author>
		 <author>
			 <first>Corey</first>
			 <last>Miller</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>294–302</pages>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1922</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Tzoukermann-Miller:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1923">
		 <title>Portable Speech-to-Speech Translation on an Android Smartphone: The MFLTS System</title>
		 <author>
			 <first>Ralf</first>
			 <last>Meermeier</last>
		 </author>
		 <author>
			 <first>Sean</first>
			 <last>Colbath</last>
		 </author>
		 <author>
			 <first>Martha</first>
			 <last>Lillie</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>303–308</pages>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1923</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Meermeier-Colbath-Lillie:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1924">
		 <title>Tutorial: De-mystifying Neural MT</title>
		 <author>
			 <first>Dragos</first>
			 <last>Munteanu</last>
		 </author>
		 <author>
			 <first>Ling</first>
			 <last>Tsou</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1924</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Munteanu-Tsou:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1925">
		 <title>Tutorial: MQM-DQF: A Good Marriage (Translation Quality for the 21st Century)</title>
		 <author>
			 <first>Arle</first>
			 <last>Lommel</last>
		 </author>
		 <author>
			 <first>Alan</first>
			 <last>Melby</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1925</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Lommel-Melby:2018:AMTA</bibkey>
	 </paper>
	 <paper id="1926">
		 <title>Tutorial: Corpora Quality Management for MT - Practices and Roles</title>
		 <author>
			 <first>Silvio</first>
			 <last>Picinini</last>
		 </author>
		 <author>
			 <first>Pete</first>
			 <last>Smith</last>
		 </author>
		 <author>
			 <first>Nicola</first>
			 <last>Ueffing</last>
		 </author>
		 <booktitle>Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 2: User Papers)</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <url>http://www.aclweb.org/anthology/W/W18/W18-1926</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Picinini-Smith-Ueffing:2018:AMTA</bibkey>
	 </paper>

	 <paper id="2000">
		 <title>Proceedings of the AMTA 2018 Workshop on The Role of Authoritative Standards in the MT Environment</title>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <url>http://www.aclweb.org/anthology/W18-2000</url>
		 <bibtype>book</bibtype>
		 <bibkey>2018:AMTA</bibkey>
		 <editor><first>Jennifer</first><last>DeCamp</last></editor>
	 </paper>
	 <paper id="2001">
		 <title>Language Codes</title>
		 <author>
			 <first>Jennifer</first>
			 <last>DeCamp</last>
		 </author>
		 <booktitle>Proceedings of the AMTA 2018 Workshop on The Role of Authoritative Standards in the MT Environment</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>1–24</pages>
		 <url>http://www.aclweb.org/anthology/W18-2001</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>DeCamp:2018:AMTA</bibkey>
	 </paper>
	 <paper id="2002">
		 <title>Termbase Exchange (TBX)</title>
		 <author>
			 <first>Sue</first>
			 <last>Wright</last>
		 </author>
		 <booktitle>Proceedings of the AMTA 2018 Workshop on The Role of Authoritative Standards in the MT Environment</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>25–47</pages>
		 <url>http://www.aclweb.org/anthology/W18-2002</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Wright:2018:AMTA</bibkey>
	 </paper>
	 <paper id="2003">
		 <title>XLIFF 2</title>
		 <author>
			 <first>David</first>
			 <last>Filip</last>
		 </author>
		 <booktitle>Proceedings of the AMTA 2018 Workshop on The Role of Authoritative Standards in the MT Environment</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>48–55</pages>
		 <url>http://www.aclweb.org/anthology/W18-2003</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Filip:2018:AMTA</bibkey>
	 </paper>
	 <paper id="2004">
		 <title>Translation Quality Standards</title>
		 <author>
			 <first>Bill</first>
			 <last>Rivers</last>
		 </author>
		 <booktitle>Proceedings of the AMTA 2018 Workshop on The Role of Authoritative Standards in the MT Environment</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>56–68</pages>
		 <url>http://www.aclweb.org/anthology/W18-2004</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Rivers:2018:AMTA</bibkey>
	 </paper>
	 <paper id="2005">
		 <title>Translation Quality Metrics</title>
		 <author>
			 <first>Arle</first>
			 <last>Lommel</last>
		 </author>
		 <booktitle>Proceedings of the AMTA 2018 Workshop on The Role of Authoritative Standards in the MT Environment</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>69–94</pages>
		 <url>http://www.aclweb.org/anthology/W18-2005</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Lommel:2018:AMTA</bibkey>
	 </paper>
	 <paper id="2006">
		 <title>Translation API Cases and Classes (TAPICC)</title>
		 <author>
			 <first>Alan</first>
			 <last>Melby</last>
		 </author>
		 <booktitle>Proceedings of the AMTA 2018 Workshop on The Role of Authoritative Standards in the MT Environment</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>95–112</pages>
		 <url>http://www.aclweb.org/anthology/W18-2006</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Melby:2018:AMTA</bibkey>
	 </paper>

	 <paper id="2100">
		 <title>Proceedings of the AMTA 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing</title>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <url>http://www.aclweb.org/anthology/W18-2100</url>
		 <bibtype>book</bibtype>
		 <bibkey>2018:AMTA</bibkey>
		 <editor><first>Ramón</first><last>Astudillo</last></editor>
		 <editor><first>João</first><last>Graça</last></editor>
		 <editor><first>André</first><last>Martins</last></editor>
	 </paper>
	 <paper id="2101">
		 <title>Automatic Post-Editing and Machine Translation Quality Estimation at eBay</title>
		 <author>
			 <first>Nicola</first>
			 <last>Ueffing</last>
		 </author>
		 <booktitle>Proceedings of the AMTA 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>1–34</pages>
		 <url>http://www.aclweb.org/anthology/W18-2101</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Ueffing:2018:AMTA</bibkey>
	 </paper>
	 <paper id="2102">
		 <title>Lightweight Word-Level Confidence Estimation for Neural Interactive Translation Prediction</title>
		 <author>
			 <first>Rebecca</first>
			 <last>Knowles</last>
		 </author>
		 <author>
			 <first>Philipp</first>
			 <last>Koehn</last>
		 </author>
		 <booktitle>Proceedings of the AMTA 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>35–40</pages>
		 <url>http://www.aclweb.org/anthology/W18-2102</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Knowles-Koehn:2018:AMTA</bibkey>
	 </paper>
	 <paper id="2103">
		 <title>Unbabel: How to combine AI with the crowd to scale professional-quality translation</title>
		 <author>
			 <first>João</first>
			 <last>Graça</last>
		 </author>
		 <booktitle>Proceedings of the AMTA 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>41–85</pages>
		 <url>http://www.aclweb.org/anthology/W18-2103</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Graça:2018:AMTA</bibkey>
	 </paper>
	 <paper id="2104">
		 <title>Machine translation at Booking.com: what’s next? </title>
		 <author>
			 <first>Maxim</first>
			 <last>Khalilov</last>
		 </author>
		 <booktitle>Proceedings of the AMTA 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>86–143</pages>
		 <url>http://www.aclweb.org/anthology/W18-2104</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Khalilov:2018:AMTA</bibkey>
	 </paper>
	 <paper id="2105">
		 <title>Are we experiencing the Golden Age of Automatic Post-Editing?</title>
		 <author>
			 <first>Marcin</first>
			 <last>Junczys-Dowmunt</last>
		 </author>
		 <booktitle>Proceedings of the AMTA 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>144–206</pages>
		 <url>http://www.aclweb.org/anthology/W18-2105</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Junczys-Dowmunt:2018:AMTA</bibkey>
	 </paper>
	 <paper id="2106">
		 <title>Challenges in Adaptive Neural Machine Translation</title>
		 <author>
			 <first>Marcello</first>
			 <last>Federico</last>
		 </author>
		 <booktitle>Proceedings of the AMTA 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>207–242</pages>
		 <url>http://www.aclweb.org/anthology/W18-2106</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Federico:2018:AMTA</bibkey>
	 </paper>
	 <paper id="2107">
		 <title>Fine-grained evaluation of Quality Estimation for Machine translation based on a linguistically motivated Test Suite </title>
		 <author>
			 <first>Eleftherios</first>
			 <last>Avramidis</last>
		 </author>
		 <author>
			 <first>Vivien</first>
			 <last>Macketanz</last>
		 </author>
		 <author>
			 <first>Arle</first>
			 <last>Lommel</last>
		 </author>
		 <author>
			 <first>Hans</first>
			 <last>Uszkoreit</last>
		 </author>
		 <booktitle>Proceedings of the AMTA 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>243–248</pages>
		 <url>http://www.aclweb.org/anthology/W18-2107</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Avramidis-Macketanz-Lommel-Uszkoreit:2018:AMTA</bibkey>
	 </paper>
	 <paper id="2108">
		 <title>A Comparison of Machine Translation Paradigms for Use in Black-Box Fuzzy-Match Repair</title>
		 <author>
			 <first>Rebecca</first>
			 <last>Knowles</last>
		 </author>
		 <author>
			 <first>John</first>
			 <last>Ortega</last>
		 </author>
		 <author>
			 <first>Philipp</first>
			 <last>Koehn</last>
		 </author>
		 <booktitle>Proceedings of the AMTA 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing</booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>249–255</pages>
		 <url>http://www.aclweb.org/anthology/W18-2108</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Knowles-Ortega-Koeh:2018:AMTA</bibkey>
	 </paper>

	 <paper id="2200">
		 <title>Proceedings of the AMTA 2018 Workshop on Technologies for MT of Low Resource Languages (LoResMT 2018) </title>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <url>http://www.aclweb.org/anthology/W18-2200</url>
		 <bibtype>book</bibtype>
		 <bibkey>2018:AMTA</bibkey>
		 <editor><first>Chao-Hong</first><last>Liu</last></editor>
	 </paper>
	 <paper id="2201">
		 <title>Using Morphemes from Agglutinative Languages like Quechua and Finnish to Aid in Low-Resource Translation</title>
		 <author>
			 <first>John</first>
			 <last>Ortega</last>
		 </author>
		 <author>
			 <first>Krishnan</first>
			 <last>Pillaipakkamnatt</last>
		 </author>
		 <booktitle>Proceedings of the AMTA 2018 Workshop on Technologies for MT of Low Resource Languages (LoResMT 2018) </booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>1–11</pages>
		 <url>http://www.aclweb.org/anthology/W18-2201</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Ortega-Pillaipakkamnatt:2018:AMTA</bibkey>
	 </paper>
	 <paper id="2202">
		 <title>SMT versus NMT: Preliminary comparisons for Irish</title>
		 <author>
			 <first>Meghan</first>
			 <last>Dowling</last>
		 </author>
		 <author>
			 <first>Teresa</first>
			 <last>Lynn</last>
		 </author>
		 <author>
			 <first>Alberto</first>
			 <last>Poncelas</last>
		 </author>
		 <author>
			 <first>Andy</first>
			 <last>Way</last>
		 </author>
		 <booktitle>Proceedings of the AMTA 2018 Workshop on Technologies for MT of Low Resource Languages (LoResMT 2018) </booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>12–20</pages>
		 <url>http://www.aclweb.org/anthology/W18-2202</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Dowling-Lynn-Poncelas-Way:2018:AMTA</bibkey>
	 </paper>
	 <paper id="2203">
		 <title>Tibetan-Chinese Neural Machine Translation based on Syllable Segmentation</title>
		 <author>
			 <first>Wen</first>
			 <last>Lai</last>
		 </author>
		 <author>
			 <first>Xiaobing</first>
			 <last>Zhao</last>
		 </author>
		 <author>
			 <first>Wei</first>
			 <last>Bao</last>
		 </author>
		 <booktitle>Proceedings of the AMTA 2018 Workshop on Technologies for MT of Low Resource Languages (LoResMT 2018) </booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>21–29</pages>
		 <url>http://www.aclweb.org/anthology/W18-2203</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Lai-Zhao-Bao:2018:AMTA</bibkey>
	 </paper>
	 <paper id="2204">
		 <title>A Survey of Machine Translation Work in the Philippines: From 1998 to 2018</title>
		 <author>
			 <first>Nathaniel</first>
			 <last>Oco</last>
		 </author>
		 <author>
			 <first>Rachel</first>
			 <last>Roxas</last>
		 </author>
		 <booktitle>Proceedings of the AMTA 2018 Workshop on Technologies for MT of Low Resource Languages (LoResMT 2018) </booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>30–36</pages>
		 <url>http://www.aclweb.org/anthology/W18-2204</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Oco-Roxas:2018:AMTA</bibkey>
	 </paper>
	 <paper id="2205">
		 <title>Semi-Supervised Neural Machine Translation with Language Models</title>
		 <author>
			 <first>Ivan</first>
			 <last>Skorokhodov</last>
		 </author>
		 <author>
			 <first>Anton</first>
			 <last>Rykachevskiy</last>
		 </author>
		 <author>
			 <first>Dmitry</first>
			 <last>Emelyanenko</last>
		 </author>
		 <author>
			 <first>Sergey</first>
			 <last>Slotin</last>
		 </author>
		 <author>
			 <first>Anton</first>
			 <last>Ponkratov</last>
		 </author>
		 <booktitle>Proceedings of the AMTA 2018 Workshop on Technologies for MT of Low Resource Languages (LoResMT 2018) </booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>37–44</pages>
		 <url>http://www.aclweb.org/anthology/W18-2205</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Skorokhodov-Rykachevskiy-Emelyanenko-Slotin-Ponkratov:2018:AMTA</bibkey>
	 </paper>
	 <paper id="2206">
		 <title>System Description of Supervised and Unsupervised Neural Machine Translation Approaches from “NL Processing” Team at DeepHack.Babel Task</title>
		 <author>
			 <first>Ilya</first>
			 <last>Gusev</last>
		 </author>
		 <author>
			 <first>Artem</first>
			 <last>Oboturov</last>
		 </author>
		 <booktitle>Proceedings of the AMTA 2018 Workshop on Technologies for MT of Low Resource Languages (LoResMT 2018) </booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>45–52</pages>
		 <url>http://www.aclweb.org/anthology/W18-2206</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Gusev-Oboturov:2018:AMTA</bibkey>
	 </paper>
	 <paper id="2207">
		 <title>Apertium’s Web Toolchain for Low-Resource Language Technology</title>
		 <author>
			 <first>Sushain</first>
			 <last>Cherivirala</last>
		 </author>
		 <author>
			 <first>Shardul</first>
			 <last>Chiplunkar</last>
		 </author>
		 <author>
			 <first>Jonathan</first>
			 <last>Washington</last>
		 </author>
		 <author>
			 <first>Kevin</first>
			 <last>Unhammer</last>
		 </author>
		 <booktitle>Proceedings of the AMTA 2018 Workshop on Technologies for MT of Low Resource Languages (LoResMT 2018) </booktitle>
		 <publisher>Association for Machine Translation in the Americas</publisher>
		 <pages>53–62</pages>
		 <url>http://www.aclweb.org/anthology/W18-2207</url>
		 <bibtype>inproceedings</bibtype>
		 <bibkey>Cherivirala-Chiplunkar-Washington-Unhammer:2018:AMTA</bibkey>
	 </paper>
    <paper id="2300">
	 <title>Proceedings of the BioNLP 2018 workshop</title>
	 <editor><first>Dina</first><last>Demner-Fushman</last></editor>
	 <editor><first>Kevin Bretonnel</first><last>Cohen</last></editor>
	 <editor><first>Sophia</first><last>Ananiadou</last></editor>
	 <editor><first>Junichi</first><last>Tsujii</last></editor>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <url>http://www.aclweb.org/anthology/W18-23</url>
	 <bibtype>book</bibtype>
	 <bibkey>BioNLP18:2018</bibkey>
    </paper>
    <paper id="2301">
	 <title>Embedding Transfer for Low-Resource Medical Named Entity Recognition: A Case Study on Patient Mobility</title>
	 <author><first>Denis</first><last>Newman-Griffis</last></author>
	 <author><first>Ayah</first><last>Zirikly</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>1–11</pages>
	 <abstract>Functioning is gaining recognition as an important indicator of global health, but remains under-studied in medical natural language processing research. We present the first analysis of automatically extracting descriptions of patient mobility, using a recently-developed dataset of free text electronic health records. We frame the task as a named entity recognition (NER) problem, and investigate the applicability of NER techniques to mobility extraction. As text corpora focused on patient functioning are scarce, we explore domain adaptation of word embeddings for use in a recurrent neural network NER system. We find that embeddings trained on a small in-domain corpus perform nearly as well as those learned from large out-of-domain corpora, and that domain adaptation techniques yield additional improvements in both precision and recall. Our analysis identifies several significant challenges in extracting descriptions of patient mobility, including the length and complexity of annotated entities and high linguistic variability in mobility descriptions.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2301</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>newmangriffis-zirikly:2018:BioNLP18</bibkey>
    </paper>
    <paper id="2302">
	 <title>Multi-task learning for interpretable cause of death classification using key phrase prediction</title>
	 <author><first>Serena</first><last>Jeblee</last></author>
	 <author><first>Mireille</first><last>Gomes</last></author>
	 <author><first>Graeme</first><last>Hirst</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>12–17</pages>
	 <abstract>We introduce a multi-task learning model for cause-of-death classification of verbal autopsy narratives that jointly learns to output interpretable key phrases. Adding these key phrases outperforms the baseline model and topic modeling features.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2302</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>jeblee-gomes-hirst:2018:BioNLP18</bibkey>
    </paper>
    <paper id="2303">
	 <title>Identifying Risk Factors For Heart Disease in Electronic Medical Records: A Deep Learning Approach</title>
	 <author><first>Thanat</first><last>Chokwijitkul</last></author>
	 <author><first>Anthony</first><last>Nguyen</last></author>
	 <author><first>Hamed</first><last>Hassanzadeh</last></author>
	 <author><first>Siegfried</first><last>Perez</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>18–27</pages>
	 <abstract>Automatic identification of heart disease risk factors in clinical narratives can expedite disease progression modelling and support clinical decisions. Existing practical solutions for cardiovascular risk detection are mostly hybrid systems entailing the integration of knowledge-driven and data-driven methods, relying on dictionaries, rules and machine learning methods that require a substantial amount of human effort. This paper proposes a comparative analysis on the applicability of deep learning, a re-emerged data-driven technique, in the context of clinical text classification. Various deep learning architectures were devised and evaluated for extracting heart disease risk factors from clinical documents. The data provided for the 2014 i2b2/UTHealth shared task focusing on identifying risk factors for heart disease was used for system development and evaluation. Results have shown that a relatively simple deep learning model can achieve a high micro-averaged F-measure of 0.9081, which is comparable to the best systems from the shared task. This is highly encouraging given the simplicity of the deep learning approach compared to the heavily feature-engineered hybrid approaches that were required to achieve state-of-the-art performances.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2303</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>chokwijitkul-EtAl:2018:BioNLP18</bibkey>
    </paper>
    <paper id="2304">
	 <title>Keyphrases Extraction from User-Generated Contents in Healthcare Domain Using Long Short-Term Memory Networks</title>
	 <author><first>Ilham Fathy</first><last>Saputra</last></author>
	 <author><first>Rahmad</first><last>Mahendra</last></author>
	 <author><first>Alfan Farizki</first><last>Wicaksono</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>28–34</pages>
	 <abstract>We propose keyphrases extraction technique to extract important terms from the healthcare user-generated contents. We employ deep learning architecture, i.e. Long Short-Term Memory, and leverage word embeddings, medical concepts from a knowledge base, and linguistic components as our features. The proposed model achieves 61.37% F-1 score. Experimental results indicate that our proposed approach outperforms the baseline methods, i.e. RAKE and CRF, on the task of extracting keyphrases from Indonesian health forum posts.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2304</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>saputra-mahendra-wicaksono:2018:BioNLP18</bibkey>
    </paper>
    <paper id="2305">
	 <title>Identifying Key Sentences for Precision Oncology Using Semi-Supervised Learning</title>
	 <author><first>Jurica</first><last>Ševa</last></author>
	 <author><first>Martin</first><last>Wackerbauer</last></author>
	 <author><first>Ulf</first><last>Leser</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>35–46</pages>
	 <abstract>We present a machine learning pipeline that identifies key sentences in abstracts of oncological articles to aid evidence-based medicine. This problem is characterized by the lack of gold standard datasets, data imbalance and thematic differences between available silver standard corpora. Additionally, available training and target data differs with regard to their domain (professional summaries vs. sentences in abstracts). This makes supervised machine learning inapplicable. We propose the use of two semi-supervised machine learning approaches: To mitigate difficulties arising from heterogeneous data sources, overcome data imbalance and create reliable training data we propose using transductive learning from positive and unlabelled data (PU Learning). For obtaining a realistic classification model, we propose the use of abstracts summarised in relevant sentences as unlabelled examples through Self-Training. The best model achieves 84% accuracy and 0.84 F1 score on our dataset</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2305</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>eva-wackerbauer-leser:2018:BioNLP18</bibkey>
    </paper>
    <paper id="2306">
	 <title>Ontology alignment in the biomedical domain using entity definitions and context</title>
	 <author><first>Lucy</first><last>Wang</last></author>
	 <author><first>Chandra</first><last>Bhagavatula</last></author>
	 <author><first>Mark</first><last>Neumann</last></author>
	 <author><first>Kyle</first><last>Lo</last></author>
	 <author><first>Chris</first><last>Wilhelm</last></author>
	 <author><first>Waleed</first><last>Ammar</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>47–55</pages>
	 <abstract>Ontology alignment is the task of identifying semantically equivalent entities from two given ontologies. Different ontologies have different representations of the same entity, resulting in a need to de-duplicate entities when merging ontologies. We propose a method for enriching entities in an ontology with external definition and context information, and use this additional information for ontology alignment. We develop a neural architecture capable of encoding the additional information when available, and show that the addition of external data results in an F1-score of 0.69 on the Ontology Alignment Evaluation Initiative (OAEI) largebio SNOMED-NCI subtask, comparable with the entity-level matchers in a SOTA system.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2306</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>wang-EtAl:2018:BioNLP181</bibkey>
    </paper>
    <paper id="2307">
	 <title>Sub-word information in pre-trained biomedical word representations: evaluation and hyper-parameter optimization</title>
	 <author><first>Dieter</first><last>Galea</last></author>
	 <author><first>Ivan</first><last>Laponogov</last></author>
	 <author><first>Kirill</first><last>Veselkov</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>56–66</pages>
	 <abstract>Word2vec embeddings are limited to computing vectors for in-vocabulary terms and do not take into account sub-word information. Character-based representations, such as fastText, mitigate such limitations. We optimize and compare these representations for the biomedical domain. fastText was found to consistently outperform word2vec in named entity recognition tasks for entities such as chemicals and genes. This is likely due to gained information from computed out-of-vocabulary term vectors, as well as the word compositionality of such entities. Contrastingly, performance varied on intrinsic datasets. Optimal hyper-parameters were intrinsic dataset-dependent, likely due to differences in term types distributions. This indicates embeddings should be chosen based on the task at hand. We therefore provide a number of optimized hyper-parameter sets and pre-trained word2vec and fastText models, available on https://github.com/dterg/bionlp-embed.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2307</url>
	 <attachment type="note">W18-2307.Notes.zip</attachment>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>galea-laponogov-veselkov:2018:BioNLP18</bibkey>
    </paper>
    <paper id="2308">
	 <title>PICO Element Detection in Medical Text via Long Short-Term Memory Neural Networks</title>
	 <author><first>Di</first><last>Jin</last></author>
	 <author><first>Peter</first><last>Szolovits</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>67–75</pages>
	 <abstract>Successful evidence-based medicine (EBM) applications rely on answering clinical questions by analyzing large medical literature databases. In order to formulate a well-defined, focused clinical question, a framework called PICO is widely used, which identifies the sentences in a given medical text that belong to the four components: Participants/Problem (P), Intervention (I), Comparison (C) and Outcome (O). In this work, we present a Long Short-Term Memory (LSTM) neural network based model to automatically detect PICO elements. By jointly classifying subsequent sentences in the given text, we achieve state-of-the-art results on PICO element classification compared to several strong baseline models. We also make our curated data public as a benchmarking dataset so that the community can benefit from it.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2308</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>jin-szolovits:2018:BioNLP18</bibkey>
    </paper>
    <paper id="2309">
	 <title>Coding Structures and Actions with the COSTA Scheme in Medical Conversations</title>
	 <author><first>Nan</first><last>Wang</last></author>
	 <author><first>Yan</first><last>Song</last></author>
	 <author><first>Fei</first><last>Xia</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>76–86</pages>
	 <abstract>This paper describes the COSTA scheme for coding structures and actions in conversation. Informed by Conversation Analysis, the scheme introduces an innovative method for marking multi-layer structural organization of conversation and a structure-informed taxonomy of actions. In addition, we create a corpus of naturally occurring medical conversations, containing 318 video-recorded and manually transcribed pediatric consultations. Based on the annotated corpus, we investigate 1) treatment decision-making process in medical conversations, and 2) effects of physician-caregiver communication behaviors on antibiotic over-prescribing. Although the COSTA annotation scheme is developed based on data from the task-specific domain of pediatric consultations, it can be easily extended to apply to more general domains and other languages.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2309</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>wang-song-xia:2018:BioNLP18</bibkey>
    </paper>
    <paper id="2310">
	 <title>A Neural Autoencoder Approach for Document Ranking and Query Refinement in Pharmacogenomic Information Retrieval</title>
	 <author><first>Jonas</first><last>Pfeiffer</last></author>
	 <author><first>Samuel</first><last>Broscheit</last></author>
	 <author><first>Rainer</first><last>Gemulla</last></author>
	 <author><first>Mathias</first><last>Göschl</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>87–97</pages>
	 <abstract>In this study, we investigate learning-to-rank and query refinement approaches for information retrieval in the pharmacogenomic domain. The goal is to improve the information retrieval process of biomedical curators, who manually build knowledge bases for personalized medicine. We study how to exploit the relationships between genes, variants, drugs, diseases and outcomes as features for document ranking and query refinement. For a supervised approach, we are faced with a small amount of annotated data and a large amount of unannotated data. Therefore, we explore ways to use a neural document auto-encoder in a semi-supervised approach. We show that a combination of established algorithms, feature-engineering and a neural auto-encoder model yield promising results in this setting.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2310</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>pfeiffer-EtAl:2018:BioNLP18</bibkey>
    </paper>
    <paper id="2311">
	 <title>Biomedical Event Extraction Using Convolutional Neural Networks and Dependency Parsing</title>
	 <author><first>Jari</first><last>Björne</last></author>
	 <author><first>Tapio</first><last>Salakoski</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>98–108</pages>
	 <abstract>Event and relation extraction are central tasks in biomedical text mining. Where relation extraction concerns the detection of semantic connections between pairs of entities, event extraction expands this concept with the addition of trigger words, multiple arguments and nested events, in order to more accurately model the diversity of natural language. In this work we develop a convolutional neural network that can be used for both event and relation extraction. We use a linear representation of the input text, where information is encoded with various vector space embeddings. Most notably, we encode the parse graph into this linear space using dependency path embeddings. We integrate our neural network into the open source Turku Event Extraction System (TEES) framework. Using this system, our machine learning model can be easily applied to a large set of corpora from e.g. the BioNLP, DDI Extraction and BioCreative shared tasks. We evaluate our system on 12 different event, relation and NER corpora, showing good generalizability to many tasks and achieving improved performance on several corpora.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2311</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>bjrne-salakoski:2018:BioNLP18</bibkey>
    </paper>
    <paper id="2312">
	 <title>BioAMA: Towards an End to End BioMedical Question Answering System</title>
	 <author><first>Vasu</first><last>Sharma</last></author>
	 <author><first>Nitish</first><last>Kulkarni</last></author>
	 <author><first>Srividya</first><last>Pranavi</last></author>
	 <author><first>Gabriel</first><last>Bayomi</last></author>
	 <author><first>Eric</first><last>Nyberg</last></author>
	 <author><first>Teruko</first><last>Mitamura</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>109–117</pages>
	 <abstract>In this paper, we present a novel Biomedical Question Answering system, BioAMA: “Biomedical Ask Me Anything” on task 5b of the annual BioASQ challenge. In this work, we focus on a wide variety of question types including factoid, list based, summary and yes/no type questions that generate both exact and well-formed ‘ideal’ answers. For summary-type questions, we combine effective IR-based techniques for retrieval and diversification of relevant snippets for a question to create an end-to-end system which achieves a ROUGE-2 score of 0.72 and a ROUGE-SU4 score of 0.71 on ideal answer questions (7% improvement over the previous best model). Additionally, we propose a novel NLI-based framework to answer the yes/no questions. To train the NLI model, we also devise a transfer-learning technique by cross-domain projection of word embeddings. Finally, we present a two-stage approach to address the factoid and list type questions by first generating a candidate set using NER taggers and ranking them using both supervised or unsupervised techniques.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2312</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>sharma-EtAl:2018:BioNLP18</bibkey>
    </paper>
    <paper id="2313">
	 <title>Phrase2VecGLM: Neural generalized language model–based semantic tagging for complex query reformulation in medical IR</title>
	 <author><first>Manirupa</first><last>Das</last></author>
	 <author><first>Eric</first><last>Fosler-Lussier</last></author>
	 <author><first>Simon</first><last>Lin</last></author>
	 <author><first>Soheil</first><last>Moosavinasab</last></author>
	 <author><first>David</first><last>Chen</last></author>
	 <author><first>Steve</first><last>Rust</last></author>
	 <author><first>Yungui</first><last>Huang</last></author>
	 <author><first>Rajiv</first><last>Ramnath</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>118–128</pages>
	 <abstract>In this work, we develop a novel, completely unsupervised, neural language model-based document ranking approach to semantic tagging of documents, using the document to be tagged as a query into the GLM to retrieve candidate phrases from top-ranked related documents, thus associating every document with novel related concepts extracted from the text. For this we extend the word embedding-based general language model due to Ganguly et al 2015, to employ phrasal embeddings, and use the semantic tags thus obtained for downstream query expansion, both directly and in feedback loop settings. Our method, evaluated using the TREC 2016 clinical decision support challenge dataset, shows statistically significant improvement not only over various baselines that use standard MeSH terms and UMLS concepts for query expansion, but also over baselines using human expert–assigned concept tags for the queries, run on top of a standard Okapi BM25–based document retrieval system.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2313</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>das-EtAl:2018:BioNLP18</bibkey>
    </paper>
    <paper id="2314">
	 <title>Convolutional neural networks for chemical-disease relation extraction are improved with character-based word embeddings</title>
	 <author><first>Dat Quoc</first><last>Nguyen</last></author>
	 <author><first>Karin</first><last>Verspoor</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>129–136</pages>
	 <abstract>We investigate the incorporation of character-based word representations into a standard CNN-based relation extraction model. We experiment with two common neural architectures, CNN and LSTM, to learn word vector representations from character embeddings. Through a task on the BioCreative-V CDR corpus, extracting relationships between chemicals and diseases, we show that models exploiting the character-based word representations improve on models that do not use this information, obtaining state-of-the-art result relative to previous neural approaches.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2314</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>nguyen-verspoor:2018:BioNLP18</bibkey>
    </paper>
    <paper id="2315">
	 <title>Domain Adaptation for Disease Phrase Matching with Adversarial Networks</title>
	 <author><first>Miaofeng</first><last>Liu</last></author>
	 <author><first>Jialong</first><last>Han</last></author>
	 <author><first>Haisong</first><last>Zhang</last></author>
	 <author><first>Yan</first><last>Song</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>137–141</pages>
	 <abstract>With the development of medical information management, numerous medical data are being classified, indexed, and searched in various systems. Disease phrase matching, i.e., deciding whether two given disease phrases interpret each other, is a basic but crucial preprocessing step for the above tasks. Being capable of relieving the scarceness of annotations, domain adaptation is generally considered useful in medical systems. However, efforts on applying it to phrase matching remain limited. This paper presents a domain-adaptive matching network for disease phrases. Our network achieves domain adaptation by adversarial training, i.e., preferring features indicating whether the two phrases match, rather than which domain they come from. Experiments suggest that our model has the best performance among the very few non-adaptive or adaptive methods that can benefit from out-of-domain annotations.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2315</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>liu-EtAl:2018:BioNLP18</bibkey>
    </paper>
    <paper id="2316">
	 <title>Predicting Discharge Disposition Using Patient Complaint Notes in Electronic Medical Records</title>
	 <author><first>Mohamad</first><last>Salimi</last></author>
	 <author><first>Alla</first><last>Rozovskaya</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>142–146</pages>
	 <abstract>Overcrowding in emergency rooms is a major challenge faced by hospitals across the United States. Overcrowding can result in longer wait times, which, in turn, has been shown to adversely affect patient satisfaction, clinical outcomes, and procedure reimbursements. This paper presents research that aims to automatically predict discharge disposition of patients who received medical treatment in an emergency department. We make use of a corpus that consists of notes containing patient complaints, diagnosis information, and disposition, entered by health care providers. We use this corpus to develop a model that uses the complaint and diagnosis information to predict patient disposition. We show that the proposed model substantially outperforms the baseline of predicting the most common disposition type. The long-term goal of this research is to build a model that can be implemented as a real-time service in an application to predict disposition as patients arrive.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2316</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>salimi-rozovskaya:2018:BioNLP18</bibkey>
    </paper>
    <paper id="2317">
	 <title>Bacteria and Biotope Entity Recognition Using A Dictionary-Enhanced Neural Network Model</title>
	 <author><first>Qiuyue</first><last>Wang</last></author>
	 <author><first>Xiaofeng</first><last>Meng</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>147–150</pages>
	 <abstract>Automatic recognition of biomedical entities in text is the crucial initial step in biomedical text mining. In this pa-per, we investigate employing modern neural network models for recognizing biomedical entities. To compensate for the small amount of training data in biomedical domain, we propose to integrate dictionaries into the neural model. Our experiments on BB3 data sets demonstrate that state-of-the-art neural network model is promising in recognizing biomedical entities even with very little training data. When integrated with dictionaries, its performance could be greatly improved, achieving the competitive performance compared with the best dictionary-based system on the entities with specific terminology, and much higher performance on the entities with more general terminology.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2317</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>wang-meng:2018:BioNLP18</bibkey>
    </paper>
    <paper id="2318">
	 <title>SingleCite: Towards an improved Single Citation Search in PubMed</title>
	 <author><first>Lana</first><last>Yeganova</last></author>
	 <author><first>Donald C</first><last>Comeau</last></author>
	 <author><first>Won</first><last>Kim</last></author>
	 <author><first>W John</first><last>Wilbur</last></author>
	 <author><first>Zhiyong</first><last>Lu</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>151–155</pages>
	 <abstract>A search that is targeted at finding a specific document in databases is called a Single Citation search. Single citation searches are particularly important for scholarly databases, such as PubMed, because users are frequently searching for a specific publication. In this work we describe SingleCite, a single citation matching system designed to facilitate user’s search for a specific document. We report on the progress that has been achieved towards building that functionality.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2318</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>yeganova-EtAl:2018:BioNLP18</bibkey>
    </paper>
    <paper id="2319">
	 <title>A Framework for Developing and Evaluating Word Embeddings of Drug-named Entity</title>
	 <author><first>Mengnan</first><last>Zhao</last></author>
	 <author><first>Aaron J.</first><last>Masino</last></author>
	 <author><first>Christopher C.</first><last>Yang</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>156–160</pages>
	 <abstract>We investigate the quality of task specific word embeddings created with relatively small, targeted corpora. We present a comprehensive evaluation framework including both intrinsic and extrinsic evaluation that can be expanded to named entities beyond drug name. Intrinsic evaluation results tell that drug name embeddings created with a domain specific document corpus outperformed the previously published versions that derived from a very large general text corpus. Extrinsic evaluation uses word embedding for the task of drug name recognition with Bi-LSTM model and the results demonstrate the advantage of using domain-specific word embeddings as the only input feature for drug name recognition with F1-score achieving 0.91. This work suggests that it may be advantageous to derive domain specific embeddings for certain tasks even when the domain specific corpus is of limited size.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2319</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>zhao-masino-yang:2018:BioNLP18</bibkey>
    </paper>
    <paper id="2320">
	 <title>MeSH-based dataset for measuring the relevance of text retrieval</title>
	 <author><first>Won Gyu</first><last>KIM</last></author>
	 <author><first>Lana</first><last>Yeganova</last></author>
	 <author><first>Donald</first><last>comeau</last></author>
	 <author><first>W John</first><last>Wilbur</last></author>
	 <author><first>Zhiyong</first><last>Lu</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>161–165</pages>
	 <abstract>Creating simulated search environments has been of a significant interest in infor-mation retrieval, in both general and bio-medical search domains. Existing collec-tions include modest number of queries and are constructed by manually evaluat-ing retrieval results. In this work we pro-pose leveraging MeSH term assignments for creating synthetic test beds. We select a suitable subset of MeSH terms as queries, and utilize MeSH term assignments as pseudo-relevance rankings for retrieval evaluation. Using well studied retrieval functions, we show that their performance on the proposed data is consistent with similar findings in previous work. We further use the proposed retrieval evaluation framework to better understand how to combine heterogeneous sources of textual information.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2320</url>
	 <attachment type="note">W18-2320.Notes.pdf</attachment>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>kim-EtAl:2018:BioNLP18</bibkey>
    </paper>
    <paper id="2321">
	 <title>CRF-LSTM Text Mining Method Unveiling the Pharmacological Mechanism of Off-target Side Effect of Anti-Multiple Myeloma Drugs</title>
	 <author><first>Kaiyin</first><last>Zhou</last></author>
	 <author><first>Sheng</first><last>Zhang</last></author>
	 <author><first>Xiangyu</first><last>Meng</last></author>
	 <author><first>Qi</first><last>Luo</last></author>
	 <author><first>Yuxing</first><last>Wang</last></author>
	 <author><first>Ke</first><last>Ding</last></author>
	 <author><first>Yukun</first><last>Feng</last></author>
	 <author><first>Mo</first><last>Chen</last></author>
	 <author><first>Kevin</first><last>Cohen</last></author>
	 <author><first>Jingbo</first><last>Xia</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>166–171</pages>
	 <abstract>Sequence labeling of biomedical entities, e.g., side effects or phenotypes, was a long-term task in BioNLP and MedNLP communities. Thanks to effects made among these communities, adverse reaction NER has developed dramatically in recent years. As an illuminative application, to achieve knowledge discovery via the combination of the text mining result and bioinformatics idea shed lights on the pharmacological mechanism research.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2321</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>zhou-EtAl:2018:BioNLP18</bibkey>
    </paper>
    <paper id="2322">
	 <title>Prediction Models for Risk of Type-2 Diabetes Using Health Claims</title>
	 <author><first>Masatoshi</first><last>Nagata</last></author>
	 <author><first>Kohichi</first><last>Takai</last></author>
	 <author><first>Keiji</first><last>Yasuda</last></author>
	 <author><first>Panikos</first><last>Heracleous</last></author>
	 <author><first>Akio</first><last>Yoneyama</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>172–176</pages>
	 <abstract>This study focuses on highly accurate prediction of the onset of type-2 diabetes. We investigated whether prediction accuracy can be improved by utilizing lab test data obtained from health checkups and incorporating health claim text data such as medically diagnosed diseases with ICD10 codes and pharmacy information. In a previous study, prediction accuracy was increased slightly by adding diagnosis disease name and independent variables such as prescription medicine. Therefore, in the current study we explored more suitable models for prediction by using state-of-the-art techniques such as XGBoost and long short-term memory (LSTM) based on recurrent neural networks. In the current study, text data was vectorized using word2vec, and the prediction model was compared with logistic regression. The results obtained confirmed that onset of type-2 diabetes can be predicted with a high degree of accuracy when the XGBoost model is used.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2322</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>nagata-EtAl:2018:BioNLP18</bibkey>
    </paper>
    <paper id="2323">
	 <title>On Learning Better Embeddings from Chinese Clinical Records: Study on Combining In-Domain and Out-Domain Data</title>
	 <author><first>Yaqiang</first><last>Wang</last></author>
	 <author><first>Yunhui</first><last>Chen</last></author>
	 <author><first>Hongping</first><last>Shu</last></author>
	 <author><first>Yongguang</first><last>Jiang</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>177–182</pages>
	 <abstract>High quality word embeddings are of great significance to advance applications of biomedical natural language processing. In recent years, a surge of interest on how to learn good embeddings and evaluate embedding quality based on English medical text has become increasing evident, however a limited number of studies based on Chinese medical text, particularly Chinese clinical records, were performed. Herein, we proposed a novel approach of improving the quality of learned embeddings using out-domain data as a supplementary in the case of limited Chinese clinical records. Moreover, the embedding quality evaluation method was conducted based on Medical Conceptual Similarity Property. The experimental results revealed that selecting good training samples was necessary, and collecting right amount of out-domain data and trading off between the quality of embeddings and the training time consumption were essential factors for better embeddings.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2323</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>wang-EtAl:2018:BioNLP182</bibkey>
    </paper>
    <paper id="2324">
	 <title>Investigating Domain-Specific Information for Neural Coreference Resolution on Biomedical Texts</title>
	 <author><first>Long</first><last>Trieu</last></author>
	 <author><first>Nhung</first><last>Nguyen</last></author>
	 <author><first>Makoto</first><last>Miwa</last></author>
	 <author><first>Sophia</first><last>Ananiadou</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>183–188</pages>
	 <abstract>Existing biomedical coreference resolu- tion systems depend on features and/or rules based on syntactic parsers. In this pa- per, we investigate the utility of the state- of-the-art general domain neural coref- erence resolution system on biomedical texts. The system is an end-to-end sys- tem without depending on any syntactic parsers. We also investigate the domain specific features to enhance the system for biomedical texts. Experimental results on the BioNLP Protein Coreference dataset and the CRAFT corpus show that, with no parser information, the adapted sys- tem compared favorably with the systems that depend on parser information on these datasets, achieving 51.23% on the BioNLP dataset and 36.33% on the CRAFT corpus in F1 score. In-domain embeddings and domain-specific features helped improve the performance on the BioNLP dataset, but they did not on the CRAFT corpus.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2324</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>trieu-EtAl:2018:BioNLP18</bibkey>
    </paper>
    <paper id="2325">
	 <title>Toward Cross-Domain Engagement Analysis in Medical Notes</title>
	 <author><first>Sara</first><last>Rosenthal</last></author>
	 <author><first>Adam</first><last>Faulkner</last></author>
	 <booktitle>Proceedings of the BioNLP 2018 workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>189–193</pages>
	 <abstract>We present a novel annotation task evaluating a patient’s engagement with their health care regimen. The concept of engagement supplements the traditional concept of adherence with a focus on the patient’s affect, lifestyle choices, and health goal status. We describe an engagement annotation task across two patient note domains: traditional clinical notes and a novel domain, care manager notes, where we find engagement to be more common. The annotation task resulted in a kappa of .53, suggesting strong annotator intuitions regarding engagement-bearing language. In addition, we report the results of a series of preliminary engagement classification experiments using domain adaptation.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2325</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>rosenthal-faulkner:2018:BioNLP18</bibkey>
    </paper>
    <paper id="2400">
	 <title>Proceedings of the Seventh Named Entities Workshop</title>
	 <editor><first>Nancy</first><last>Chen</last></editor>
	 <editor><first>Rafael E.</first><last>Banchs</last></editor>
	 <editor><first>Xiangyu</first><last>Duan</last></editor>
	 <editor><first>Min</first><last>Zhang</last></editor>
	 <editor><first>Haizhou</first><last>Li</last></editor>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <url>http://www.aclweb.org/anthology/W18-24</url>
	 <bibtype>book</bibtype>
	 <bibkey>NEWS2018:2018</bibkey>
    </paper>
    <paper id="2401">
	 <title>Automatic Extraction of Entities and Relation from Legal Documents</title>
	 <author><first>Judith Jeyafreeda</first><last>Andrew</last></author>
	 <booktitle>Proceedings of the Seventh Named Entities Workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>1–8</pages>
	 <abstract>In recent years, the journalists and computer sciences speak to each other to identify useful technologies which would help them in extracting useful information. This is called “computational Journalism”. In this paper, we present a method that will enable the journalists to automatically identifies and annotates entities such as names of people, organizations, role and functions of people in legal documents; the relationship between these entities are also explored. The system uses a combination of both statistical and rule based technique. The statistical method used is Conditional Random Fields and for the rule based technique, document and language specific regular expressions are used.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2401</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>andrew:2018:NEWS2018</bibkey>
    </paper>
    <paper id="2402">
	 <title>Connecting Distant Entities with Induction through Conditional Random Fields for Named Entity Recognition: Precursor-Induced CRF</title>
	 <author><first>Wangjin</first><last>Lee</last></author>
	 <author><first>Jinwook</first><last>Choi</last></author>
	 <booktitle>Proceedings of the Seventh Named Entities Workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>9–13</pages>
	 <abstract>This paper presents a method of designing specific high-order dependency factor on the linear chain conditional random fields (CRFs) for named entity recognition (NER). Named entities tend to be separated from each other by multiple outside tokens in a text, and thus the first-order CRF, as well as the second-order CRF, may innately lose transition information between distant named entities. The proposed design uses outside label in NER as a transmission medium of precedent entity information on the CRF. Then, empirical results apparently demonstrate that it is possible to exploit long-distance label dependency in the original first-order linear chain CRF structure upon NER while reducing computational loss rather than in the second-order CRF.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2402</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>lee-choi:2018:NEWS2018</bibkey>
    </paper>
    <paper id="2403">
	 <title>A Sequence Learning Method for Domain-Specific Entity Linking</title>
	 <author><first>Emrah</first><last>Inan</last></author>
	 <author><first>Oguz</first><last>Dikenelli</last></author>
	 <booktitle>Proceedings of the Seventh Named Entities Workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>14–21</pages>
	 <abstract>Recent collective Entity Linking studies usually promote global coherence of all the mapped entities in the same document by using semantic embeddings and graph-based approaches. Although graph-based approaches are shown to achieve remarkable results, they are computationally expensive for general datasets. Also, semantic embeddings only indicate relatedness between entity pairs without considering sequences. In this paper, we address these problems by introducing a two-fold neural model. First, we match easy mention-entity pairs and using the domain information of this pair to filter candidate entities of closer mentions. Second, we resolve more ambiguous pairs using bidirectional Long Short-Term Memory and CRF models for the entity disambiguation. Our proposed system outperforms state-of-the-art systems on the generated domain-specific evaluation dataset.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2403</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>inan-dikenelli:2018:NEWS2018</bibkey>
    </paper>
    <paper id="2404">
	 <title>Attention-based Semantic Priming for Slot-filling</title>
	 <author><first>Jiewen</first><last>Wu</last></author>
	 <author><first>Rafael E.</first><last>Banchs</last></author>
	 <author><first>Luis Fernando</first><last>D’Haro</last></author>
	 <author><first>Pavitra</first><last>Krishnaswamy</last></author>
	 <author><first>Nancy</first><last>Chen</last></author>
	 <booktitle>Proceedings of the Seventh Named Entities Workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>22–26</pages>
	 <abstract>The problem of sequence labelling in language understanding would benefit from approaches inspired by semantic priming phenomena. We propose that an attention-based RNN architecture can be used to simulate semantic priming for sequence labelling. Specifically, we employ pre-trained word embeddings to characterize the semantic relationship between utterances and labels. We validate the approach using varying sizes of the ATIS and MEDIA datasets, and show up to 1.4-1.9% improvement in F1 score. The developed framework can enable more explainable and generalizable spoken language understanding systems.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2404</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>wu-EtAl:2018:NEWS2018</bibkey>
    </paper>
    <paper id="2405">
	 <title>Named Entity Recognition for Hindi-English Code-Mixed Social Media Text</title>
	 <author><first>Vinay</first><last>Singh</last></author>
	 <author><first>Deepanshu</first><last>Vijay</last></author>
	 <author><first>Syed Sarfaraz</first><last>Akhtar</last></author>
	 <author><first>Manish</first><last>Shrivastava</last></author>
	 <booktitle>Proceedings of the Seventh Named Entities Workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>27–35</pages>
	 <abstract>Named Entity Recognition (NER) is a major task in the field of Natural Language Processing (NLP), and also is a sub-task of Information Extraction. The challenge of NER for tweets lie in the insufficient information available in a tweet. There has been a significant amount of work done related to entity extraction, but only for resource rich languages and domains such as newswire. Entity extraction is, in general, a challenging task for such an informal text, and code-mixed text further complicates the process with it’s unstructured and incomplete information. We propose experiments with different machine learning classification algorithms with word, character and lexical features. The algorithms we experimented with are Decision tree, Long Short-Term Memory (LSTM), and Conditional Random Field (CRF). In this paper, we present a corpus for NER in Hindi-English Code-Mixed along with extensive experiments on our machine learning models which achieved the best f1-score of 0.95 with both CRF and LSTM.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2405</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>singh-EtAl:2018:NEWS2018</bibkey>
    </paper>
    <paper id="2406">
	 <title>Forms of Anaphoric Reference to Organisational Named Entities: Hoping to widen appeal, they diversified</title>
	 <author><first>Christian</first><last>Hardmeier</last></author>
	 <author><first>Luca</first><last>Bevacqua</last></author>
	 <author><first>Sharid</first><last>Loáiciga</last></author>
	 <author><first>Hannah</first><last>Rohde</last></author>
	 <booktitle>Proceedings of the Seventh Named Entities Workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>36–40</pages>
	 <abstract>Proper names of organisations are a special case of collective nouns. Their meaning can be conceptualised as a collective unit or as a plurality of persons, allowing for different morphological marking of coreferent anaphoric pronouns. This paper explores the variability of references to organisation names with 1) a corpus analysis and 2) two crowd-sourced story continuation experiments. The first shows that the preference for singular vs. plural conceptualisation is dependent on the level of formality of a text. In the second, we observe a strong preference for the plural they otherwise typical of informal speech. Using edited corpus data instead of constructed sentences as stimuli reduces this preference.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2406</url>
	 <attachment type="note">W18-2406.Notes.pdf</attachment>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>hardmeier-EtAl:2018:NEWS2018</bibkey>
    </paper>
    <paper id="2407">
	 <title>Named-Entity Tagging and Domain adaptation for Better Customized Translation</title>
	 <author><first>Zhongwei</first><last>Li</last></author>
	 <author><first>Xuancong</first><last>Wang</last></author>
	 <author><first>AiTi</first><last>Aw</last></author>
	 <author><first>Eng Siong</first><last>Chng</last></author>
	 <author><first>Haizhou</first><last>Li</last></author>
	 <booktitle>Proceedings of the Seventh Named Entities Workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>41–46</pages>
	 <abstract>Customized translation need pay spe-cial attention to the target domain ter-minology especially the named-entities for the domain. Adding linguistic features to neural machine translation (NMT) has been shown to benefit translation in many studies. In this paper, we further demonstrate that adding named-entity (NE) feature with named-entity recognition (NER) into the source language produces better translation with NMT. Our experiments show that by just including the different NE classes and boundary tags, we can increase the BLEU score by around 1 to 2 points using the standard test sets from WMT2017. We also show that adding NE tags using NER and applying in-domain adaptation can be combined to further improve customized machine translation.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2407</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>li-EtAl:2018:NEWS2018</bibkey>
    </paper>
    <paper id="2408">
	 <title>NEWS 2018 Whitepaper</title>
	 <author><first>Nancy</first><last>Chen</last></author>
	 <author><first>Xiangyu</first><last>Duan</last></author>
	 <author><first>Min</first><last>Zhang</last></author>
	 <author><first>Rafael E.</first><last>Banchs</last></author>
	 <author><first>Haizhou</first><last>Li</last></author>
	 <booktitle>Proceedings of the Seventh Named Entities Workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>47–54</pages>
	 <abstract>Transliteration is defined as phonetic translation of names across languages. Transliteration of Named Entities (NEs) is necessary in many applications, such as machine translation, corpus alignment, cross-language IR, information extraction and automatic lexicon acquisition. All such systems call for high-performance transliteration, which is the focus of shared task in the NEWS 2018 workshop. The objective of the shared task is to promote machine transliteration research by providing a common benchmarking platform for the community to evaluate the state-of-the-art technologies.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2408</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>chen-EtAl:2018:NEWS20181</bibkey>
    </paper>
    <paper id="2409">
	 <title>Report of NEWS 2018 Named Entity Transliteration Shared Task</title>
	 <author><first>Nancy</first><last>Chen</last></author>
	 <author><first>Rafael E.</first><last>Banchs</last></author>
	 <author><first>Min</first><last>Zhang</last></author>
	 <author><first>Xiangyu</first><last>Duan</last></author>
	 <author><first>Haizhou</first><last>Li</last></author>
	 <booktitle>Proceedings of the Seventh Named Entities Workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>55–73</pages>
	 <abstract>This report presents the results from the Named Entity Transliteration Shared Task conducted as part of The Seventh Named Entities Workshop (NEWS 2018) held at ACL 2018 in Melbourne, Australia. Similar to previous editions of NEWS, the Shared Task featured 19 tasks on proper name transliteration, including 13 different languages and two different Japanese scripts. A total of 6 teams from 8 different institutions participated in the evaluation, submitting 424 runs, involving different transliteration methodologies. Four performance metrics were used to report the evaluation results. The NEWS shared task on machine transliteration has successfully achieved its objectives by providing a common ground for the research community to conduct comparative evaluations of state-of-the-art technologies that will benefit the future research and development in this area.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2409</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>chen-EtAl:2018:NEWS20182</bibkey>
    </paper>
    <paper id="2410">
	 <title>Statistical Machine Transliteration Baselines for NEWS 2018</title>
	 <author><first>Snigdha</first><last>Singhania</last></author>
	 <author><first>Minh</first><last>Nguyen</last></author>
	 <author><first>Gia H</first><last>Ngo</last></author>
	 <author><first>Nancy</first><last>Chen</last></author>
	 <booktitle>Proceedings of the Seventh Named Entities Workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>74–78</pages>
	 <abstract>This paper reports the results of our trans-literation experiments conducted on NEWS 2018 Shared Task dataset. We focus on creating the baseline systems trained using two open-source, statistical transliteration tools, namely Sequitur and Moses. We discuss the pre-processing steps performed on this dataset for both the systems. We also provide a re-ranking system which uses top hypotheses from Sequitur and Moses to create a consolidated list of transliterations. The results obtained from each of these models can be used to present a good starting point for the participating teams.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2410</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>singhania-EtAl:2018:NEWS2018</bibkey>
    </paper>
    <paper id="2411">
	 <title>A Deep Learning Based Approach to Transliteration</title>
	 <author><first>Soumyadeep</first><last>Kundu</last></author>
	 <author><first>Sayantan</first><last>Paul</last></author>
	 <author><first>Santanu</first><last>Pal</last></author>
	 <booktitle>Proceedings of the Seventh Named Entities Workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>79–83</pages>
	 <abstract>In this paper, we propose different architectures for language independent machine transliteration which is extremely important for natural language processing (NLP) applications. Though a number of statistical models for transliteration have already been proposed in the past few decades, we proposed some neural network based deep learning architectures for the transliteration of named entities. Our transliteration systems adapt two different neural machine translation (NMT) frameworks: recurrent neural network and convolutional sequence to sequence based NMT. It is shown that our method provides quite satisfactory results when it comes to multi lingual machine transliteration. Our submitted runs are an ensemble of different transliteration systems for all the language pairs. In the NEWS 2018 Shared Task on Transliteration, our method achieves top performance for the En–Pe and Pe–En language pairs and comparable results for other cases.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2411</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>kundu-paul-pal:2018:NEWS2018</bibkey>
    </paper>
    <paper id="2412">
	 <title>Comparison of Assorted Models for Transliteration</title>
	 <author><first>Saeed</first><last>Najafi</last></author>
	 <author><first>Bradley</first><last>Hauer</last></author>
	 <author><first>Rashed Rubby</first><last>Riyadh</last></author>
	 <author><first>Leyuan</first><last>Yu</last></author>
	 <author><first>Grzegorz</first><last>Kondrak</last></author>
	 <booktitle>Proceedings of the Seventh Named Entities Workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>84–88</pages>
	 <abstract>We report the results of our experiments in the context of the NEWS 2018 Shared Task on Transliteration. We focus on the comparison of several diverse systems, including three neural MT models. A combination of discriminative, generative, and neural models obtains the best results on the development sets. We also put forward ideas for improving the shared task.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2412</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>najafi-EtAl:2018:NEWS2018</bibkey>
    </paper>
    <paper id="2413">
	 <title>Neural Machine Translation Techniques for Named Entity Transliteration</title>
	 <author><first>Roman</first><last>Grundkiewicz</last></author>
	 <author><first>Kenneth</first><last>Heafield</last></author>
	 <booktitle>Proceedings of the Seventh Named Entities Workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>89–94</pages>
	 <abstract>Transliterating named entities from one language into another can be approached as neural machine translation (NMT) problem, for which we use deep attentional RNN encoder-decoder models. To build a strong transliteration system, we apply well-established techniques from NMT, such as dropout regularization, model ensembling, rescoring with right-to-left models, and back-translation. Our submission to the NEWS 2018 Shared Task on Named Entity Transliteration ranked first in several tracks.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2413</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>grundkiewicz-heafield:2018:NEWS2018</bibkey>
    </paper>
    <paper id="2414">
	 <title>Low-Resource Machine Transliteration Using Recurrent Neural Networks of Asian Languages</title>
	 <author><first>Ngoc Tan</first><last>Le</last></author>
	 <author><first>Fatiha</first><last>Sadat</last></author>
	 <booktitle>Proceedings of the Seventh Named Entities Workshop</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>95–100</pages>
	 <abstract>Grapheme-to-phoneme models are key components in automatic speech recognition and text-to-speech systems. With low-resource language pairs that do not have available and well-developed pronunciation lexicons, grapheme-to-phoneme models are particularly useful. These models are based on initial alignments between grapheme source and phoneme target sequences. Inspired by sequence-to-sequence recurrent neural network-based translation methods, the current research presents an approach that applies an alignment representation for input sequences and pre-trained source and target embeddings to overcome the transliteration problem for a low-resource languages pair. We participated in the NEWS 2018 shared task for the English-Vietnamese transliteration task.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2414</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>le-sadat:2018:NEWS2018</bibkey>
    </paper>
    <paper id="2500">
	 <title>Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</title>
	 <editor><first>Eunjeong L.</first><last>Park</last></editor>
	 <editor><first>Masato</first><last>Hagiwara</last></editor>
	 <editor><first>Dmitrijs</first><last>Milajevs</last></editor>
	 <editor><first>Liling</first><last>Tan</last></editor>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <url>http://www.aclweb.org/anthology/W18-25</url>
	 <bibtype>book</bibtype>
	 <bibkey>NLP-OSS:2018</bibkey>
    </paper>
    <paper id="2501">
	 <title>AllenNLP: A Deep Semantic Natural Language Processing Platform</title>
	 <author><first>Matt</first><last>Gardner</last></author>
	 <author><first>Joel</first><last>Grus</last></author>
	 <author><first>Mark</first><last>Neumann</last></author>
	 <author><first>Oyvind</first><last>Tafjord</last></author>
	 <author><first>Pradeep</first><last>Dasigi</last></author>
	 <author><first>Nelson</first><last>F. Liu</last></author>
	 <author><first>Matthew</first><last>Peters</last></author>
	 <author><first>Michael</first><last>Schmitz</last></author>
	 <author><first>Luke</first><last>Zettlemoyer</last></author>
	 <booktitle>Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>1–6</pages>
	 <abstract>Modern natural language processing (NLP) research requires writing code. Ideally this code would provide a precise definition of the approach, easy repeatability of results, and a basis for extending the research. However, many research codebases bury high-level parameters under implementation details, are challenging to run and debug, and are difficult enough to extend that they are more likely to be rewritten. This paper describes AllenNLP, a library for applying deep learning methods to NLP research that addresses these issues with easy-to-use command-line tools, declarative configuration-driven experiments, and modular NLP abstractions. AllenNLP has already increased the rate of research experimentation and the sharing of NLP components at the Allen Institute for Artificial Intelligence, and we are working to have the same impact across the field.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2501</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>gardner-EtAl:2018:NLP-OSS</bibkey>
    </paper>
    <paper id="2502">
	 <title>Stop Word Lists in Free Open-source Software Packages</title>
	 <author><first>Joel</first><last>Nothman</last></author>
	 <author><first>Hanmin</first><last>Qin</last></author>
	 <author><first>Roman</first><last>Yurchak</last></author>
	 <booktitle>Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>7–12</pages>
	 <abstract>Open-source software packages for language processing often include stop word lists. Users may apply them without awareness of their surprising omissions (e.g. “hasn’t” but not “hadn’t”) and inclusions (“computer”), or their incompatibility with a particular tokenizer. Motivated by issues raised about the Scikit-learn stop list, we investigate variation among and consistency within 52 popular English-language stop lists, and propose strategies for mitigating these issues.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2502</url>
	 <software>W18-2502.Software.zip</software>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>nothman-qin-yurchak:2018:NLP-OSS</bibkey>
    </paper>
    <paper id="2503">
	 <title>Texar: A Modularized, Versatile, and Extensible Toolbox for Text Generation</title>
	 <author><first>Zhiting</first><last>Hu</last></author>
	 <author><first>Zichao</first><last>Yang</last></author>
	 <author><first>Tiancheng</first><last>Zhao</last></author>
	 <author><first>Haoran</first><last>Shi</last></author>
	 <author><first>Junxian</first><last>He</last></author>
	 <author><first>Di</first><last>Wang</last></author>
	 <author><first>Xuezhe</first><last>Ma</last></author>
	 <author><first>Zhengzhong</first><last>Liu</last></author>
	 <author><first>Xiaodan</first><last>Liang</last></author>
	 <author><first>Lianhui</first><last>Qin</last></author>
	 <author><first>Devendra Singh</first><last>Chaplot</last></author>
	 <author><first>Bowen</first><last>Tan</last></author>
	 <author><first>Xingjiang</first><last>Yu</last></author>
	 <author><first>Eric</first><last>Xing</last></author>
	 <booktitle>Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>13–22</pages>
	 <abstract>We introduce Texar, an open-source toolkit aiming to support the broad set of text generation tasks. Different from many existing toolkits that are specialized for specific applications (e.g., neural machine translation), Texar is designed to be highly flexible and versatile. This is achieved by abstracting the common patterns underlying the diverse tasks and methodologies, creating a library of highly reusable modules and functionalities, and enabling arbitrary model architectures and various algorithmic paradigms. The features make Texar particularly suitable for technique sharing and generalization across different text generation applications. The toolkit emphasizes heavily on extensibility and modularized system design, so that components can be freely plugged in or swapped out. We conduct extensive experiments and case studies to demonstrate the use and advantage of the toolkit.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2503</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>hu-EtAl:2018:NLP-OSS</bibkey>
    </paper>
    <paper id="2504">
	 <title>The ACL Anthology: Current State and Future Directions</title>
	 <author><first>Daniel</first><last>Gildea</last></author>
	 <author><first>Min-Yen</first><last>Kan</last></author>
	 <author><first>Nitin</first><last>Madnani</last></author>
	 <author><first>Christoph</first><last>Teichmann</last></author>
	 <author><first>Martin</first><last>Villalba</last></author>
	 <booktitle>Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>23–28</pages>
	 <abstract>The Association of Computational Linguistic’s Anthology is the open source archive, and the main source for computational linguistics and natural language processing’s scientific literature. The ACL Anthology is currently maintained exclusively by community volunteers and has to be available and up-to-date at all times. We first discuss the current, open source approach used to achieve this, and then discuss how the planned use of Docker images will improve the Anthology’s long-term stability. This change will make it easier for researchers to utilize Anthology data for experimentation. We believe the ACL community can directly benefit from the extension-friendly architecture of the Anthology. We end by issuing an open challenge of reviewer matching we encourage the community to rally towards.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2504</url>
	 <revision id="2">W18-2504v2</revision>
	 <attachment type="presentation">W18-2504.Presentation.pdf</attachment>
	 <attachment type="poster">W18-2504.Poster.pdf</attachment>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>gildea-EtAl:2018:NLP-OSS</bibkey>
    </paper>
    <paper id="2505">
	 <title>The risk of sub-optimal use of Open Source NLP Software: UKB is inadvertently state-of-the-art in knowledge-based WSD</title>
	 <author><first>Eneko</first><last>Agirre</last></author>
	 <author><first>Oier</first><last>Lopez de Lacalle</last></author>
	 <author><first>Aitor</first><last>Soroa</last></author>
	 <booktitle>Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>29–33</pages>
	 <abstract>UKB is an open source collection of programs for performing, among other tasks, Knowledge-Based Word Sense Disambiguation (WSD). Since it was released in 2009 it has been often used out-of-the-box in sub-optimal settings. We show that nine years later it is the state-of-the-art on knowledge-based WSD. This case shows the pitfalls of releasing open source NLP software without optimal default settings and precise instructions for reproducibility.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2505</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>agirre-lopezdelacalle-soroa:2018:NLP-OSS</bibkey>
    </paper>
    <paper id="2506">
	 <title>Baseline: A Library for Rapid Modeling, Experimentation and Development of Deep Learning Algorithms targeting NLP</title>
	 <author><first>Daniel</first><last>Pressel</last></author>
	 <author><first>Sagnik</first><last>Ray Choudhury</last></author>
	 <author><first>Brian</first><last>Lester</last></author>
	 <author><first>Yanjie</first><last>Zhao</last></author>
	 <author><first>Matt</first><last>Barta</last></author>
	 <booktitle>Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>34–40</pages>
	 <abstract>We introduce Baseline: a library for reproducible deep learning research and fast model development for NLP. The library provides easily extensible abstractions and implementations for data loading, model development, training and export of deep learning architectures. It also provides implementations for simple, high-performance, deep learning models for various NLP tasks, against which newly developed models can be compared. Deep learning experiments are hard to reproduce, Baseline provides functionalities to track them. The goal is to allow a researcher to focus on model development, delegating the repetitive tasks to the library.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2506</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>pressel-EtAl:2018:NLP-OSS</bibkey>
    </paper>
    <paper id="2507">
	 <title>OpenSeq2Seq: Extensible Toolkit for Distributed and Mixed Precision Training of Sequence-to-Sequence Models</title>
	 <author><first>Oleksii</first><last>Kuchaiev</last></author>
	 <author><first>Boris</first><last>Ginsburg</last></author>
	 <author><first>Igor</first><last>Gitman</last></author>
	 <author><first>Vitaly</first><last>Lavrukhin</last></author>
	 <author><first>Carl</first><last>Case</last></author>
	 <author><first>Paulius</first><last>Micikevicius</last></author>
	 <booktitle>Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>41–46</pages>
	 <abstract>We present OpenSeq2Seq – an open-source toolkit for training sequence-to-sequence models. The main goal of our toolkit is to allow researchers to most effectively explore different sequence-to-sequence architectures. The efficiency is achieved by fully supporting distributed and mixed-precision training. OpenSeq2Seq provides building blocks for training encoder-decoder models for neural machine translation and automatic speech recognition. We plan to extend it with other modalities in the future.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2507</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>kuchaiev-EtAl:2018:NLP-OSS</bibkey>
    </paper>
    <paper id="2508">
	 <title>Integrating Multiple NLP Technologies into an Open-source Platform for Multilingual Media Monitoring</title>
	 <author><first>Ulrich</first><last>Germann</last></author>
	 <author><first>Renars</first><last>Liepins</last></author>
	 <author><first>Didzis</first><last>Gosko</last></author>
	 <author><first>Guntis</first><last>Barzdins</last></author>
	 <booktitle>Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>47–51</pages>
	 <abstract>The open-source SUMMA Platform is a highly scalable distributed architecture for monitoring a large number of media broadcasts in parallel, with a lag behind actual broadcast time of at most a few minutes. It assembles numerous state-of-the-art NLP technologies into a fully automated media ingestion pipeline that can record live broadcasts, detect and transcribe spoken content, translate from several languages (original text or transcribed speech) into English, recognize Named Entities, detect topics, cluster and summarize documents across language barriers, and extract and store factual claims in these news items. This paper describes the intended use cases and discusses the system design decisions that allowed us to integrate state-of-the-art NLP modules into an effective workflow with comparatively little effort.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2508</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>germann-EtAl:2018:NLP-OSS</bibkey>
    </paper>
    <paper id="2509">
	 <title>The Annotated Transformer</title>
	 <author><first>Alexander</first><last>Rush</last></author>
	 <booktitle>Proceedings of Workshop for NLP Open Source Software (NLP-OSS)</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>52–60</pages>
	 <abstract>(Note this is not being submitted blind. The chair of the workshop requested this submission unblinded from me on twitter, so assuming that is okay.) A major goal of open-source NLP is to quickly and accurately reproduce the results of new work, in a manner that the community can easily use and modify. While most papers publish enough detail for replication, it still may be difficult to achieve good results in practice. This paper presents a worked exercise of paper reproduction with the goal of implementing the results of the recent Transformer model. The replication exercise aims at simple code structure that follows closely with the original work, while achieving an efficient usable system.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2509</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>rush:2018:NLP-OSS</bibkey>
    </paper>
    <paper id="2600">
	 <title>Proceedings of the Workshop on Machine Reading for Question Answering</title>
	 <editor><first>Eunsol</first><last>Choi</last></editor>
	 <editor><first>Minjoon</first><last>Seo</last></editor>
	 <editor><first>Danqi</first><last>Chen</last></editor>
	 <editor><first>Robin</first><last>Jia</last></editor>
	 <editor><first>Jonathan</first><last>Berant</last></editor>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <url>http://www.aclweb.org/anthology/W18-26</url>
	 <bibtype>book</bibtype>
	 <bibkey>W18-26:2018</bibkey>
    </paper>
    <paper id="2601">
	 <title>Ruminating Reader: Reasoning with Gated Multi-hop Attention</title>
	 <author><first>Yichen</first><last>Gong</last></author>
	 <author><first>Samuel</first><last>Bowman</last></author>
	 <booktitle>Proceedings of the Workshop on Machine Reading for Question Answering</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>1–11</pages>
	 <abstract>To answer the question in machine comprehension (MC) task, the models need to establish the interaction between the question and the context. To tackle the problem that the single-pass model cannot reflect on and correct its answer, we present Ruminating Reader. Ruminating Reader adds a second pass of attention and a novel information fusion component to the Bi-Directional Attention Flow model (BiDAF). We propose novel layer structures that construct a query aware context vector representation and fuse encoding representation with intermediate representation on top of BiDAF model. We show that a multi-hop attention mechanism can be applied to a bi-directional attention structure. In experiments on SQuAD, we find that the Reader outperforms the BiDAF baseline by 2.1 F1 score and 2.7 EM score. Our analysis shows that different hops of the attention have different responsibilities in selecting answers.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2601</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>gong-bowman:2018:W18-26</bibkey>
    </paper>
    <paper id="2602">
	 <title>Systematic Error Analysis of the Stanford Question Answering Dataset</title>
	 <author><first>Marc-Antoine</first><last>Rondeau</last></author>
	 <author><first>T. J.</first><last>Hazen</last></author>
	 <booktitle>Proceedings of the Workshop on Machine Reading for Question Answering</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>12–20</pages>
	 <abstract>We analyzed the outputs of multiple question answering (QA) models applied to the Stanford Question Answering Dataset (SQuAD) to identify the core challenges for QA systems on this data set. Through an iterative process, challenging aspects were hypothesized through qualitative analysis of the common error cases. A classifier was then constructed to predict whether SQuAD test examples were likely to be difficult for systems to answer based on features associated with the hypothesized aspects. The classifier’s performance was used to accept or reject each aspect as an indicator of difficulty. With this approach, we ensured that our hypotheses were systematically tested and not simply accepted based on our pre-existing biases. Our explanations are not accepted based on human evaluation of individual examples. This process also enabled us to identify the primary QA strategy learned by the models, i.e., systems determined the acceptable answer type for a question and then selected the acceptable answer span of that type containing the highest density of words present in the question within its local vicinity in the passage.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2602</url>
	 <attachment type="note">W18-2602.Notes.pdf</attachment>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>rondeau-hazen:2018:W18-26</bibkey>
    </paper>
    <paper id="2603">
	 <title>A Multi-Stage Memory Augmented Neural Network for Machine Reading Comprehension</title>
	 <author><first>Seunghak</first><last>Yu</last></author>
	 <author><first>Sathish Reddy</first><last>Indurthi</last></author>
	 <author><first>Seohyun</first><last>Back</last></author>
	 <author><first>Haejun</first><last>Lee</last></author>
	 <booktitle>Proceedings of the Workshop on Machine Reading for Question Answering</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>21–30</pages>
	 <abstract>Reading Comprehension (RC) of text is one of the fundamental tasks in natural language processing. In recent years, several end-to-end neural network models have been proposed to solve RC tasks. However, most of these models suffer in reasoning over long documents. In this work, we propose a novel Memory Augmented Machine Comprehension Network (MAMCN) to address long-range dependencies present in machine reading comprehension. We perform extensive experiments to evaluate proposed method with the renowned benchmark datasets such as SQuAD, QUASAR-T, and TriviaQA. We achieve the state of the art performance on both the document-level (QUASAR-T, TriviaQA) and paragraph-level (SQuAD) datasets compared to all the previously published approaches.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2603</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>yu-EtAl:2018:W18-26</bibkey>
    </paper>
    <paper id="2604">
	 <title>Tackling Adversarial Examples in QA via Answer Sentence Selection</title>
	 <author><first>Yuanhang</first><last>Ren</last></author>
	 <author><first>Ye</first><last>Du</last></author>
	 <author><first>Di</first><last>Wang</last></author>
	 <booktitle>Proceedings of the Workshop on Machine Reading for Question Answering</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>31–36</pages>
	 <abstract>Question answering systems deteriorate dramatically in the presence of adversarial sentences in articles. According to Jia and Liang (2017), the single BiDAF system (Seo et al., 2016) only achieves an F1 score of 4.8 on the ADDANY adversarial dataset. In this paper, we present a method to tackle this problem via answer sentence selection. Given a paragraph of an article and a corresponding query, instead of directly feeding the whole paragraph to the single BiDAF system, a sentence that most likely contains the answer to the query is first selected, which is done via a deep neural network based on TreeLSTM (Tai et al., 2015). Experiments on ADDANY adversarial dataset validate the effectiveness of our method. The F1 score has been improved to 52.3.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2604</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>ren-du-wang:2018:W18-26</bibkey>
    </paper>
    <paper id="2605">
	 <title>DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications</title>
	 <author><first>Wei</first><last>He</last></author>
	 <author><first>Kai</first><last>Liu</last></author>
	 <author><first>Jing</first><last>Liu</last></author>
	 <author><first>Yajuan</first><last>Lyu</last></author>
	 <author><first>Shiqi</first><last>Zhao</last></author>
	 <author><first>Xinyan</first><last>Xiao</last></author>
	 <author><first>Yuan</first><last>Liu</last></author>
	 <author><first>Yizhong</first><last>Wang</last></author>
	 <author><first>Hua</first><last>Wu</last></author>
	 <author><first>Qiaoqiao</first><last>She</last></author>
	 <author><first>Xuan</first><last>Liu</last></author>
	 <author><first>Tian</first><last>Wu</last></author>
	 <author><first>Haifeng</first><last>Wang</last></author>
	 <booktitle>Proceedings of the Workshop on Machine Reading for Question Answering</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>37–46</pages>
	 <abstract>This paper introduces DuReader, a new large-scale, open-domain Chinese machine reading comprehension (MRC) dataset, designed to address real-world MRC. DuReader has three advantages over previous MRC datasets: (1) data sources: questions and documents are based on Baidu Search and Baidu Zhidao; answers are manually generated. (2) question types: it provides rich annotations for more question types, especially yes-no and opinion questions, that leaves more opportunity for the research community. (3) scale: it contains 200K questions, 420K answers and 1M documents; it is the largest Chinese MRC dataset so far. Experiments show that human performance is well above current state-of-the-art baseline systems, leaving plenty of room for the community to make improvements. To help the community make these improvements, both DuReader and baseline systems have been posted online. We also organize a shared competition to encourage the exploration of more models. Since the release of the task, there are significant improvements over the baselines.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2605</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>he-EtAl:2018:W18-26</bibkey>
    </paper>
    <paper id="2606">
	 <title>Robust and Scalable Differentiable Neural Computer for Question Answering</title>
	 <author><first>Jörg</first><last>Franke</last></author>
	 <author><first>Jan</first><last>Niehues</last></author>
	 <author><first>Alex</first><last>Waibel</last></author>
	 <booktitle>Proceedings of the Workshop on Machine Reading for Question Answering</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>47–59</pages>
	 <abstract>Deep learning models are often not easily adaptable to new tasks and require task-specific adjustments. The differentiable neural computer (DNC), a memory-augmented neural network, is designed as a general problem solver which can be used in a wide range of tasks. But in reality, it is hard to apply this model to new tasks. We analyze the DNC and identify possible improvements within the application of question answering. This motivates a more robust and scalable DNC (rsDNC). The objective precondition is to keep the general character of this model intact while making its application more reliable and speeding up its required training time. The rsDNC is distinguished by a more robust training, a slim memory unit and a bidirectional architecture. We not only achieve new state-of-the-art performance on the bAbI task, but also minimize the performance variance between different initializations. Furthermore, we demonstrate the simplified applicability of the rsDNC to new tasks with passable results on the CNN RC task without adaptions.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2606</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>franke-niehues-waibel:2018:W18-26</bibkey>
    </paper>
    <paper id="2607">
	 <title>A Systematic Classification of Knowledge, Reasoning, and Context within the ARC Dataset</title>
	 <author><first>Michael</first><last>Boratko</last></author>
	 <author><first>Harshit</first><last>Padigela</last></author>
	 <author><first>Divyendra</first><last>Mikkilineni</last></author>
	 <author><first>Pritish</first><last>Yuvraj</last></author>
	 <author><first>Rajarshi</first><last>Das</last></author>
	 <author><first>Andrew</first><last>McCallum</last></author>
	 <author><first>Maria</first><last>Chang</last></author>
	 <author><first>Achille</first><last>Fokoue-Nkoutche</last></author>
	 <author><first>Pavan</first><last>Kapanipathi</last></author>
	 <author><first>Nicholas</first><last>Mattei</last></author>
	 <author><first>Ryan</first><last>Musa</last></author>
	 <author><first>Kartik</first><last>Talamadupula</last></author>
	 <author><first>Michael</first><last>Witbrock</last></author>
	 <booktitle>Proceedings of the Workshop on Machine Reading for Question Answering</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>60–70</pages>
	 <abstract>The recent work of Clark et al. (2018) introduces the AI2 Reasoning Challenge (ARC) and the associated ARC dataset that partitions open domain, complex science questions into easy and challenge sets. That paper includes an analysis of 100 questions with respect to the types of knowledge and reasoning required to answer them; however, it does not include clear definitions of these types, nor does it offer information about the quality of the labels. We propose a comprehensive set of definitions of knowledge and reasoning types necessary for answering the questions in the ARC dataset. Using ten annotators and a sophisticated annotation interface, we analyze the distribution of labels across the challenge set and statistics related to them. Additionally, we demonstrate that although naive information retrieval methods return sentences that are irrelevant to answering the query, sufficient supporting text is often present in the (ARC) corpus. Evaluating with human-selected relevant sentences improves the performance of a neural machine comprehension model by 42 points.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2607</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>boratko-EtAl:2018:W18-26</bibkey>
    </paper>
    <paper id="2608">
	 <title>RECIPE: Applying Open Domain Question Answering to Privacy Policies</title>
	 <author><first>Yan</first><last>Shvartzshanider</last></author>
	 <author><first>Ananth</first><last>Balashankar</last></author>
	 <author><first>Thomas</first><last>Wies</last></author>
	 <author><first>Lakshminarayanan</first><last>Subramanian</last></author>
	 <booktitle>Proceedings of the Workshop on Machine Reading for Question Answering</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>71–77</pages>
	 <abstract>We describe our experiences in using an open domain question answering model (Chen et al., 2017) to evaluate an out-of-domain QA task of assisting in analyzing privacy policies of companies. Specifically, Relevant CI Parameters Extractor (RECIPE) seeks to answer questions posed by the theory of contextual integrity (CI) regarding the information flows described in the privacy statements. These questions have a simple syntactic structure and the answers are factoids or descriptive in nature. The model achieved an F1 score of 72.33, but we noticed that combining the results of this model with a neural dependency parser based approach yields a significantly higher F1 score of 92.35 compared to manual annotations. This indicates that future work which in-corporates signals from parsing like NLP tasks more explicitly can generalize better on out-of-domain tasks.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2608</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>shvartzshanider-EtAl:2018:W18-26</bibkey>
    </paper>
    <paper id="2609">
	 <title>Neural Models for Key Phrase Extraction and Question Generation</title>
	 <author><first>Sandeep</first><last>Subramanian</last></author>
	 <author><first>Tong</first><last>Wang</last></author>
	 <author><first>Xingdi</first><last>Yuan</last></author>
	 <author><first>Saizheng</first><last>Zhang</last></author>
	 <author><first>Adam</first><last>Trischler</last></author>
	 <author><first>Yoshua</first><last>Bengio</last></author>
	 <booktitle>Proceedings of the Workshop on Machine Reading for Question Answering</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>78–88</pages>
	 <abstract>We propose a two-stage neural model to tackle question generation from documents. First, our model estimates the probability that word sequences in a document are ones that a human would pick when selecting candidate answers by training a neural key-phrase extractor on the answers in a question-answering corpus. Predicted key phrases then act as target answers and condition a sequence-to-sequence question-generation model with a copy mechanism. Empirically, our key-phrase extraction model significantly outperforms an entity-tagging baseline and existing rule-based approaches. We further demonstrate that our question generation system formulates fluent, answerable questions from key phrases. This two-stage system could be used to augment or generate reading comprehension datasets, which may be leveraged to improve machine reading systems or in educational settings.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2609</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>subramanian-EtAl:2018:W18-26</bibkey>
    </paper>
    <paper id="2610">
	 <title>Comparative Analysis of Neural QA models on SQuAD</title>
	 <author><first>Soumya</first><last>Wadhwa</last></author>
	 <author><first>Khyathi</first><last>Chandu</last></author>
	 <author><first>Eric</first><last>Nyberg</last></author>
	 <booktitle>Proceedings of the Workshop on Machine Reading for Question Answering</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>89–97</pages>
	 <abstract>The task of Question Answering has gained prominence in the past few decades for testing the ability of machines to understand natural language. Large datasets for Machine Reading have led to the development of neural models that cater to deeper language understanding compared to information retrieval tasks. Different components in these neural architectures are intended to tackle different challenges. As a first step towards achieving generalization across multiple domains, we attempt to understand and compare the peculiarities of existing end-to-end neural models on the Stanford Question Answering Dataset (SQuAD) by performing quantitative as well as qualitative analysis of the results attained by each of them. We observed that prediction errors reflect certain model-specific biases, which we further discuss in this paper.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2610</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>wadhwa-chandu-nyberg:2018:W18-26</bibkey>
    </paper>
    <paper id="2611">
	 <title>Adaptations of ROUGE and BLEU to Better Evaluate Machine Reading Comprehension Task</title>
	 <author><first>An</first><last>Yang</last></author>
	 <author><first>Kai</first><last>Liu</last></author>
	 <author><first>Jing</first><last>Liu</last></author>
	 <author><first>Yajuan</first><last>Lyu</last></author>
	 <author><first>Sujian</first><last>Li</last></author>
	 <booktitle>Proceedings of the Workshop on Machine Reading for Question Answering</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>98–104</pages>
	 <abstract>Current evaluation metrics to question answering based machine reading comprehension (MRC) systems generally focus on the lexical overlap between candidate and reference answers, such as ROUGE and BLEU. However, bias may appear when these metrics are used for specific question types, especially questions inquiring yes-no opinions and entity lists. In this paper, we make adaptations on the metrics to better correlate <tex-math>n</tex-math>-gram overlap with the human judgment for answers to these two question types. Statistical analysis proves the effectiveness of our approach. Our adaptations may provide positive guidance for the development of real-scene MRC systems.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2611</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>yang-EtAl:2018:W18-26</bibkey>
    </paper>
    <paper id="2700">
	 <title>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</title>
	 <editor><first>Alexandra</first><last>Birch</last></editor>
	 <editor><first>Andrew</first><last>Finch</last></editor>
	 <editor><first>Thang</first><last>Luong</last></editor>
	 <editor><first>Graham</first><last>Neubig</last></editor>
	 <editor><first>Yusuke</first><last>Oda</last></editor>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <url>http://www.aclweb.org/anthology/W18-27</url>
	 <bibtype>book</bibtype>
	 <bibkey>WNMT2018:2018</bibkey>
    </paper>
    <paper id="2701">
	 <title>Findings of the Second Workshop on Neural Machine Translation and Generation</title>
	 <author><first>Alexandra</first><last>Birch</last></author>
	 <author><first>Andrew</first><last>Finch</last></author>
	 <author><first>Minh-Thang</first><last>Luong</last></author>
	 <author><first>Graham</first><last>Neubig</last></author>
	 <author><first>Yusuke</first><last>Oda</last></author>
	 <booktitle>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>1–10</pages>
	 <abstract>This document describes the findings of the Second Workshop on Neural Machine Translation and Generation, held in concert with the annual conference of the Association for Computational Linguistics (ACL 2018). First, we summarize the research trends of papers presented in the proceedings, and note that there is particular interest in linguistic structure, domain adaptation, data augmentation, handling inadequate resources, and analysis of models. Second, we describe the results of the workshop’s shared task on efficient neural machine translation, where participants were tasked with creating MT systems that are both accurate and efficient.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2701</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>birch-EtAl:2018:WNMT2018</bibkey>
    </paper>
    <paper id="2702">
	 <title>A Shared Attention Mechanism for Interpretation of Neural Automatic Post-Editing Systems</title>
	 <author><first>Inigo</first><last>Jauregi Unanue</last></author>
	 <author><first>Ehsan</first><last>Zare Borzeshi</last></author>
	 <author><first>Massimo</first><last>Piccardi</last></author>
	 <booktitle>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>11–17</pages>
	 <abstract>Automatic post-editing (APE) systems aim to correct the systematic errors made by machine translators. In this paper, we propose a neural APE system that encodes the source (src) and machine translated (mt) sentences with two separate encoders, but leverages a shared attention mechanism to better understand how the two inputs contribute to the generation of the post-edited (pe) sentences. Our empirical observations have showed that when the mt is incorrect, the attention shifts weight toward tokens in the src sentence to properly edit the incorrect translation. The model has been trained and evaluated on the official data from the WMT16 and WMT17 APE IT domain English-German shared tasks. Additionally, we have used the extra 500K artificial data provided by the shared task. Our system has been able to reproduce the accuracies of systems trained with the same data, while at the same time providing better interpretability.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2702</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>jauregiunanue-zareborzeshi-piccardi:2018:WNMT2018</bibkey>
    </paper>
    <paper id="2703">
	 <title>Iterative Back-Translation for Neural Machine Translation</title>
	 <author><first>Vu Cong Duy</first><last>Hoang</last></author>
	 <author><first>Philipp</first><last>Koehn</last></author>
	 <author><first>Gholamreza</first><last>Haffari</last></author>
	 <author><first>Trevor</first><last>Cohn</last></author>
	 <booktitle>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>18–24</pages>
	 <abstract>We present iterative back-translation, a method for generating increasingly better synthetic parallel data from monolingual data to train neural machine translation systems. Our proposed method is very simple yet effective and highly applicable in practice. We demonstrate improvements in neural machine translation quality in both high and low resourced scenarios, including the best reported BLEU scores for the WMT 2017 German↔English tasks.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2703</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>hoang-EtAl:2018:WNMT20181</bibkey>
    </paper>
    <paper id="2704">
	 <title>Inducing Grammars with and for Neural Machine Translation</title>
	 <author><first>Yonatan</first><last>Bisk</last></author>
	 <author><first>Ke</first><last>Tran</last></author>
	 <booktitle>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>25–35</pages>
	 <abstract>Machine translation systems require semantic knowledge and grammatical understanding. Neural machine translation (NMT) systems often assume this information is captured by an attention mechanism and a decoder that ensures fluency. Recent work has shown that incorporating explicit syntax alleviates the burden of modeling both types of knowledge. However, requiring parses is expensive and does not explore the question of what syntax a model needs during translation. To address both of these issues we introduce a model that simultaneously translates while inducing dependency trees. In this way, we leverage the benefits of structure while investigating what syntax NMT must induce to maximize performance. We show that our dependency trees are 1. language pair dependent and 2. improve translation quality.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2704</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>bisk-tran:2018:WNMT2018</bibkey>
    </paper>
    <paper id="2705">
	 <title>Regularized Training Objective for Continued Training for Domain Adaptation in Neural Machine Translation</title>
	 <author><first>Huda</first><last>Khayrallah</last></author>
	 <author><first>Brian</first><last>Thompson</last></author>
	 <author><first>Kevin</first><last>Duh</last></author>
	 <author><first>Philipp</first><last>Koehn</last></author>
	 <booktitle>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>36–44</pages>
	 <abstract>Supervised domain adaptation—where a large generic corpus and a smaller in-domain corpus are both available for training—is a challenge for neural machine translation (NMT). Standard practice is to train a generic model and use it to initialize a second model, then continue training the second model on in-domain data to produce an in-domain model. We add an auxiliary term to the training objective during continued training that minimizes the cross entropy between the in-domain model’s output word distribution and that of the out-of-domain model to prevent the model’s output from differing too much from the original out-of-domain model. We perform experiments on EMEA (descriptions of medicines) and TED (rehearsed presentations), initialized from a general domain (WMT) model. Our method shows improvements over standard continued training by up to 1.5 BLEU.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2705</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>khayrallah-EtAl:2018:WNMT2018</bibkey>
    </paper>
    <paper id="2706">
	 <title>Controllable Abstractive Summarization</title>
	 <author><first>Angela</first><last>Fan</last></author>
	 <author><first>David</first><last>Grangier</last></author>
	 <author><first>Michael</first><last>Auli</last></author>
	 <booktitle>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>45–54</pages>
	 <abstract>Current models for document summarization disregard user preferences such as the desired length, style, the entities that the user might be interested in, or how much of the document the user has already read. We present a neural summarization model with a simple but effective mechanism to enable users to specify these high level attributes in order to control the shape of the final summaries to better suit their needs. With user input, our system can produce high quality summaries that follow user preferences. Without user input, we set the control variables automatically – on the full text CNN-Dailymail dataset, we outperform state of the art abstractive systems (both in terms of F1-ROUGE1 40.38 vs. 39.53 F1-ROUGE and human evaluation.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2706</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>fan-grangier-auli:2018:WNMT2018</bibkey>
    </paper>
    <paper id="2707">
	 <title>Enhancement of Encoder and Attention Using Target Monolingual Corpora in Neural Machine Translation</title>
	 <author><first>Kenji</first><last>Imamura</last></author>
	 <author><first>Atsushi</first><last>Fujita</last></author>
	 <author><first>Eiichiro</first><last>Sumita</last></author>
	 <booktitle>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>55–63</pages>
	 <abstract>A large-scale parallel corpus is required to train encoder-decoder neural machine translation. The method of using synthetic parallel texts, in which target monolingual corpora are automatically translated into source sentences, is effective in improving the decoder, but is unreliable for enhancing the encoder. In this paper, we propose a method that enhances the encoder and attention using target monolingual corpora by generating multiple source sentences via sampling. By using multiple source sentences, diversity close to that of humans is achieved. Our experimental results show that the translation quality is improved by increasing the number of synthetic source sentences for each given target sentence, and quality close to that using a manually created parallel corpus was achieved.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2707</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>imamura-fujita-sumita:2018:WNMT2018</bibkey>
    </paper>
    <paper id="2708">
	 <title>Document-Level Adaptation for Neural Machine Translation</title>
	 <author><first>Sachith Sri Ram</first><last>Kothur</last></author>
	 <author><first>Rebecca</first><last>Knowles</last></author>
	 <author><first>Philipp</first><last>Koehn</last></author>
	 <booktitle>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>64–73</pages>
	 <abstract>It is common practice to adapt machine translation systems to novel domains, but even a well-adapted system may be able to perform better on a particular document if it were to learn from a translator’s corrections within the document itself. We focus on adaptation within a single document – appropriate for an interactive translation scenario where a model adapts to a human translator’s input over the course of a document. We propose two methods: single-sentence adaptation (which performs online adaptation one sentence at a time) and dictionary adaptation (which specifically addresses the issue of translating novel words). Combining the two models results in improvements over both approaches individually, and over baseline systems, even on short documents. On WMT news test data, we observe an improvement of +1.8 BLEU points and +23.3% novel word translation accuracy and on EMEA data (descriptions of medications) we observe an improvement of +2.7 BLEU points and +49.2% novel word translation accuracy.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2708</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>kothur-knowles-koehn:2018:WNMT2018</bibkey>
    </paper>
    <paper id="2709">
	 <title>On the Impact of Various Types of Noise on Neural Machine Translation</title>
	 <author><first>Huda</first><last>Khayrallah</last></author>
	 <author><first>Philipp</first><last>Koehn</last></author>
	 <booktitle>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>74–83</pages>
	 <abstract>We examine how various types of noise in the parallel training data impact the quality of neural machine translation systems. We create five types of artificial noise and analyze how they degrade performance in neural and statistical machine translation. We find that neural models are generally more harmed by noise than statistical models. For one especially egregious type of noise they learn to just copy the input sentence.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2709</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>khayrallah-koehn:2018:WNMT2018</bibkey>
    </paper>
    <paper id="2710">
	 <title>Bi-Directional Neural Machine Translation with Synthetic Parallel Data</title>
	 <author><first>Xing</first><last>Niu</last></author>
	 <author><first>Michael</first><last>Denkowski</last></author>
	 <author><first>Marine</first><last>Carpuat</last></author>
	 <booktitle>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>84–91</pages>
	 <abstract>Despite impressive progress in high-resource settings, Neural Machine Translation (NMT) still struggles in low-resource and out-of-domain scenarios, often failing to match the quality of phrase-based translation. We propose a novel technique that combines back-translation and multilingual NMT to improve performance in these difficult cases. Our technique trains a single model for both directions of a language pair, allowing us to back-translate source or target monolingual data without requiring an auxiliary model. We then continue training on the augmented parallel data, enabling a cycle of improvement for a single model that can incorporate any source, target, or parallel data to improve both translation directions. As a byproduct, these models can reduce training and deployment costs significantly compared to uni-directional models. Extensive experiments show that our technique outperforms standard back-translation in low-resource scenarios, improves quality on cross-domain tasks, and effectively reduces costs across the board.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2710</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>niu-denkowski-carpuat:2018:WNMT2018</bibkey>
    </paper>
    <paper id="2711">
	 <title>Multi-Source Neural Machine Translation with Missing Data</title>
	 <author><first>Yuta</first><last>Nishimura</last></author>
	 <author><first>Katsuhito</first><last>Sudoh</last></author>
	 <author><first>Graham</first><last>Neubig</last></author>
	 <author><first>Satoshi</first><last>Nakamura</last></author>
	 <booktitle>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>92–99</pages>
	 <abstract>Multi-source translation is an approach to exploit multiple inputs (e.g. in two different languages) to increase translation accuracy. In this paper, we examine approaches for multi-source neural machine translation (NMT) using an incomplete multilingual corpus in which some translations are missing. In practice, many multilingual corpora are not complete due to the difficulty to provide translations in all of the relevant languages (for example, in TED talks, most English talks only have subtitles for a small portion of the languages that TED supports). Existing studies on multi-source translation did not explicitly handle such situations. This study focuses on the use of incomplete multilingual corpora in multi-encoder NMT and mixture of NMT experts and examines a very simple implementation where missing source translations are replaced by a special symbol &lt;NULL&gt;. These methods allow us to use incomplete corpora both at training time and test time. In experiments with real incomplete multilingual corpora of TED Talks, the multi-source NMT with the &lt;NULL&gt; tokens achieved higher translation accuracies measured by BLEU than those by any one-to-one NMT systems.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2711</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>nishimura-EtAl:2018:WNMT2018</bibkey>
    </paper>
    <paper id="2712">
	 <title>Towards one-shot learning for rare-word translation with external experts</title>
	 <author><first>Ngoc-Quan</first><last>Pham</last></author>
	 <author><first>Jan</first><last>Niehues</last></author>
	 <author><first>Alexander</first><last>Waibel</last></author>
	 <booktitle>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>100–109</pages>
	 <abstract>Neural machine translation (NMT) has significantly improved the quality of automatic translation models. One of the main challenges in current systems is the translation of rare words. We present a generic approach to address this weakness by having external models annotate the training data as Experts, and control the model-expert interaction with a pointer network and reinforcement learning. Our experiments using phrase-based models to simulate Experts to complement neural machine translation models show that the model can be trained to copy the annotations into the output consistently. We demonstrate the benefit of our proposed framework in outof domain translation scenarios with only lexical resources, improving more than 1.0 BLEU point in both translation directions English-Spanish and German-English.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2712</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>pham-niehues-waibel:2018:WNMT2018</bibkey>
    </paper>
    <paper id="2713">
	 <title>NICT Self-Training Approach to Neural Machine Translation at NMT-2018</title>
	 <author><first>Kenji</first><last>Imamura</last></author>
	 <author><first>Eiichiro</first><last>Sumita</last></author>
	 <booktitle>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>110–115</pages>
	 <abstract>This paper describes the NICT neural machine translation system submitted at the NMT-2018 shared task. A characteristic of our approach is the introduction of self-training. Since our self-training does not change the model structure, it does not influence the efficiency of translation, such as the translation speed. The experimental results showed that the translation quality improved not only in the sequence-to-sequence (seq-to-seq) models but also in the transformer models.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2713</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>imamura-sumita:2018:WNMT2018</bibkey>
    </paper>
    <paper id="2714">
	 <title>Fast Neural Machine Translation Implementation</title>
	 <author><first>Hieu</first><last>Hoang</last></author>
	 <author><first>Tomasz</first><last>Dwojak</last></author>
	 <author><first>Rihards</first><last>Krislauks</last></author>
	 <author><first>Daniel</first><last>Torregrosa</last></author>
	 <author><first>Kenneth</first><last>Heafield</last></author>
	 <booktitle>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>116–121</pages>
	 <abstract>This paper describes the submissions to the efficiency track for GPUs at the Workshop for Neural Machine Translation and Generation by members of the University of Edinburgh, Adam Mickiewicz University, Tilde and University of Alicante. We focus on efficient implementation of the recurrent deep-learning model as implemented in Amun, the fast inference engine for neural machine translation. We improve the performance with an efficient mini-batching algorithm, and by fusing the softmax operation with the k-best extraction algorithm. Submissions using Amun were first, second and third fastest in the GPU efficiency track.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2714</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>hoang-EtAl:2018:WNMT20182</bibkey>
    </paper>
    <paper id="2715">
	 <title>OpenNMT System Description for WNMT 2018: 800 words/sec on a single-core CPU</title>
	 <author><first>Jean</first><last>Senellart</last></author>
	 <author><first>Dakun</first><last>Zhang</last></author>
	 <author><first>Bo</first><last>WANG</last></author>
	 <author><first>Guillaume</first><last>KLEIN</last></author>
	 <author><first>Jean-Pierre</first><last>Ramatchandirin</last></author>
	 <author><first>Josep</first><last>Crego</last></author>
	 <author><first>Alexander</first><last>Rush</last></author>
	 <booktitle>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>122–128</pages>
	 <abstract>We present a system description of the OpenNMT Neural Machine Translation entry for the WNMT 2018 evaluation. In this work, we developed a heavily optimized NMT inference model targeting a high-performance CPU system. The final system uses a combination of four techniques, all of them lead to significant speed-ups in combination: (a) sequence distillation, (b) architecture modifications, (c) precomputation, particularly of vocabulary, and (d) CPU targeted quantization. This work achieves the fastest performance of the shared task, and led to the development of new features that have been integrated to OpenNMT and available to the community.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2715</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>senellart-EtAl:2018:WNMT2018</bibkey>
    </paper>
    <paper id="2716">
	 <title>Marian: Cost-effective High-Quality Neural Machine Translation in C++</title>
	 <author><first>Marcin</first><last>Junczys-Dowmunt</last></author>
	 <author><first>Kenneth</first><last>Heafield</last></author>
	 <author><first>Hieu</first><last>Hoang</last></author>
	 <author><first>Roman</first><last>Grundkiewicz</last></author>
	 <author><first>Anthony</first><last>Aue</last></author>
	 <booktitle>Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>129–135</pages>
	 <abstract>This paper describes the submissions of the “Marian” team to the WNMT 2018 shared task. We investigate combinations of teacher-student training, low-precision matrix products, auto-tuning and other methods to optimize the Transformer model on GPU and CPU. By further integrating these methods with the new averaging attention networks, a recently introduced faster Transformer variant, we create a number of high-quality, high-performance models on the GPU and CPU, dominating the Pareto frontier for this shared task.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2716</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>junczysdowmunt-EtAl:2018:WNMT2018</bibkey>
    </paper>
    <paper id="2800">
	 <title>Proceedings of the Eight Workshop on Cognitive Aspects of Computational Language Learning and Processing</title>
	 <editor><first>Marco</first><last>Idiart</last></editor>
	 <editor><first>Alessandro</first><last>Lenci</last></editor>
	 <editor><first>Thierry</first><last>Poibeau</last></editor>
	 <editor><first>Aline</first><last>Villavicencio</last></editor>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <url>http://www.aclweb.org/anthology/W18-28</url>
	 <bibtype>book</bibtype>
	 <bibkey>CogACLL:2018</bibkey>
    </paper>
    <paper id="2801">
	 <title>Predicting Brain Activation with WordNet Embeddings</title>
	 <author><first>João</first><last>António Rodrigues</last></author>
	 <author><first>Ruben</first><last>Branco</last></author>
	 <author><first>João</first><last>Silva</last></author>
	 <author><first>Chakaveh</first><last>Saedi</last></author>
	 <author><first>Antõnio</first><last>Branco</last></author>
	 <booktitle>Proceedings of the Eight Workshop on Cognitive Aspects of Computational Language Learning and Processing</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>1–5</pages>
	 <abstract>The task of taking a semantic representation of a noun and predicting the brain activity triggered by it in terms of fMRI spatial patterns was pioneered by Mitchell et al. 2008. That seminal work used word co-occurrence features to represent the meaning of the nouns. Even though the task does not impose any specific type of semantic representation, the vast majority of subsequent approaches resort to feature-based models or to semantic spaces (aka word embeddings). We address this task, with competitive results, by using instead a semantic network to encode lexical semantics, thus providing further evidence for the cognitive plausibility of this approach to model lexical meaning.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2801</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>antniorodrigues-EtAl:2018:CogACLL</bibkey>
    </paper>
    <paper id="2802">
	 <title>Do Speakers Produce Discourse Connectives Rationally?</title>
	 <author><first>Frances</first><last>Yung</last></author>
	 <author><first>Vera</first><last>Demberg</last></author>
	 <booktitle>Proceedings of the Eight Workshop on Cognitive Aspects of Computational Language Learning and Processing</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>6–16</pages>
	 <abstract>A number of different discourse connectives can be used to mark the same discourse relation, but it is unclear what factors affect connective choice. One recent account is the Rational Speech Acts theory, which predicts that speakers try to maximize the informativeness of an utterance such that the listener can interpret the intended meaning correctly. Existing prior work uses referential language games to test the rational account of speakers’ production of concrete meanings, such as identification of objects within a picture. Building on the same paradigm, we design a novel Discourse Continuation Game to investigate speakers’ production of abstract discourse relations. Experimental results reveal that speakers significantly prefer a more informative connective, in line with predictions of the RSA model.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2802</url>
	 <attachment type="note">W18-2802.Notes.pdf</attachment>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>yung-demberg:2018:CogACLL</bibkey>
    </paper>
    <paper id="2803">
	 <title>Language Production Dynamics with Recurrent Neural Networks</title>
	 <author><first>Jesús</first><last>Calvillo</last></author>
	 <author><first>Matthew</first><last>Crocker</last></author>
	 <booktitle>Proceedings of the Eight Workshop on Cognitive Aspects of Computational Language Learning and Processing</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>17–26</pages>
	 <abstract>We present an analysis of the internal mechanism of the recurrent neural model of sentence production presented by Calvillo et al. (2016). The results show clear patterns of computation related to each layer in the network allowing to infer an algorithmic account, where the semantics activates the semantically related words, then each word generated at each time step activates syntactic and semantic constraints on possible continuations, while the recurrence preserves information through time. We propose that such insights could generalize to other models with similar architecture, including some used in computational linguistics for language modeling, machine translation and image caption generation.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2803</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>calvillo-crocker:2018:CogACLL</bibkey>
    </paper>
    <paper id="2804">
	 <title>Multi-glance Reading Model for Text Understanding</title>
	 <author><first>Pengcheng</first><last>Zhu</last></author>
	 <author><first>Yujiu</first><last>Yang</last></author>
	 <author><first>Wenqiang</first><last>Gao</last></author>
	 <author><first>Yi</first><last>Liu</last></author>
	 <booktitle>Proceedings of the Eight Workshop on Cognitive Aspects of Computational Language Learning and Processing</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>27–35</pages>
	 <abstract>In recent years, a variety of recurrent neural networks have been proposed, e.g LSTM. However, existing models only read the text once, it cannot describe the situation of repeated reading in reading comprehension. In fact, when reading or analyzing a text, we may read the text several times rather than once if we couldn’t well understand it. So, how to model this kind of the reading behavior? To address the issue, we propose a multi-glance mechanism (MGM) for modeling the habit of reading behavior. In the proposed framework, the actual reading process can be fully simulated, and then the obtained information can be consistent with the task. Based on the multi-glance mechanism, we design two types of recurrent neural network models for repeated reading: Glance Cell Model (GCM) and Glance Gate Model (GGM). Visualization analysis of the GCM and the GGM demonstrates the effectiveness of multi-glance mechanisms. Experiments results on the large-scale datasets show that the proposed methods can achieve better performance.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2804</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>zhu-EtAl:2018:CogACLL</bibkey>
    </paper>
    <paper id="2805">
	 <title>Predicting Japanese Word Order in Double Object Constructions</title>
	 <author><first>Masayuki</first><last>Asahara</last></author>
	 <author><first>Satoshi</first><last>Nambu</last></author>
	 <author><first>Shin-Ichiro</first><last>Sano</last></author>
	 <booktitle>Proceedings of the Eight Workshop on Cognitive Aspects of Computational Language Learning and Processing</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>36–40</pages>
	 <abstract>This paper presents a statistical model to predict Japanese word order in the double object constructions. We employed a Bayesian linear mixed model with manually annotated predicate-argument structure data. The findings from the refined corpus analysis confirmed the effects of information status of an NP as ‘givennew ordering’ in addition to the effects of ‘long-before-short’ as a tendency of the general Japanese word order.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2805</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>asahara-nambu-sano:2018:CogACLL</bibkey>
    </paper>
    <paper id="2806">
	 <title>Affordances in Grounded Language Learning</title>
	 <author><first>Stephen</first><last>McGregor</last></author>
	 <author><first>KyungTae</first><last>Lim</last></author>
	 <booktitle>Proceedings of the Eight Workshop on Cognitive Aspects of Computational Language Learning and Processing</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>41–46</pages>
	 <abstract>We present a novel methodology involving mappings between different modes of semantic representation. We propose distributional semantic models as a mechanism for representing the kind of world knowledge inherent in the system of abstract symbols characteristic of a sophisticated community of language users. Then, motivated by insight from ecological psychology, we describe a model approximating affordances, by which we mean a language learner’s direct perception of opportunities for action in an environment. We present a preliminary experiment involving mapping between these two representational modalities, and propose that our methodology can become the basis for a cognitively inspired model of grounded language learning.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2806</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>mcgregor-lim:2018:CogACLL</bibkey>
    </paper>
    <paper id="2807">
	 <title>Rating Distributions and Bayesian Inference: Enhancing Cognitive Models of Spatial Language Use</title>
	 <author><first>Thomas</first><last>Kluth</last></author>
	 <author><first>Holger</first><last>Schultheis</last></author>
	 <booktitle>Proceedings of the Eight Workshop on Cognitive Aspects of Computational Language Learning and Processing</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>47–55</pages>
	 <abstract>We present two methods that improve the assessment of cognitive models. The first method is applicable to models computing average acceptability ratings. For these models, we propose an extension that simulates a full rating distribution (instead of average ratings) and allows generating individual ratings. Our second method enables Bayesian inference for models generating individual data. To this end, we propose to use the cross-match test (Rosenbaum, 2005) as a likelihood function. We exemplarily present both methods using cognitive models from the domain of spatial language use. For spatial language use, determining linguistic acceptability judgments of a spatial preposition for a depicted spatial relation is assumed to be a crucial process (Logan and Sadler, 1996). Existing models of this process compute an average acceptability rating. We extend the models and – based on existing data – show that the extended models allow extracting more information from the empirical data and yield more readily interpretable information about model successes and failures. Applying Bayesian inference, we find that model performance relies less on mechanisms of capturing geometrical aspects than on mapping the captured geometry to a rating interval.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2807</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>kluth-schultheis:2018:CogACLL</bibkey>
    </paper>
    <paper id="2808">
	 <title>The Role of Syntax During Pronoun Resolution: Evidence from fMRI</title>
	 <author><first>Jixing</first><last>Li</last></author>
	 <author><first>Murielle</first><last>Fabre</last></author>
	 <author><first>Wen-Ming</first><last>Luh</last></author>
	 <author><first>John</first><last>Hale</last></author>
	 <booktitle>Proceedings of the Eight Workshop on Cognitive Aspects of Computational Language Learning and Processing</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>56–64</pages>
	 <abstract>The current study examined the role of syntactic structure during pronoun resolution. We correlated complexity measures derived by the syntax-sensitive Hobbs algorithm and a neural network model for pronoun resolution with brain activity of participants listening to an audiobook during fMRI recording. Compared to the neural network model, the Hobbs algorithm is associated with larger clusters of brain activation in a network including the left Broca’s area.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2808</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>li-EtAl:2018:CogACLL</bibkey>
    </paper>
    <paper id="2809">
	 <title>A Sound and Complete Left-Corner Parsing for Minimalist Grammars</title>
	 <author><first>Miloš</first><last>Stanojević</last></author>
	 <author><first>Edward</first><last>Stabler</last></author>
	 <booktitle>Proceedings of the Eight Workshop on Cognitive Aspects of Computational Language Learning and Processing</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>65–74</pages>
	 <abstract>This paper presents a left-corner parser for minimalist grammars. The relation between the parser and the grammar is transparent in the sense that there is a very simple 1-1 correspondence between derivations and parses. Like left-corner context-free parsers, left-corner minimalist parsers can be non-terminating when the grammar has empty left corners, so an easily computed left-corner oracle is defined to restrict the search.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2809</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>stanojevi-stabler:2018:CogACLL</bibkey>
    </paper>
    <paper id="2900">
	 <title>Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP</title>
	 <editor><first>Georgiana</first><last>Dinu</last></editor>
	 <editor><first>Miguel</first><last>Ballesteros</last></editor>
	 <editor><first>Avirup</first><last>Sil</last></editor>
	 <editor><first>Sam</first><last>Bowman</last></editor>
	 <editor><first>Wael</first><last>Hamza</last></editor>
	 <editor><first>Anders</first><last>Sogaard</last></editor>
	 <editor><first>Tahira</first><last>Naseem</last></editor>
	 <editor><first>Yoav</first><last>Goldberg</last></editor>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <url>http://www.aclweb.org/anthology/W18-29</url>
	 <bibtype>book</bibtype>
	 <bibkey>W18-29:2018</bibkey>
    </paper>
    <paper id="2901">
	 <title>Compositional Morpheme Embeddings with Affixes as Functions and Stems as Arguments</title>
	 <author><first>Daniel</first><last>Edmiston</last></author>
	 <author><first>Karl</first><last>Stratos</last></author>
	 <booktitle>Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>1–5</pages>
	 <abstract>This work introduces a novel, linguistically motivated architecture for composing morphemes to derive word embeddings. The principal novelty in the work is to treat stems as vectors and affixes as functions over vectors. In this way, our model’s architecture more closely resembles the compositionality of morphemes in natural language. Such a model stands in opposition to models which treat morphemes uniformly, making no distinction between stem and affix. We run this new architecture on a dependency parsing task in Korean—a language rich in derivational morphology—and compare it against a lexical baseline,along with other sub-word architectures. StAffNet, the name of our architecture, shows competitive performance with the state-of-the-art on this task.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2901</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>edmiston-stratos:2018:W18-29</bibkey>
    </paper>
    <paper id="2902">
	 <title>Unsupervised Source Hierarchies for Low-Resource Neural Machine Translation</title>
	 <author><first>Anna</first><last>Currey</last></author>
	 <author><first>Kenneth</first><last>Heafield</last></author>
	 <booktitle>Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>6–12</pages>
	 <abstract>Incorporating source syntactic information into neural machine translation (NMT) has recently proven successful (Eriguchi et al., 2016; Luong et al., 2016). However, this is generally done using an outside parser to syntactically annotate the training data, making this technique difficult to use for languages or domains for which a reliable parser is not available. In this paper, we introduce an unsupervised tree-to-sequence (tree2seq) model for neural machine translation; this model is able to induce an unsupervised hierarchical structure on the source sentence based on the downstream task of neural machine translation. We adapt the Gumbel tree-LSTM of Choi et al. (2018) to NMT in order to create the encoder. We evaluate our model against sequential and supervised parsing baselines on three low- and medium-resource language pairs. For low-resource cases, the unsupervised tree2seq encoder significantly outperforms the baselines; no improvements are seen for medium-resource translation.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2902</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>currey-heafield:2018:W18-29</bibkey>
    </paper>
    <paper id="2903">
	 <title>Latent Tree Learning with Differentiable Parsers: Shift-Reduce Parsing and Chart Parsing</title>
	 <author><first>Jean</first><last>Maillard</last></author>
	 <author><first>Stephen</first><last>Clark</last></author>
	 <booktitle>Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>13–18</pages>
	 <abstract>Latent tree learning models represent sentences by composing their words according to an induced parse tree, all based on a downstream task. These models often outperform baselines which use (externally provided) syntax trees to drive the composition order. This work contributes (a) a new latent tree learning model based on shift-reduce parsing, with competitive downstream performance and non-trivial induced trees, and (b) an analysis of the trees learned by our shift-reduce model and by a chart-based model.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2903</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>maillard-clark:2018:W18-29</bibkey>
    </paper>
    <paper id="2904">
	 <title>Syntax Helps ELMo Understand Semantics: Is Syntax Still Relevant in a Deep Neural Architecture for SRL?</title>
	 <author><first>Emma</first><last>Strubell</last></author>
	 <author><first>Andrew</first><last>McCallum</last></author>
	 <booktitle>Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>19–27</pages>
	 <abstract>Do unsupervised methods for learning rich, contextualized token representations obviate the need for explicit modeling of linguistic structure in neural network models for semantic role labeling (SRL)? We address this question by incorporating the massively successful ELMo embeddings (Peters et al., 2018) into LISA (Strubell and McCallum, 2018), a strong, linguistically-informed neural network architecture for SRL. In experiments on the CoNLL-2005 shared task we find that though ELMo out-performs typical word embeddings, beginning to close the gap in F1 between LISA with predicted and gold syntactic parses, syntactically-informed models still out-perform syntax-free models when both use ELMo, especially on out-of-domain data. Our results suggest that linguistic structures are indeed still relevant in this golden age of deep learning for NLP.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2904</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>strubell-mccallum:2018:W18-29</bibkey>
    </paper>
    <paper id="2905">
	 <title>Subcharacter Information in Japanese Embeddings: When Is It Worth It?</title>
	 <author><first>Marzena</first><last>Karpinska</last></author>
	 <author><first>Bofang</first><last>Li</last></author>
	 <author><first>Anna</first><last>Rogers</last></author>
	 <author><first>Aleksandr</first><last>Drozd</last></author>
	 <booktitle>Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>28–37</pages>
	 <abstract>Languages with logographic writing systems present a difficulty for traditional character-level models. Leveraging the subcharacter information was recently shown to be beneficial for a number of intrinsic and extrinsic tasks in Chinese. We examine whether the same strategies could be applied for Japanese, and contribute a new analogy dataset for this language.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2905</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>karpinska-EtAl:2018:W18-29</bibkey>
    </paper>
    <paper id="2906">
	 <title>A neural parser as a direct classifier for head-final languages</title>
	 <author><first>Hiroshi</first><last>Kanayama</last></author>
	 <author><first>Masayasu</first><last>Muraoka</last></author>
	 <author><first>Ryosuke</first><last>Kohita</last></author>
	 <booktitle>Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>38–46</pages>
	 <abstract>This paper demonstrates a neural parser implementation suitable for consistently head-final languages such as Japanese. Unlike the transition- and graph-based algorithms in most state-of-the-art parsers, our parser directly selects the head word of a dependent from a limited number of candidates. This method drastically simplifies the model so that we can easily interpret the output of the neural model. Moreover, by exploiting grammatical knowledge to restrict possible modification types, we can control the output of the parser to reduce specific errors without adding annotated corpora. The neural parser performed well both on conventional Japanese corpora and the Japanese version of Universal Dependency corpus, and the advantages of distributed representations were observed in the comparison with the non-neural conventional model.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2906</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>kanayama-muraoka-kohita:2018:W18-29</bibkey>
    </paper>
    <paper id="2907">
	 <title>Syntactic Dependency Representations in Neural Relation Classification</title>
	 <author><first>Farhad</first><last>Nooralahzadeh</last></author>
	 <author><first>Lilja</first><last>Øvrelid</last></author>
	 <booktitle>Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>47–53</pages>
	 <abstract>We investigate the use of different syntactic dependency representations in a neural relation classification task and compare the CoNLL, Stanford Basic and Universal Dependencies schemes. We further compare with a syntax-agnostic approach and perform an error analysis in order to gain a better understanding of the results.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-2907</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>nooralahzadeh-vrelid:2018:W18-29</bibkey>
    </paper>
   <paper id="3000">
     <title>Proceedings of The Third Workshop on Representation Learning for NLP</title>
     <editor><first>Isabelle</first><last>Augenstein</last></editor>
     <editor><first>Kris</first><last>Cao</last></editor>
     <editor><first>He</first><last>He</last></editor>
     <editor><first>Felix</first><last>Hill</last></editor>
     <editor><first>Spandana</first><last>Gella</last></editor>
     <editor><first>Jamie</first><last>Kiros</last></editor>
     <editor><first>Hongyuan</first><last>Mei</last></editor>
     <editor><first>Dipendra</first><last>Misra</last></editor>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-30</url>
     <bibtype>book</bibtype>
     <bibkey>W18-30:2018</bibkey>
   </paper>

   <paper id="3001">
     <title>Corpus Specificity in LSA and Word2vec: The Role of Out-of-Domain Documents</title>
     <author><first>Edgar</first><last>Altszyler</last></author>
     <author><first>Mariano</first><last>Sigman</last></author>
     <author><first>Diego</first><last>Fernandez Slezak</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–10</pages>
     <url>http://www.aclweb.org/anthology/W18-3001</url>
     <abstract>Despite the popularity of word embeddings, the precise way by which they acquire semantic relations between words remain unclear. In the present article, we investigate whether LSA and word2vec capacity to identify relevant semantic relations increases with corpus size. One intuitive hypothesis is that the capacity to identify relevant associations should increase as the amount of data increases. However, if corpus size grows in topics which are not specific to the domain of interest, signal to noise ratio may weaken. Here we investigate the effect of corpus specificity and size in word-embeddings, and for this, we study two ways for progressive elimination of documents: the elimination of random documents vs. the elimination of documents unrelated to a specific task. We show that word2vec can take advantage of all the documents, obtaining its best performance when it is trained with the whole corpus. On the contrary, the specialization (removal of out-of-domain documents) of the training corpus, accompanied by a decrease of dimensionality, can increase LSA word-representation quality while speeding up the processing time. From a cognitive-modeling point of view, we point out that LSA’s word-knowledge acquisitions may not be efficiently exploiting higher-order co-occurrences and global relations, whereas word2vec does.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>altszyler-sigman-fernandezslezak:2018:W18-30</bibkey>
   </paper>

   <paper id="3002">
     <title>Hierarchical Convolutional Attention Networks for Text Classification</title>
     <author><first>Shang</first><last>Gao</last></author>
     <author><first>Arvind</first><last>Ramanathan</last></author>
     <author><first>Georgia</first><last>Tourassi</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>11–23</pages>
     <url>http://www.aclweb.org/anthology/W18-3002</url>
     <abstract>Recent work in machine translation has demonstrated that self-attention mechanisms can be used in place of recurrent neural networks to increase training speed without sacrificing model accuracy. We propose combining this approach with the benefits of convolutional filters and a hierarchical structure to create a document classification model that is both highly accurate and fast to train – we name our method Hierarchical Convolutional Attention Networks. We demonstrate the effectiveness of this architecture by surpassing the accuracy of the current state-of-the-art on several classification tasks while being twice as fast to train.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>gao-ramanathan-tourassi:2018:W18-30</bibkey>
   </paper>

   <paper id="3003">
     <title>Extrofitting: Enriching Word Representation and its Vector Space with Semantic Lexicons</title>
     <author><first>Hwiyeol</first><last>Jo</last></author>
     <author><first>Stanley Jungkyu</first><last>Choi</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>24–29</pages>
     <url>http://www.aclweb.org/anthology/W18-3003</url>
     <abstract>We propose post-processing method for enriching not only word representation but also its vector space using semantic lexicons, which we call extrofitting. The method consists of 3 steps as follows: (i) Expanding 1 or more dimension(s) on all the word vectors, filling with their representative value. (ii) Transferring semantic knowledge by averaging each representative values of synonyms and filling them in the expanded dimension(s). These two steps make representations of the synonyms close together. (iii) Projecting the vector space using Linear Discriminant Analysis, which eliminates the expanded dimension(s) with semantic knowledge. When experimenting with GloVe, we find that our method outperforms Faruqui’s retrofitting on some of word similarity task. We also report further analysis on our method in respect to word vector dimensions, vocabulary size as well as other well-known pretrained word vectors (e.g., Word2Vec, Fasttext).</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>jo-choi:2018:W18-30</bibkey>
   </paper>

   <paper id="3004">
     <title>Chat Discrimination for Intelligent Conversational Agents with a Hybrid CNN-LMTGRU Network</title>
     <author><first>Dennis Singh</first><last>Moirangthem</last></author>
     <author><first>Minho</first><last>Lee</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>30–40</pages>
     <url>http://www.aclweb.org/anthology/W18-3004</url>
     <abstract>Recently, intelligent dialog systems and smart assistants have attracted the attention of many, and development of novel dialogue agents have become a research challenge. Intelligent agents that can handle both domain-specific task-oriented and open-domain chit-chat dialogs are one of the major requirements in the current systems. In order to address this issue and to realize such smart hybrid dialogue systems, we develop a model to discriminate user utterance between task-oriented and chit-chat conversations. We introduce a hybrid of convolutional neural network (CNN) and a lateral multiple timescale gated recurrent units (LMTGRU) that can represent multiple temporal scale dependencies for the discrimination task. With the help of the combined slow and fast units of the LMTGRU, our model effectively determines whether a user will have a chit-chat conversation or a task-specific conversation with the system. We also show that the LMTGRU structure helps the model to perform well on longer text inputs. We address the lack of dataset by constructing a dataset using Twitter and Maluuba Frames data. The results of the experiments demonstrate that the proposed hybrid network outperforms the conventional models on the chat discrimination task as well as performed comparable to the baselines on various benchmark datasets.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>moirangthem-lee:2018:W18-30</bibkey>
   </paper>

   <paper id="3005">
     <title>Text Completion using Context-Integrated Dependency Parsing</title>
     <author><first>Amr Rekaby</first><last>Salama</last></author>
     <author><first>Özge</first><last>Alacam</last></author>
     <author><first>Wolfgang</first><last>Menzel</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>41–49</pages>
     <url>http://www.aclweb.org/anthology/W18-3005</url>
     <abstract>Incomplete linguistic input, i.e. due to a noisy environment, is one of the challenges that a successful communication system has to deal with. In this paper, we study text completion with a data set composed of sentences with gaps where a successful completion cannot be achieved through a uni-modal (language-based) approach. We present a solution based on a context-integrating dependency parser incorporating an additional non-linguistic modality. An incompleteness in one channel is compensated by information from another one and the parser learns the association between the two modalities from a multiple level knowledge representation. We examined several model variations by adjusting the degree of influence of different modalities in the decision making on possible filler words and their exact reference to a non-linguistic context element. Our model is able to fill the gap with 95.4% word and 95.2% exact reference accuracy hence the successful prediction can be achieved not only on the word level (such as mug) but also with respect to the correct identification of its context reference (such as mug 2 among several mug instances).</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>salama-alacam-menzel:2018:W18-30</bibkey>
   </paper>

   <paper id="3006">
     <title>Quantum-Inspired Complex Word Embedding</title>
     <author><first>Qiuchi</first><last>Li</last></author>
     <author><first>Sagar</first><last>Uprety</last></author>
     <author><first>Benyou</first><last>Wang</last></author>
     <author><first>Dawei</first><last>Song</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>50–57</pages>
     <url>http://www.aclweb.org/anthology/W18-3006</url>
     <abstract>A challenging task for word embeddings is to capture the emergent meaning or polarity of a combination of individual words. For example, existing approaches in word embeddings will assign high probabilities to the words “Penguin” and “Fly” if they frequently co-occur, but it fails to capture the fact that they occur in an opposite sense - Penguins do not fly. We hypothesize that humans do not associate a single polarity or sentiment to each word. The word contributes to the overall polarity of a combination of words depending upon which other words it is combined with. This is analogous to the behavior of microscopic particles which exist in all possible states at the same time and interfere with each other to give rise to new states depending upon their relative phases. We make use of the Hilbert Space representation of such particles in Quantum Mechanics where we subscribe a relative phase to each word, which is a complex number, and investigate two such quantum inspired models to derive the meaning of a combination of words. The proposed models achieve better performances than state-of-the-art non-quantum models on binary sentence classification tasks.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>li-EtAl:2018:W18-30</bibkey>
   </paper>

   <paper id="3007">
     <title>Natural Language Inference with Definition Embedding Considering Context On the Fly</title>
     <author><first>Kosuke</first><last>Nishida</last></author>
     <author><first>Kyosuke</first><last>Nishida</last></author>
     <author><first>Hisako</first><last>Asano</last></author>
     <author><first>Junji</first><last>Tomita</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>58–63</pages>
     <url>http://www.aclweb.org/anthology/W18-3007</url>
     <abstract>Natural language inference (NLI) is one of the most important tasks in NLP. In this study, we propose a novel method using word dictionaries, which are pairs of a word and its definition, as external knowledge. Our neural definition embedding mechanism encodes input sentences with the definitions of each word of the sentences on the fly. It can encode the definition of words considering the context of input sentences by using an attention mechanism. We evaluated our method using WordNet as a dictionary and confirmed that our method performed better than baseline models when using the full or a subset of 100d GloVe as word embeddings.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>nishida-EtAl:2018:W18-30</bibkey>
   </paper>

   <paper id="3008">
     <title>Comparison of Representations of Named Entities for Document Classification</title>
     <author><first>Lidia</first><last>Pivovarova</last></author>
     <author><first>Roman</first><last>Yangarber</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>64–68</pages>
     <url>http://www.aclweb.org/anthology/W18-3008</url>
     <abstract>We explore representations for multi-word names in text classification tasks, on Reuters (RCV1) topic and sector classification. We find that: the best way to treat names is to split them into tokens and use each token as a separate feature; NEs have more impact on sector classification than topic classification; replacing NEs with entity types is not an effective strategy; representing tokens by different embeddings for proper names vs. common nouns does not improve results. We highlight the improvements over state-of-the-art results that our CNN models yield.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>pivovarova-yangarber:2018:W18-30</bibkey>
   </paper>

   <paper id="3009">
     <title>Speeding up Context-based Sentence Representation Learning with Non-autoregressive Convolutional Decoding</title>
     <author><first>Shuai</first><last>Tang</last></author>
     <author><first>Hailin</first><last>Jin</last></author>
     <author><first>Chen</first><last>Fang</last></author>
     <author><first>Zhaowen</first><last>Wang</last></author>
     <author><first>Virginia</first><last>de Sa</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>69–78</pages>
     <url>http://www.aclweb.org/anthology/W18-3009</url>
     <abstract>We propose an asymmetric encoder-decoder structure, which keeps an RNN as the encoder and has a CNN as the decoder, and the model only explores the subsequent context information as the supervision. The asymmetry in both model architecture and training pair reduces a large amount of the training time. The contribution of our work is summarized as 1. We design experiments to show that an autoregressive decoder or an RNN decoder is not necessary for the encoder-decoder type of models in terms of learning sentence representations, and based on our results, we present 2 findings. 2. The two interesting findings lead to our final model design, which has an RNN encoder and a CNN decoder, and it learns to encode the current sentence and decode the subsequent contiguous words all at once. 3. With a suite of techniques, our model performs good on downstream tasks and can be trained efficiently on a large unlabelled corpus.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>tang-EtAl:2018:W18-30</bibkey>
   </paper>

   <paper id="3010">
     <title>Connecting Supervised and Unsupervised Sentence Embeddings</title>
     <author><first>Gil</first><last>Levi</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>79–83</pages>
     <url>http://www.aclweb.org/anthology/W18-3010</url>
     <abstract>Representing sentences as numerical vectors while capturing their semantic context is an important and useful intermediate step in natural language processing. Representations that are both general and discriminative can serve as a tool for tackling various NLP tasks. While common sentence representation methods are unsupervised in nature, recently, an approach for learning universal sentence representation in a supervised setting was presented in (Conneau et al.,2017). We argue that although promising results were obtained, an improvement can be reached by adding various unsupervised constraints that are motivated by auto-encoders and by language models. We show that by adding such constraints, superior sentence embeddings can be achieved. We compare our method with the original implementation and show improvements in several tasks.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>levi:2018:W18-30</bibkey>
   </paper>

   <paper id="3011">
     <title>A Hybrid Learning Scheme for Chinese Word Embedding</title>
     <author><first>Wenfan</first><last>Chen</last></author>
     <author><first>Weiguo</first><last>Sheng</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>84–90</pages>
     <url>http://www.aclweb.org/anthology/W18-3011</url>
     <abstract>To improve word embedding, subword information has been widely employed in state-of-the-art methods. These methods can be classified to either compositional or predictive models. In this paper, we propose a hybrid learning scheme, which integrates compositional and predictive model for word embedding. Such a scheme can take advantage of both models, thus effectively learning word embedding. The proposed scheme has been applied to learn word representation on Chinese. Our results show that the proposed scheme can significantly improve the performance of word embedding in terms of analogical reasoning and is robust to the size of training data.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>chen-sheng:2018:W18-30</bibkey>
   </paper>

   <paper id="3012">
     <title>Unsupervised Random Walk Sentence Embeddings: A Strong but Simple Baseline</title>
     <author><first>Kawin</first><last>Ethayarajh</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>91–100</pages>
     <url>http://www.aclweb.org/anthology/W18-3012</url>
     <abstract>Using a random walk model of text generation, Arora et al. (2017) proposed a strong baseline for computing sentence embeddings: take a weighted average of word embeddings and modify with SVD. This simple method even outperforms far more complex approaches such as LSTMs on textual similarity tasks. In this paper, we first show that word vector length has a confounding effect on the probability of a sentence being generated in Arora et al.’s model. We propose a random walk model that is robust to this confound, where the probability of word generation is inversely related to the angular distance between the word and sentence embeddings. Our approach beats Arora et al.’s by up to 44.4% on textual similarity tasks and is competitive with state-of-the-art methods. Unlike Arora et al.’s method, ours requires no hyperparameter tuning, which means it can be used when there is no labelled data.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>ethayarajh:2018:W18-30</bibkey>
   </paper>

   <paper id="3013">
     <title>Evaluating Word Embeddings in Multi-label Classification Using Fine-Grained Name Typing</title>
     <author><first>Yadollah</first><last>Yaghoobzadeh</last></author>
     <author><first>Katharina</first><last>Kann</last></author>
     <author><first>Hinrich</first><last>Schütze</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>101–106</pages>
     <url>http://www.aclweb.org/anthology/W18-3013</url>
     <abstract>Embedding models typically associate each word with a single real-valued vector, representing its different properties. Evaluation methods, therefore, need to analyze the accuracy and completeness of these properties in embeddings. This requires fine-grained analysis of embedding subspaces. Multi-label classification is an appropriate way to do so. We propose a new evaluation method for word embeddings based on multi-label classification given a word embedding. The task we use is fine-grained name typing: given a large corpus, find all types that a name can refer to based on the name embedding. Given the scale of entities in knowledge bases, we can build datasets for this task that are complementary to the current embedding evaluation datasets in: they are very large, contain fine-grained classes, and allow the direct evaluation of embeddings without confounding factors like sentence context.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>yaghoobzadeh-kann-schtze:2018:W18-30</bibkey>
   </paper>

   <paper id="3015">
     <title>Exploiting Common Characters in Chinese and Japanese to Learn Cross-Lingual Word Embeddings via Matrix Factorization</title>
     <author><first>Jilei</first><last>Wang</last></author>
     <author><first>Shiying</first><last>Luo</last></author>
     <author><first>Weiyan</first><last>Shi</last></author>
     <author><first>Tao</first><last>Dai</last></author>
     <author><first>Shu-Tao</first><last>Xia</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>113–121</pages>
     <url>http://www.aclweb.org/anthology/W18-3015</url>
     <abstract>Learning vector space representation of words (i.e., word embeddings) has recently attracted wide research interests, and has been extended to cross-lingual scenario. Currently most cross-lingual word embedding learning models are based on sentence alignment, which inevitably introduces much noise. In this paper, we show in Chinese and Japanese, the acquisition of semantic relation among words can benefit from the large number of common characters shared by both languages; inspired by this unique feature, we design a method named CJC targeting to generate cross-lingual context of words. We combine CJC with GloVe based on matrix factorization, and then propose an integrated model named CJ-Glo. Taking two sentence-aligned models and CJ-BOC (also exploits common characters but is based on CBOW) as baseline algorithms, we compare them with CJ-Glo on a series of NLP tasks including cross-lingual synonym, word analogy and sentence alignment. The result indicates CJ-Glo achieves the best performance among these methods, and is more stable in cross-lingual tasks; moreover, compared with CJ-BOC, CJ-Glo is less sensitive to the alteration of parameters.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>wang-EtAl:2018:W18-30</bibkey>
   </paper>

   <paper id="3016">
     <title>WordNet Embeddings</title>
     <author><first>Chakaveh</first><last>Saedi</last></author>
     <author><first>António</first><last>Branco</last></author>
     <author><first>João</first><last>António Rodrigues</last></author>
     <author><first>João</first><last>Silva</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>122–131</pages>
     <url>http://www.aclweb.org/anthology/W18-3016</url>
     <abstract>Semantic networks and semantic spaces have been two prominent approaches to represent lexical semantics. While a unified account of the lexical meaning relies on one being able to convert between these representations, in both directions, the conversion direction from semantic networks into semantic spaces started to attract more attention recently. In this paper we present a methodology for this conversion and assess it with a case study. When it is applied over WordNet, the performance of the resulting embeddings in a mainstream semantic similarity task is very good, substantially superior to the performance of word embeddings based on very large collections of texts like word2vec.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>saedi-EtAl:2018:W18-30</bibkey>
   </paper>

   <paper id="3017">
     <title>Knowledge Graph Embedding with Numeric Attributes of Entities</title>
     <author><first>Yanrong</first><last>Wu</last></author>
     <author><first>Zhichun</first><last>Wang</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>132–136</pages>
     <url>http://www.aclweb.org/anthology/W18-3017</url>
     <abstract>Knowledge Graph (KG) embedding projects entities and relations into low dimensional vector space, which has been successfully applied in KG completion task. The previous embedding approaches only model entities and their relations, ignoring a large number of entities’ numeric attributes in KGs. In this paper, we propose a new KG embedding model which jointly model entity relations and numeric attributes. Our approach combines an attribute embedding model with a translation-based structure embedding model, which learns the embeddings of entities, relations, and attributes simultaneously. Experiments of link prediction on YAGO and Freebase show that the performance is effectively improved by adding entities’ numeric attributes in the embedding model.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>wu-wang:2018:W18-30</bibkey>
   </paper>

   <paper id="3018">
     <title>Injecting Lexical Contrast into Word Vectors by Guiding Vector Space Specialisation</title>
     <author><first>Ivan</first><last>Vulić</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>137–143</pages>
     <url>http://www.aclweb.org/anthology/W18-3018</url>
     <abstract>Word vector space specialisation models offer a portable, light-weight approach to fine-tuning arbitrary distributional vector spaces to discern between synonymy and antonymy. Their effectiveness is drawn from external linguistic constraints that specify the exact lexical relation between words. In this work, we show that a careful selection of the external constraints can steer and improve the specialisation. By simply selecting appropriate constraints, we report state-of-the-art results on a suite of tasks with well-defined benchmarks where modeling lexical contrast is crucial: 1) true semantic similarity, with highest reported scores on SimLex-999 and SimVerb-3500 to date; 2) detecting antonyms; and 3) distinguishing antonyms from synonyms.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>vuli:2018:W18-30</bibkey>
   </paper>

   <paper id="3019">
     <title>Characters or Morphemes: How to Represent Words?</title>
     <author><first>Ahmet</first><last>Üstün</last></author>
     <author><first>Murathan</first><last>Kurfalı</last></author>
     <author><first>Burcu</first><last>Can</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>144–153</pages>
     <url>http://www.aclweb.org/anthology/W18-3019</url>
     <abstract>In this paper, we investigate the effects of using subword information in representa- tion learning. We argue that using syntactic subword units effects the quality of the word representations positively. We introduce a morpheme-based model and compare it against to word-based, character-based, and character n-gram level models. Our model takes a list of candidate segmentations of a word and learns the representation of the word based on different segmentations that are weighted by an attention mechanism. We performed experiments on Turkish as a morphologically rich language and English with a comparably poorer morphology. The results show that morpheme-based models are better at learning word representations of morphologically complex languages compared to character-based and character n-gram level models since the morphemes help to incorporate more syntactic knowledge in learning, that makes morpheme-based models better at syntactic tasks.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>stn-kurfal-can:2018:W18-30</bibkey>
   </paper>

   <paper id="3020">
     <title>Learning Hierarchical Structures On-The-Fly with a Recurrent-Recursive Model for Sequences</title>
     <author><first>Athul Paul</first><last>Jacob</last></author>
     <author><first>Zhouhan</first><last>Lin</last></author>
     <author><first>Alessandro</first><last>Sordoni</last></author>
     <author><first>Yoshua</first><last>Bengio</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>154–158</pages>
     <url>http://www.aclweb.org/anthology/W18-3020</url>
     <abstract>We propose a hierarchical model for sequential data that learns a tree on-the-fly, i.e. while reading the sequence. In the model, a recurrent network adapts its structure and reuses recurrent weights in a recursive manner. This creates adaptive skip-connections that ease the learning of long-term dependencies. The tree structure can either be inferred without supervision through reinforcement learning, or learned in a supervised manner. We provide preliminary experiments in a novel Math Expression Evaluation (MEE) task, which is created to have a hierarchical tree structure that can be used to study the effectiveness of our model. Additionally, we test our model in a well-known propositional logic and language modelling tasks. Experimental results have shown the potential of our approach.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>jacob-EtAl:2018:W18-30</bibkey>
   </paper>

   <paper id="3021">
     <title>Limitations of Cross-Lingual Learning from Image Search</title>
     <author><first>Mareike</first><last>Hartmann</last></author>
     <author><first>Anders</first><last>Søgaard</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>159–163</pages>
     <url>http://www.aclweb.org/anthology/W18-3021</url>
     <abstract>Cross-lingual representation learning is an important step in making NLP scale to all the world’s languages. Previous work on bilingual lexicon induction suggests that it is possible to learn cross-lingual representations of words based on similarities between images associated with these words. However, that work focused (almost exclusively) on the translation of nouns only. Here, we investigate whether the meaning of other parts-of-speech (POS), in particular adjectives and verbs, can be learned in the same way. Our experiments across five language pairs indicate that previous work does not scale to the problem of learning cross-lingual representations beyond simple nouns.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>hartmann-sgaard:2018:W18-30</bibkey>
   </paper>

   <paper id="3022">
     <title>Learning Semantic Textual Similarity from Conversations</title>
     <author><first>Yinfei</first><last>Yang</last></author>
     <author><first>Steve</first><last>Yuan</last></author>
     <author><first>Daniel</first><last>Cer</last></author>
     <author><first>Sheng-Yi</first><last>Kong</last></author>
     <author><first>Noah</first><last>Constant</last></author>
     <author><first>Petr</first><last>Pilar</last></author>
     <author><first>Heming</first><last>Ge</last></author>
     <author><first>Yun-hsuan</first><last>Sung</last></author>
     <author><first>Brian</first><last>Strope</last></author>
     <author><first>Ray</first><last>Kurzweil</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>164–174</pages>
     <url>http://www.aclweb.org/anthology/W18-3022</url>
     <abstract>We present a novel approach to learn representations for sentence-level semantic similarity using conversational data. Our method trains an unsupervised model to predict conversational responses. The resulting sentence embeddings perform well on the Semantic Textual Similarity (STS) Benchmark and SemEval 2017’s Community Question Answering (CQA) question similarity subtask. Performance is further improved by introducing multitask training, combining conversational response prediction and natural language inference. Extensive experiments show the proposed model achieves the best performance among all neural models on the STS Benchmark and is competitive with the state-of-the-art feature engineered and mixed systems for both tasks.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>yang-EtAl:2018:W18-30</bibkey>
   </paper>

   <paper id="3023">
     <title>Multilingual Seq2seq Training with Similarity Loss for Cross-Lingual Document Classification</title>
     <author><first>Katherine</first><last>Yu</last></author>
     <author><first>Haoran</first><last>Li</last></author>
     <author><first>Barlas</first><last>Oguz</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>175–179</pages>
     <url>http://www.aclweb.org/anthology/W18-3023</url>
     <abstract>In this paper we continue experiments where neural machine translation training is used to produce joint cross-lingual fixed-dimensional sentence embeddings. In this framework we introduce a simple method of adding a loss to the learning objective which penalizes distance between representations of bilingually aligned sentences. We evaluate cross-lingual transfer using two approaches, cross-lingual similarity search on an aligned corpus (Europarl) and cross-lingual document classification on a recently published benchmark Reuters corpus, and we find the similarity loss significantly improves performance on both. Furthermore, we notice that while our Reuters results are very competitive, our English results are not as competitive, showing room for improvement in the current cross-lingual state-ofthe-art. Our results are based on a set of 6 European languages.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>yu-li-oguz:2018:W18-30</bibkey>
   </paper>

   <paper id="3024">
     <title>LSTMs Exploit Linguistic Attributes of Data</title>
     <author><first>Nelson F.</first><last>Liu</last></author>
     <author><first>Omer</first><last>Levy</last></author>
     <author><first>Roy</first><last>Schwartz</last></author>
     <author><first>Chenhao</first><last>Tan</last></author>
     <author><first>Noah A.</first><last>Smith</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>180–186</pages>
     <url>http://www.aclweb.org/anthology/W18-3024</url>
     <abstract>While recurrent neural networks have found success in a variety of natural language processing applications, they are general models of sequential data. We investigate how the properties of natural language data affect an LSTM’s ability to learn a nonlinguistic task: recalling elements from its input. We find that models trained on natural language data are able to recall tokens from much longer sequences than models trained on non-language sequential data. Furthermore, we show that the LSTM learns to solve the memorization task by explicitly using a subset of its neurons to count timesteps in the input. We hypothesize that the patterns and structure in natural language data enable LSTMs to learn by providing approximate ways of reducing loss, but understanding the effect of different training data on the learnability of LSTMs remains an open question.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>liu-EtAl:2018:W18-30</bibkey>
   </paper>

   <paper id="3025">
     <title>Learning Distributional Token Representations from Visual Features</title>
     <author><first>Samuel</first><last>Broscheit</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>187–194</pages>
     <url>http://www.aclweb.org/anthology/W18-3025</url>
     <abstract>In this study, we compare token representations constructed from visual features (i.e., pixels) with standard lookup-based embeddings. Our goal is to gain insight about the challenges of encoding a text representation from low-level features, e.g. from characters or pixels. We focus on Chinese, which—as a logographic language—has properties that make a representation via visual features challenging and interesting. To train and evaluate different models for the token representation, we chose the task of character-based neural machine translation (NMT) from Chinese to English. We found that a token representation computed only from visual features can achieve competitive results to lookup embeddings. However, we also show different strengths and weaknesses in the models’ performance in a part-of-speech tagging task and also a semantic similarity task. In summary, we show that it is possible to achieve a <i>text representation</i> only from pixels. We hope that this is a useful stepping stone for future studies that exclusively rely on visual input, or aim at exploiting visual features of written language.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>broscheit:2018:W18-30</bibkey>
   </paper>

   <paper id="3026">
     <title>Jointly Embedding Entities and Text with Distant Supervision</title>
     <author><first>Denis</first><last>Newman-Griffis</last></author>
     <author><first>Albert M.</first><last>Lai</last></author>
     <author><first>Eric</first><last>Fosler-Lussier</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>195–206</pages>
     <url>http://www.aclweb.org/anthology/W18-3026</url>
     <abstract>Learning representations for knowledge base entities and concepts is becoming increasingly important for NLP applications. However, recent entity embedding methods have relied on structured resources that are expensive to create for new domains and corpora. We present a distantly-supervised method for jointly learning embeddings of entities and text from an unnanotated corpus, using only a list of mappings between entities and surface forms. We learn embeddings from open-domain and biomedical corpora, and compare against prior methods that rely on human-annotated text or large knowledge graph structure. Our embeddings capture entity similarity and relatedness better than prior work, both in existing biomedical datasets and a new Wikipedia-based dataset that we release to the community. Results on analogy completion and entity sense disambiguation indicate that entities and words capture complementary information that can be effectively combined for downstream use.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>newmangriffis-lai-foslerlussier:2018:W18-30</bibkey>
   </paper>

   <paper id="3027">
     <title>A Sequence-to-Sequence Model for Semantic Role Labeling</title>
     <author><first>Angel</first><last>Daza</last></author>
     <author><first>Anette</first><last>Frank</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>207–216</pages>
     <url>http://www.aclweb.org/anthology/W18-3027</url>
     <abstract>We explore a novel approach for Semantic Role Labeling (SRL) by casting it as a sequence-to-sequence process. We employ an attention-based model enriched with a copying mechanism to ensure faithful regeneration of the input sequence, while enabling interleaved generation of argument role labels. We apply this model in a monolingual setting, performing PropBank SRL on English language data. The constrained sequence generation set-up enforced with the copying mechanism allows us to analyze the performance and special properties of the model on manually labeled data and benchmarking against state-of-the-art sequence labeling models. We show that our model is able to solve the SRL argument labeling task on English data, yet further structural decoding constraints will need to be added to make the model truly competitive. Our work represents the first step towards more advanced, generative SRL labeling setups.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>daza-frank:2018:W18-30</bibkey>
   </paper>

   <paper id="3028">
     <title>Predicting Concreteness and Imageability of Words Within and Across Languages via Word Embeddings</title>
     <author><first>Nikola</first><last>Ljubešić</last></author>
     <author><first>Darja</first><last>Fišer</last></author>
     <author><first>Anita</first><last>Peti-Stantić</last></author>
     <booktitle>Proceedings of The Third Workshop on Representation Learning for NLP</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>217–222</pages>
     <url>http://www.aclweb.org/anthology/W18-3028</url>
     <abstract>The notions of concreteness and imageability, traditionally important in psycholinguistics, are gaining significance in semantic-oriented natural language processing tasks. In this paper we investigate the predictability of these two concepts via supervised learning, using word embeddings as explanatory variables. We perform predictions both within and across languages by exploiting collections of cross-lingual embeddings aligned to a single vector space. We show that the notions of concreteness and imageability are highly predictable both within and across languages, with a moderate loss of up to 20% in correlation when predicting across languages. We further show that the cross-lingual transfer via word embeddings is more efficient than the simple transfer via bilingual dictionaries.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>ljubei-fier-petistanti:2018:W18-30</bibkey>
   </paper>

    <paper id="3100">
	 <title>Proceedings of the First Workshop on Economics and Natural Language Processing</title>
	 <editor><first>Udo</first><last>Hahn</last></editor>
	 <editor><first>Véronique</first><last>Hoste</last></editor>
	 <editor><first>Ming-Feng</first><last>Tsai</last></editor>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <url>http://www.aclweb.org/anthology/W18-31</url>
	 <bibtype>book</bibtype>
	 <bibkey>W18-31:2018</bibkey>
    </paper>
    <paper id="3101">
	 <title>Economic Event Detection in Company-Specific News Text</title>
	 <author><first>Gilles</first><last>Jacobs</last></author>
	 <author><first>Els</first><last>Lefever</last></author>
	 <author><first>Véronique</first><last>Hoste</last></author>
	 <booktitle>Proceedings of the First Workshop on Economics and Natural Language Processing</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>1–10</pages>
	 <abstract>This paper presents a dataset and supervised classification approach for economic event detection in English news articles. Currently, the economic domain is lacking resources and methods for data-driven supervised event detection. The detection task is conceived as a sentence-level classification task for 10 different economic event types. Two different machine learning approaches were tested: a rich feature set Support Vector Machine (SVM) set-up and a word-vector-based long short-term memory recurrent neural network (RNN-LSTM) set-up. We show satisfactory results for most event types, with the linear kernel SVM outperforming the other experimental set-ups</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3101</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>jacobs-lefever-hoste:2018:W18-31</bibkey>
    </paper>
    <paper id="3102">
	 <title>Causality Analysis of Twitter Sentiments and Stock Market Returns</title>
	 <author><first>Narges</first><last>Tabari</last></author>
	 <author><first>Piyusha</first><last>Biswas</last></author>
	 <author><first>Bhanu</first><last>Praneeth</last></author>
	 <author><first>Armin</first><last>Seyeditabari</last></author>
	 <author><first>Mirsad</first><last>Hadzikadic</last></author>
	 <author><first>Wlodek</first><last>Zadrozny</last></author>
	 <booktitle>Proceedings of the First Workshop on Economics and Natural Language Processing</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>11–19</pages>
	 <abstract>Sentiment analysis is the process of identifying the opinion expressed in text. Recently, it has been used to study behavioral finance, and in particular the effect of opinions and emotions on economic or financial decisions. In this paper, we use a public dataset of labeled tweets that has been labeled by Amazon Mechanical Turk and then we propose a baseline classification model. Then, by using Granger causality of both sentiment datasets with the different stocks, we shows that there is causality between social media and stock market returns (in both directions) for many stocks. Finally, We evaluate this causality analysis by showing that in the event of a specific news on certain dates, there are evidences of trending the same news on Twitter for that stock.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3102</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>tabari-EtAl:2018:W18-31</bibkey>
    </paper>
    <paper id="3103">
	 <title>A Corpus of Corporate Annual and Social Responsibility Reports: 280 Million Tokens of Balanced Organizational Writing</title>
	 <author><first>Sebastian G.M.</first><last>Händschke</last></author>
	 <author><first>Sven</first><last>Buechel</last></author>
	 <author><first>Jan</first><last>Goldenstein</last></author>
	 <author><first>Philipp</first><last>Poschmann</last></author>
	 <author><first>Tinghui</first><last>Duan</last></author>
	 <author><first>Peter</first><last>Walgenbach</last></author>
	 <author><first>Udo</first><last>Hahn</last></author>
	 <booktitle>Proceedings of the First Workshop on Economics and Natural Language Processing</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>20–31</pages>
	 <abstract>We introduce JOCo, a novel text corpus for NLP analytics in the field of economics, business and management. This corpus is composed of corporate annual and social responsibility reports of the top 30 US, UK and German companies in the major (DJIA, FTSE 100, DAX), middle-sized (S&amp;P 500, FTSE 250, MDAX) and technology (NASDAQ, FTSE AIM 100, TECDAX) stock indices, respectively. Altogether, this adds up to 5,000 reports from 270 companies headquartered in three of the world’s most important economies. The corpus spans a time frame from 2000 up to 2015 and contains, in total, 282M tokens. We also feature JOCo in a small-scale experiment to demonstrate its potential for NLP-fueled studies in economics, business and management research.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3103</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>hndschke-EtAl:2018:W18-31</bibkey>
    </paper>
    <paper id="3104">
	 <title>Word Embeddings-Based Uncertainty Detection in Financial Disclosures</title>
	 <author><first>Christoph Kilian</first><last>Theil</last></author>
	 <author><first>Sanja</first><last>Stajner</last></author>
	 <author><first>Heiner</first><last>Stuckenschmidt</last></author>
	 <booktitle>Proceedings of the First Workshop on Economics and Natural Language Processing</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>32–37</pages>
	 <abstract>In this paper, we use NLP techniques to detect linguistic uncertainty in financial disclosures. Leveraging general-domain and domain-specific word embedding models, we automatically expand an existing dictionary of uncertainty triggers. We furthermore examine how an expert filtering affects the quality of such an expansion. We show that the dictionary expansions significantly improve regressions on stock return volatility. Lastly, we prove that the expansions significantly boost the automatic detection of uncertain sentences.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3104</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>theil-stajner-stuckenschmidt:2018:W18-31</bibkey>
    </paper>
    <paper id="3105">
	 <title>A Simple End-to-End Question Answering Model for Product Information</title>
	 <author><first>Tuan</first><last>Lai</last></author>
	 <author><first>Trung</first><last>Bui</last></author>
	 <author><first>Sheng</first><last>Li</last></author>
	 <author><first>Nedim</first><last>Lipka</last></author>
	 <booktitle>Proceedings of the First Workshop on Economics and Natural Language Processing</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>38–43</pages>
	 <abstract>When evaluating a potential product purchase, customers may have many questions in mind. They want to get adequate information to determine whether the product of interest is worth their money. In this paper we present a simple deep learning model for answering questions regarding product facts and specifications. Given a question and a product specification, the model outputs a score indicating their relevance. To train and evaluate our proposed model, we collected a dataset of 7,119 questions that are related to 153 different products. Experimental results demonstrate that –despite its simplicity– the performance of our model is shown to be comparable to a more complex state-of-the-art baseline.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3105</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>lai-EtAl:2018:W18-31</bibkey>
    </paper>
    <paper id="3106">
	 <title>Sentence Classification for Investment Rules Detection</title>
	 <author><first>Youness</first><last>Mansar</last></author>
	 <author><first>Sira</first><last>Ferradans</last></author>
	 <booktitle>Proceedings of the First Workshop on Economics and Natural Language Processing</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>44–48</pages>
	 <abstract>In the last years, compliance requirements for the banking sector have greatly augmented, making the current compliance processes difficult to maintain. Any process that allows to accelerate the identification and implementation of compliance requirements can help address this issues. The contributions of the paper are twofold: we propose a new NLP task that is the investment rule detection, and a group of methods identify them. We show that the proposed methods are highly performing and fast, thus can be deployed in production.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3106</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>mansar-ferradans:2018:W18-31</bibkey>
    </paper>
    <paper id="3107">
	 <title>Leveraging News Sentiment to Improve Microblog Sentiment Classification in the Financial Domain</title>
	 <author><first>Tobias</first><last>Daudert</last></author>
	 <author><first>Paul</first><last>Buitelaar</last></author>
	 <author><first>Sapna</first><last>Negi</last></author>
	 <booktitle>Proceedings of the First Workshop on Economics and Natural Language Processing</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>49–54</pages>
	 <abstract>With the rising popularity of social media in the society and in research, analysing texts short in length, such as microblogs, becomes an increasingly important task. As a medium of communication, microblogs carry peoples sentiments and express them to the public. Given that sentiments are driven by multiple factors including the news media, the question arises if the sentiment expressed in news and the news article themselves can be leveraged to detect and classify sentiment in microblogs. Prior research has highlighted the impact of sentiments and opinions on the market dynamics, making the financial domain a prime case study for this approach. Therefore, this paper describes ongoing research dealing with the exploitation of news contained sentiment to improve microblog sentiment classification in a financial context.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3107</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>daudert-buitelaar-negi:2018:W18-31</bibkey>
    </paper>
    <paper id="3108">
	 <title>Implicit and Explicit Aspect Extraction in Financial Microblogs</title>
	 <author><first>Thomas</first><last>Gaillat</last></author>
	 <author><first>Bernardo</first><last>Stearns</last></author>
	 <author><first>Gopal</first><last>Sridhar</last></author>
	 <author><first>Ross</first><last>McDermott</last></author>
	 <author><first>Manel</first><last>Zarrouk</last></author>
	 <author><first>Brian</first><last>Davis</last></author>
	 <booktitle>Proceedings of the First Workshop on Economics and Natural Language Processing</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>55–61</pages>
	 <abstract>This paper focuses on aspect extraction which is a sub-task of Aspect-based Sentiment Analysis. The goal is to report an extraction method of financial aspects in microblog messages. Our approach uses a stock-investment taxonomy for the identification of explicit and implicit aspects. We compare supervised and unsupervised methods to assign predefined categories at message level. Results on 7 aspect classes show 0.71 accuracy, while the 32 class classification gives 0.82 accuracy for messages containing explicit aspects and 0.35 for implicit aspects.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3108</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>gaillat-EtAl:2018:W18-31</bibkey>
    </paper>
    <paper id="3109">
	 <title>Unsupervised Word Influencer Networks from News Streams</title>
	 <author><first>Ananth</first><last>Balashankar</last></author>
	 <author><first>Sunandan</first><last>Chakraborty</last></author>
	 <author><first>Lakshminarayanan</first><last>Subramanian</last></author>
	 <booktitle>Proceedings of the First Workshop on Economics and Natural Language Processing</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>62–68</pages>
	 <abstract>In this paper, we propose a new unsuper- vised learning framework to use news events for predicting trends in stock prices. We present Word Influencer Networks (WIN), a graph framework to extract longitudinal tem- poral relationships between any pair of infor- mative words from news streams. Using the temporal occurrence of words, WIN measures how the appearance of one word in a news stream influences the emergence of another set of words in the future. The latent word-word influencer relationships in WIN are the build- ing blocks for causal reasoning and predic- tive modeling. We demonstrate the efficacy of WIN by using it for unsupervised extraction of latent features for stock price prediction and obtain 2 orders lower prediction error com- pared to a similar causal graph based method. WIN discovered influencer links from seem- ingly unrelated words from topics like poli- tics to finance. WIN also validated 67% of the causal evidence found manually in the text through a direct edge and the rest 33% through a path of length 2.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3109</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>balashankar-chakraborty-subramanian:2018:W18-31</bibkey>
    </paper>
    <paper id="3200">
	 <title>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</title>
	 <editor><first>Gustavo</first><last>Aguilar</last></editor>
	 <editor><first>Fahad</first><last>AlGhamdi</last></editor>
	 <editor><first>Victor</first><last>Soto</last></editor>
	 <editor><first>Thamar</first><last>Solorio</last></editor>
	 <editor><first>Mona</first><last>Diab</last></editor>
	 <editor><first>Julia</first><last>Hirschberg</last></editor>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <url>http://www.aclweb.org/anthology/W18-32</url>
	 <bibtype>book</bibtype>
	 <bibkey>W18-32:2018</bibkey>
    </paper>
    <paper id="3201">
	 <title>Joint Part-of-Speech and Language ID Tagging for Code-Switched Data</title>
	 <author><first>Victor</first><last>Soto</last></author>
	 <author><first>Julia</first><last>Hirschberg</last></author>
	 <booktitle>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>1–10</pages>
	 <abstract>Code-switching is the fluent alternation between two or more languages in conversation between bilinguals. Large populations of speakers code-switch during communication, but little effort has been made to develop tools for code-switching, including part-of-speech taggers. In this paper, we propose an approach to POS tagging of code-switched English-Spanish data based on recurrent neural networks. We test our model on known monolingual benchmarks to demonstrate that our neural POS tagging model is on par with state-of-the-art methods. We next test our code-switched methods on the Miami Bangor corpus of English Spanish conversation, focusing on two types of experiments: POS tagging alone, for which we achieve 96.34% accuracy, and joint part-of-speech and language ID tagging, which achieves similar POS tagging accuracy (96.39%) and very high language ID accuracy (98.78%). Finally, we show that our proposed models outperform other state-of-the-art code-switched taggers.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3201</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>soto-hirschberg:2018:W18-32</bibkey>
    </paper>
    <paper id="3202">
	 <title>Phone Merging For Code-Switched Speech Recognition</title>
	 <author><first>Sunit</first><last>Sivasankaran</last></author>
	 <author><first>Brij Mohan Lal</first><last>Srivastava</last></author>
	 <author><first>Sunayana</first><last>Sitaram</last></author>
	 <author><first>Kalika</first><last>Bali</last></author>
	 <author><first>Monojit</first><last>Choudhury</last></author>
	 <booktitle>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>11–19</pages>
	 <abstract>Speakers in multilingual communities often switch between or mix multiple languages in the same conversation. Automatic Speech Recognition (ASR) of code-switched speech faces many challenges including the influence of phones of different languages on each other. This paper shows evidence that phone sharing between languages improves the Acoustic Model performance for Hindi-English code-switched speech. We compare baseline system built with separate phones for Hindi and English with systems where the phones were manually merged based on linguistic knowledge. Encouraged by the improved ASR performance after manually merging the phones, we further investigate multiple data-driven methods to identify phones to be merged across the languages. We show detailed analysis of automatic phone merging in this language pair and the impact it has on individual phone accuracies and WER. Though the best performance gain of 1.2% WER was observed with manually merged phones, we show experimentally that the manual phone merge is not optimal.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3202</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>sivasankaran-EtAl:2018:W18-32</bibkey>
    </paper>
    <paper id="3203">
	 <title>Improving Neural Network Performance by Injecting Background Knowledge: Detecting Code-switching and Borrowing in Algerian texts</title>
	 <author><first>Wafia</first><last>Adouane</last></author>
	 <author><first>Jean-Philippe</first><last>Bernardy</last></author>
	 <author><first>Simon</first><last>Dobnik</last></author>
	 <booktitle>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>20–28</pages>
	 <abstract>We explore the effect of injecting background knowledge to different deep neural network (DNN) configurations in order to mitigate the problem of the scarcity of annotated data when applying these models on datasets of low-resourced languages. The background knowledge is encoded in the form of lexicons and pre-trained sub-word embeddings. The DNN models are evaluated on the task of detecting code-switching and borrowing points in non-standardised user-generated Algerian texts. Overall results show that DNNs benefit from adding background knowledge. However, the gain varies between models and categories. The proposed DNN architectures are generic and could be applied to other low-resourced languages.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3203</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>adouane-bernardy-dobnik:2018:W18-32</bibkey>
    </paper>
    <paper id="3204">
	 <title>Code-Mixed Question Answering Challenge: Crowd-sourcing Data and Techniques</title>
	 <author><first>Khyathi</first><last>Chandu</last></author>
	 <author><first>Ekaterina</first><last>Loginova</last></author>
	 <author><first>Vishal</first><last>Gupta</last></author>
	 <author><first>Josef van</first><last>Genabith</last></author>
	 <author><first>Günter</first><last>Neuman</last></author>
	 <author><first>Manoj</first><last>Chinnakotla</last></author>
	 <author><first>Eric</first><last>Nyberg</last></author>
	 <author><first>Alan W.</first><last>Black</last></author>
	 <booktitle>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>29–38</pages>
	 <abstract>Code-Mixing (CM) is the phenomenon of alternating between two or more languages which is prevalent in bi- and multi-lingual communities. Most NLP applications today are still designed with the assumption of a single interaction language and are most likely to break given a CM utterance with multiple languages mixed at a morphological, phrase or sentence level. For example, popular commercial search engines do not yet fully understand the intents expressed in CM queries. As a first step towards fostering research which supports CM in NLP applications, we systematically crowd-sourced and curated an evaluation dataset for factoid question answering in three CM languages - Hinglish (Hindi+English), Tenglish (Telugu+English) and Tamlish (Tamil+English) which belong to two language families (Indo-Aryan and Dravidian). We share the details of our data collection process, techniques which were used to avoid inducing lexical bias amongst the crowd workers and other CM specific linguistic properties of the dataset. Our final dataset, which is available freely for research purposes, has 1,694 Hinglish, 2,848 Tamlish and 1,391 Tenglish factoid questions and their answers. We discuss the techniques used by the participants for the first edition of this ongoing challenge.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3204</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>chandu-EtAl:2018:W18-321</bibkey>
    </paper>
    <paper id="3205">
	 <title>Transliteration Better than Translation? Answering Code-mixed Questions over a Knowledge Base</title>
	 <author><first>Vishal</first><last>Gupta</last></author>
	 <author><first>Manoj</first><last>Chinnakotla</last></author>
	 <author><first>Manish</first><last>Shrivastava</last></author>
	 <booktitle>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>39–50</pages>
	 <abstract>Humans can learn multiple languages. If they know a fact in one language, they can answer a question in another language they understand. They can also answer Code-mix (CM) questions: questions which contain both languages. This behavior is attributed to the unique learning ability of humans. Our task aims to study if machines can achieve this. We demonstrate how effectively a machine can answer CM questions. In this work, we adopt a two phase approach: candidate generation and candidate re-ranking to answer questions. We propose a Triplet-Siamese-Hybrid CNN (TSHCNN) to re-rank candidate answers. We show experiments on the SimpleQuestions dataset. Our network is trained only on English questions provided in this dataset and noisy Hindi translations of these questions and can answer English-Hindi CM questions effectively without the need of translation into English. Back-transliterated CM questions outperform their lexical and sentence level translated counterparts by 5% &amp; 35% in accuracy respectively, highlighting the efficacy of our approach in a resource constrained setting.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3205</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>gupta-chinnakotla-shrivastava:2018:W18-32</bibkey>
    </paper>
    <paper id="3206">
	 <title>Language Identification and Analysis of Code-Switched Social Media Text</title>
	 <author><first>Deepthi</first><last>Mave</last></author>
	 <author><first>Suraj</first><last>Maharjan</last></author>
	 <author><first>Thamar</first><last>Solorio</last></author>
	 <booktitle>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>51–61</pages>
	 <abstract>In this paper, we detail our work on comparing different word-level language identification systems for code-switched Hindi-English data and a standard Spanish-English dataset. In this regard, we build a new code-switched dataset for Hindi-English. To understand the code-switching patterns in these language pairs, we investigate different code-switching metrics. We find that the CRF model outperforms the neural network based models by a margin of 2-5 percentage points for Spanish-English and 3-5 percentage points for Hindi-English.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3206</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>mave-maharjan-solorio:2018:W18-32</bibkey>
    </paper>
    <paper id="3207">
	 <title>Code-Switching Language Modeling using Syntax-Aware Multi-Task Learning</title>
	 <author><first>Genta Indra</first><last>Winata</last></author>
	 <author><first>Andrea</first><last>Madotto</last></author>
	 <author><first>Chien-Sheng</first><last>Wu</last></author>
	 <author><first>Pascale</first><last>Fung</last></author>
	 <booktitle>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>62–67</pages>
	 <abstract>Lack of text data has been the major issue on code-switching language modeling. In this paper, we introduce multi-task learning based language model which shares syntax representation of languages to leverage linguistic information and tackle the low resource data issue. Our model jointly learns both language modeling and Part-of-Speech tagging on code-switched utterances. In this way, the model is able to identify the location of code-switching points and improves the prediction of next word. Our approach outperforms standard LSTM based language model, with an improvement of 9.7% and 7.4% in perplexity on SEAME Phase I and Phase II dataset respectively.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3207</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>winata-EtAl:2018:W18-321</bibkey>
    </paper>
    <paper id="3208">
	 <title>Predicting the presence of a Matrix Language in code-switching</title>
	 <author><first>Barbara</first><last>Bullock</last></author>
	 <author><first>Wally</first><last>Guzman</last></author>
	 <author><first>Jacqueline</first><last>Serigos</last></author>
	 <author><first>Vivek</first><last>Sharath</last></author>
	 <author><first>Almeida Jacqueline</first><last>Toribio</last></author>
	 <booktitle>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>68–75</pages>
	 <abstract>One language is often assumed to be dominant in code-switching but this assumption has not been empirically tested. We operationalize the matrix language (ML) at the level of the sentence, using three common definitions from linguistics. We test whether these converge and then model this convergence via a set of metrics that together quantify the nature of C-S. We conduct our experiment on four Spanish-English corpora. Our results demonstrate that our model can separate some corpora according to whether they have a dominant ML or not but that the corpora span a range of mixing types that cannot be sorted neatly into an insertional vs. alternational dichotomy.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3208</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>bullock-EtAl:2018:W18-32</bibkey>
    </paper>
    <paper id="3209">
	 <title>Automatic Detection of Code-switching Style from Acoustics</title>
	 <author><first>SaiKrishna</first><last>Rallabandi</last></author>
	 <author><first>Sunayana</first><last>Sitaram</last></author>
	 <author><first>Alan W.</first><last>Black</last></author>
	 <booktitle>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>76–81</pages>
	 <abstract>Multilingual speakers switch between languages in an non-trivial fashion displaying inter sentential, intra sentential, and congruent lexicalization based transitions. While monolingual ASR systems may be capable of recognizing a few words from a foreign language, they are usually not robust enough to handle these varied styles of code-switching. There is also a lack of large code-switched speech corpora capturing all these styles making it difficult to build code-switched speech recognition systems. We hypothesize that it may be useful for an ASR system to be able to first detect the switching style of a particular utterance from acoustics, and then use specialized language models or other adaptation techniques for decoding the speech. In this paper, we look at the first problem of detecting code-switching style from acoustics. We classify code-switched Spanish-English and Hindi-English corpora using two metrics and show that features extracted from acoustics alone can distinguish between different kinds of code-switching in these language pairs.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3209</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>rallabandi-sitaram-black:2018:W18-32</bibkey>
    </paper>
    <paper id="3210">
	 <title>Accommodation of Conversational Code-Choice</title>
	 <author><first>Anshul</first><last>Bawa</last></author>
	 <author><first>Monojit</first><last>Choudhury</last></author>
	 <author><first>Kalika</first><last>Bali</last></author>
	 <booktitle>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>82–91</pages>
	 <abstract>Bilingual speakers often freely mix languages. However, in such bilingual conversations, are the language choices of the speakers coordinated? How much does one speaker’s choice of language affect other speakers? In this paper, we formulate code-choice as a linguistic style, and show that speakers are indeed sensitive to and accommodating of each other’s code-choice. We find that the saliency or markedness of a language in context directly affects the degree of accommodation observed. More importantly, we discover that accommodation of code-choices persists over several conversational turns. We also propose an alternative interpretation of conversational accommodation as a retrieval problem, and show that the differences in accommodation characteristics of code-choices are based on their markedness in context.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3210</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>bawa-choudhury-bali:2018:W18-32</bibkey>
    </paper>
    <paper id="3211">
	 <title>Language Informed Modeling of Code-Switched Text</title>
	 <author><first>Khyathi</first><last>Chandu</last></author>
	 <author><first>Thomas</first><last>Manzini</last></author>
	 <author><first>Sumeet</first><last>Singh</last></author>
	 <author><first>Alan W.</first><last>Black</last></author>
	 <booktitle>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>92–97</pages>
	 <abstract>Code-switching (CS), the practice of alternating between two or more languages in conversations, is pervasive in most multi-lingual communities. CS texts have a complex interplay between languages and occur in informal contexts that make them harder to collect and construct NLP tools for. We approach this problem through Language Modeling (LM) on a new Hindi-English mixed corpus containing 59,189 unique sentences collected from blogging websites. We implement and discuss different Language Models derived from a multi-layered LSTM architecture. We hypothesize that encoding language information strengthens a language model by helping to learn code-switching points. We show that our highest performing model achieves a test perplexity of 19.52 on the CS corpus that we collected and processed. On this data we demonstrate that our performance is an improvement over AWD-LSTM LM (a recent state of the art on monolingual English).</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3211</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>chandu-EtAl:2018:W18-322</bibkey>
    </paper>
    <paper id="3212">
	 <title>GHHT at CALCS 2018: Named Entity Recognition for Dialectal Arabic Using Neural Networks</title>
	 <author><first>Mohammed</first><last>Attia</last></author>
	 <author><first>Younes</first><last>Samih</last></author>
	 <author><first>Wolfgang</first><last>Maier</last></author>
	 <booktitle>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>98–102</pages>
	 <abstract>This paper describes our system submission to the CALCS 2018 shared task on named entity recognition on code-switched data for the language variant pair of Modern Standard Arabic and Egyptian dialectal Arabic. We build a a Deep Neural Network that combines word and character-based representations in convolutional and recurrent networks with a CRF layer. The model is augmented with stacked layers of enriched information such pre-trained embeddings, Brown clusters and named entity gazetteers. Our system is ranked second among those participating in the shared task achieving an FB1 average of 70.09%.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3212</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>attia-samih-maier:2018:W18-32</bibkey>
    </paper>
    <paper id="3213">
	 <title>Simple Features for Strong Performance on Named Entity Recognition in Code-Switched Twitter Data</title>
	 <author><first>Devanshu</first><last>Jain</last></author>
	 <author><first>Maria</first><last>Kustikova</last></author>
	 <author><first>Mayank</first><last>Darbari</last></author>
	 <author><first>Rishabh</first><last>Gupta</last></author>
	 <author><first>Stephen</first><last>Mayhew</last></author>
	 <booktitle>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>103–109</pages>
	 <abstract>In this work, we address the problem of Named Entity Recognition (NER) in code-switched tweets as a part of the Workshop on Computational Approaches to Linguistic Code-switching (CALCS) at ACL’18. Code-switching is the phenomenon where a speaker switches between two languages or variants of the same language within or across utterances, known as intra-sentential or inter-sentential code-switching, respectively. Processing such data is challenging using state of the art methods since such technology is generally geared towards processing monolingual text. In this paper we explored ways to use language identification and translation to recognize named entities in such data, however, utilizing simple features (sans multi-lingual features) with Conditional Random Field (CRF) classifier achieved the best results. Our experiments were mainly aimed at the (ENG-SPA) English-Spanish dataset but we submitted a language-independent version of our system to the (MSA-EGY) Arabic-Egyptian dataset as well and achieved good results.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3213</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>jain-EtAl:2018:W18-32</bibkey>
    </paper>
    <paper id="3214">
	 <title>Bilingual Character Representation for Efficiently Addressing Out-of-Vocabulary Words in Code-Switching Named Entity Recognition</title>
	 <author><first>Genta Indra</first><last>Winata</last></author>
	 <author><first>Chien-Sheng</first><last>Wu</last></author>
	 <author><first>Andrea</first><last>Madotto</last></author>
	 <author><first>Pascale</first><last>Fung</last></author>
	 <booktitle>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>110–114</pages>
	 <abstract>We propose an LSTM-based model with hierarchical architecture on named entity recognition from code-switching Twitter data. Our model uses bilingual character representation and transfer learning to address out-of-vocabulary words. In order to mitigate data noise, we propose to use token replacement and normalization. In the 3rd Workshop on Computational Approaches to Linguistic Code-Switching Shared Task, we achieved second place with 62.76% harmonic mean F1-score for English-Spanish language pair without using any gazetteer and knowledge-based information.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3214</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>winata-EtAl:2018:W18-322</bibkey>
    </paper>
    <paper id="3215">
	 <title>Named Entity Recognition on Code-Switched Data Using Conditional Random Fields</title>
	 <author><first>Utpal Kumar</first><last>Sikdar</last></author>
	 <author><first>Biswanath</first><last>Barik</last></author>
	 <author><first>Björn</first><last>Gambäck</last></author>
	 <booktitle>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>115–119</pages>
	 <abstract>Named Entity Recognition is an important information extraction task that identifies proper names in unstructured texts and classifies them into some pre-defined categories. Identification of named entities in code-mixed social media texts is a more difficult and challenging task as the contexts are short, ambiguous and often noisy. This work proposes a Conditional Random Fields based named entity recognition system to identify proper names in code-switched data and classify them into nine categories. The system ranked fifth among nine participant systems and achieved a 59.25% F1-score.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3215</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>sikdar-barik-gambck:2018:W18-32</bibkey>
    </paper>
    <paper id="3216">
	 <title>The University of Texas System Submission for the Code-Switching Workshop Shared Task 2018</title>
	 <author><first>Florian</first><last>Janke</last></author>
	 <author><first>Tongrui</first><last>Li</last></author>
	 <author><first>Eric</first><last>Rincón</last></author>
	 <author><first>Gualberto</first><last>Guzmán</last></author>
	 <author><first>Barbara</first><last>Bullock</last></author>
	 <author><first>Almeida Jacqueline</first><last>Toribio</last></author>
	 <booktitle>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>120–125</pages>
	 <abstract>This paper describes the system for the Named Entity Recognition Shared Task of the Third Workshop on Computational Approaches to Linguistic Code-Switching (CALCS) submitted by the Bilingual Annotations Tasks (BATs) research group of the University of Texas. Our system uses several features to train a Conditional Random Field (CRF) model for classifying input words as Named Entities (NEs) using the Inside-Outside-Beginning (IOB) tagging scheme. We participated in the Modern Standard Arabic-Egyptian Arabic (MSA-EGY) and English-Spanish (ENG-SPA) tasks, achieving weighted average F-scores of 65.62 and 54.16 respectively. We also describe the performance of a deep neural network (NN) trained on a subset of the CRF features, which did not surpass CRF performance.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3216</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>janke-EtAl:2018:W18-32</bibkey>
    </paper>
    <paper id="3217">
	 <title>Tackling Code-Switched NER: Participation of CMU</title>
	 <author><first>Parvathy</first><last>Geetha</last></author>
	 <author><first>Khyathi</first><last>Chandu</last></author>
	 <author><first>Alan W.</first><last>Black</last></author>
	 <booktitle>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>126–131</pages>
	 <abstract>Named Entity Recognition plays a major role in several downstream applications in NLP. Though this task has been heavily studied in formal monolingual texts and also noisy texts like Twitter data, it is still an emerging task in code-switched (CS) content on social media. This paper describes our participation in the shared task of NER on code-switched data for Spanglish (Spanish + English) and Arabish (Arabic + English). In this paper we describe models that intuitively developed from the data for the shared task Named Entity Recognition on Code-switched Data. Owing to the sparse and non-linear relationships between words in Twitter data, we explored neural architectures that are capable of non-linearities fairly well. In specific, we trained character level models and word level models based on Bidirectional LSTMs (Bi-LSTMs) to perform sequential tagging. We train multiple models to identify nominal mentions and subsequently use this information to predict the labels of named entity in a sequence. Our best model is a character level model along with word level pre-trained multilingual embeddings that gave an F-score of 56.72 in Spanglish and a word level model that gave an F-score of 65.02 in Arabish on the test data.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3217</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>geetha-chandu-black:2018:W18-32</bibkey>
    </paper>
    <paper id="3218">
	 <title>Multilingual Named Entity Recognition on Spanish-English Code-switched Tweets using Support Vector Machines</title>
	 <author><first>Daniel</first><last>Claeser</last></author>
	 <author><first>Samantha</first><last>Kent</last></author>
	 <author><first>Dennis</first><last>Felske</last></author>
	 <booktitle>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>132–137</pages>
	 <abstract>This paper describes our system submission for the ACL 2018 shared task on named entity recognition (NER) in code-switched Twitter data. Our best result (F1 = 53.65) was obtained using a Support Vector Machine (SVM) with 14 features combined with rule-based post processing.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3218</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>claeser-kent-felske:2018:W18-32</bibkey>
    </paper>
    <paper id="3219">
	 <title>Named Entity Recognition on Code-Switched Data: Overview of the CALCS 2018 Shared Task</title>
	 <author><first>Gustavo</first><last>Aguilar</last></author>
	 <author><first>Fahad</first><last>AlGhamdi</last></author>
	 <author><first>Victor</first><last>Soto</last></author>
	 <author><first>Mona</first><last>Diab</last></author>
	 <author><first>Julia</first><last>Hirschberg</last></author>
	 <author><first>Thamar</first><last>Solorio</last></author>
	 <booktitle>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>138–147</pages>
	 <abstract>In the third shared task of the Computational Approaches to Linguistic Code-Switching (CALCS) workshop, we focus on Named Entity Recognition (NER) on code-switched social-media data. We divide the shared task into two competitions based on the English-Spanish (ENG-SPA) and Modern Standard Arabic-Egyptian (MSA-EGY) language pairs. We use Twitter data and 9 entity types to establish a new dataset for code-switched NER benchmarks. In addition to the CS phenomenon, the diversity of the entities and the social media challenges make the task considerably hard to process. As a result, the best scores of the competitions are 63.76% and 71.61% for ENG-SPA and MSA-EGY, respectively. We present the scores of 9 participants and discuss the most common challenges among submissions.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3219</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>aguilar-EtAl:2018:W18-32</bibkey>
    </paper>
    <paper id="3220">
	 <title>IIT (BHU) Submission for the ACL Shared Task on Named Entity Recognition on Code-switched Data</title>
	 <author><first>Shashwat</first><last>Trivedi</last></author>
	 <author><first>Harsh</first><last>Rangwani</last></author>
	 <author><first>Anil</first><last>Kumar Singh</last></author>
	 <booktitle>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>148–153</pages>
	 <abstract>This paper describes the best performing system for the shared task on Named Entity Recognition (NER) on code-switched data for the language pair Spanish-English (ENG-SPA). We introduce a gated neural architecture for the NER task. Our final model achieves an F1 score of 63.76%, outperforming the baseline by 10%.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3220</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>trivedi-rangwani-kumarsingh:2018:W18-32</bibkey>
    </paper>
    <paper id="3221">
	 <title>Code-Switched Named Entity Recognition with Embedding Attention</title>
	 <author><first>Changhan</first><last>Wang</last></author>
	 <author><first>Kyunghyun</first><last>Cho</last></author>
	 <author><first>Douwe</first><last>Kiela</last></author>
	 <booktitle>Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>154–158</pages>
	 <abstract>We describe our work for the CALCS 2018 shared task on named entity recognition on code-switched data. Our system ranked first place for MS Arabic-Egyptian named entity recognition and third place for English-Spanish.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3221</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>wang-cho-kiela:2018:W18-32</bibkey>
    </paper>
    <paper id="3300">
	 <title>Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)</title>
	 <editor><first>Amir</first><last>Zadeh</last></editor>
	 <editor><first>Paul Pu</first><last>Liang</last></editor>
	 <editor><first>Louis-Philippe</first><last>Morency</last></editor>
	 <editor><first>Soujanya</first><last>Poria</last></editor>
	 <editor><first>Erik</first><last>Cambria</last></editor>
	 <editor><first>Stefan</first><last>Scherer</last></editor>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <url>http://www.aclweb.org/anthology/W18-33</url>
	 <bibtype>book</bibtype>
	 <bibkey>W18-33:2018</bibkey>
    </paper>
    <paper id="3301">
	 <title>Getting the subtext without the text: Scalable multimodal sentiment classification from visual and acoustic modalities</title>
	 <author><first>Nathaniel</first><last>Blanchard</last></author>
	 <author><first>Daniel</first><last>Moreira</last></author>
	 <author><first>Aparna</first><last>Bharati</last></author>
	 <author><first>Walter</first><last>Scheirer</last></author>
	 <booktitle>Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>1–10</pages>
	 <abstract>In the last decade, video blogs (vlogs) have become an extremely popular method through which people express sentiment. The ubiquitousness of these videos has increased the importance of multimodal fusion models, which incorporate video and audio features with traditional text features for automatic sentiment detection. Multimodal fusion offers a unique opportunity to build models that learn from the full depth of expression available to human viewers. In the detection of sentiment in these videos, acoustic and video features provide clarity to otherwise ambiguous transcripts. In this paper, we present a multimodal fusion model that exclusively uses high-level video and audio features to analyze spoken sentences for sentiment. We discard traditional transcription features in order to minimize human intervention and to maximize the deployability of our model on at-scale real-world data. We select high-level features for our model that have been successful in non-affect domains in order to test their generalizability in the sentiment detection domain. We train and test our model on the newly released CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset, obtaining an F1 score of 0.8049 on the validation set and an F1 score of 0.6325 on the held-out challenge test set.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3301</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>blanchard-EtAl:2018:W18-33</bibkey>
    </paper>
    <paper id="3302">
	 <title>Recognizing Emotions in Video Using Multimodal DNN Feature Fusion</title>
	 <author><first>Jennifer</first><last>Williams</last></author>
	 <author><first>Steven</first><last>Kleinegesse</last></author>
	 <author><first>Ramona</first><last>Comanescu</last></author>
	 <author><first>Oana</first><last>Radu</last></author>
	 <booktitle>Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>11–19</pages>
	 <abstract>We present our system description of input-level multimodal fusion of audio, video, and text for recognition of emotions and their intensities for the 2018 First Grand Challenge on Computational Modeling of Human Multimodal Language. Our proposed approach is based on input-level feature fusion with sequence learning from Bidirectional Long-Short Term Memory (BLSTM) deep neural networks (DNNs). We show that our fusion approach outperforms unimodal predictors. Our system performs 6-way simultaneous classification and regression, allowing for overlapping emotion labels in a video segment. This leads to an overall binary accuracy of 90%, overall 4-class accuracy of 89.2% and an overall mean-absolute-error (MAE) of 0.12. Our work shows that an early fusion technique can effectively predict the presence of multi-label emotions as well as their coarse-grained intensities. The presented multimodal approach creates a simple and robust baseline on this new Grand Challenge dataset. Furthermore, we provide a detailed analysis of emotion intensity distributions as output from our DNN, as well as a related discussion concerning the inherent difficulty of this task.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3302</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>williams-EtAl:2018:W18-331</bibkey>
    </paper>
    <paper id="3303">
	 <title>Multimodal Relational Tensor Network for Sentiment and Emotion Classification</title>
	 <author><first>Saurav</first><last>Sahay</last></author>
	 <author><first>Shachi H</first><last>Kumar</last></author>
	 <author><first>Rui</first><last>Xia</last></author>
	 <author><first>Jonathan</first><last>Huang</last></author>
	 <author><first>Lama</first><last>Nachman</last></author>
	 <booktitle>Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>20–27</pages>
	 <abstract>Understanding Affect from video segments has brought researchers from the language, audio and video domains together. Most of the current multimodal research in this area deals with various techniques to fuse the modalities, and mostly treat the segments of a video independently. Motivated by the work of (Zadeh et al., 2017) and (Poria et al., 2017), we present our architecture, Relational Tensor Network, where we use the inter-modal interactions within a segment (intra-segment) and also consider the sequence of segments in a video to model the inter-segment inter-modal interactions. We also generate rich representations of text and audio modalities by leveraging richer audio and linguistic context alongwith fusing fine-grained knowledge based polarity scores from text. We present the results of our model on CMU-MOSEI dataset and show that our model outperforms many baselines and state of the art methods for sentiment classification and emotion recognition.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3303</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>sahay-EtAl:2018:W18-33</bibkey>
    </paper>
    <paper id="3304">
	 <title>Convolutional Attention Networks for Multimodal Emotion Recognition from Speech and Text Data</title>
	 <author><first>Woo Yong</first><last>Choi</last></author>
	 <author><first>Kyu Ye</first><last>Song</last></author>
	 <author><first>Chan Woo</first><last>Lee</last></author>
	 <booktitle>Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>28–34</pages>
	 <abstract>Emotion recognition has become a popular topic of interest, especially in the field of human computer interaction. Previous works involve unimodal analysis of emotion, while recent efforts focus on multimodal emotion recognition from vision and speech. In this paper, we propose a new method of learning about the hidden representations between just speech and text data using convolutional attention networks. Compared to the shallow model which employs simple concatenation of feature vectors, the proposed attention model performs much better in classifying emotion from speech and text data contained in the CMU-MOSEI dataset.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3304</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>choi-song-lee:2018:W18-33</bibkey>
    </paper>
    <paper id="3305">
	 <title>Sentiment Analysis using Imperfect Views from Spoken Language and Acoustic Modalities</title>
	 <author><first>Imran</first><last>Sheikh</last></author>
	 <author><first>Sri Harsha</first><last>Dumpala</last></author>
	 <author><first>Rupayan</first><last>Chakraborty</last></author>
	 <author><first>Sunil Kumar</first><last>Kopparapu</last></author>
	 <booktitle>Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>35–39</pages>
	 <abstract>Multimodal sentiment classification in practical applications may have to rely on erroneous and imperfect views, namely (a) language transcription from a speech recognizer and (b) under-performing acoustic views. This work focuses on improving the representations of these views by performing a deep canonical correlation analysis with the representations of the better performing manual transcription view. Enhanced representations of the imperfect views can be obtained even in absence of the perfect views and give an improved performance during test conditions. Evaluations on the CMU-MOSI and CMU-MOSEI datasets demonstrate the effectiveness of the proposed approach.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3305</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>sheikh-EtAl:2018:W18-33</bibkey>
    </paper>
    <paper id="3306">
	 <title>Polarity and Intensity: the Two Aspects of Sentiment Analysis</title>
	 <author><first>Leimin</first><last>Tian</last></author>
	 <author><first>Catherine</first><last>Lai</last></author>
	 <author><first>Johanna</first><last>Moore</last></author>
	 <booktitle>Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>40–47</pages>
	 <abstract>Current multimodal sentiment analysis frames sentiment score prediction as a general Machine Learning task. However, what the sentiment score actually represents has often been overlooked. As a measurement of opinions and affective states, a sentiment score generally consists of two aspects: polarity and intensity. We decompose sentiment scores into these two aspects and study how they are conveyed through individual modalities and combined multimodal models in a naturalistic monologue setting. In particular, we build unimodal and multimodal multi-task learning models with sentiment score prediction as the main task and polarity and/or intensity classification as the auxiliary tasks. Our experiments show that sentiment analysis benefits from multi-task learning, and individual modalities differ when conveying the polarity and intensity aspects of sentiment.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3306</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>tian-lai-moore:2018:W18-33</bibkey>
    </paper>
    <paper id="3307">
	 <title>ASR-based Features for Emotion Recognition: A Transfer Learning Approach</title>
	 <author><first>Noé</first><last>Tits</last></author>
	 <author><first>Kevin</first><last>El Haddad</last></author>
	 <author><first>Thierry</first><last>Dutoit</last></author>
	 <booktitle>Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>48–52</pages>
	 <abstract>During the last decade, the applications of signal processing have drastically improved with deep learning. However areas of affecting computing such as emotional speech synthesis or emotion recognition from spoken language remains challenging. In this paper, we investigate the use of a neural Automatic Speech Recognition (ASR) as a feature extractor for emotion recognition. We show that these features outperform the eGeMAPS feature set to predict the valence and arousal emotional dimensions, which means that the audio-to-text mapping learned by the ASR system contains information related to the emotional dimensions in spontaneous speech. We also examine the relationship between first layers (closer to speech) and last layers (closer to text) of the ASR and valence/arousal.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3307</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>tits-elhaddad-dutoit:2018:W18-33</bibkey>
    </paper>
    <paper id="3308">
	 <title>Seq2Seq2Sentiment: Multimodal Sequence to Sequence Models for Sentiment Analysis</title>
	 <author><first>Hai</first><last>Pham</last></author>
	 <author><first>Thomas</first><last>Manzini</last></author>
	 <author><first>Paul Pu</first><last>Liang</last></author>
	 <author><first>Barnabas</first><last>Poczos</last></author>
	 <booktitle>Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>53–63</pages>
	 <abstract>Multimodal machine learning is a core research area spanning the language, visual and acoustic modalities. The central challenge in multimodal learning involves learning representations that can process and relate information from multiple modalities. In this paper, we propose two methods for unsupervised learning of joint multimodal representations using sequence to sequence (Seq2Seq) methods: a Seq2Seq Modality Translation Model and a Hierarchical Seq2Seq Modality Translation Model. We also explore multiple different variations on the multimodal inputs and outputs of these seq2seq models. Our experiments on multimodal sentiment analysis using the CMU-MOSI dataset indicate that our methods learn informative multimodal representations that outperform the baselines and achieve improved performance on multimodal sentiment analysis, specifically in the Bimodal case where our model is able to improve F1 Score by twelve points. We also discuss future directions for multimodal Seq2Seq methods.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3308</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>pham-EtAl:2018:W18-33</bibkey>
    </paper>
    <paper id="3309">
	 <title>DNN Multimodal Fusion Techniques for Predicting Video Sentiment</title>
	 <author><first>Jennifer</first><last>Williams</last></author>
	 <author><first>Ramona</first><last>Comanescu</last></author>
	 <author><first>Oana</first><last>Radu</last></author>
	 <author><first>Leimin</first><last>Tian</last></author>
	 <booktitle>Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>64–72</pages>
	 <abstract>We present our work on sentiment prediction using the benchmark MOSI dataset from the CMU-MultimodalDataSDK. Previous work on multimodal sentiment analysis have been focused on input-level feature fusion or decision-level fusion for multimodal fusion. Here, we propose an intermediate-level feature fusion, which merges weights from each modality (audio, video, and text) during training with subsequent additional training. Moreover, we tested principle component analysis (PCA) for feature selection. We found that applying PCA increases unimodal performance, and multimodal fusion outperforms unimodal models. Our experiments show that our proposed intermediate-level feature fusion outperforms other fusion techniques, and it achieves the best performance with an overall binary accuracy of 74.0% on video+text modalities. Our work also improves feature selection for unimodal sentiment analysis, while proposing a novel and effective multimodal fusion architecture for this task.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3309</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>williams-EtAl:2018:W18-332</bibkey>
    </paper>
    <paper id="3400">
	 <title>Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP</title>
	 <editor><first>Reza</first><last>Haffari</last></editor>
	 <editor><first>Colin</first><last>Cherry</last></editor>
	 <editor><first>George</first><last>Foster</last></editor>
	 <editor><first>Shahram</first><last>Khadivi</last></editor>
	 <editor><first>Bahar</first><last>Salehi</last></editor>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <url>http://www.aclweb.org/anthology/W18-34</url>
	 <bibtype>book</bibtype>
	 <bibkey>W18-34:2018</bibkey>
    </paper>
    <paper id="3401">
	 <title>Character-level Supervision for Low-resource POS Tagging</title>
	 <author><first>Katharina</first><last>Kann</last></author>
	 <author><first>Johannes</first><last>Bjerva</last></author>
	 <author><first>Isabelle</first><last>Augenstein</last></author>
	 <author><first>Barbara</first><last>Plank</last></author>
	 <author><first>Anders</first><last>Søgaard</last></author>
	 <booktitle>Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>1–11</pages>
	 <abstract>Neural part-of-speech (POS) taggers are known to not perform well with little training data. As a step towards overcoming this problem, we present an architecture for learning more robust neural POS taggers by jointly training a hierarchical, recurrent model and a recurrent character-based sequence-to-sequence network supervised using an auxiliary objective. This way, we introduce stronger character-level supervision into the model, which enables better generalization to unseen words and provides regularization, making our encoding less prone to overfitting. We experiment with three auxiliary tasks: lemmatization, character-based word autoencoding, and character-based random string autoencoding. Experiments with minimal amounts of labeled data on 34 languages show that our new architecture outperforms a single-task baseline and, sur- prisingly, that, on average, raw text autoencoding can be as beneficial for low-resource POS tagging as using lemma information. Our neural POS tagger closes the gap to a state-of-the-art POS tagger (MarMoT) for low-resource scenarios by 43%, even outperforming it on languages with templatic morphology, e.g., Arabic, Hebrew, and Turkish, by some margin.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3401</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>kann-EtAl:2018:W18-34</bibkey>
    </paper>
    <paper id="3402">
	 <title>Training a Neural Network in a Low-Resource Setting on Automatically Annotated Noisy Data</title>
	 <author><first>Michael A.</first><last>Hedderich</last></author>
	 <author><first>Dietrich</first><last>Klakow</last></author>
	 <booktitle>Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>12–18</pages>
	 <abstract>Manually labeled corpora are expensive to create and often not available for low-resource languages or domains. Automatic labeling approaches are an alternative way to obtain labeled data in a quicker and cheaper way. However, these labels often contain more errors which can deteriorate a classifier’s performance when trained on this data. We propose a noise layer that is added to a neural network architecture. This allows modeling the noise and train on a combination of clean and noisy data. We show that in a low-resource NER task we can improve performance by up to 35% by using additional, noisy data and handling the noise.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3402</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>hedderich-klakow:2018:W18-34</bibkey>
    </paper>
    <paper id="3403">
	 <title>Multi-task learning for historical text normalization: Size matters</title>
	 <author><first>Marcel</first><last>Bollmann</last></author>
	 <author><first>Anders</first><last>Søgaard</last></author>
	 <author><first>Joachim</first><last>Bingel</last></author>
	 <booktitle>Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>19–24</pages>
	 <abstract>Historical text normalization suffers from small datasets that exhibit high variance, and previous work has shown that multi-task learning can be used to leverage data from related problems in order to obtain more robust models. Previous work has been limited to datasets from a specific language and a specific historical period, and it is not clear whether results generalize. It therefore remains an open problem, when historical text normalization benefits from multi-task learning. We explore the benefits of multi-task learning across 10 different datasets, representing different languages and periods. Our main finding—contrary to what has been observed for other NLP tasks—is that multi-task learning mainly works when target task data is very scarce.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3403</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>bollmann-sgaard-bingel:2018:W18-34</bibkey>
    </paper>
    <paper id="3404">
	 <title>Compositional Language Modeling for Icon-Based Augmentative and Alternative Communication</title>
	 <author><first>Shiran</first><last>Dudy</last></author>
	 <author><first>Steven</first><last>Bedrick</last></author>
	 <booktitle>Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>25–32</pages>
	 <abstract>Icon-based communication systems are widely used in the field of Augmentative and Alternative Communication. Typically, icon-based systems have lagged behind word- and character-based systems in terms of predictive typing functionality, due to the challenges inherent to training icon-based language models. We propose a method for synthesizing training data for use in icon-based language models, and explore two different modeling strategies. We propose a method to generate language models for corpus-less symbol-set.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3404</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>dudy-bedrick:2018:W18-34</bibkey>
    </paper>
    <paper id="3405">
	 <title>Multimodal Neural Machine Translation for Low-resource Language Pairs using Synthetic Data</title>
	 <author><first>Koel</first><last>Dutta Chowdhury</last></author>
	 <author><first>Mohammed</first><last>Hasanuzzaman</last></author>
	 <author><first>Qun</first><last>Liu</last></author>
	 <booktitle>Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>33–42</pages>
	 <abstract>In this paper, we investigate the effectiveness of training a multimodal neu- ral machine translation (MNMT) system with image features for a low-resource language pair, Hindi and English, using synthetic data. A three-way parallel corpus which contains bilingual texts and corresponding images is required to train a MNMT system with image features. However, such a corpus is not available for low resource language pairs. To address this, we developed both a synthetic training dataset and a manually curated development/test dataset for Hindi based on an existing English-image parallel corpus. We used these datasets to build our image description translation system by adopting state-of-the-art MNMT models. Our results show that it is possible to train a MNMT system for low-resource language pairs through the use of synthetic data and that such a system can benefit from image features.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3405</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>duttachowdhury-hasanuzzaman-liu:2018:W18-34</bibkey>
    </paper>
    <paper id="3406">
	 <title>Multi-Task Active Learning for Neural Semantic Role Labeling on Low Resource Conversational Corpus</title>
	 <author><first>Fariz</first><last>Ikhwantri</last></author>
	 <author><first>Samuel</first><last>Louvan</last></author>
	 <author><first>Kemal</first><last>Kurniawan</last></author>
	 <author><first>Bagas</first><last>Abisena</last></author>
	 <author><first>Valdi</first><last>Rachman</last></author>
	 <author><first>Alfan Farizki</first><last>Wicaksono</last></author>
	 <author><first>Rahmad</first><last>Mahendra</last></author>
	 <booktitle>Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>43–50</pages>
	 <abstract>Most Semantic Role Labeling (SRL) approaches are supervised methods which require a significant amount of annotated corpus, and the annotation requires linguistic expertise. In this paper, we propose a Multi-Task Active Learning framework for Semantic Role Labeling with Entity Recognition (ER) as the auxiliary task to alleviate the need for extensive data and use additional information from ER to help SRL. We evaluate our approach on Indonesian conversational dataset. Our experiments show that multi-task active learning can outperform single-task active learning method and standard multi-task learning. According to our results, active learning is more efficient by using 12% less of training data compared to passive learning in both single-task and multi-task setting. We also introduce a new dataset for SRL in Indonesian conversational domain to encourage further research in this area.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3406</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>ikhwantri-EtAl:2018:W18-34</bibkey>
    </paper>
    <paper id="3407">
	 <title>Domain Adapted Word Embeddings for Improved Sentiment Classification</title>
	 <author><first>Prathusha</first><last>Kameswara Sarma</last></author>
	 <author><first>Yingyu</first><last>Liang</last></author>
	 <author><first>Bill</first><last>Sethares</last></author>
	 <booktitle>Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>51–59</pages>
	 <abstract>Generic word embeddings are trained on large-scale generic corpora; Domain Specific (DS) word embeddings are trained only on data from a domain of interest. This paper proposes a method to combine the breadth of generic embeddings with the specificity of domain specific embeddings. The resulting embeddings, called Domain Adapted (DA) word embeddings, are formed by first aligning corresponding word vectors using Canonical Correlation Analysis (CCA) or the related nonlinear Kernel CCA (KCCA) and then combining them via convex optimization. Results from evaluation on sentiment classification tasks show that the DA embeddings substantially outperform both generic, DS embeddings when used as input features to standard or state-of-the-art sentence encoding algorithms for classification.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3407</url>
	 <attachment type="note">W18-3407.Notes.pdf</attachment>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>kameswarasarma-liang-sethares:2018:W18-34</bibkey>
    </paper>
    <paper id="3408">
	 <title>Investigating Effective Parameters for Fine-tuning of Word Embeddings Using Only a Small Corpus</title>
	 <author><first>Kanako</first><last>Komiya</last></author>
	 <author><first>Hiroyuki</first><last>Shinnou</last></author>
	 <booktitle>Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>60–67</pages>
	 <abstract>Fine-tuning is a popular method to achieve better performance when only a small target corpus is available. However, it requires tuning of a number of metaparameters and thus it might carry risk of adverse effect when inappropriate metaparameters are used. Therefore, we investigate effective parameters for fine-tuning when only a small target corpus is available. In the current study, we target at improving Japanese word embeddings created from a huge corpus. First, we demonstrate that even the word embeddings created from the huge corpus are affected by domain shift. After that, we investigate effective parameters for fine-tuning of the word embeddings using a small target corpus. We used perplexity of a language model obtained from a Long Short-Term Memory network to assess the word embeddings input into the network. The experiments revealed that fine-tuning sometimes give adverse effect when only a small target corpus is used and batch size is the most important parameter for fine-tuning. In addition, we confirmed that effect of fine-tuning is higher when size of a target corpus was larger.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3408</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>komiya-shinnou:2018:W18-34</bibkey>
    </paper>
    <paper id="3409">
	 <title>Semi-Supervised Learning with Auxiliary Evaluation Component for Large Scale e-Commerce Text Classification</title>
	 <author><first>Mingkuan</first><last>Liu</last></author>
	 <author><first>Musen</first><last>Wen</last></author>
	 <author><first>Selcuk</first><last>Kopru</last></author>
	 <author><first>Xianjing</first><last>Liu</last></author>
	 <author><first>Alan</first><last>Lu</last></author>
	 <booktitle>Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>68–76</pages>
	 <abstract>The lack of high-quality labeled training data has been one of the critical challenges facing many industrial machine learning tasks. To tackle this challenge, in this paper, we propose a semi-supervised learning method to utilize unlabeled data and user feedback signals to improve the performance of ML models. The method employs a primary model Main and an auxiliary evaluation model Eval, where Main and Eval models are trained iteratively by automatically generating labeled data from unlabeled data and/or users’ feedback signals. The proposed approach is applied to different text classification tasks. We report results on both the publicly available Yahoo! Answers dataset and our e-commerce product classification dataset. The experimental results show that the proposed method reduces the classification error rate by 4% and up to 15% across various experimental setups and datasets. A detailed comparison with other semi-supervised learning approaches is also presented later in the paper. The results from various text classification tasks demonstrate that our method outperforms those developed in previous related studies.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3409</url>
	 <software>W18-3409.Software.zip</software>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>liu-EtAl:2018:W18-34</bibkey>
    </paper>
    <paper id="3410">
	 <title>Low-rank passthrough neural networks</title>
	 <author><first>Antonio Valerio</first><last>Miceli Barone</last></author>
	 <booktitle>Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>77–86</pages>
	 <abstract>Various common deep learning architectures, such as LSTMs, GRUs, Resnets and Highway Networks, employ state passthrough connections that support training with high feed-forward depth or recurrence over many time steps. These “Passthrough Networks” architectures also enable the decoupling of the network state size from the number of parameters of the network, a possibility has been studied by Sak et al. (2014) with their low-rank parametrization of the LSTM. In this work we extend this line of research, proposing effective, low-rank and low-rank plus diagonal matrix parametrizations for Passthrough Networks which exploit this decoupling property, reducing the data complexity and memory requirements of the network while preserving its memory capacity. This is particularly beneficial in low-resource settings as it supports expressive models with a compact parametrization less susceptible to overfitting. We present competitive experimental results on several tasks, including language modeling and a near state of the art result on sequential randomly-permuted MNIST classification, a hard task on natural data.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3410</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>micelibarone:2018:W18-34</bibkey>
    </paper>
    <paper id="3500">
	 <title>Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media</title>
	 <editor><first>Lun-Wei</first><last>Ku</last></editor>
	 <editor><first>Cheng-Te</first><last>Li</last></editor>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <url>http://www.aclweb.org/anthology/W18-35</url>
	 <bibtype>book</bibtype>
	 <bibkey>SocialNLP2018:2018</bibkey>
    </paper>
    <paper id="3501">
	 <title>Sociolinguistic Corpus of WhatsApp Chats in Spanish among College Students</title>
	 <author><first>Alejandro</first><last>Dorantes</last></author>
	 <author><first>Gerardo</first><last>Sierra</last></author>
	 <author><first>Tlauhlia Yamín</first><last>Donohue Pérez</last></author>
	 <author><first>Gemma</first><last>Bel-Enguix</last></author>
	 <author><first>Mónica</first><last>Jasso Rosales</last></author>
	 <booktitle>Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>1–6</pages>
	 <abstract>This work presents the Sociolinguistic Corpus of WhatsApp Chats in Spanish among College Students, a corpus of raw data for general use. Its purpose is to offer data for the study of of language and interactions via Instant Messaging (IM) among bachelors. Our paper consists of an overview of both the corpus’s content and demographic metadata. Furthermore, it presents the current research being conducted with it —namely parenthetical expressions, orality traits, and code-switching. This work also includes a brief outline of similar corpora and recent studies in the field of IM.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3501</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>dorantes-EtAl:2018:SocialNLP2018</bibkey>
    </paper>
    <paper id="3502">
	 <title>A Crowd-Annotated Spanish Corpus for Humor Analysis</title>
	 <author><first>Santiago</first><last>Castro</last></author>
	 <author><first>Luis</first><last>Chiruzzo</last></author>
	 <author><first>Aiala</first><last>Rosá</last></author>
	 <author><first>Diego</first><last>Garat</last></author>
	 <author><first>Guillermo</first><last>Moncecchi</last></author>
	 <booktitle>Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>7–11</pages>
	 <abstract>Computational Humor involves several tasks, such as humor recognition, humor generation, and humor scoring, for which it is useful to have human-curated data. In this work we present a corpus of 27,000 tweets written in Spanish and crowd-annotated by their humor value and funniness score, with about four annotations per tweet, tagged by 1,300 people over the Internet. It is equally divided between tweets coming from humorous and non-humorous accounts. The inter-annotator agreement Krippendorff’s alpha value is 0.5710. The dataset is available for general usage and can serve as a basis for humor detection and as a first step to tackle subjectivity.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3502</url>
	 <attachment type="presentation">W18-3502.Presentation.pdf</attachment>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>castro-EtAl:2018:SocialNLP2018</bibkey>
    </paper>
    <paper id="3503">
	 <title>A Twitter Corpus for Hindi-English Code Mixed POS Tagging</title>
	 <author><first>Kushagra</first><last>Singh</last></author>
	 <author><first>Indira</first><last>Sen</last></author>
	 <author><first>Ponnurangam</first><last>Kumaraguru</last></author>
	 <booktitle>Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>12–17</pages>
	 <abstract>Code-mixing is a linguistic phenomenon where multiple languages are used in the same occurrence that is increasingly common in multilingual societies. Code-mixed content on social media is also on the rise, prompting the need for tools to automatically understand such content. Automatic Parts-of-Speech (POS) tagging is an essential step in any Natural Language Processing (NLP) pipeline, but there is a lack of annotated data to train such models. In this work, we present a unique language tagged and POS-tagged dataset of code-mixed English-Hindi tweets related to five incidents in India that led to a lot of Twitter activity. Our dataset is unique in two dimensions: (i) it is larger than previous annotated datasets and (ii) it closely resembles typical real-world tweets. Additionally, we present a POS tagging model that is trained on this dataset to provide an example of how this dataset can be used. The model also shows the efficacy of our dataset in enabling the creation of code-mixed social media POS taggers.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3503</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>singh-sen-kumaraguru:2018:SocialNLP2018</bibkey>
    </paper>
    <paper id="3504">
	 <title>Detecting Offensive Tweets in Hindi-English Code-Switched Language</title>
	 <author><first>Puneet</first><last>Mathur</last></author>
	 <author><first>Rajiv</first><last>Shah</last></author>
	 <author><first>Ramit</first><last>Sawhney</last></author>
	 <author><first>Debanjan</first><last>Mahata</last></author>
	 <booktitle>Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>18–26</pages>
	 <abstract>The exponential rise of social media websites like Twitter, Facebook and Reddit in linguistically diverse geographical regions has led to hybridization of popular native languages with English in an effort to ease communication. The paper focuses on the classification of offensive tweets written in Hinglish language, which is a portmanteau of the Indic language Hindi with the Roman script. The paper introduces a novel tweet dataset, titled Hindi-English Offensive Tweet (HEOT) dataset, consisting of tweets in Hindi-English code switched language split into three classes: non-offensive, abusive and hate-speech. Further, we approach the problem of classification of the tweets in HEOT dataset using transfer learning wherein the proposed model employing Convolutional Neural Networks is pre-trained on tweets in English followed by retraining on Hinglish tweets.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3504</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>mathur-EtAl:2018:SocialNLP2018</bibkey>
    </paper>
    <paper id="3505">
	 <title>SocialNLP 2018 EmotionX Challenge Overview: Recognizing Emotions in Dialogues</title>
	 <author><first>Chao-Chun</first><last>Hsu</last></author>
	 <author><first>Lun-Wei</first><last>Ku</last></author>
	 <booktitle>Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>27–31</pages>
	 <abstract>This paper describes an overview of the Dialogue Emotion Recognition Challenge, EmotionX, at the Sixth SocialNLP Workshop, which recognizes the emotion of each utterance in dialogues. This challenge offers the EmotionLines dataset as the experimental materials. The EmotionLines dataset contains conversations from Friends TV show transcripts (Friends) and real chatting logs (EmotionPush), where every dialogue utterance is labeled with emotions. Organizers provide baseline results. 18 teams registered in this challenge and 5 of them submitted their results successfully. The best team achieves the unweighted accuracy 62.48 and 62.5 on EmotionPush and Friends, respectively. In this paper we present the task definition, test collection, the evaluation results of the groups that participated in this challenge, and their approach.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3505</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>hsu-ku:2018:SocialNLP2018</bibkey>
    </paper>
    <paper id="3506">
	 <title>EmotionX-DLC: Self-Attentive BiLSTM for Detecting Sequential Emotions in Dialogues</title>
	 <author><first>Linkai</first><last>Luo</last></author>
	 <author><first>Haiqin</first><last>Yang</last></author>
	 <author><first>Francis Y. L.</first><last>Chin</last></author>
	 <booktitle>Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>32–36</pages>
	 <abstract>In this paper, we propose a self-attentive bidirectional long short-term memory (SA-BiLSTM) network to predict multiple emotions for the EmotionX challenge. The BiLSTM exhibits the power of modeling the word dependencies, and extracting the most relevant features for emotion classification. Building on top of BiLSTM, the self-attentive network can model the contextual dependencies between utterances which are helpful for classifying the ambiguous emotions. We achieve 59.6 and 55.0 unweighted accuracy scores in the Friends and the EmotionPush test sets, respectively.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3506</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>luo-yang-chin:2018:SocialNLP2018</bibkey>
    </paper>
    <paper id="3507">
	 <title>EmotionX-AR: CNN-DCNN autoencoder based Emotion Classifier</title>
	 <author><first>Sopan</first><last>Khosla</last></author>
	 <booktitle>Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>37–44</pages>
	 <abstract>In this paper, we model emotions in EmotionLines dataset using a convolutional-deconvolutional autoencoder (CNN-DCNN) framework. We show that adding a joint reconstruction loss improves performance. Quantitative evaluation with jointly trained network, augmented with linguistic features, reports best accuracies for emotion prediction; namely joy, sadness, anger, and neutral emotion in text.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3507</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>khosla:2018:SocialNLP2018</bibkey>
    </paper>
    <paper id="3508">
	 <title>EmotionX-SmartDubai_NLP: Detecting User Emotions In Social Media Text</title>
	 <author><first>Hessa</first><last>AlBalooshi</last></author>
	 <author><first>Shahram</first><last>Rahmanian</last></author>
	 <author><first>Rahul</first><last>Venkatesh Kumar</last></author>
	 <booktitle>Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>45–49</pages>
	 <abstract>This paper describes the working note on “EmotionX” shared task. It is hosted by SocialNLP 2018. The objective of this task is to detect the emotions, based on each speaker’s utterances that are in English. Taking this as multiclass text classification problem, we have experimented to develop a model to classify the target class. The primary challenge in this task is to detect the emotions in short messages, communicated through social media. This paper describes the participation of SmartDubai_NLP team in EmotionX shared task and our investigation to detect the emotions from utterance using Neural networks and Natural language understanding.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3508</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>albalooshi-rahmanian-venkateshkumar:2018:SocialNLP2018</bibkey>
    </paper>
    <paper id="3509">
	 <title>EmotionX-Area66: Predicting Emotions in Dialogues using Hierarchical Attention Network with Sequence Labeling</title>
	 <author><first>Rohit</first><last>Saxena</last></author>
	 <author><first>Savita</first><last>Bhat</last></author>
	 <author><first>Niranjan</first><last>Pedanekar</last></author>
	 <booktitle>Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>50–55</pages>
	 <abstract>This paper presents our system submitted to the EmotionX challenge. It is an emotion detection task on dialogues in the EmotionLines dataset. We formulate this as a hierarchical network where network learns data representation at both utterance level and dialogue level. Our model is inspired by Hierarchical Attention network (HAN) and uses pre-trained word embeddings as features. We formulate emotion detection in dialogues as a sequence labeling problem to capture the dependencies among labels. We report the performance accuracy for four emotions (anger, joy, neutral and sadness). The model achieved unweighted accuracy of 55.38% on Friends test dataset and 56.73% on EmotionPush test dataset. We report an improvement of 22.51% in Friends dataset and 36.04% in EmotionPush dataset over baseline results.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3509</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>saxena-bhat-pedanekar:2018:SocialNLP2018</bibkey>
    </paper>
    <paper id="3510">
	 <title>EmotionX-JTML: Detecting emotions with Attention</title>
	 <author><first>Johnny</first><last>Torres</last></author>
	 <booktitle>Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>56–60</pages>
	 <abstract>This paper addresses the problem of automatic recognition of emotions in conversational text datasets for the EmotionX challenge. Emotion is a human characteristic expressed through several modalities (e.g., auditory, visual, tactile). Trying to detect emotions only from the text becomes a difficult task even for humans. This paper evaluates several neural architectures based on Attention Models, which allow extracting relevant parts of the context within a conversation to identify the emotion associated with each utterance. Empirical results in the validation datasets demonstrate the effectiveness of the approach compared to the reference models for some instances, and other cases show better results with simpler models.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3510</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>torres:2018:SocialNLP2018</bibkey>
    </paper>
    <paper id="3511">
	 <title>Towards Automation of Sense-type Identification of Verbs in OntoSenseNet</title>
	 <author><first>Sreekavitha</first><last>Parupalli</last></author>
	 <author><first>Vijjini</first><last>Anvesh Rao</last></author>
	 <author><first>Radhika</first><last>Mamidi</last></author>
	 <booktitle>Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>61–66</pages>
	 <abstract>In this paper, we discuss the enrichment of a manually developed resource, OntoSenseNet for Telugu. OntoSenseNet is a sense annotated resource that marks each verb of Telugu with a primary and a secondary sense. The area of research is relatively recent but has a large scope of development. We provide an introductory work to enrich the OntoSenseNet to promote further research in Telugu. Classifiers are adopted to learn the sense relevant features of the words in the resource and also to automate the tagging of sense-types for verbs. We perform a comparative analysis of different classifiers applied on OntoSenseNet. The results of the experiment prove that automated enrichment of the resource is effective using SVM classifiers and Adaboost ensemble.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3511</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>parupalli-anveshrao-mamidi:2018:SocialNLP2018</bibkey>
    </paper>
    <paper id="3512">
	 <title>Improving Classification of Twitter Behavior During Hurricane Events</title>
	 <author><first>Kevin</first><last>Stowe</last></author>
	 <author><first>Jennings</first><last>Anderson</last></author>
	 <author><first>Martha</first><last>Palmer</last></author>
	 <author><first>Leysia</first><last>Palen</last></author>
	 <author><first>Ken</first><last>Anderson</last></author>
	 <booktitle>Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>67–75</pages>
	 <abstract>A large amount of social media data is generated during natural disasters, and identifying the relevant portions of this data is critical for researchers attempting to understand human behavior, the effects of information sources, and preparatory actions undertaken during these events. In order to classify human behavior during hazard events, we employ machine learning for two tasks: identifying hurricane related tweets and classifying user evacuation behavior during hurricanes. We show that feature-based and deep learning methods provide different benefits for tweet classification, and ensemble-based methods using linguistic, temporal, and geospatial features can effectively classify user behavior.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3512</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>stowe-EtAl:2018:SocialNLP2018</bibkey>
    </paper>
    <paper id="3513">
	 <title>Political discourse classification in social networks using context sensitive convolutional neural networks</title>
	 <author><first>Aritz</first><last>Bilbao-Jayo</last></author>
	 <author><first>Aitor</first><last>Almeida</last></author>
	 <booktitle>Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>76–85</pages>
	 <abstract>In this study we propose a new approach to analyse the political discourse in on-line social networks such as Twitter. To do so, we have built a discourse classifier using Convolutional Neural Networks. Our model has been trained using election manifestos annotated manually by political scientists following the Regional Manifestos Project (RMP) methodology. In total, it has been trained with more than 88,000 sentences extracted from more that 100 annotated manifestos. Our approach takes into account the context of the phrase in order to classify it, like what was previously said and the political affiliation of the transmitter. To improve the classification results we have used a simplified political message taxonomy developed within the Electronic Regional Manifestos Project (E-RMP). Using this taxonomy, we have validated our approach analysing the Twitter activity of the main Spanish political parties during 2015 and 2016 Spanish general election and providing a study of their discourse.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3513</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>bilbaojayo-almeida:2018:SocialNLP2018</bibkey>
    </paper>
    <paper id="3600">
	 <title>Proceedings of the First Workshop on Multilingual Surface Realisation</title>
	 <editor><first>Simon</first><last>Mille</last></editor>
	 <editor><first>Anja</first><last>Belz</last></editor>
	 <editor><first>Bernd</first><last>Bohnet</last></editor>
	 <editor><first>Emily</first><last>Pitler</last></editor>
	 <editor><first>Leo</first><last>Wanner</last></editor>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <url>http://www.aclweb.org/anthology/W18-36</url>
	 <revision id="2">W18-36v2</revision>
	 <bibtype>book</bibtype>
	 <bibkey>MSR:2018</bibkey>
    </paper>
    <paper id="3601">
	 <title>The First Multilingual Surface Realisation Shared Task (SR’18): Overview and Evaluation Results</title>
	 <author><first>Simon</first><last>Mille</last></author>
	 <author><first>Anja</first><last>Belz</last></author>
	 <author><first>Bernd</first><last>Bohnet</last></author>
	 <author><first>Yvette</first><last>Graham</last></author>
	 <author><first>Emily</first><last>Pitler</last></author>
	 <author><first>Leo</first><last>Wanner</last></author>
	 <booktitle>Proceedings of the First Workshop on Multilingual Surface Realisation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>1–12</pages>
	 <abstract>We report results from the SR’18 Shared Task, a new multilingual surface realisation task organised as part of the ACL’18 Workshop on Multilingual Surface Realisation. As in its English-only predecessor task SR’11, the shared task comprised two tracks with different levels of complexity: (a) a shallow track where the inputs were full UD structures with word order information removed and tokens lemmatised; and (b) a deep track where additionally, functional words and morphological information were removed. The shallow track was offered in ten, and the deep track in three languages. Systems were evaluated (a) automatically, using a range of intrinsic metrics, and (b) by human judges in terms of readability and meaning similarity. This report presents the evaluation results, along with descriptions of the SR’18 tracks, data and evaluation methods. For full descriptions of the participating systems, please see the separate system reports elsewhere in this volume.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3601</url>
	 <revision id="2">W18-3601v2</revision>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>mille-EtAl:2018:MSR</bibkey>
    </paper>
    <paper id="3602">
	 <title>BinLin: A Simple Method of Dependency Tree Linearization</title>
	 <author><first>Yevgeniy</first><last>Puzikov</last></author>
	 <author><first>Iryna</first><last>Gurevych</last></author>
	 <booktitle>Proceedings of the First Workshop on Multilingual Surface Realisation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>13–28</pages>
	 <abstract>Surface Realization Shared Task 2018 is a workshop on generating sentences from lemmatized sets of dependency triples. This paper describes the results of our participation in the challenge. We develop a data-driven pipeline system which first orders the lemmas and then conjugates the words to finish the surface realization process. Our contribution is a novel sequential method of ordering lemmas, which, despite its simplicity, achieves promising results. We demonstrate the effectiveness of the proposed approach, describe its limitations and outline ways to improve it.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3602</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>puzikov-gurevych:2018:MSR</bibkey>
    </paper>
    <paper id="3603">
	 <title>IIT (BHU) Varanasi at MSR-SRST 2018: A Language Model Based Approach for Natural Language Generation</title>
	 <author><first>Shreyansh</first><last>Singh</last></author>
	 <author><first>Ayush</first><last>Sharma</last></author>
	 <author><first>Avi</first><last>Chawla</last></author>
	 <author><first>A.K.</first><last>Singh</last></author>
	 <booktitle>Proceedings of the First Workshop on Multilingual Surface Realisation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>29–34</pages>
	 <abstract>This paper describes our submission system for the Shallow Track of Surface Realization Shared Task 2018 (SRST’18). The task was to convert genuine UD structures, from which word order information had been removed and the tokens had been lemmatized, into their correct sentential form. We divide the problem statement into two parts, word reinflection and correct word order prediction. For the first sub-problem, we use a Long Short Term Memory based Encoder-Decoder approach. For the second sub-problem, we present a Language Model (LM) based approach. We apply two different sub-approaches in the LM Based approach and the combined result of these two approaches is considered as the final output of the system.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3603</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>singh-EtAl:2018:MSR</bibkey>
    </paper>
    <paper id="3604">
	 <title>Surface Realization Shared Task 2018 (SR18): The Tilburg University Approach</title>
	 <author><first>Thiago</first><last>Castro Ferreira</last></author>
	 <author><first>Sander</first><last>Wubben</last></author>
	 <author><first>Emiel</first><last>Krahmer</last></author>
	 <booktitle>Proceedings of the First Workshop on Multilingual Surface Realisation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>35–38</pages>
	 <abstract>This study describes the approach developed by the Tilburg University team to the shallow task of the Multilingual Surface Realization Shared Task 2018 (SR18). Based on (Castro Ferreira et al., 2017), the approach works by first preprocessing an input dependency tree into an ordered linearized string, which is then realized using a statistical machine translation model. Our approach shows promising results, with BLEU scores above 50 for 5 different languages (English, French, Italian, Portuguese and Spanish) and above 35 for the Dutch language.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3604</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>castroferreira-wubben-krahmer:2018:MSR</bibkey>
    </paper>
    <paper id="3605">
	 <title>The OSU Realizer for SRST ‘18: Neural Sequence-to-Sequence Inflection and Incremental Locality-Based Linearization</title>
	 <author><first>David</first><last>King</last></author>
	 <author><first>Michael</first><last>White</last></author>
	 <booktitle>Proceedings of the First Workshop on Multilingual Surface Realisation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>39–48</pages>
	 <abstract>Surface realization is a nontrivial task as it involves taking structured data and producing grammatically and semantically correct utterances. Many competing grammar-based and statistical models for realization still struggle with relatively simple sentences. For our submission to the 2018 Surface Realization Shared Task, we tackle the shallow task by first generating inflected wordforms with a neural sequence-to-sequence model before incrementally linearizing them. For linearization, we use a global linear model trained using early update that makes use of features that take into account the dependency structure and dependency locality. Using this pipeline sufficed to produce surprisingly strong results in the shared task. In future work, we intend to pursue joint approaches to linearization and morphological inflection and incorporating a neural language model into the linearization choices.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3605</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>king-white:2018:MSR</bibkey>
    </paper>
    <paper id="3606">
	 <title>Generating High-Quality Surface Realizations Using Data Augmentation and Factored Sequence Models</title>
	 <author><first>Henry</first><last>Elder</last></author>
	 <author><first>Chris</first><last>Hokamp</last></author>
	 <booktitle>Proceedings of the First Workshop on Multilingual Surface Realisation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>49–53</pages>
	 <abstract>This work presents state of the art results in reconstruction of surface realizations from obfuscated text. We identify the lack of sufficient training data as the major obstacle to training high-performing models, and solve this issue by generating large amounts of synthetic training data. We also propose preprocessing techniques which make the structure contained in the input features more accessible to sequence models. Our models were ranked first on all evaluation metrics in the English portion of the 2018 Surface Realization shared task.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3606</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>elder-hokamp:2018:MSR</bibkey>
    </paper>
    <paper id="3607">
	 <title>AX Semantics’ Submission to the Surface Realization Shared Task 2018</title>
	 <author><first>Andreas</first><last>Madsack</last></author>
	 <author><first>Johanna</first><last>Heininger</last></author>
	 <author><first>Nyamsuren</first><last>Davaasambuu</last></author>
	 <author><first>Vitaliia</first><last>Voronik</last></author>
	 <author><first>Michael</first><last>Käufl</last></author>
	 <author><first>Robert</first><last>Weißgraeber</last></author>
	 <booktitle>Proceedings of the First Workshop on Multilingual Surface Realisation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>54–57</pages>
	 <abstract>In this paper we describe our system and experimental results on the development set of the Surface Realisation Shared Task. Our system is an entry for the Shallow-Task, with two different models based on deep-learning implementations for building the sentence combined with a rule-based morphology component.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3607</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>madsack-EtAl:2018:MSR</bibkey>
    </paper>
    <paper id="3608">
	 <title>NILC-SWORNEMO at the Surface Realization Shared Task: Exploring Syntax-Based Word Ordering using Neural Models</title>
	 <author><first>Marco Antonio</first><last>Sobrevilla Cabezudo</last></author>
	 <author><first>Thiago</first><last>Pardo</last></author>
	 <booktitle>Proceedings of the First Workshop on Multilingual Surface Realisation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>58–64</pages>
	 <abstract>This paper describes the submission by the NILC Computational Linguistics research group of the University of S ̃ao Paulo/Brazil to the Track 1 of the Surface Realization Shared Task (SRST Track 1). We present a neural-based method that works at the syntactic level to order the words (which we refer by NILC-SWORNEMO, standing for “Syntax-based Word ORdering using NEural MOdels”). Additionally, we apply a bottom-up approach to build the sentence and, using language-specific lexicons, we produce the proper word form of each lemma in the sentence. The results obtained by our method outperformed the average of the results for English, Portuguese and Spanish in the track.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3608</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>sobrevillacabezudo-pardo:2018:MSR</bibkey>
    </paper>
    <paper id="3609">
	 <title>The DipInfo-UniTo system for SRST 2018</title>
	 <author><first>Valerio</first><last>Basile</last></author>
	 <author><first>Alessandro</first><last>Mazzei</last></author>
	 <booktitle>Proceedings of the First Workshop on Multilingual Surface Realisation</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>65–71</pages>
	 <abstract>This paper describes the system developed by the DipInfo-UniTo team to participate to the shallow track of the Surface Realization Shared Task 2018. The system employs two separate neural networks with different architectures to predict the word ordering and the morphological inflection independently from each other. The UniTO realizer is language independent, and its simple architecture allowed it to be scored in the central part of the final ranking of the shared task.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3609</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>basile-mazzei:2018:MSR</bibkey>
    </paper>
    <paper id="3700">
	 <title>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</title>
	 <editor><first>Yuen-Hsien</first><last>Tseng</last></editor>
	 <editor><first>Hsin-Hsi</first><last>Chen</last></editor>
	 <editor><first>Vincent</first><last>Ng</last></editor>
	 <editor><first>Mamoru</first><last>Komachi</last></editor>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <url>http://www.aclweb.org/anthology/W18-37</url>
	 <bibtype>book</bibtype>
	 <bibkey>NLPTEA:2018</bibkey>
    </paper>
    <paper id="3701">
	 <title>Generating Questions for Reading Comprehension using Coherence Relations</title>
	 <author><first>Takshak</first><last>Desai</last></author>
	 <author><first>Parag</first><last>Dakle</last></author>
	 <author><first>Dan</first><last>Moldovan</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>1–10</pages>
	 <abstract>In this paper, we have proposed a technique for generating complex reading comprehension questions from a discourse that are more useful than factual ones derived from assertions. Our system produces a set of general-level questions using coherence relations and a set of well-defined syntactic transformations on the input text. Generated questions evaluate comprehension abilities like a comprehensive analysis of the text and its structure, correct identification of the author’s intent, a thorough evaluation of stated arguments; and a deduction of the high-level semantic relations that hold between text spans. Experiments performed on the RST-DT corpus allow us to conclude that our system possesses a strong aptitude for generating intricate questions. These questions are capable of effectively assessing a student’s interpretation of the text.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3701</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>desai-dakle-moldovan:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3702">
	 <title>Syntactic and Lexical Approaches to Reading Comprehension</title>
	 <author><first>Henry</first><last>Lin</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>11–19</pages>
	 <abstract>Among the challenges of teaching reading comprehension in K – 12 are identifying the portions of a text that are difficult for a student, comprehending major critical ideas, and understanding context-dependent polysemous words. We present a simple, unsupervised but robust and accurate syntactic method for achieving the first objective and a modified hierarchical lexical method for the second objective. Focusing on pinpointing troublesome sentences instead of the overall readability and on concepts central to a reading, we believe these methods will greatly facilitate efforts to help students improve reading skills</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3702</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>lin:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3703">
	 <title>Feature Optimization for Predicting Readability of Arabic L1 and L2</title>
	 <author><first>Hind</first><last>Saddiki</last></author>
	 <author><first>Nizar</first><last>Habash</last></author>
	 <author><first>Violetta</first><last>Cavalli-Sforza</last></author>
	 <author><first>Muhamed</first><last>Al Khalil</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>20–29</pages>
	 <abstract>Advances in automatic readability assessment can impact the way people consume information in a number of domains. Arabic, being a low-resource and morphologically complex language, presents numerous challenges to the task of automatic readability assessment. In this paper, we present the largest and most in-depth computational readability study for Arabic to date. We study a large set of features with varying depths, from shallow words to syntactic trees, for both L1 and L2 readability tasks. Our best L1 readability accuracy result is 94.8% (75% error reduction from a commonly used baseline). The comparable results for L2 are 72.4% (45% error reduction). We also demonstrate the added value of leveraging L1 features for L2 readability prediction.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3703</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>saddiki-EtAl:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3704">
	 <title>A Tutorial Markov Analysis of Effective Human Tutorial Sessions</title>
	 <author><first>Nabin</first><last>Maharjan</last></author>
	 <author><first>Vasile</first><last>Rus</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>30–34</pages>
	 <abstract>This paper investigates what differentiates effective tutorial sessions from less effective sessions. Towards this end, we characterize and explore human tutors’ actions in tutorial dialogue sessions by mapping the tutor-tutee interactions, which are streams of dialogue utterances, into streams of actions, based on the language-as-action theory. Next, we use human expert judgment measures, evidence of learning (EL) and evidence of soundness (ES), to identify effective and ineffective sessions. We perform sub-sequence pattern mining to identify sub-sequences of dialogue modes that discriminate good sessions from bad sessions. We finally use the results of sub-sequence analysis method to generate a tutorial Markov process for effective tutorial sessions.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3704</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>maharjan-rus:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3705">
	 <title>Thank “Goodness”! A Way to Measure Style in Student Essays</title>
	 <author><first>Sandeep</first><last>Mathias</last></author>
	 <author><first>Pushpak</first><last>Bhattacharyya</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>35–41</pages>
	 <abstract>Essays have two major components for scoring - content and style. In this paper, we describe a property of the essay, called goodness, and use it to predict the score given for the style of student essays. We compare our approach to solve this problem with baseline approaches, like language modeling and also a state-of-the-art deep learning system. We show that, despite being quite intuitive, our approach is very powerful in predicting the style of the essays.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3705</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>mathias-bhattacharyya:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3706">
	 <title>Overview of NLPTEA-2018 Share Task Chinese Grammatical Error Diagnosis</title>
	 <author><first>Gaoqi</first><last>RAO</last></author>
	 <author><first>Qi</first><last>Gong</last></author>
	 <author><first>Baolin</first><last>Zhang</last></author>
	 <author><first>Endong</first><last>Xun</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>42–51</pages>
	 <abstract>This paper presents the NLPTEA 2018 shared task for Chinese Grammatical Error Diagnosis (CGED) which seeks to identify grammatical error types, their range of occurrence and recommended corrections within sentences written by learners of Chinese as foreign language. We describe the task definition, data preparation, performance metrics, and evaluation results. Of the 20 teams registered for this shared task, 13 teams developed the system and submitted a total of 32 runs. Progress in system performances was obviously, reaching F1 of 36.12% in position level and 25.27% in correction level. All data sets with gold standards and scoring scripts are made publicly available to researchers.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3706</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>rao-EtAl:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3707">
	 <title>Chinese Grammatical Error Diagnosis using Statistical and Prior Knowledge driven Features with Probabilistic Ensemble Enhancement</title>
	 <author><first>Ruiji</first><last>Fu</last></author>
	 <author><first>Zhengqi</first><last>Pei</last></author>
	 <author><first>Jiefu</first><last>Gong</last></author>
	 <author><first>Wei</first><last>Song</last></author>
	 <author><first>Dechuan</first><last>Teng</last></author>
	 <author><first>Wanxiang</first><last>Che</last></author>
	 <author><first>Shijin</first><last>Wang</last></author>
	 <author><first>Guoping</first><last>Hu</last></author>
	 <author><first>Ting</first><last>Liu</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>52–59</pages>
	 <abstract>This paper describes our system at NLPTEA-2018 Task #1: Chinese Grammatical Error Diagnosis. Grammatical Error Diagnosis is one of the most challenging NLP tasks，which is to locate grammar errors and tell error types. Our system is built on the model of bidirectional Long Short-Term Memory with a conditional random field layer (BiLSTM-CRF) but integrates with several new features. First, richer features are considered in the BiLSTM-CRF model; second, a probabilistic ensemble approach is adopted; third, Template Matcher are used during a post-processing to bring in human knowledge. In official evaluation, our system obtains the highest F1 scores at identifying error types and locating error positions, the second highest F1 score at sentence level error detection. We also recommend error corrections for specific error types and achieve the best F1 performance among all participants.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3707</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>fu-EtAl:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3708">
	 <title>A Hybrid System for Chinese Grammatical Error Diagnosis and Correction</title>
	 <author><first>Chen</first><last>Li</last></author>
	 <author><first>Junpei</first><last>Zhou</last></author>
	 <author><first>Zuyi</first><last>Bao</last></author>
	 <author><first>Hengyou</first><last>Liu</last></author>
	 <author><first>Guangwei</first><last>Xu</last></author>
	 <author><first>Linlin</first><last>Li</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>60–69</pages>
	 <abstract>This paper introduces the DM_NLP team’s system for NLPTEA 2018 shared task of Chinese Grammatical Error Diagnosis (CGED), which can be used to detect and correct grammatical errors in texts written by Chinese as a Foreign Language (CFL) learners. This task aims at not only detecting four types of grammatical errors including redundant words (R), missing words (M), bad word selection (S) and disordered words (W), but also recommending corrections for errors of M and S types. We proposed a hybrid system including four models for this task with two stages: the detection stage and the correction stage. In the detection stage, we first used a BiLSTM-CRF model to tag potential errors by sequence labeling, along with some handcraft features. Then we designed three Grammatical Error Correction (GEC) models to generate corrections, which could help to tune the detection result. In the correction stage, candidates were generated by the three GEC models and then merged to output the final corrections for M and S types. Our system reached the highest precision in the correction subtask, which was the most challenging part of this shared task, and got top 3 on F1 scores for position detection of errors.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3708</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>li-EtAl:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3709">
	 <title>Ling@CASS Solution to the NLP-TEA CGED Shared Task 2018</title>
	 <author><first>Qinan</first><last>Hu</last></author>
	 <author><first>Yongwei</first><last>Zhang</last></author>
	 <author><first>Fang</first><last>Liu</last></author>
	 <author><first>Yueguo</first><last>Gu</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>70–76</pages>
	 <abstract>In this study, we employ the sequence to sequence learning to model the task of grammar error correction. The system takes potentially erroneous sentences as inputs, and outputs correct sentences. To breakthrough the bottlenecks of very limited size of manually labeled data, we adopt a semi-supervised approach. Specifically, we adapt correct sentences written by native Chinese speakers to generate pseudo grammatical errors made by learners of Chinese as a second language. We use the pseudo data to pre-train the model, and the CGED data to fine-tune it. Being aware of the significance of precision in a grammar error correction system in real scenarios, we use ensembles to boost precision. When using inputs as simple as Chinese characters, the ensembled system achieves a precision at 86.56% in the detection of erroneous sentences, and a precision at 51.53% in the correction of errors of Selection and Missing types.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3709</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>hu-EtAl:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3710">
	 <title>Chinese Grammatical Error Diagnosis Based on Policy Gradient LSTM Model</title>
	 <author><first>Changliang</first><last>Li</last></author>
	 <author><first>Ji</first><last>Qi</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>77–82</pages>
	 <abstract>Chinese Grammatical Error Diagnosis (CGED) is a natural language processing task for the NLPTEA2018 workshop held during ACL2018. The goal of this task is to diagnose Chinese sentences containing four kinds of grammatical errors through the model and find out the sentence errors. Chinese grammatical error diagnosis system is a very important tool, which can help Chinese learners automatically diagnose grammatical errors in many scenarios. However, due to the limitations of the Chinese language’s own characteristics and datasets, the traditional model faces the problem of extreme imbalances in the positive and negative samples and the disappearance of gradients. In this paper, we propose a sequence labeling method based on the Policy Gradient LSTM model and apply it to this task to solve the above problems. The results show that our model can achieve higher precision scores in the case of lower False positive rate (FPR) and it is convenient to optimize the model on-line.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3710</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>li-qi:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3711">
	 <title>The Importance of Recommender and Feedback Features in a Pronunciation Learning Aid</title>
	 <author><first>Dzikri</first><last>Fudholi</last></author>
	 <author><first>Hanna</first><last>Suominen</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>83–87</pages>
	 <abstract>Verbal communication — and pronunciation as its part — is a core skill that can be developed through guided learning. An artificial intelligence system can take a role in these guided learning approaches as an enabler of an application for pronunciation learning with a recommender system to guide language learners through exercises and feedback system to correct their pronunciation. In this paper, we report on a user study on language learners’ perceived usefulness of the application. 16 international students who spoke non-native English and lived in Australia participated. 13 of them said they need to improve their pronunciation skills in English because of their foreign accent. The feedback system with features for pronunciation scoring, speech replay, and giving a pronunciation example was deemed essential by most of the respondents. In contrast, a clear dichotomy between the recommender system perceived as useful or useless existed; the system had features to prompt new common words or old poorly-scored words. These results can be used to target research and development from information retrieval and reinforcement learning for better and better recommendations to speech recognition and speech analytics for accent acquisition.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3711</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>fudholi-suominen:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3712">
	 <title>Selecting NLP Techniques to Evaluate Learning Design Objectives in Collaborative Multi-perspective Elaboration Activities</title>
	 <author><first>Aneesha</first><last>Bakharia</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>88–92</pages>
	 <abstract>PerspectivesX is a multi-perspective elaboration tool designed to encourage learner submission and curation across a range of collaborative learning activities. In this paper, it is shown that the learning design objectives of collaborative learning activities can be evaluated using NLP techniques, but that careful analysis of learner impact and pedagogical intent are required in order to select appropriate techniques. In particular, this paper focuses on the NLP techniques required to deliver an instructor dashboard, personalized learner feedback and content recommendation within multi-perspective elaboration activities. Key NLP techniques considered for inclusion include summarization, topic modeling, paraphrase detection and diversified content recommendation.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3712</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>bakharia:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3713">
	 <title>Augmenting Textual Qualitative Features in Deep Convolution Recurrent Neural Network for Automatic Essay Scoring</title>
	 <author><first>Tirthankar</first><last>Dasgupta</last></author>
	 <author><first>Abir</first><last>Naskar</last></author>
	 <author><first>Lipika</first><last>Dey</last></author>
	 <author><first>Rupsa</first><last>Saha</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>93–102</pages>
	 <abstract>In this paper we present a qualitatively enhanced deep convolution recurrent neural network for computing the quality of a text in an automatic essay scoring task. The novelty of the work lies in the fact that instead of considering only the word and sentence representation of a text, we try to augment the different complex linguistic, cognitive and psycological features associated within a text document along with a hierarchical convolution recurrent neural network framework. Our preliminary investigation shows that incorporation of such qualitative feature vectors along with standard word/sentence embeddings can give us better understanding about improving the overall evaluation of the input essays.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3713</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>dasgupta-EtAl:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3714">
	 <title>Joint learning of frequency and word embeddings for multilingual readability assessment</title>
	 <author><first>Dieu-Thu</first><last>Le</last></author>
	 <author><first>Cam-Tu</first><last>Nguyen</last></author>
	 <author><first>Xiaoliang</first><last>Wang</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>103–107</pages>
	 <abstract>This paper describes two models that employ word frequency embeddings to deal with the problem of readability assessment in multiple languages. The task is to determine the difficulty level of a given document, i.e., how hard it is for a reader to fully comprehend the text. The proposed models show how frequency information can be integrated to improve the readability assessment. The experimental results testing on both English and Chinese datasets show that the proposed models improve the results notably when comparing to those using only traditional word embeddings.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3714</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>le-nguyen-wang:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3715">
	 <title>MULLE: A grammar-based Latin language learning tool to supplement the classroom setting</title>
	 <author><first>Herbert</first><last>Lange</last></author>
	 <author><first>Peter</first><last>Ljunglöf</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>108–112</pages>
	 <abstract>MULLE is a tool for language learning that focuses on teaching Latin as a foreign language. It is aimed for easy integration into the traditional classroom setting and syllabus, which makes it distinct from other language learning tools that provide standalone learning experience. It uses grammar-based lessons and embraces methods of gamification to improve the learner motivation. The main type of exercise provided by our application is to practice translation, but it is also possible to shift the focus to vocabulary or morphology training.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3715</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>lange-ljunglf:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3716">
	 <title>Textual Features Indicative of Writing Proficiency in Elementary School Spanish Documents</title>
	 <author><first>Gemma</first><last>Bel-Enguix</last></author>
	 <author><first>Diana</first><last>Dueñas Chavez</last></author>
	 <author><first>Arturo</first><last>Curiel Díaz</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>113–118</pages>
	 <abstract>Childhood acquisition of written language is not straightforward. Writing skills evolve differently depending on external factors, such as the conditions in which children practice their productions and the quality of their instructors’ guidance. This can be challenging in low-income areas, where schools may struggle to ensure ideal acquisition conditions. Developing computational tools to support the learning process may counterweight negative environmental influences; however, few work exists on the use of information technologies to improve childhood literacy. This work centers around the computational study of Spanish word and syllable structure in documents written by 2nd and 3rd year elementary school students. The studied texts were compared against a corpus of short stories aimed at the same age group, so as to observe whether the children tend to produce similar written patterns as the ones they are expected to interpret at their literacy level. The obtained results show some significant differences between the two kinds of texts, pointing towards possible strategies for the implementation of new education software in support of written language acquisition.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3716</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>belenguix-dueaschavez-curieldaz:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3717">
	 <title>Assessment of an Index for Measuring Pronunciation Difficulty</title>
	 <author><first>Katsunori</first><last>Kotani</last></author>
	 <author><first>Takehiko</first><last>Yoshimi</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>119–124</pages>
	 <abstract>This study assesses an index for measur-ing the pronunciation difficulty of sen-tences (henceforth, pronounceability) based on the normalized edit distance from a reference sentence to a transcrip-tion of learners’ pronunciation. Pro-nounceability should be examined when language teachers use a computer-assisted language learning system for pronunciation learning to maintain the motivation of learners. However, unlike the evaluation of learners’ pronunciation performance, previous research did not focus on pronounceability not only for English but also for Asian languages. This study found that the normalized edit distance was reliable but not valid. The lack of validity appeared to be because of an English test used for determining the proficiency of learners.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3717</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>kotani-yoshimi:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3718">
	 <title>A Short Answer Grading System in Chinese by Support Vector Approach</title>
	 <author><first>Shih-Hung</first><last>Wu</last></author>
	 <author><first>Wen-Feng</first><last>Shih</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>125–129</pages>
	 <abstract>In this paper, we report a short answer grading system in Chinese. We build a system based on standard machine learning approaches and test it with translated corpus from two publicly available corpus in English. The experiment results show similar results on two different corpus as in English.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3718</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>wu-shih:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3719">
	 <title>From Fidelity to Fluency: Natural Language Processing for Translator Training</title>
	 <author><first>Oi Yee</first><last>Kwong</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>130–134</pages>
	 <abstract>This study explores the use of natural language processing techniques to enhance bilingual lexical access beyond simple equivalents, to enable translators to navigate along a wider cross-lingual lexical space and more examples showing different translation strategies, which is essential for them to learn to produce not only faithful but also fluent translations.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3719</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>kwong:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3720">
	 <title>Countering Position Bias in Instructor Interventions in MOOC Discussion Forums</title>
	 <author><first>Muthu Kumar</first><last>Chandrasekaran</last></author>
	 <author><first>Min-Yen</first><last>Kan</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>135–142</pages>
	 <abstract>We systematically confirm that instructors are strongly influenced by the user interface presentation of Massive Online Open Course (MOOC) discussion forums. In a large scale dataset, we conclusively show that instructor interventions exhibit strong position bias, as measured by the position where the thread appeared on the user interface at the time of intervention. We measure and remove this bias, enabling unbiased statistical modelling and evaluation. We show that our de-biased classifier improves predicting interventions over the state-of-the-art on courses with sufficient number of interventions by 8.2% in F1 and 24.4% in recall on average.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3720</url>
	 <attachment type="poster">W18-3720.Poster.pdf</attachment>
	 <revision id="2">W18-3720v2</revision>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>chandrasekaran-kan:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3721">
	 <title>Measuring Beginner Friendliness of Japanese Web Pages explaining Academic Concepts by Integrating Neural Image Feature and Text Features</title>
	 <author><first>Hayato</first><last>Shiokawa</last></author>
	 <author><first>Kota</first><last>Kawaguchi</last></author>
	 <author><first>Bingcai</first><last>Han</last></author>
	 <author><first>Takehito</first><last>Utsuro</last></author>
	 <author><first>Yasuhide</first><last>Kawada</last></author>
	 <author><first>Masaharu</first><last>Yoshioka</last></author>
	 <author><first>Noriko</first><last>Kando</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>143–151</pages>
	 <abstract>Search engine is an important tool of modern academic study, but the results are lack of measurement of beginner friendliness. In order to improve the efficiency of using search engine for academic study, it is necessary to invent a technique of measuring the beginner friendliness of a Web page explaining academic concepts and to build an automatic measurement system. This paper studies how to integrate heterogeneous features such as a neural image feature generated from the image of the Web page by a variant of CNN (convolutional neural network) as well as text features extracted from the body text of the HTML file of the Web page. Integration is performed through the framework of the SVM classifier learning. Evaluation results show that heterogeneous features perform better than each individual type of features.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3721</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>shiokawa-EtAl:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3722">
	 <title>Learning to Automatically Generate Fill-In-The-Blank Quizzes</title>
	 <author><first>Edison</first><last>Marrese-Taylor</last></author>
	 <author><first>Ai</first><last>Nakajima</last></author>
	 <author><first>Yutaka</first><last>Matsuo</last></author>
	 <author><first>Ono</first><last>Yuichi</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>152–156</pages>
	 <abstract>In this paper we formalize the problem automatic fill-in-the-blank question generation using two standard NLP machine learning schemes, proposing concrete deep learning models for each. We present an empirical study based on data obtained from a language learning platform showing that both of our proposed settings offer promising results.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3722</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>marresetaylor-EtAl:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3723">
	 <title>Multilingual Short Text Responses Clustering for Mobile Educational Activities: a Preliminary Exploration</title>
	 <author><first>Yuen-Hsien</first><last>Tseng</last></author>
	 <author><first>Lung-Hao</first><last>Lee</last></author>
	 <author><first>Yu-Ta</first><last>Chien</last></author>
	 <author><first>Chun-Yen</first><last>Chang</last></author>
	 <author><first>Tsung-Yen</first><last>Li</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>157–164</pages>
	 <abstract>Text clustering is a powerful technique to detect topics from document corpora, so as to provide information browsing, analysis, and organization. On the other hand, the Instant Response System (IRS) has been widely used in recent years to enhance student engagement in class and thus improve their learning effectiveness. However, the lack of functions to process short text responses from the IRS prevents the further application of IRS in classes. Therefore, this study aims to propose a proper short text clustering module for the IRS, and demonstrate our implemented techniques through real-world examples, so as to provide experiences and insights for further study. In particular, we have compared three clustering methods and the result shows that theoretically better methods need not lead to better results, as there are various factors that may affect the final performance.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3723</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>tseng-EtAl:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3724">
	 <title>Chinese Grammatical Error Diagnosis Based on CRF and LSTM-CRF model</title>
	 <author><first>Yujie</first><last>Zhou</last></author>
	 <author><first>Yinan</first><last>Shao</last></author>
	 <author><first>Yong</first><last>Zhou</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>165–171</pages>
	 <abstract>When learning Chinese as a foreign language, the learners may have some grammatical errors due to negative migration of their native languages. However, few grammar checking applications have been developed to support the learners. The goal of this paper is to develop a tool to automatically diagnose four types of grammatical errors which are redundant words (R), missing words (M), bad word selection (S) and disordered words (W) in Chinese sentences written by those foreign learners. In this paper, a conventional linear CRF model with specific feature engineering and a LSTM-CRF model are used to solve the CGED (Chinese Grammatical Error Diagnosis) task. We make some improvement on both models and the submitted results have better performance on false positive rate and accuracy than the average of all runs from CGED2018 for all three evaluation levels.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3724</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>zhou-shao-zhou:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3725">
	 <title>Contextualized Character Representation for Chinese Grammatical Error Diagnosis</title>
	 <author><first>Jianbo</first><last>Zhao</last></author>
	 <author><first>Si</first><last>Li</last></author>
	 <author><first>Zhiqing</first><last>Lin</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>172–179</pages>
	 <abstract>Nowadays, more and more people are learning Chinese as their second language. Establishing an automatic diagnosis system for Chinese grammatical error has become an important challenge. In this paper, we propose a Chinese grammatical error diagnosis (CGED) model with contextualized character representation. Compared to the traditional model using LSTM (Long-Short Term Memory), our model have better performance and there is no need to add too many artificial features.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3725</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>zhao-li-lin:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3726">
	 <title>CMMC-BDRC Solution to the NLP-TEA-2018 Chinese Grammatical Error Diagnosis Task</title>
	 <author><first>Zhang</first><last>Yongwei</last></author>
	 <author><first>Hu</first><last>Qinan</last></author>
	 <author><first>Liu</first><last>Fang</last></author>
	 <author><first>Gu</first><last>Yueguo</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>180–187</pages>
	 <abstract>Chinese grammatical error diagnosis is an important natural language processing (NLP) task, which is also an important application using artificial intelligence technology in language education. This paper introduces a system developed by the Chinese Multilingual &amp; Multimodal Corpus and Big Data Research Center for the NLP-TEA shared task, named Chinese Grammar Error Diagnosis (CGED). This system regards diagnosing errors task as a sequence tagging problem, while takes correction task as a text classification problem. Finally, in the 12 teams, this system gets the highest F1 score in the detection task and the second highest F1 score in mean in the identification task, position task and the correction task.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3726</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>yongwei-EtAl:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3727">
	 <title>Detecting Simultaneously Chinese Grammar Errors Based on a BiLSTM-CRF Model</title>
	 <author><first>Yajun</first><last>Liu</last></author>
	 <author><first>Hongying</first><last>Zan</last></author>
	 <author><first>Mengjie</first><last>Zhong</last></author>
	 <author><first>Hongchao</first><last>Ma</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>188–193</pages>
	 <abstract>In the process of learning and using Chinese, many learners of Chinese as foreign language(CFL) may have grammar errors due to negative migration of their native languages. This paper introduces our system that can simultaneously diagnose four types of grammatical errors including redundant (R), missing (M), selection (S), disorder (W) in NLPTEA-5 shared task. We proposed a Bidirectional LSTM CRF neural network (BiLSTM-CRF) that combines BiLSTM and CRF without hand-craft features for Chinese Grammatical Error Diagnosis (CGED). Evaluation includes three levels, which are detection level, identification level and position level. At the detection level and identification level, our system got the third recall scores, and achieved good F1 values.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3727</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>liu-EtAl:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3728">
	 <title>A Hybrid Approach Combining Statistical Knowledge with Conditional Random Fields for Chinese Grammatical Error Detection</title>
	 <author><first>Yiyi</first><last>Wang</last></author>
	 <author><first>Chilin</first><last>Shih</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>194–198</pages>
	 <abstract>This paper presents a method of combining Conditional Random Fields (CRFs) model with a post-processing layer using Google n-grams statistical information tailored to detect word selection and word order errors made by learners of Chinese as Foreign Language (CFL). We describe the architecture of the model and its performance in the shared task of the ACL 2018 Workshop on Natural Language Processing Techniques for Educational Applications (NLPTEA). This hybrid approach yields comparably high false positive rate (FPR = 0.1274) and precision (Pd= 0.7519; Pi= 0.6311), but low recall (Rd = 0.3035; Ri = 0.1696 ) in grammatical error detection and identification tasks. Additional statistical information and linguistic rules can be added to enhance the model performance in the future.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3728</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>wang-shih:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3729">
	 <title>CYUT-III Team Chinese Grammatical Error Diagnosis System Report in NLPTEA-2018 CGED Shared Task</title>
	 <author><first>Shih-Hung</first><last>Wu</last></author>
	 <author><first>JUN-WEI</first><last>WANG</last></author>
	 <author><first>Liang-Pu</first><last>Chen</last></author>
	 <author><first>Ping-Che</first><last>Yang</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>199–202</pages>
	 <abstract>This paper reports how we build a Chinese Grammatical Error Diagnosis system in the NLPTEA-2018 CGED shared task. In 2018, we sent three runs with three different approaches. The first one is a pattern-based approach by frequent error pattern matching. The second one is a sequential labelling approach by conditional random fields (CRF). The third one is a rewriting approach by sequence to sequence (seq2seq) model. The three approaches have different properties that aim to optimize different performance metrics and the formal run results show the differences as we expected.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3729</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>wu-EtAl:2018:NLPTEA</bibkey>
    </paper>
    <paper id="3730">
	 <title>Detecting Grammatical Errors in the NTOU CGED System by Identifying Frequent Subsentences</title>
	 <author><first>Chuan-Jie</first><last>Lin</last></author>
	 <author><first>Shao-Heng</first><last>Chen</last></author>
	 <booktitle>Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications</booktitle>
	 <month>July</month>
	 <year>2018</year>
	 <address>Melbourne, Australia</address>
	 <publisher>Association for Computational Linguistics</publisher>
	 <pages>203–206</pages>
	 <abstract>The main goal of Chinese grammatical error diagnosis task is to detect word er-rors in the sentences written by Chinese-learning students. Our previous system would generate error-corrected sentences as candidates and their sentence likeli-hood were measured based on a large scale Chinese n-gram dataset. This year we further tried to identify long frequent-ly-seen subsentences and label them as correct in order to avoid propose too many error candidates. Two new methods for suggesting missing and selection er-rors were also tested.</abstract>
	 <url>http://www.aclweb.org/anthology/W18-3730</url>
	 <bibtype>inproceedings</bibtype>
	 <bibkey>lin-chen:2018:NLPTEA</bibkey>
    </paper>
   <paper id="3800">
     <title>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</title>
     <editor><first>Peter</first><last>Machonis</last></editor>
     <editor><first>Anabela</first><last>Barreiro</last></editor>
     <editor><first>Kristina</first><last>Kocijan</last></editor>
     <editor><first>Max</first><last>Silberztein</last></editor>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-38</url>
     <bibtype>book</bibtype>
     <bibkey>W18-38:2018</bibkey>
   </paper>

   <paper id="3801">
     <title>Corpus Phonetics: Past, Present, and Future</title>
     <author><first>Mark</first><last>Liberman</last></author>
     <booktitle>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1</pages>
     <url>http://www.aclweb.org/anthology/W18-3801</url>
     <abstract>Invited talk</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>liberman:2018:W18-38</bibkey>
   </paper>

   <paper id="3802">
     <title>Using Linguistic Resources to Evaluate the Quality of Annotated Corpora</title>
     <author><first>Max</first><last>Silberztein</last></author>
     <booktitle>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>2–11</pages>
     <url>http://www.aclweb.org/anthology/W18-3802</url>
     <abstract>Statistical and neural-network-based methods that compute their results by comparing a given text to be analyzed with a reference corpus assume that the reference corpus is complete and reliable enough. In this article, I conduct several experiments on an extract of the Open American National Corpus to verify this assumption.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>silberztein:2018:W18-38</bibkey>
   </paper>

   <paper id="3803">
     <title>Rule-based vs. Neural Net Approaches to Semantic Textual Similarity</title>
     <author><first>Linrui</first><last>Zhang</last></author>
     <author><first>Dan</first><last>Moldovan</last></author>
     <booktitle>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>12–17</pages>
     <url>http://www.aclweb.org/anthology/W18-3803</url>
     <abstract>This paper presents a neural net approach to determine Semantic Textual Similarity (STS) using attention-based bidirectional Long Short-Term Memory Networks (Bi-LSTM). To this date, most of the traditional STS systems were rule-based that built on top of excessive use of linguistic features and resources. In this paper, we present an end-to-end attention-based Bi-LSTM neural network system that solely takes word-level features, without expensive feature engineering work or the usage of external resources. By comparing its performance with traditional rule-based systems against SemEval-2012 benchmark, we make an assessment on the limitations and strengths of neural net systems to rule-based systems on Semantic Textual Similarity.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>zhang-moldovan:2018:W18-38</bibkey>
   </paper>

   <paper id="3804">
     <title>Linguistic Resources for Phrasal Verb Identification</title>
     <author><first>Peter</first><last>Machonis</last></author>
     <booktitle>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>18–27</pages>
     <url>http://www.aclweb.org/anthology/W18-3804</url>
     <abstract>This paper shows how a Lexicon-Grammar dictionary of English phrasal verbs (PV) can be transformed into an electronic dictionary, and with the help of multiple grammars, dictionaries, and filters within the linguistic development environment, NooJ, how to accurately identify PV in large corpora. The NooJ program is an alternative to statistical methods commonly used in NLP: all PV are listed in a dictionary and then located by means of a PV grammar in both continuous and discontinuous format. Results are then refined with a series of dictionaries, disambiguating grammars, and other linguistics recourses. The main advantage of such a program is that all PV can be identified in any corpus. The only drawback is that PV not listed in the dictionary (e.g., archaic forms, recent neologisms) are not identified; however, new PV can easily be added to the electronic dictionary, which is freely available to all.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>machonis:2018:W18-38</bibkey>
   </paper>

   <paper id="3805">
     <title>Designing a Croatian Aspectual Derivatives Dictionary: Preliminary Stages</title>
     <author><first>Kristina</first><last>Kocijan</last></author>
     <author><first>Krešimir</first><last>Šojat</last></author>
     <author><first>Dario</first><last>Poljak</last></author>
     <booktitle>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>28–37</pages>
     <url>http://www.aclweb.org/anthology/W18-3805</url>
     <abstract>The paper focusses on derivationally connected verbs in Croatian, i.e. on verbs that share the same lexical morpheme and are derived from other verbs via prefixation, suffixation and/or stem alternations. As in other Slavic languages with rich derivational morphology, each verb is marked for aspect, either perfective or imperfective. Some verbs, mostly of foreign origin, are marked as bi-aspectual verbs. The main objective of this paper is to detect and to describe major derivational processes and affixes used in the derivation of aspectually connected verbs with NooJ. Annotated chains are exported into a format adequate for web database system and further used to enhance the aspectual and derivational information for each verb.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>kocijan-ojat-poljak:2018:W18-38</bibkey>
   </paper>

   <paper id="3806">
     <title>A Rule-Based System for Disambiguating French Locative Verbs and Their Translation into Arabic</title>
     <author><first>Safa</first><last>Boudhina</last></author>
     <author><first>Héla</first><last>Fehri</last></author>
     <booktitle>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>38–46</pages>
     <url>http://www.aclweb.org/anthology/W18-3806</url>
     <abstract>This paper presents a rule-based system for disambiguating frensh locative verbs and their translation to Arabic language. The disambiguation phase is based on the use of the French Verb dictionary (LVF) of Dubois and Dubois Charlier as a linguistic resource, from which a base of disambiguation rules is extracted. The extracted rules thus take the form of transducers which will be subsequently applied to texts. The translation phase consists in translating the disambiguated locative verbs returned by the disambiguation phase. The translation takes into account the verb’s tense used as well as the inflected form of the verb. This phase is based on bilingual dictionaries that contain the different French locative verbs and their translation into the Arabic language. The experimentation and the evaluation are done in the linguistic platform NooJ. The obtained results are satisfactory.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>boudhina-fehri:2018:W18-38</bibkey>
   </paper>

   <paper id="3807">
     <title>A Pedagogical Application of NooJ in Language Teaching: The Adjective in Spanish and Italian</title>
     <author><first>Andrea</first><last>Rodrigo</last></author>
     <author><first>Mario</first><last>Monteleone</last></author>
     <author><first>Silvia</first><last>Reyes</last></author>
     <booktitle>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>47–56</pages>
     <url>http://www.aclweb.org/anthology/W18-3807</url>
     <abstract>In this paper, a pedagogical application of NooJ to the teaching and learning of Spanish as a foreign language is presented, which is directed to a specific addressee: learners whose mother tongue is Italian. The category ‘adjective’ has been chosen on account of its lower frequency of occurrence in texts written in Spanish, and particularly in the Argentine Rioplatense variety, and with the aim of developing strategies to increase its use. In addition, the features that the adjective shares with other grammatical categories render it extremely productive and provide elements that enrich the learners’ proficiency. The reference corpus contains the front pages of the Argentinian newspaper Clarín related to an emblematic historical moment, whose starting point is 24 March 1976, when a military coup began, and covers a thirty year period until 24 March 2006. It can be seen how the term desaparecido emerges with all its cultural and social charge, providing a context which allows an approach to Rioplatense Spanish from a more comprehensive perspective. Finally, a pedagogical proposal accounting for the application of the NooJ platform in language teaching is included.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>rodrigo-monteleone-reyes:2018:W18-38</bibkey>
   </paper>

   <paper id="3808">
     <title>STYLUS: A Resource for Systematically Derived Language Usage</title>
     <author><first>Bonnie</first><last>Dorr</last></author>
     <author><first>Clare</first><last>Voss</last></author>
     <booktitle>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>57–64</pages>
     <url>http://www.aclweb.org/anthology/W18-3808</url>
     <abstract>We describe a resource derived through extraction of a set of argument realizations from an existing lexical-conceptual structure (LCS) Verb Database of 500 verb classes (containing a total of 9525 verb entries) to include information about realization of arguments for a range of different verb classes. We demonstrate that our extended resource, called STYLUS (SysTematicallY Derived Language USe), enables systematic derivation of regular patterns of language usage without requiring manual annotation. We posit that both spatially oriented applications such as robot navigation and more general applications such as narrative generation require a layered representation scheme where a set of primitives (often grounded in space/motion such as GO) is coupled with a representation of constraints at the syntax-semantics interface. We demonstrate that the resulting resource covers three cases of lexico-semantic operations applicable to both language understanding and language generation.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>dorr-voss:2018:W18-38</bibkey>
   </paper>

   <paper id="3809">
     <title>Contemporary Amharic Corpus: Automatically Morpho-Syntactically Tagged Amharic Corpus</title>
     <author><first>Andargachew Mekonnen</first><last>Gezmu</last></author>
     <author><first>Binyam Ephrem</first><last>Seyoum</last></author>
     <author><first>Michael</first><last>Gasser</last></author>
     <author><first>Andreas</first><last>Nürnberger</last></author>
     <booktitle>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>65–70</pages>
     <url>http://www.aclweb.org/anthology/W18-3809</url>
     <abstract>We introduced the contemporary Amharic corpus, which is automatically tagged for morpho-syntactic information. Texts are collected from 25,199 documents from different domains and about 24 million orthographic words are tokenized. Since it is partly a web corpus, we made some automatic spelling error correction. We have also modified the existing morphological analyzer, HornMorpho, to use it for the automatic tagging.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>gezmu-EtAl:2018:W18-38</bibkey>
   </paper>

   <paper id="3810">
     <title>Gold Corpus for Telegraphic Summarization</title>
     <author><first>Chanakya</first><last>Malireddy</last></author>
     <author><first>Srivenkata N M</first><last>Somisetty</last></author>
     <author><first>Manish</first><last>Shrivastava</last></author>
     <booktitle>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>71–77</pages>
     <url>http://www.aclweb.org/anthology/W18-3810</url>
     <abstract>Most extractive summarization techniques operate by ranking all the source sentences and then select the top ranked sentences as the summary. Such methods are known to produce good summaries, especially when applied to news articles and scientific texts. However, they don’t fare so well when applied to texts such as fictional narratives, which don’t have a single central or recurrent theme. This is because usually the information or plot of the story is spread across several sentences. In this paper, we discuss a different summarization technique called Telegraphic Summarization. Here, we don’t select whole sentences, rather pick short segments of text spread across sentences, as the summary. We have tailored a set of guidelines to create such summaries and, using the same, annotate a gold corpus of 200 English short stories.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>malireddy-somisetty-shrivastava:2018:W18-38</bibkey>
   </paper>

   <paper id="3811">
     <title>Design of a Tigrinya Language Speech Corpus for Speech Recognition</title>
     <author><first>Hafte</first><last>Abera</last></author>
     <author><first>Sebsibe</first><last>H/Mariam</last></author>
     <booktitle>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>78–82</pages>
     <url>http://www.aclweb.org/anthology/W18-3811</url>
     <abstract>In this paper, we describe the first Tigrinya Languages speech corpora designed and development for speech recognition purposes. Tigrinya, often written as Tigrigna (ትግርኛ) /tɪˈɡrinjə/ belongs to the Semitic branch of the Afro-Asiatic languages where it shows the characteristic features of a Semitic language. It is spoken by ethnic Tigray-Tigrigna people in the Horn of Africa. The paper outlines different corpus designing process analysis of related work on speech corpora creation for different languages. The authors provide also procedures that were used for the creation of Tigrinya speech recognition corpus which is the under-resourced language. One hundred and thirty speakers, native to Tigrinya language, were recorded for training and test dataset set. Each speaker read 100 texts, which consisted of syllabically rich and balanced sentences. Ten thousand sets of sentences were used to prompt sheets. These sentences contained all of the contextual syllables and phones.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>abera-hmariam:2018:W18-38</bibkey>
   </paper>

   <paper id="3812">
     <title>Parallel Corpora for bi-Directional Statistical Machine Translation for Seven Ethiopian Language Pairs</title>
     <author><first>Solomon</first><last>Teferra Abate</last></author>
     <author><first>Michael</first><last>Melese</last></author>
     <author><first>Martha</first><last>Yifiru Tachbelie</last></author>
     <author><first>Million</first><last>Meshesha</last></author>
     <author><first>Solomon</first><last>Atinafu</last></author>
     <author><first>Wondwossen</first><last>Mulugeta</last></author>
     <author><first>Yaregal</first><last>Assabie</last></author>
     <author><first>Hafte</first><last>Abera</last></author>
     <author><first>Binyam</first><last>Ephrem</last></author>
     <author><first>Tewodros</first><last>Abebe</last></author>
     <author><first>Wondimagegnhue</first><last>Tsegaye</last></author>
     <author><first>Amanuel</first><last>Lemma</last></author>
     <author><first>Tsegaye</first><last>Andargie</last></author>
     <author><first>Seifedin</first><last>Shifaw</last></author>
     <booktitle>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>83–90</pages>
     <url>http://www.aclweb.org/anthology/W18-3812</url>
     <abstract>In this paper, we describe the development of parallel corpora for Ethiopian Languages: Amharic, Tigrigna, Afan-Oromo, Wolaytta and Geez. To check the usability of all the corpora we conducted baseline bi-directional statistical machine translation (SMT) experiments for seven language pairs. The performance of the bi-directional SMT systems shows that all the corpora can be used for further investigations. We have also shown that the morphological complexity of the Ethio-Semitic languages has a negative impact on the performance of the SMT especially when they are target languages. Based on the results we obtained, we are currently working towards handling the morphological complexities to improve the performance of statistical machine translation among the Ethiopian languages.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>teferraabate-EtAl:2018:W18-38</bibkey>
   </paper>

   <paper id="3813">
     <title>Using Embeddings to Compare FrameNet Frames Across Languages</title>
     <author><first>Jennifer</first><last>Sikos</last></author>
     <author><first>Sebastian</first><last>Padó</last></author>
     <booktitle>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>91–101</pages>
     <url>http://www.aclweb.org/anthology/W18-3813</url>
     <abstract>Much interest in Frame Semantics is fueled by the substantial extent of its applicability across languages. At the same time, lexicographic studies have found that the applicability of individual frames can be diminished by cross-lingual divergences regarding polysemy, syntactic valency, and lexicalization. Due to the large effort involved in manual investigations, there are so far no broad-coverage resources with “problematic” frames for any language pair. Our study investigates to what extent multilingual vector representations of frames learned from manually annotated corpora can address this need by serving as a wide coverage source for such divergences. We present a case study for the language pair English — German using the FrameNet and SALSA corpora and find that inferences can be made about cross-lingual frame applicability using a vector space model.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>sikos-pad:2018:W18-38</bibkey>
   </paper>

   <paper id="3814">
     <title>Construction of a Multilingual Corpus Annotated with Translation Relations</title>
     <author><first>Yuming</first><last>Zhai</last></author>
     <author><first>Aurélien</first><last>Max</last></author>
     <author><first>Anne</first><last>Vilnat</last></author>
     <booktitle>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>102–111</pages>
     <url>http://www.aclweb.org/anthology/W18-3814</url>
     <abstract>Translation relations, which distinguish literal translation from other translation techniques, constitute an important subject of study for human translators (Chuquet and Paillard, 1989). However, automatic processing techniques based on interlingual relations, such as machine translation or paraphrase generation exploiting translational equivalence, have not exploited these relations explicitly until now. In this work, we present a categorisation of translation relations and annotate them in a parallel multilingual (English, French, Chinese) corpus of oral presentations, the TED Talks. Our long term objective will be to automatically detect these relations in order to integrate them as important characteristics for the search of monolingual segments in relation of equivalence (paraphrases) or of entailment. The annotated corpus resulting from our work will be made available to the community.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>zhai-max-vilnat:2018:W18-38</bibkey>
   </paper>

   <paper id="3815">
     <title>Towards an Automatic Classification of Illustrative Examples in a Large Japanese-French Dictionary Obtained by OCR</title>
     <author><first>Christian</first><last>Boitet</last></author>
     <author><first>Mathieu</first><last>Mangeot</last></author>
     <author><first>Mutsuko</first><last>Tomokiyo</last></author>
     <booktitle>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>112–121</pages>
     <url>http://www.aclweb.org/anthology/W18-3815</url>
     <abstract>We work on improving the Cesselin, a large and open source Japanese-French bilingual dictionary digitalized by OCR, available on the web, and contributively improvable online. Labelling its examples (about 226000) would significantly enhance their usefulness for language learners. Examples are proverbs, idiomatic constructions, normal usage examples, and, for nouns, phrases containing a quantifier. Proverbs are easy to spot, but not examples of other types. To find a method for automatically or at least semi-automatically annotating them, we have studied many entries, and hypothesized that the degree of lexical similarity between results of MT into a third language might give good cues. To confirm that hypothesis, we sampled 500 examples and used Google Translate to translate into English their Japanese expressions and their French translations. The hypothesis holds well, in particular for distinguishing examples of normal usage from idiomatic examples. Finally, we propose a detailed annotation procedure and discuss its future automatization.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>boitet-mangeot-tomokiyo:2018:W18-38</bibkey>
   </paper>

   <paper id="3816">
     <title>Contractions: To Align or Not to Align, That Is the Question</title>
     <author><first>Anabela</first><last>Barreiro</last></author>
     <author><first>Fernando</first><last>Batista</last></author>
     <booktitle>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>122–130</pages>
     <url>http://www.aclweb.org/anthology/W18-3816</url>
     <abstract>This paper performs a detailed analysis on the alignment of Portuguese contractions, based on a previously aligned bilingual corpus. The alignment task was performed manually in a subset of the English-Portuguese CLUE4Translation Alignment Collection. The initial parallel corpus was pre-processed and, a decision was made as to whether the contraction should be maintained or decomposed in the alignment. Decomposition was required in the cases in which the two words that have been concatenated, i.e., the preposition and the determiner or pronoun, go in two separate translation alignment pairs (e.g., [no seio de] [a União Europeia] | [within] [the European Union]). Most contractions required decomposition in contexts where they are positioned at the end of a multiword unit. On the other hand, contractions tend to be maintained when they occur in the beginning or in the middle of the multiword unit, i.e., in the frozen part of the multiword (e.g., [no que diz respeito a] | [with regard to] or [além disso] [in addition]. A correct alignment of multiwords and phrasal units containing contractions is instrumental for machine translation, paraphrasing, and variety adaptation.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>barreiro-batista:2018:W18-38</bibkey>
   </paper>

   <paper id="3817">
     <title>Enabling Code-Mixed Translation: Parallel Corpus Creation and MT Augmentation Approach</title>
     <author><first>Mrinal</first><last>Dhar</last></author>
     <author><first>Vaibhav</first><last>Kumar</last></author>
     <author><first>Manish</first><last>Shrivastava</last></author>
     <booktitle>Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>131–140</pages>
     <url>http://www.aclweb.org/anthology/W18-3817</url>
     <abstract>Code-mixing, use of two or more languages in a single sentence, is ubiquitous; generated by multi-lingual speakers across the world. The phenomenon presents itself prominently in social media discourse. Consequently, there is a growing need for translating code-mixed hybrid lan- guage into standard languages. However, due to the lack of gold parallel data, existing machine translation systems fail to properly translate code-mixed text. In an effort to initiate the task of machine translation of code-mixed content, we present a newly created parallel corpus of code-mixed English-Hindi and English. We selected previously avail- able English-Hindi code-mixed data as a starting point for the creation of our parallel corpus. We then chose 4 human translators, fluent in both English and Hindi, for translating the 6088 code-mixed English-Hindi sentences to English. With the help of the created parallel corpus, we analyzed the structure of English-Hindi code- mixed data and present a technique to augment run-of-the-mill machine translation (MT) ap- proaches that can help achieve superior translations without the need for specially designed trans- lation systems. We present an augmentation pipeline for existing MT approaches, like Phrase Based MT (Moses) and Neural MT, to improve the translation of code-mixed text. The augmen- tation pipeline is presented as a pre-processing step and can be plugged with any existing MT system, which we demonstrate by improving translations done by systems like Moses, Google Neural Machine Translation System (NMTS) and Bing Translator for English-Hindi code-mixed content.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>dhar-kumar-shrivastava:2018:W18-38</bibkey>
   </paper>

   <paper id="3900">
     <title>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</title>
     <editor><first>Marcos</first><last>Zampieri</last></editor>
     <editor><first>Preslav</first><last>Nakov</last></editor>
     <editor><first>Nikola</first><last>Ljubešić</last></editor>
     <editor><first>Jörg</first><last>Tiedemann</last></editor>
     <editor><first>Shervin</first><last>Malmasi</last></editor>
     <editor><first>Ahmed</first><last>Ali</last></editor>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-39</url>
     <bibtype>book</bibtype>
     <bibkey>W18-39:2018</bibkey>
   </paper>

   <paper id="3901">
     <title>Language Identification and Morphosyntactic Tagging: The Second VarDial Evaluation Campaign</title>
     <author><first>Marcos</first><last>Zampieri</last></author>
     <author><first>Shervin</first><last>Malmasi</last></author>
     <author><first>Preslav</first><last>Nakov</last></author>
     <author><first>Ahmed</first><last>Ali</last></author>
     <author><first>Suwon</first><last>Shon</last></author>
     <author><first>James</first><last>Glass</last></author>
     <author><first>Yves</first><last>Scherrer</last></author>
     <author><first>Tanja</first><last>Samardžić</last></author>
     <author><first>Nikola</first><last>Ljubešić</last></author>
     <author><first>Jörg</first><last>Tiedemann</last></author>
     <author><first>Chris</first><last>van der Lee</last></author>
     <author><first>Stefan</first><last>Grondelaers</last></author>
     <author><first>Nelleke</first><last>Oostdijk</last></author>
     <author><first>Dirk</first><last>Speelman</last></author>
     <author><first>Antal</first><last>van den Bosch</last></author>
     <author><first>Ritesh</first><last>Kumar</last></author>
     <author><first>Bornini</first><last>Lahiri</last></author>
     <author><first>Mayank</first><last>Jain</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–17</pages>
     <url>http://www.aclweb.org/anthology/W18-3901</url>
     <abstract>We present the results and the findings of the Second VarDial Evaluation Campaign on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects. The campaign was organized as part of the fifth edition of the VarDial workshop, collocated with COLING’2018. This year, the campaign included five shared tasks, including two task re-runs – Arabic Dialect Identification (ADI) and German Dialect Identification (GDI) –, and three new tasks – Morphosyntactic Tagging of Tweets (MTT), Discriminating between Dutch and Flemish in Subtitles (DFS), and Indo-Aryan Language Identification (ILI). A total of 24 teams submitted runs across the five shared tasks, and contributed 22 system description papers, which were included in the VarDial workshop proceedings and are referred to in this report.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>zampieri-EtAl:2018:W18-39</bibkey>
   </paper>

   <paper id="3902">
     <title>Encoder-Decoder Methods for Text Normalization</title>
     <author><first>Massimo</first><last>Lusetti</last></author>
     <author><first>Tatyana</first><last>Ruzsics</last></author>
     <author><first>Anne</first><last>Göhring</last></author>
     <author><first>Tanja</first><last>Samardžić</last></author>
     <author><first>Elisabeth</first><last>Stark</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>18–28</pages>
     <url>http://www.aclweb.org/anthology/W18-3902</url>
     <abstract>Text normalization is the task of mapping non-canonical language, typical of speech transcription and computer-mediated communication, to a standardized writing. It is an up-stream task necessary to enable the subsequent direct employment of standard natural language processing tools and indispensable for languages such as Swiss German, with strong regional variation and no written standard. Text normalization has been addressed with a variety of methods, most successfully with character-level statistical machine translation (CSMT). In the meantime, machine translation has changed and the new methods, known as neural encoder-decoder (ED) models, resulted in remarkable improvements. Text normalization, however, has not yet followed. A number of neural methods have been tried, but CSMT remains the state-of-the-art. In this work, we normalize Swiss German WhatsApp messages using the ED framework. We exploit the flexibility of this framework, which allows us to learn from the same training data in different ways. In particular, we modify the decoding stage of a plain ED model to include target-side language models operating at different levels of granularity: characters and words. Our systematic comparison shows that our approach results in an improvement over the CSMT state-of-the-art.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>lusetti-EtAl:2018:W18-39</bibkey>
   </paper>

   <paper id="3903">
     <title>A High Coverage Method for Automatic False Friends Detection for Spanish and Portuguese</title>
     <author><first>Santiago</first><last>Castro</last></author>
     <author><first>Jairo</first><last>Bonanata</last></author>
     <author><first>Aiala</first><last>Rosá</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>29–36</pages>
     <url>http://www.aclweb.org/anthology/W18-3903</url>
     <attachment type="presentation">W18-3903.Presentation.pdf</attachment>
     <abstract>False friends are words in two languages that look or sound similar, but have different meanings. They are a common source of confusion among language learners. Methods to detect them automatically do exist, however they make use of large aligned bilingual corpora, which are hard to find and expensive to build, or encounter problems dealing with infrequent words. In this work we propose a high coverage method that uses word vector representations to build a false friends classifier for any pair of languages, which we apply to the particular case of Spanish and Portuguese. The required resources are a large corpus for each language and a small bilingual lexicon for the pair.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>castro-bonanata-ros:2018:W18-39</bibkey>
   </paper>

   <paper id="3904">
     <title>Sub-label dependencies for Neural Morphological Tagging – The Joint Submission of University of Colorado and University of Helsinki for VarDial 2018</title>
     <author><first>Miikka</first><last>Silfverberg</last></author>
     <author><first>Senka</first><last>Drobac</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>37–45</pages>
     <url>http://www.aclweb.org/anthology/W18-3904</url>
     <abstract>This paper presents the submission of the UH&amp;CU team (Joint University of Colorado and University of Helsinki team) for the VarDial 2018 shared task on morphosyntactic tagging of Croatian, Slovenian and Serbian tweets. Our system is a bidirectional LSTM tagger which emits tags as character sequences using an LSTM generator in order to be able to handle unknown tags and combinations of several tags for one token which occur in the shared task data sets. To the best of our knowledge, using an LSTM generator is a novel approach. The system delivers sizable improvements of more than 6%-points over a baseline trigram tagger. Overall, the performance of our system is quite even for all three languages.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>silfverberg-drobac:2018:W18-39</bibkey>
   </paper>

   <paper id="3905">
     <title>Part of Speech Tagging in Luyia: A Bantu Macrolanguage</title>
     <author><first>Kenneth</first><last>Steimel</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>46–54</pages>
     <url>http://www.aclweb.org/anthology/W18-3905</url>
     <abstract>Luyia is a macrolanguage in central Kenya. The Luyia languages, like other Bantu languages, have a complex morphological system. This system can be leveraged to aid in part of speech tagging. Bag-of-characters taggers trained on a source Luyia language can be applied directly to another Luyia language with some degree of success. In addition, mixing data from the target language with data from the source language does produce more accurate predictive models compared to models trained on just the target language data when the training set size is small. However, for both of these tagging tasks, models involving the more distantly related language, Tiriki, are better at predicting part of speech tags for Wanga data. The models incorporating Bukusu data are not as successful despite the closer relationship between Bukusu and Wanga. Overlapping vocabulary between the Wanga and Tiriki corpora as well as a bias towards open class words help Tiriki outperform Bukusu.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>steimel:2018:W18-39</bibkey>
   </paper>

   <paper id="3906">
     <title>Tübingen-Oslo Team at the VarDial 2018 Evaluation Campaign: An Analysis of N-gram Features in Language Variety Identification</title>
     <author><first>Çağrı</first><last>Çöltekin</last></author>
     <author><first>Taraka</first><last>Rama</last></author>
     <author><first>Verena</first><last>Blaschke</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>55–65</pages>
     <url>http://www.aclweb.org/anthology/W18-3906</url>
     <abstract>This paper describes our systems for the VarDial 2018 evaluation campaign. We participated in all language identification tasks, namely, Arabic dialect identification (ADI), German dialect identification (GDI), discriminating between Dutch and Flemish in Subtitles (DFS), and Indo-Aryan Language Identification (ILI). In all of the tasks, we only used textual transcripts (not using audio features for ADI). We submitted system runs based on support vector machine classifiers (SVMs) with bag of character and word n-grams as features, and gated bidirectional recurrent neural networks (RNNs) using units of characters and words. Our SVM models outperformed our RNN models in all tasks, obtaining the first place on the DFS task, third place on the ADI task, and second place on others according to the official rankings. As well as describing the models we used in the shared task participation, we present an analysis of the n-gram features used by the SVM models in each task, and also report additional results (that were run after the official competition deadline) on the GDI surprise dialect track.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>ltekin-rama-blaschke:2018:W18-39</bibkey>
   </paper>

   <paper id="3907">
     <title>Iterative Language Model Adaptation for Indo-Aryan Language Identification</title>
     <author><first>Tommi</first><last>Jauhiainen</last></author>
     <author><first>Heidi</first><last>Jauhiainen</last></author>
     <author><first>Krister</first><last>Lindén</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>66–75</pages>
     <url>http://www.aclweb.org/anthology/W18-3907</url>
     <abstract>This paper presents the experiments and results obtained by the SUKI team in the Indo-Aryan Language Identification shared task of the VarDial 2018 Evaluation Campaign. The shared task was an open one, but we did not use any corpora other than what was distributed by the organizers. A total of eight teams provided results for this shared task. Our submission using a HeLI-method based language identifier with iterative language model adaptation obtained the best results in the shared task with a macro F1-score of 0.958.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>jauhiainen-jauhiainen-lindn:2018:W18-391</bibkey>
   </paper>

   <paper id="3908">
     <title>Language and the Shifting Sands of Domain, Space and Time (Invited Talk)</title>
     <author><first>Timothy</first><last>Baldwin</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>76</pages>
     <url>http://www.aclweb.org/anthology/W18-3908</url>
     <abstract>In this talk, I will first present recent work on domain debiasing in the context of language identification, then discuss a new line of work on language variety analysis in the form of dialect map generation. Finally, I will reflect on the interplay between time and space on language variation, and speculate on how these can be captured in a single model.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>baldwin:2018:W18-39</bibkey>
   </paper>

   <paper id="3909">
     <title>UnibucKernel Reloaded: First Place in Arabic Dialect Identification for the Second Year in a Row</title>
     <author><first>Andrei</first><last>Butnaru</last></author>
     <author><first>Radu Tudor</first><last>Ionescu</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>77–87</pages>
     <url>http://www.aclweb.org/anthology/W18-3909</url>
     <abstract>We present a machine learning approach that ranked on the first place in the Arabic Dialect Identification (ADI) Closed Shared Tasks of the 2018 VarDial Evaluation Campaign. The proposed approach combines several kernels using multiple kernel learning. While most of our kernels are based on character p-grams (also known as n-grams) extracted from speech or phonetic transcripts, we also use a kernel based on dialectal embeddings generated from audio recordings by the organizers. In the learning stage, we independently employ Kernel Discriminant Analysis (KDA) and Kernel Ridge Regression (KRR). Preliminary experiments indicate that KRR provides better classification results. Our approach is shallow and simple, but the empirical results obtained in the 2018 ADI Closed Shared Task prove that it achieves the best performance. Furthermore, our top macro-F1 score (58.92%) is significantly better than the second best score (57.59%) in the 2018 ADI Shared Task, according to the statistical significance test performed by the organizers. Nevertheless, we obtain even better post-competition results (a macro-F1 score of 62.28%) using the audio embeddings released by the organizers after the competition. With a very similar approach (that did not include phonetic features), we also ranked first in the ADI Closed Shared Tasks of the 2017 VarDial Evaluation Campaign, surpassing the second best method by 4.62%. We therefore conclude that our multiple kernel learning method is the best approach to date for Arabic dialect identification.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>butnaru-ionescu:2018:W18-39</bibkey>
   </paper>

   <paper id="3910">
     <title>Varying image description tasks: spoken versus written descriptions</title>
     <author><first>Emiel</first><last>van Miltenburg</last></author>
     <author><first>Ruud</first><last>Koolen</last></author>
     <author><first>Emiel</first><last>Krahmer</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>88–100</pages>
     <url>http://www.aclweb.org/anthology/W18-3910</url>
     <abstract>Automatic image description systems are commonly trained and evaluated on written image descriptions. At the same time, these systems are often used to provide spoken descriptions (e.g. for visually impaired users) through apps like TapTapSee or Seeing AI. This is not a problem, as long as spoken and written descriptions are very similar. However, linguistic research suggests that spoken language often differs from written language. These differences are not regular, and vary from context to context. Therefore, this paper investigates whether there are differences between written and spoken image descriptions, even if they are elicited through similar tasks. We compare descriptions produced in two languages (English and Dutch), and in both languages observe substantial differences between spoken and written descriptions. Future research should see if users prefer the spoken over the written style and, if so, aim to emulate spoken descriptions.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>vanmiltenburg-koolen-krahmer:2018:W18-39</bibkey>
   </paper>

   <paper id="3911">
     <title>Transfer Learning for British Sign Language Modelling</title>
     <author><first>Boris</first><last>Mocialov</last></author>
     <author><first>Helen</first><last>Hastie</last></author>
     <author><first>Graham</first><last>Turner</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>101–110</pages>
     <url>http://www.aclweb.org/anthology/W18-3911</url>
     <abstract>Automatic speech recognition and spoken dialogue systems have made great advances through the use of deep machine learning methods. This is partly due to greater computing power but also through the large amount of data available in common languages, such as English. Conversely, research in minority languages, including sign languages, is hampered by the severe lack of data. This has led to work on transfer learning methods, whereby a model developed for one language is reused as the starting point for a model on a second language, which is less resourced. In this paper, we examine two transfer learning techniques of fine-tuning and layer substitution for language modelling of British Sign Language. Our results show improvement in perplexity when using transfer learning with standard stacked LSTM models, trained initially using a large corpus for standard English from the Penn Treebank corpus.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>mocialov-hastie-turner:2018:W18-39</bibkey>
   </paper>

   <paper id="3912">
     <title>Paraphrastic Variance between European and Brazilian Portuguese</title>
     <author><first>Anabela</first><last>Barreiro</last></author>
     <author><first>Cristina</first><last>Mota</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>111–121</pages>
     <url>http://www.aclweb.org/anthology/W18-3912</url>
     <abstract>This paper presents a methodology to extract a paraphrase database for the European and Brazilian varieties of Portuguese, and discusses a set of paraphrastic categories of multiwords and phrasal units, such as the compounds “toda a gente” versus “todo o mundo” ‘everybody’ or the gerundive constructions [estar a + V-Inf] versus [ficar + V-Ger] (e.g., “estive a observar” | “fiquei observando” ‘I was observing’), which are extremely relevant to high quality paraphrasing. The variants were manually aligned in the e-PACT corpus, using the CLUE-Aligner tool. The methodology, inspired in the Logos Model, focuses on a semantico-syntactic analysis of each paraphrastic unit and constitutes a subset of the Gold-CLUE-Paraphrases. The construction of a larger dataset of paraphrastic contrasts among the distinct varieties of the Portuguese language is indispensable for variety adaptation, i.e., for dealing with the cultural, linguistic and stylistic differences between them, making it possible to convert texts (semi-)automatically from one variety into another, a key function in paraphrasing systems. This topic represents an interesting new line of research with valuable applications in language learning, language generation, question-answering, summarization, and machine translation, among others. The paraphrastic units are the first resource of its kind for Portuguese to become available to the scientific community for research purposes.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>barreiro-mota:2018:W18-39</bibkey>
   </paper>

   <paper id="3913">
     <title>Character Level Convolutional Neural Network for Arabic Dialect Identification</title>
     <author><first>Mohamed</first><last>Ali</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>122–127</pages>
     <url>http://www.aclweb.org/anthology/W18-3913</url>
     <abstract>This submission is for the description paper for our system in the ADI shared task.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>ali:2018:W18-391</bibkey>
   </paper>

   <paper id="3914">
     <title>Neural Network Architectures for Arabic Dialect Identification</title>
     <author><first>Elise</first><last>Michon</last></author>
     <author><first>Minh Quang</first><last>Pham</last></author>
     <author><first>Josep</first><last>Crego</last></author>
     <author><first>Jean</first><last>Senellart</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>128–136</pages>
     <url>http://www.aclweb.org/anthology/W18-3914</url>
     <abstract>SYSTRAN competes this year for the first time to the DSL shared task, in the Arabic Dialect Identification subtask. We participate by training several Neural Network models showing that we can obtain competitive results despite the limited amount of training data available for learning. We report our experiments and detail the network architecture and parameters of our 3 runs: our best performing system consists in a Multi-Input CNN that learns separate embeddings for lexical, phonetic and acoustic input features (F1: 0.5289); we also built a CNN-biLSTM network aimed at capturing both spatial and sequential features directly from speech spectrograms (F1: 0.3894 at submission time, F1: 0.4235 with later found parameters); and finally a system relying on binary CNN-biLSTMs (F1: 0.4339).</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>michon-EtAl:2018:W18-39</bibkey>
   </paper>

   <paper id="3915">
     <title>HeLI-based Experiments in Discriminating Between Dutch and Flemish Subtitles</title>
     <author><first>Tommi</first><last>Jauhiainen</last></author>
     <author><first>Heidi</first><last>Jauhiainen</last></author>
     <author><first>Krister</first><last>Lindén</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>137–144</pages>
     <url>http://www.aclweb.org/anthology/W18-3915</url>
     <abstract>This paper presents the experiments and results obtained by the SUKI team in the Discriminating between Dutch and Flemish in Subtitles shared task of the VarDial 2018 Evaluation Campaign. Our best submission was ranked 8th, obtaining macro F1-score of 0.61. Our best results were produced by a language identifier implementing the HeLI method without any modifications. We describe, in addition to the best method we used, some of the experiments we did with unsupervised clustering.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>jauhiainen-jauhiainen-lindn:2018:W18-392</bibkey>
   </paper>

   <paper id="3916">
     <title>Measuring language distance among historical varieties using perplexity. Application to European Portuguese.</title>
     <author><first>José Ramom</first><last>Pichel Campos</last></author>
     <author><first>Pablo</first><last>Gamallo</last></author>
     <author><first>Iñaki</first><last>Alegria</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>145–155</pages>
     <url>http://www.aclweb.org/anthology/W18-3916</url>
     <abstract>The objective of this work is to quantify, with a simple and robust measure, the distance between historical varieties of a language. The measure will be inferred from text corpora corresponding to historical periods. Different approaches have been proposed for similar aims: Language Identification, Phylogenetics, Historical Linguistics or Dialectology. In our approach, we used a perplexity-based measure to calculate language distance between all the historical periods of a specific language: European Portuguese. Perplexity has also proven to be a robust metric to calculate distance between languages. However, this measure has not been tested yet to identify diachronic periods within the historical evolution of a specific language. For this purpose, a historical Portuguese corpus has been constructed from different open sources containing texts with close original spelling. The results of our experiments show that Portuguese keeps an important degree of homogeneity over time. We anticipate this metric to be a starting point to be applied to other languages.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>pichelcampos-gamallo-alegria:2018:W18-39</bibkey>
   </paper>

   <paper id="3917">
     <title>Comparing CRF and LSTM performance on the task of morphosyntactic tagging of non-standard varieties of South Slavic languages</title>
     <author><first>Nikola</first><last>Ljubešić</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>156–163</pages>
     <url>http://www.aclweb.org/anthology/W18-3917</url>
     <abstract>This paper presents two systems taking part in the Morphosyntactic Tagging of Tweets shared task on Slovene, Croatian and Serbian data, organized inside the VarDial Evaluation Campaign. While one system relies on the traditional method for sequence labeling (conditional random fields), the other relies on its neural alternative (bidirectional long short-term memory). We investigate the similarities and differences of these two approaches, showing that both methods yield very good and quite similar results, with the neural model outperforming the traditional one more as the level of non-standardness of the text increases. Through an error analysis we show that the neural system is better at long-range dependencies, while the traditional system excels and slightly outperforms the neural system at the local ones. We present in the paper new state-of-the-art results in morphosyntactic annotation of non-standard text for Slovene, Croatian and Serbian.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>ljubei:2018:W18-39</bibkey>
   </paper>

   <paper id="3918">
     <title>Computationally efficient discrimination between language varieties with large feature vectors and regularized classifiers</title>
     <author><first>Adrien</first><last>Barbaresi</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>164–171</pages>
     <url>http://www.aclweb.org/anthology/W18-3918</url>
     <abstract>The present contribution revolves around efficient approaches to language classification which have been field-tested in the Vardial evaluation campaign. The methods used in several language identification tasks comprising different language types are presented and their results are discussed, giving insights on real-world application of regularization, linear classifiers and corresponding linguistic features. The use of a specially adapted Ridge classifier proved useful in 2 tasks out of 3. The overall approach (XAC) has slightly outperformed most of the other systems on the DFS task (Dutch and Flemish) and on the ILI task (Indo-Aryan languages), while its comparative performance was poorer in on the GDI task (Swiss German dialects).</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>barbaresi:2018:W18-39</bibkey>
   </paper>

   <paper id="3919">
     <title>Character Level Convolutional Neural Network for German Dialect Identification</title>
     <author><first>Mohamed</first><last>Ali</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>172–177</pages>
     <url>http://www.aclweb.org/anthology/W18-3919</url>
     <abstract>This submission is a description paper for our system in GDI shared task</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>ali:2018:W18-392</bibkey>
   </paper>

   <paper id="3920">
     <title>Discriminating between Indo-Aryan Languages Using SVM Ensembles</title>
     <author><first>Alina Maria</first><last>Ciobanu</last></author>
     <author><first>Marcos</first><last>Zampieri</last></author>
     <author><first>Shervin</first><last>Malmasi</last></author>
     <author><first>Santanu</first><last>Pal</last></author>
     <author><first>Liviu P.</first><last>Dinu</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>178–184</pages>
     <url>http://www.aclweb.org/anthology/W18-3920</url>
     <abstract>In this paper we present a system based on SVM ensembles trained on characters and words to discriminate between five similar languages of the Indo-Aryan family: Hindi, Braj Bhasha, Awadhi, Bhojpuri, and Magahi. The system competed in the Indo-Aryan Language Identification (ILI) shared task organized within the VarDial Evaluation Campaign 2018. Our best entry in the competition, named ILIdentification, scored 88.95% F1 score and it was ranked 3rd out of 8 teams.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>ciobanu-EtAl:2018:W18-39</bibkey>
   </paper>

   <paper id="3921">
     <title>IIT (BHU) System for Indo-Aryan Language Identification (ILI) at VarDial 2018</title>
     <author><first>Divyanshu</first><last>Gupta</last></author>
     <author><first>Gourav</first><last>Dhakad</last></author>
     <author><first>Jayprakash</first><last>Gupta</last></author>
     <author><first>Anil Kumar</first><last>Singh</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>185–190</pages>
     <url>http://www.aclweb.org/anthology/W18-3921</url>
     <abstract>Text language Identification is a Natural Language Processing task of identifying and recognizing a given language out of many different languages from a piece of text. This paper describes our submission to the ILI 2018 shared-task, which includes the identification of 5 closely related Indo-Aryan languages. We developed a word-level LSTM(Long Short-term Memory) model, a specific type of Recurrent Neural Network model, for this task. Given a sentence, our model embeds each word of the sentence and convert into its trainable word embedding, feeds them into our LSTM network and finally predict the language. We obtained an F1 macro score of 0.836, ranking 5th in the task.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>gupta-EtAl:2018:W18-39</bibkey>
   </paper>

   <paper id="3922">
     <title>Exploring Classifier Combinations for Language Variety Identification</title>
     <author><first>Tim</first><last>Kreutz</last></author>
     <author><first>Walter</first><last>Daelemans</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>191–198</pages>
     <url>http://www.aclweb.org/anthology/W18-3922</url>
     <abstract>This paper describes CLiPS’s submissions for the Discriminating between Dutch and Flemish in Subtitles (DFS) shared task at VarDial 2018. We explore different ways to combine classifiers trained on different feature groups. Our best system uses two Linear SVM classifiers; one trained on lexical features (word n-grams) and one trained on syntactic features (PoS n-grams). The final prediction for a document to be in Flemish Dutch or Netherlandic Dutch is made by the classifier that outputs the highest probability for one of the two labels. This confidence vote approach outperforms a meta-classifier on the development data and on the test data.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>kreutz-daelemans:2018:W18-39</bibkey>
   </paper>

   <paper id="3923">
     <title>Identification of Differences between Dutch Language Varieties with the VarDial2018 Dutch-Flemish Subtitle Data</title>
     <author><first>Hans</first><last>van Halteren</last></author>
     <author><first>Nelleke</first><last>Oostdijk</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>199–209</pages>
     <url>http://www.aclweb.org/anthology/W18-3923</url>
     <abstract>With the goal of discovering differences between Belgian and Netherlandic Dutch, we participated as Team Taurus in the Dutch-Flemish Subtitles task of VarDial2018. We used a rather simple marker-based method, but a wide range of features, including lexical, lexico-syntactic and syntactic ones, and achieved a second position in the ranking. Inspection of highly distin-guishing features did point towards differences between the two language varieties, but because of the nature of the experimental data, we have to treat our observations as very tentative and in need of further investigation.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>vanhalteren-oostdijk:2018:W18-39</bibkey>
   </paper>

   <paper id="3924">
     <title>Birzeit Arabic Dialect Identification System for the 2018 VarDial Challenge</title>
     <author><first>Rabee</first><last>Naser</last></author>
     <author><first>Abualsoud</first><last>Hanani</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>210–217</pages>
     <url>http://www.aclweb.org/anthology/W18-3924</url>
     <abstract>This paper describes our Automatic Dialect Recognition (ADI) system for the VarDial 2018 challenge, with the goal of distinguishing four major Arabic dialects, as well as Modern Standard Arabic (MSA). The training and development ADI VarDial 2018 data consists of 16,157 utterances, their words transcription, their phonetic transcriptions obtained with four non-Arabic phoneme recognizers and acoustic embedding data. Our overall system is a combination of four different systems. One system uses the words transcriptions and tries to recognize the speaker dialect by modeling the sequence of words for each dialect. Another system tries to recognize the dialect by modeling the phones sequence produced by non-Arabic phone recognizers, whereas, the other two systems use GMM trained on the acoustic features for recognizing the dialect. The best performance was achieved by the fused system which combines four systems together, with F1 micro of 68.77%.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>naser-hanani:2018:W18-39</bibkey>
   </paper>

   <paper id="3925">
     <title>Twist Bytes - German Dialect Identification with Data Mining Optimization</title>
     <author><first>Fernando</first><last>Benites</last></author>
     <author><first>Ralf</first><last>Grubenmann</last></author>
     <author><first>Pius</first><last>von Däniken</last></author>
     <author><first>Dirk</first><last>von Grünigen</last></author>
     <author><first>Jan</first><last>Deriu</last></author>
     <author><first>Mark</first><last>Cieliebak</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>218–227</pages>
     <url>http://www.aclweb.org/anthology/W18-3925</url>
     <abstract>We describe our approaches used in the German Dialect Identification (GDI) task at the VarDial Evaluation Campaign 2018. The goal was to identify to which out of four dialects spoken in German speaking part of Switzerland a sentence belonged to. We adopted two different meta classifier approaches and used some data mining insights to improve the preprocessing and the meta classifier parameters. Especially, we focused on using different feature extraction methods and how to combine them, since they influenced very differently the performance of the system. Our system achieved second place out of 8 teams, with a macro averaged F-1 of 64.6%.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>benites-EtAl:2018:W18-39</bibkey>
   </paper>

   <paper id="3926">
     <title>STEVENDU2018’s system in VarDial 2018: Discriminating between Dutch and Flemish in Subtitles</title>
     <author><first>Steven</first><last>Du</last></author>
     <author><first>Yuan Yuan</first><last>Wang</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>228–234</pages>
     <url>http://www.aclweb.org/anthology/W18-3926</url>
     <abstract>This paper introduces the submitted system for team STEVENDU2018 during VarDial 2018 Discriminating between Dutch and Flemish in Subtitles(DFS). Post evaluation analyses are also presented, the results obtained indicate that it is a challenging task to discriminate Dutch and Flemish.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>du-wang:2018:W18-39</bibkey>
   </paper>

   <paper id="3927">
     <title>Using Neural Transfer Learning for Morpho-syntactic Tagging of South-Slavic Languages Tweets</title>
     <author><first>Sara</first><last>Meftah</last></author>
     <author><first>Nasredine</first><last>Semmar</last></author>
     <author><first>Fatiha</first><last>Sadat</last></author>
     <author><first>Stephan</first><last>Raaijmakers</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>235–243</pages>
     <url>http://www.aclweb.org/anthology/W18-3927</url>
     <abstract>In this paper, we describe a morpho-syntactic tagger of tweets, an important component of the CEA List DeepLIMA tool which is a multilingual text analysis platform based on deep learning. This tagger is built for the Morpho-syntactic Tagging of Tweets (MTT) Shared task of the 2018 VarDial Evaluation Campaign. The MTT task focuses on morpho-syntactic annotation of non-canonical Twitter varieties of three South-Slavic languages: Slovene, Croatian and Serbian. We propose to use a neural network model trained in an end-to-end manner for the three languages without any need for task or domain specific features engineering. The proposed approach combines both character and word level representations. Considering the lack of annotated data in the social media domain for South-Slavic languages, we have also implemented a cross-domain Transfer Learning (TL) approach to exploit any available related out-of-domain annotated data.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>meftah-EtAl:2018:W18-39</bibkey>
   </paper>

   <paper id="3928">
     <title>When Simple n-gram Models Outperform Syntactic Approaches: Discriminating between Dutch and Flemish</title>
     <author><first>Martin</first><last>Kroon</last></author>
     <author><first>Masha</first><last>Medvedeva</last></author>
     <author><first>Barbara</first><last>Plank</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>244–253</pages>
     <url>http://www.aclweb.org/anthology/W18-3928</url>
     <abstract>In this paper we present the results of our participation in the Discriminating between Dutch and Flemish in Subtitles VarDial 2018 shared task. We try techniques proven to work well for discriminating between language varieties as well as explore the potential of using syntactic features, i.e. hierarchical syntactic subtrees. We experiment with different combinations of features. Discriminating between these two languages turned out to be a very hard task, not only for a machine: human performance is only around 0.51 F1 score; our best system is still a simple Naive Bayes model with word unigrams and bigrams. The system achieved an F1 score (macro) of 0.62, which ranked us 4th in the shared task.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>kroon-medvedeva-plank:2018:W18-39</bibkey>
   </paper>

   <paper id="3929">
     <title>HeLI-based Experiments in Swiss German Dialect Identification</title>
     <author><first>Tommi</first><last>Jauhiainen</last></author>
     <author><first>Heidi</first><last>Jauhiainen</last></author>
     <author><first>Krister</first><last>Lindén</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>254–262</pages>
     <url>http://www.aclweb.org/anthology/W18-3929</url>
     <abstract>In this paper we present the experiments and results by the SUKI team in the German Dialect Identification shared task of the VarDial 2018 Evaluation Campaign. Our submission using HeLI with adaptive language models obtained the best results in the shared task with a macro F1-score of 0.686, which is clearly higher than the other submitted results. Without some form of unsupervised adaptation on the test set, it might not be possible to reach as high an F1-score with the level of domain difference between the datasets of the shared task. We describe the methods used in detail, as well as some additional experiments carried out during the shared task.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>jauhiainen-jauhiainen-lindn:2018:W18-393</bibkey>
   </paper>

   <paper id="3930">
     <title>Deep Models for Arabic Dialect Identification on Benchmarked Data</title>
     <author><first>Mohamed</first><last>Elaraby</last></author>
     <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>263–274</pages>
     <url>http://www.aclweb.org/anthology/W18-3930</url>
     <abstract>The Arabic Online Commentary (AOC) (Zaidan and Callison-Burch, 2011) is a large-scale repos-itory of Arabic dialects with manual labels for4varieties of the language. Existing dialect iden-tification models exploiting the dataset pre-date the recent boost deep learning brought to NLPand hence the data are not benchmarked for use with deep learning, nor is it clear how much neural networks can help tease the categories in the data apart. We treat these two limitations:We (1) benchmark the data, and (2) empirically test6different deep learning methods on thetask, comparing peformance to several classical machine learning models under different condi-tions (i.e., both binary and multi-way classification). Our experimental results show that variantsof (attention-based) bidirectional recurrent neural networks achieve best accuracy (acc) on thetask, significantly outperforming all competitive baselines. On blind test data, our models reach87.65%acc on the binary task (MSA vs. dialects),87.4%acc on the 3-way dialect task (Egyptianvs. Gulf vs. Levantine), and82.45%acc on the 4-way variants task (MSA vs. Egyptian vs. Gulfvs. Levantine). We release our benchmark for future work on the dataset</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>elaraby-abdulmageed:2018:W18-39</bibkey>
   </paper>

   <paper id="3931">
     <title>A Neural Approach to Language Variety Translation</title>
     <author><first>Marta R.</first><last>Costa-jussà</last></author>
     <author><first>Marcos</first><last>Zampieri</last></author>
     <author><first>Santanu</first><last>Pal</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>275–282</pages>
     <url>http://www.aclweb.org/anthology/W18-3931</url>
     <abstract>In this paper we present the first neural-based machine translation system trained to translate between standard national varieties of the same language. We take the pair Brazilian - European Portuguese as an example and compare the performance of this method to a phrase-based statistical machine translation system. We report a performance improvement of 0.9 BLEU points in translating from European to Brazilian Portuguese and 0.2 BLEU points when translating in the opposite direction. We also carried out a human evaluation experiment with native speakers of Brazilian Portuguese which indicates that humans prefer the output produced by the neural-based system in comparison to the statistical system.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>costajuss-zampieri-pal:2018:W18-39</bibkey>
   </paper>

   <paper id="3932">
     <title>Character Level Convolutional Neural Network for Indo-Aryan Language Identification</title>
     <author><first>Mohamed</first><last>Ali</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>283–287</pages>
     <url>http://www.aclweb.org/anthology/W18-3932</url>
     <abstract>This submission is a description paper for our system in ILI shared task</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>ali:2018:W18-393</bibkey>
   </paper>

   <paper id="3933">
     <title>German Dialect Identification Using Classifier Ensembles</title>
     <author><first>Alina Maria</first><last>Ciobanu</last></author>
     <author><first>Shervin</first><last>Malmasi</last></author>
     <author><first>Liviu P.</first><last>Dinu</last></author>
     <booktitle>Proceedings of the Fifth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>288–294</pages>
     <url>http://www.aclweb.org/anthology/W18-3933</url>
     <abstract>In this paper we present the GDI classification entry to the second German Dialect Identification (GDI) shared task organized within the scope of the VarDial Evaluation Campaign 2018. We present a system based on SVM classifier ensembles trained on characters and words. The system was trained on a collection of speech transcripts of five Swiss-German dialects provided by the organizers. The transcripts included in the dataset contained speakers from Basel, Bern, Lucerne, and Zurich. Our entry in the challenge reached 62.03% F1 score and was ranked third out of eight teams.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>ciobanu-malmasi-dinu:2018:W18-39</bibkey>
   </paper>

   <paper id="4000">
     <title>Proceedings of the Third Workshop on Semantic Deep Learning</title>
     <editor><first>Luis Espinosa</first><last>Anke</last></editor>
     <editor><first>Dagmar</first><last>Gromann</last></editor>
     <editor><first>Thierry</first><last>Declerck</last></editor>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-40</url>
     <bibtype>book</bibtype>
     <bibkey>W18-40:2018</bibkey>
   </paper>

   <paper id="4001">
     <title>Replicated Siamese LSTM in Ticketing System for Similarity Learning and Retrieval in Asymmetric Texts</title>
     <author><first>Pankaj</first><last>Gupta</last></author>
     <author><first>Bernt</first><last>Andrassy</last></author>
     <author><first>Hinrich</first><last>Schütze</last></author>
     <booktitle>Proceedings of the Third Workshop on Semantic Deep Learning</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–11</pages>
     <url>http://www.aclweb.org/anthology/W18-4001</url>
     <abstract>The goal of our industrial ticketing system is to retrieve a relevant solution for an input query, by matching with historical tickets stored in knowledge base. A query is comprised of subject and description, while a historical ticket consists of subject, description and solution. To retrieve a relevant solution, we use textual similarity paradigm to learn similarity in the query and historical tickets. The task is challenging due to significant term mismatch in the query and ticket pairs of asymmetric lengths, where subject is a short text but description and solution are multi-sentence texts. We present a novel Replicated Siamese LSTM model to learn similarity in asymmetric text pairs, that gives 22% and 7% gain (Accuracy@10) for retrieval task, respectively over unsupervised and supervised baselines. We also show that the topic and distributed semantic features for short and long texts improved both similarity learning and retrieval.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>gupta-andrassy-schtze:2018:W18-40</bibkey>
   </paper>

   <paper id="4002">
     <title>Word-Embedding based Content Features for Automated Oral Proficiency Scoring</title>
     <author><first>Su-Youn</first><last>Yoon</last></author>
     <author><first>Anastassia</first><last>Loukina</last></author>
     <author><first>Chong Min</first><last>Lee</last></author>
     <author><first>Matthew</first><last>Mulholland</last></author>
     <author><first>Xinhao</first><last>Wang</last></author>
     <author><first>Ikkyu</first><last>Choi</last></author>
     <booktitle>Proceedings of the Third Workshop on Semantic Deep Learning</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>12–22</pages>
     <url>http://www.aclweb.org/anthology/W18-4002</url>
     <abstract>In this study, we develop content features for an automated scoring system of non-native English speakers’ spontaneous speech. The features calculate the lexical similarity between the question text and the ASR word hypothesis of the spoken response, based on traditional word vector models or word embeddings. The proposed features do not require any sample training responses for each question, and this is a strong advantage since collecting question-specific data is an expensive task, and sometimes even impossible due to concerns about question exposure. We explore the impact of these new features on the automated scoring of two different question types: (a) providing opinions on familiar topics and (b) answering a question about a stimulus material. The proposed features showed statistically significant correlations with the oral proficiency scores, and the combination of new features with the speech-driven features achieved a small but significant further improvement for the latter question type. Further analyses suggested that the new features were effective in assigning more accurate scores for responses with serious content issues.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>yoon-EtAl:2018:W18-40</bibkey>
   </paper>

   <paper id="4003">
     <title>Automatically Linking Lexical Resources with Word Sense Embedding Models</title>
     <author><first>Luis</first><last>Nieto Piña</last></author>
     <author><first>Richard</first><last>Johansson</last></author>
     <booktitle>Proceedings of the Third Workshop on Semantic Deep Learning</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>23–29</pages>
     <url>http://www.aclweb.org/anthology/W18-4003</url>
     <abstract>Automatically learnt word sense embeddings are developed as an attempt to refine the capabilities of coarse word embeddings. The word sense representations obtained this way are, however, sensitive to underlying corpora and parameterizations, and they might be difficult to relate to formally defined word senses. We propose to tackle this problem by devising a mechanism to establish links between word sense embeddings and lexical resources created by experts. We evaluate the applicability of these links in a task to retrieve instances of word sense unlisted in the lexicon.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>nietopia-johansson:2018:W18-40</bibkey>
   </paper>

   <paper id="4004">
     <title>Transferred Embeddings for Igbo Similarity, Analogy, and Diacritic Restoration Tasks</title>
     <author><first>Ignatius</first><last>Ezeani</last></author>
     <author><first>Ikechukwu</first><last>Onyenwe</last></author>
     <author><first>Mark</first><last>Hepple</last></author>
     <booktitle>Proceedings of the Third Workshop on Semantic Deep Learning</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>30–38</pages>
     <url>http://www.aclweb.org/anthology/W18-4004</url>
     <abstract>Existing NLP models are mostly trained with data from well-resourced languages. Most minority languages face the challenge of lack of resources - data and technologies - for NLP research. Building these resources from scratch for each minority language will be very expensive, time-consuming and amount largely to unnecessarily re-inventing the wheel. In this paper, we applied transfer learning techniques to create Igbo word embeddings from a variety of existing English trained embeddings. Transfer learning methods were also used to build standard datasets for Igbo word similarity and analogy tasks for intrinsic evaluation of embeddings. These projected embeddings were also applied to diacritic restoration task. Our results indicate that the projected models not only outperform the trained ones on the semantic-based tasks of analogy, word-similarity, and odd-word identifying, but they also achieve enhanced performance on the diacritic restoration with learned diacritic embeddings.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>ezeani-onyenwe-hepple:2018:W18-40</bibkey>
   </paper>

   <paper id="4005">
     <title>Towards Enhancing Lexical Resource and Using Sense-annotations of OntoSenseNet for Sentiment Analysis</title>
     <author><first>Sreekavitha</first><last>Parupalli</last></author>
     <author><first>Vijjini</first><last>Anvesh Rao</last></author>
     <author><first>Radhika</first><last>Mamidi</last></author>
     <booktitle>Proceedings of the Third Workshop on Semantic Deep Learning</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>39–44</pages>
     <url>http://www.aclweb.org/anthology/W18-4005</url>
     <abstract>This paper illustrates the interface of the tool we developed for crowd sourcing and we explain the annotation procedure in detail. Our tool is named as ‘పారుపల్లి పదజాలం’ (Parupalli Padajaalam) which means web of words by Parupalli. The aim of this tool is to populate the OntoSenseNet, sentiment polarity annotated Telugu resource. Recent works have shown the importance of word-level annotations on sentiment analysis. With this as basis, we aim to analyze the importance of sense-annotations obtained from OntoSenseNet in performing the task of sentiment analysis. We explain the features extracted from OntoSenseNet (Telugu). Furthermore we compute and explain the adverbial class distribution of verbs in OntoSenseNet. This task is known to aid in disambiguating word-senses which helps in enhancing the performance of word-sense disambiguation (WSD) task(s).</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>parupalli-anveshrao-mamidi:2018:W18-40</bibkey>
   </paper>

   <paper id="4006">
     <title>Knowledge Representation with Conceptual Spaces</title>
     <author><first>Steven</first><last>Schockaert</last></author>
     <booktitle>Proceedings of the Third Workshop on Semantic Deep Learning</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>45</pages>
     <url>http://www.aclweb.org/anthology/W18-4006</url>
     <abstract>Entity embeddings are vector space representations of a given domain of interest. They are typically
 learned from text corpora (possibly in combination with any available structured knowledge), based on
 the intuition that similar entities should be represented by similar vectors. The usefulness of such entity
 embeddings largely stems from the fact that they implicitly encode a rich amount of knowledge about the
 considered domain, beyond mere similarity. In an embedding of movies, for instance, we may expect all
 movies from a given genre to be located in some low-dimensional manifold. This is particularly useful
 in supervised learning settings, where it may e.g. allow neural movie recommenders to base predictions
 on the genre of a movie, without that genre having to be specified explicitly for each movie, or without
 even the need to specify that the genre of a movie is a property that may have predictive value for
 the considered task. In unsupervised settings, however, such implicitly encoded knowledge cannot be
 leveraged.
 Conceptual spaces, as proposed by Grdenfors, are similar to entity embeddings, but provide more
 structure. In conceptual spaces, among others, dimensions are interpretable and grouped into facets, and
 properties and concepts are explicitly modelled as (vague) regions. Thanks to this additional structure,
 conceptual spaces can be used as a knowledge representation framework, which can also be effectively
 exploited in unsupervised settings. Given a conceptual space of movies, for instance, we are able to answer
 queries that ask about similarity w.r.t. a particular facet (e.g. movies which are cinematographically
 similar to Jurassic Park), that refer to a given feature (e.g. movies which are scarier than Jurassic Park
 but otherwise similar), or that refer to particular properties or concepts (e.g. thriller from the 1990s with
 a dinosaur theme). Compared to standard entity embeddings, however, conceptual spaces are more challenging
 to learn in a purely data-driven fashion. In this talk, I will give an overview of some approaches
 for learning such representations that have recently been developed within the context of the FLEXILOG
 project.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>schockaert:2018:W18-40</bibkey>
   </paper>

   <paper id="4007">
     <title>Knowledge Representation and Extraction at Scale</title>
     <author><first>Christos</first><last>Christodoulopoulos</last></author>
     <booktitle>Proceedings of the Third Workshop on Semantic Deep Learning</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>46</pages>
     <url>http://www.aclweb.org/anthology/W18-4007</url>
     <abstract>These days, most general knowledge question-answering systems rely on large-scale knowledge bases
 comprising billions of facts about millions of entities. Having a structured source of semantic knowledge
 means that we can answer questions involving single static facts (e.g. “Who was the 8th president of the
 US?”) or dynamically generated ones (e.g. “How old is Donald Trump?”). More importantly, we can
 answer questions involving multiple inference steps (“Is the queen older than the president of the US?”).
 In this talk, I’m going to be discussing some of the unique challenges that are involved with building
 and maintaining a consistent knowledge base for Alexa, extending it with new facts and using it to serve
 answers in multiple languages. I will focus on three recent projects from our group. First, a way of
 measuring the completeness of a knowledge base, that is based on usage patterns. The definition of the
 usage of the KB is done in terms of the relation distribution of entities seen in question-answer logs.
 Instead of directly estimating the relation distribution of individual entities, it is generalized to the “class
 signature” of each entity. For example, users ask for baseball players’ height, age, and batting average,
 so a knowledge base is complete (with respect to baseball players) if every entity has facts for those three
 relations.
 Second, an investigation into fact extraction from unstructured text. I will present a method for creating
 distant (weak) supervision labels for training a large-scale relation extraction system. I will also discuss
 the effectiveness of neural network approaches by decoupling the model architecture from the feature
 design of a state-of-the-art neural network system. Surprisingly, a much simpler classifier trained on
 similar features performs on par with the highly complex neural network system (at 75x reduction to the
 training time), suggesting that the features are a bigger contributor to the final performance.
 Finally, I will present the Fact Extraction and VERification (FEVER) dataset and challenge. The
 dataset comprises more than 185,000 human-generated claims extracted from Wikipedia pages. False
 claims were generated by mutating true claims in a variety of ways, some of which were meaningaltering.
 During the verification step, annotators were required to label a claim for its validity and also
 supply full-sentence textual evidence from (potentially multiple) Wikipedia articles for the label. With
 FEVER, we aim to help create a new generation of transparent and interprable knowledge extraction
 systems.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>christodoulopoulos:2018:W18-40</bibkey>
   </paper>

   <paper id="4100">
     <title>Proceedings of the First International Workshop on Language Cognition and Computational Models</title>
     <editor><first>Manjira</first><last>Sinha</last></editor>
     <editor><first>Tirthankar</first><last>Dasgupta</last></editor>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-41</url>
     <bibtype>book</bibtype>
     <bibkey>W18-41:2018</bibkey>
   </paper>

   <paper id="4101">
     <title>A Compositional Bayesian Semantics for Natural Language</title>
     <author><first>Jean-Philippe</first><last>Bernardy</last></author>
     <author><first>Rasmus</first><last>Blanck</last></author>
     <author><first>Stergios</first><last>Chatzikyriakidis</last></author>
     <author><first>Shalom</first><last>Lappin</last></author>
     <booktitle>Proceedings of the First International Workshop on Language Cognition and Computational Models</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–10</pages>
     <url>http://www.aclweb.org/anthology/W18-4101</url>
     <abstract>We propose a compositional Bayesian semantics that interprets declarative sentences in a natural language by assigning them probability conditions. These are conditional probabilities that estimate the likelihood that a competent speaker would endorse an assertion, given certain hypotheses. Our semantics is implemented in a functional programming language. It estimates the marginal probability of a sentence through Markov Chain Monte Carlo (MCMC) sampling of objects in vector space models satisfying specified hypotheses. We apply our semantics to examples with several predicates and generalised quantifiers, including higher-order quantifiers. It captures the vagueness of predication (both gradable and non-gradable), without positing a precise boundary for classifier application. We present a basic account of semantic learning based on our semantic system. We compare our proposal to other current theories of probabilistic semantics, and we show that it offers several important advantages over these accounts.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>bernardy-EtAl:2018:W18-41</bibkey>
   </paper>

   <paper id="4102">
     <title>Detecting Linguistic Traces of Depression in Topic-Restricted Text: Attending to Self-Stigmatized Depression with NLP</title>
     <author><first>JT</first><last>Wolohan</last></author>
     <author><first>Misato</first><last>Hiraga</last></author>
     <author><first>Atreyee</first><last>Mukherjee</last></author>
     <author><first>Zeeshan Ali</first><last>Sayyed</last></author>
     <author><first>Matthew</first><last>Millard</last></author>
     <booktitle>Proceedings of the First International Workshop on Language Cognition and Computational Models</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>11–21</pages>
     <url>http://www.aclweb.org/anthology/W18-4102</url>
     <abstract>Natural language processing researchers have proven the ability of machine learning approaches to detect depression-related cues from language; however, to date, these efforts have primarily assumed it was acceptable to leave depression-related texts in the data. Our concerns with this are twofold: first, that the models may be overfitting on depression-related signals, which may not be present in all depressed users (only those who talk about depression on social media); and second, that these models would under-perform for users who are sensitive to the public stigma of depression. This study demonstrates the validity to those concerns. We construct a novel corpus of texts from 12,106 Reddit users and perform lexical and predictive analyses under two conditions: one where all text produced by the users is included and one where the depression data is withheld. We find significant differences in the language used by depressed users under the two conditions as well as a difference in the ability of machine learning algorithms to correctly detect depression. However, despite the lexical differences and reduced classification performance–each of which suggests that users may be able to fool algorithms by avoiding direct discussion of depression–a still respectable overall performance suggests lexical models are reasonably robust and well suited for a role in a diagnostic or monitoring capacity.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>wolohan-EtAl:2018:W18-41</bibkey>
   </paper>

   <paper id="4103">
     <title>An OpenNMT Model to Arabic Broken Plurals</title>
     <author><first>Elsayed</first><last>Issa</last></author>
     <booktitle>Proceedings of the First International Workshop on Language Cognition and Computational Models</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>22–30</pages>
     <url>http://www.aclweb.org/anthology/W18-4103</url>
     <abstract>Arabic Broken Plurals show an interesting phenomenon in Arabic morphology as they are formed by shifting the consonants of the syllables into different syllable patterns, and subsequently, the pattern of the word changes. The present paper, therefore, attempts to look at Arabic broken plurals from the perspective of neural networks by implementing an OpenNMT experiment to better understand and interpret the behavior of these plurals, especially when it comes to L2 acquisition. The results show that the model is successful in predicting the Arabic template. However, it fails to predict certain consonants such as the emphatics and the gutturals. This reinforces the fact that these consonants or sounds are the most difficult for L2 learners to acquire.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>issa:2018:W18-41</bibkey>
   </paper>

   <paper id="4104">
     <title>Enhancing Cohesion and Coherence of Fake Text to Improve Believability for Deceiving Cyber Attackers</title>
     <author><first>Prakruthi</first><last>Karuna</last></author>
     <author><first>Hemant</first><last>Purohit</last></author>
     <author><first>Ozlem</first><last>Uzuner</last></author>
     <author><first>Sushil</first><last>Jajodia</last></author>
     <author><first>Rajesh</first><last>Ganesan</last></author>
     <booktitle>Proceedings of the First International Workshop on Language Cognition and Computational Models</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>31–40</pages>
     <url>http://www.aclweb.org/anthology/W18-4104</url>
     <abstract>Ever increasing ransomware attacks and thefts of intellectual property demand cybersecurity solutions to protect critical documents. One emerging solution is to place fake text documents in the repository of critical documents for deceiving and catching cyber attackers. We can generate fake text documents by obscuring the salient information in legit text documents. However, the obscuring process can result in linguistic inconsistencies, such as broken co-references and illogical flow of ideas across the sentences, which can discern the fake document and render it unbelievable. In this paper, we propose a novel method to generate believable fake text documents by automatically improving the linguistic consistency of computer-generated fake text. Our method focuses on enhancing syntactic cohesion and semantic coherence across discourse segments. We conduct experiments with human subjects to evaluate the effect of believability improvements in distinguishing legit texts from fake texts. Results show that the probability to distinguish legit texts from believable fake texts is consistently lower than from fake texts that have not been improved in believability. This indicates the effectiveness of our method in generating believable fake text.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>karuna-EtAl:2018:W18-41</bibkey>
   </paper>

   <paper id="4105">
     <title>Addressing the Winograd Schema Challenge as a Sequence Ranking Task</title>
     <author><first>Juri</first><last>Opitz</last></author>
     <author><first>Anette</first><last>Frank</last></author>
     <booktitle>Proceedings of the First International Workshop on Language Cognition and Computational Models</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>41–52</pages>
     <url>http://www.aclweb.org/anthology/W18-4105</url>
     <abstract>The Winograd Schema Challenge targets pronominal anaphora resolution problems which require the application of cognitive inference in combination with world knowledge. These problems are easy to solve for humans but most difficult to solve for machines. Computational models that previously addressed this task rely on syntactic preprocessing and incorporation of external knowledge by manually crafted features. We address the Winograd Schema Challenge from a new perspective as a sequence ranking task, and design a Siamese neural sequence ranking model which performs significantly better than a random baseline, even when solely trained on sequences of words. We evaluate against a baseline and a state-of-the-art system on two data sets and show that anonymization of noun phrase candidates strongly helps our model to generalize.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>opitz-frank:2018:W18-41</bibkey>
   </paper>

   <paper id="4106">
     <title>Finite State Reasoning for Presupposition Satisfaction</title>
     <author><first>Jacob</first><last>Collard</last></author>
     <booktitle>Proceedings of the First International Workshop on Language Cognition and Computational Models</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>53–62</pages>
     <url>http://www.aclweb.org/anthology/W18-4106</url>
     <abstract>Sentences with presuppositions are often treated as uninterpretable or unvalued (neither true nor false) if their presuppositions are not satisfied. However, there is an open question as to how this satisfaction is calculated. In some cases, determining whether a presupposition is satisfied is not a trivial task (or even a decidable one), yet native speakers are able to quickly and confidently identify instances of presupposition failure. I propose that this can be accounted for with a form of possible world semantics that encapsulates some reasoning abilities, but is limited in its computational power, thus circumventing the need to solve computationally difficult problems. This can be modeled using a variant of the framework of finite state semantics proposed by Rooth (2017). A few modifications to this system are necessary, including its extension into a three-valued logic to account for presupposition. Within this framework, the logic necessary to calculate presupposition satisfaction is readily available, but there is no risk of needing exceptional computational power. This correctly predicts that certain presuppositions will not be calculated intuitively, while others can be easily evaluated.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>collard:2018:W18-41</bibkey>
   </paper>

   <paper id="4107">
     <title>Language-Based Automatic Assessment of Cognitive and Communicative Functions Related to Parkinson’s Disease</title>
     <author><first>Lesley</first><last>Jessiman</last></author>
     <author><first>Gabriel</first><last>Murray</last></author>
     <author><first>McKenzie</first><last>Braley</last></author>
     <booktitle>Proceedings of the First International Workshop on Language Cognition and Computational Models</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>63–74</pages>
     <url>http://www.aclweb.org/anthology/W18-4107</url>
     <abstract>We explore the use of natural language processing and machine learning for detecting evidence of Parkinson’s disease from transcribed speech of subjects who are describing everyday tasks. Experiments reveal the difficulty of treating this as a binary classification task, and a multi-class approach yields superior results. We also show that these models can be used to predict cognitive abilities across all subjects.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>jessiman-murray-braley:2018:W18-41</bibkey>
   </paper>

   <paper id="4108">
     <title>Can spontaneous spoken language disfluencies help describe syntactic dependencies? An empirical study</title>
     <author><first>M.</first><last>KURDI</last></author>
     <booktitle>Proceedings of the First International Workshop on Language Cognition and Computational Models</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>75–84</pages>
     <url>http://www.aclweb.org/anthology/W18-4108</url>
     <abstract>This paper explores the correlations between key syntactic dependencies and the occurrence of simple spoken language disfluencies such as filled pauses and incomplete words. The working hypothesis here is that interruptions caused by these phenomena are more likely to happen between weakly connected words from a syntactic point of view than between strongly connected ones. The obtained results show significant patterns with the regard to key syntactic phenomena, like confirming the positive correlation between the frequency of disfluencies and multiples measures of syntactic complexity. In addition, they show that there is a stronger relationship between the verb and its subject than with its object, which confirms the idea of a hierarchical incrementality. Also, this work uncovered an interesting role played by a verb particle as a syntactic delimiter of some verb complements. Finally, the interruptions by disfluencies patterns show that verbs have a more privileged relationship with their preposition compared to the object Noun Phrase (NP).</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>kurdi:2018:W18-41</bibkey>
   </paper>

   <paper id="4109">
     <title>Word-word Relations in Dementia and Typical Aging</title>
     <author><first>Natalia</first><last>Arias-Trejo</last></author>
     <author><first>Aline</first><last>Minto-García</last></author>
     <author><first>Diana I.</first><last>Luna-Umanzor</last></author>
     <author><first>Alma E.</first><last>Ríos-Ponce</last></author>
     <author><first>Balderas-Pliego</first><last>Mariana</last></author>
     <author><first>Gemma</first><last>Bel-Enguix</last></author>
     <booktitle>Proceedings of the First International Workshop on Language Cognition and Computational Models</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>85–93</pages>
     <url>http://www.aclweb.org/anthology/W18-4109</url>
     <abstract>Older adults tend to suffer a decline in some of their cognitive capabilities, being language one of least affected processes. Word association norms (WAN) also known as free word associations reflect word-word relations, the participant reads or hears a word and is asked to write or say the first word that comes to mind. Free word associations show how the organization of semantic memory remains almost unchanged with age. We have performed a WAN task with very small samples of older adults with Alzheimer’s disease (AD), vascular dementia (VaD) and mixed dementia (MxD), and also with a control group of typical aging adults, matched by age, sex and education. All of them are native speakers of Mexican Spanish. The results show, as expected, that Alzheimer disease has a very important impact in lexical retrieval, unlike vascular and mixed dementia. This suggests that linguistic tests elaborated from WAN can be also used for detecting AD at early stages.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>ariastrejo-EtAl:2018:W18-41</bibkey>
   </paper>

   <paper id="4110">
     <title>Part-of-Speech Annotation of English-Assamese code-mixed texts: Two Approaches</title>
     <author><first>Ritesh</first><last>Kumar</last></author>
     <author><first>Manas Jyoti</first><last>Bora</last></author>
     <booktitle>Proceedings of the First International Workshop on Language Cognition and Computational Models</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>94–103</pages>
     <url>http://www.aclweb.org/anthology/W18-4110</url>
     <abstract>In this paper, we discuss the development of a part-of-speech tagger for English-Assamese code-mixed texts. We provide a comparison of 2 approaches to annotating code-mixed data – a) annotation of the texts from the two languages using monolingual resources from each language and b) annotation of the text through a different resource created specifically for code-mixed data. We present a comparative study of the efforts required in each approach and the final performance of the system. Based on this, we argue that it might be a better approach to develop new technologies using code-mixed data instead of monolingual, ‘clean’ data, especially for those languages where we do not have significant tools and technologies available till now.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>kumar-bora:2018:W18-41</bibkey>
   </paper>

   <paper id="4200">
     <title>Proceedings of the First Workshop on Natural Language Processing for Internet Freedom</title>
     <editor><first>Chris</first><last>Brew</last></editor>
     <editor><first>Anna</first><last>Feldman</last></editor>
     <editor><first>Chris</first><last>Leberknight</last></editor>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-42</url>
     <bibtype>book</bibtype>
     <bibkey>W18-42:2018</bibkey>
   </paper>

   <paper id="4201">
     <title>The effect of information controls on developers in China: An analysis of censorship in Chinese open source projects</title>
     <author><first>Jeffrey</first><last>Knockel</last></author>
     <author><first>Masashi</first><last>Crete-Nishihata</last></author>
     <author><first>Lotus</first><last>Ruan</last></author>
     <booktitle>Proceedings of the First Workshop on Natural Language Processing for Internet Freedom</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–11</pages>
     <url>http://www.aclweb.org/anthology/W18-4201</url>
     <abstract>Censorship of Internet content in China is understood to operate through a system of intermediary liability whereby service providers are liable for the content on their platforms. Previous work studying censorship has found huge variability in the implementation of censorship across different products even within the same industry segment. In this work we explore the extent to which these censorship features are present in the open source projects of individual developers in China by collecting their blacklists and comparing their similarity. We collect files from a popular online code repository, extract lists of strings, and then classify whether each is a Chinese blacklist. Overall, we found over 1,000 Chinese blacklists comprising over 200,000 unique keywords, representing the largest dataset of Chinese blacklisted keywords to date. We found very little keyword overlap between lists, raising questions as to their origins, as the lists seem too large to have been individually curated, yet the lack of overlap suggests that they have no common source.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>knockel-cretenishihata-ruan:2018:W18-42</bibkey>
   </paper>

   <paper id="4202">
     <title>Linguistic Characteristics of Censorable Language on SinaWeibo</title>
     <author><first>Kei Yin</first><last>Ng</last></author>
     <author><first>Anna</first><last>Feldman</last></author>
     <author><first>Jing</first><last>Peng</last></author>
     <author><first>Chris</first><last>Leberknight</last></author>
     <booktitle>Proceedings of the First Workshop on Natural Language Processing for Internet Freedom</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>12–22</pages>
     <url>http://www.aclweb.org/anthology/W18-4202</url>
     <abstract>This paper investigates censorship from a linguistic perspective. We collect a corpus of censored and uncensored posts on a number of topics, build a classifier that predicts censorship decisions independent of discussion topics. Our investigation reveals that the strongest linguistic indicator of censored content of our corpus is its readability.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>ng-EtAl:2018:W18-42</bibkey>
   </paper>

   <paper id="4203">
     <title>Creative Language Encoding under Censorship</title>
     <author><first>Heng</first><last>Ji</last></author>
     <author><first>Kevin</first><last>Knight</last></author>
     <booktitle>Proceedings of the First Workshop on Natural Language Processing for Internet Freedom</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>23–33</pages>
     <url>http://www.aclweb.org/anthology/W18-4203</url>
     <abstract>People often create obfuscated language for online communication to avoid Internet censorship, share sensitive information, express strong sentiment or emotion, plan for secret actions, trade illegal products, or simply hold interesting conversations. In this position paper we systematically categorize human-created obfuscated language on various levels, investigate their basic mechanisms, give an overview on automated techniques needed to simulate human encoding. These encoders have potential to frustrate and evade, co‐evolve with dynamic human or automated decoders, and produce interesting and adoptable code words. We also summarize remaining challenges for future research on the interaction between Natural Language Processing (NLP) and encryption, and leveraging NLP techniques for encoding and decoding.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>ji-knight:2018:W18-42</bibkey>
   </paper>

   <paper id="4300">
     <title>Proceedings of the Workshop Events and Stories in the News 2018</title>
     <editor><first>Tommaso</first><last>Caselli</last></editor>
     <editor><first>Ben</first><last>Miller</last></editor>
     <editor><first>Marieke</first><last>van Erp</last></editor>
     <editor><first>Piek</first><last>Vossen</last></editor>
     <editor><first>Martha</first><last>Palmer</last></editor>
     <editor><first>Eduard</first><last>Hovy</last></editor>
     <editor><first>Teruko</first><last>Mitamura</last></editor>
     <editor><first>David</first><last>Caswell</last></editor>
     <editor><first>Susan W.</first><last>Brown</last></editor>
     <editor><first>Claire</first><last>Bonial</last></editor>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, U.S.A</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-43</url>
     <bibtype>book</bibtype>
     <bibkey>W18-43:2018</bibkey>
   </paper>

   <paper id="4301">
     <title>Every Object Tells a Story</title>
     <author><first>James</first><last>Pustejovsky</last></author>
     <author><first>Nikhil</first><last>Krishnaswamy</last></author>
     <booktitle>Proceedings of the Workshop Events and Stories in the News 2018</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, U.S.A</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–6</pages>
     <url>http://www.aclweb.org/anthology/W18-4301</url>
     <abstract>Most work within the computational event modeling community has tended to focus on the interpretation and ordering of events that are associated with verbs and event nominals in linguistic expressions. What is often overlooked in the construction of a global interpretation of a narrative is the role contributed by the objects participating in these structures, and the latent events and activities conventionally associated with them. Recently, the analysis of visual images has also enriched the scope of how events can be identified, by anchoring both linguistic expressions and ontological labels to segments, subregions, and properties of images. By semantically grounding event descriptions in their visualization, the importance of object-based attributes becomes more apparent. In this position paper, we look at the narrative structure of objects: that is, how objects reference events through their intrinsic attributes, such as affordances, purposes, and functions. We argue that, not only do objects encode conventionalized events, but that when they are composed within specific habitats, the ensemble can be viewed as modeling coherent event sequences, thereby enriching the global interpretation of the evolving narrative being constructed.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>pustejovsky-krishnaswamy:2018:W18-43</bibkey>
   </paper>

   <paper id="4302">
     <title>A Rich Annotation Scheme for Mental Events</title>
     <author><first>William</first><last>Croft</last></author>
     <author><first>Pavlina</first><last>Peskova</last></author>
     <author><first>Michael</first><last>Regan</last></author>
     <author><first>Sook-kyung</first><last>Lee</last></author>
     <booktitle>Proceedings of the Workshop Events and Stories in the News 2018</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, U.S.A</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>7–17</pages>
     <url>http://www.aclweb.org/anthology/W18-4302</url>
     <abstract>We present a rich annotation scheme for the structure of mental events. Mental events are those in which the verb describes a mental state or process, usually oriented towards an external situation. While physical events have been described in detail and there are numerous studies of their semantic analysis and annotation, mental events are less thoroughly studied. The annotation scheme proposed here is based on decompositional analyses in the semantic and typological linguistic literature. The scheme was applied to the news corpus from the 2016 Events workshop, and error analysis of the test annotation provides suggestions for refinement and clarification of the annotation scheme.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>croft-EtAl:2018:W18-43</bibkey>
   </paper>

   <paper id="4303">
     <title>Cross-Document Narrative Alignment of Environmental News: A Position Paper on the Challenge of Using Event Chains to Proxy Narrative Features</title>
     <author><first>Ben</first><last>Miller</last></author>
     <booktitle>Proceedings of the Workshop Events and Stories in the News 2018</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, U.S.A</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>18–24</pages>
     <url>http://www.aclweb.org/anthology/W18-4303</url>
     <abstract>Cross-document event chain co-referencing in corpora of news articles would achieve increased precision and generalizability from a method that consistently recognizes narrative, discursive, and phenomenological features such as tense, mood, tone, canonicity and breach, person, hermeneutic composability, speed, and time. Current models that capture primarily linguistic data such as entities, times, and relations or causal relationships may only incidentally capture narrative framing features of events. That limits efforts at narrative and event chain segmentation, among other predicate tasks for narrative search and narrative-based reasoning. It further limits research on audience engagement with journalism about complex subjects. This position paper explores the above proposition with respect to narrative theory and ongoing research on segmenting event chains into narrative units. Our own work in progress approaches this task using event segmentation, word embeddings, and variable length pattern matching in a corpus of 2,000 articles describing environmental events. Our position is that narrative features may or may not be implicitly captured by current methods explicitly focused on events as linguistic phenomena, that they are not explicitly captured, and that further research is required.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>miller:2018:W18-43</bibkey>
   </paper>

   <paper id="4304">
     <title>Identifying the Discourse Function of News Article Paragraphs</title>
     <author><first>W. Victor</first><last>Yarlott</last></author>
     <author><first>Cristina</first><last>Cornelio</last></author>
     <author><first>Tian</first><last>Gao</last></author>
     <author><first>Mark</first><last>Finlayson</last></author>
     <booktitle>Proceedings of the Workshop Events and Stories in the News 2018</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, U.S.A</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>25–33</pages>
     <url>http://www.aclweb.org/anthology/W18-4304</url>
     <abstract>Discourse structure is a key aspect of all forms of text, providing valuable information both to humans and machines. We applied the hierarchical theory of news discourse developed by van Dijk to examine how paragraphs operate as units of discourse structure within news articles—what we refer to here as document-level discourse. This document-level discourse provides a characterization of the content of each paragraph that describes its relation to the events presented in the article (such as main events, backgrounds, and consequences) as well as to other components of the story (such as commentary and evaluation). The purpose of a news discourse section is of great utility to story understanding as it affects both the importance and temporal order of items introduced in the text—therefore, if we know the news discourse purpose for different sections, we should be able to better rank events for their importance and better construct timelines. We test two hypotheses: first, that people can reliably annotate news articles with van Dijk’s theory; second, that we can reliably predict these labels using machine learning. We show that people have a high degree of agreement with each other when annotating the theory (F1 &gt; 0.8, Cohen’s kappa &gt; 0.6), demonstrating that it can be both learned and reliably applied by human annotators. Additionally, we demonstrate first steps toward machine learning of the theory, achieving a performance of F1 = 0.54, which is 65% of human performance. Moreover, we have generated a gold-standard, adjudicated corpus of 50 documents for document-level discourse annotation based on the ACE Phase 2 corpus.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>yarlott-EtAl:2018:W18-43</bibkey>
   </paper>

   <paper id="4305">
     <title>An Evaluation of Information Extraction Tools for Identifying Health Claims in News Headlines</title>
     <author><first>Shi</first><last>Yuan</last></author>
     <author><first>Bei</first><last>Yu</last></author>
     <booktitle>Proceedings of the Workshop Events and Stories in the News 2018</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, U.S.A</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>34–43</pages>
     <url>http://www.aclweb.org/anthology/W18-4305</url>
     <abstract>This study evaluates the performance of four information extraction tools (extractors) on identifying health claims in health news headlines. A health claim is defined as a triplet: IV (what is being manipulated), DV (what is being measured) and their relation. Tools that can identify health claims provide the foundation for evaluating the accuracy of these claims against authoritative resources. The evaluation result shows that 26% headlines do not in-clude health claims, and all extractors face difficulty separating them from the rest. For those with health claims, OPENIE-5.0 performed the best with F-measure at 0.6 level for ex-tracting “IV-relation-DV”. However, the characteristic linguistic structures in health news headlines, such as incomplete sentences and non-verb relations, pose particular challenge to existing tools.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>yuan-yu:2018:W18-43</bibkey>
   </paper>

   <paper id="4306">
     <title>Crowdsourcing StoryLines: Harnessing the Crowd for Causal Relation Annotation</title>
     <author><first>Tommaso</first><last>Caselli</last></author>
     <author><first>Oana</first><last>Inel</last></author>
     <booktitle>Proceedings of the Workshop Events and Stories in the News 2018</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, U.S.A</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>44–54</pages>
     <url>http://www.aclweb.org/anthology/W18-4306</url>
     <abstract>This paper describes a crowdsourcing experiment on the annotation of plot-like structures in English news articles. CrowdThruth methodology and metrics have been applied to select valid annotations from the crowd. We further run an in-depth analysis of the annotated data by comparing them with available expert data. Our results show a valuable use of crowdsourcing annotations for such complex semantic tasks, and suggest a new annotation approach which combine crowd and experts.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>caselli-inel:2018:W18-43</bibkey>
   </paper>

   <paper id="4307">
     <title>Can You Spot the Semantic Predicate in this Video?</title>
     <author><first>Christopher</first><last>Reale</last></author>
     <author><first>Claire</first><last>Bonial</last></author>
     <author><first>Heesung</first><last>Kwon</last></author>
     <author><first>Clare</first><last>Voss</last></author>
     <booktitle>Proceedings of the Workshop Events and Stories in the News 2018</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, U.S.A</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>55–60</pages>
     <url>http://www.aclweb.org/anthology/W18-4307</url>
     <abstract>We propose a method to improve human activity recognition in video by leveraging semantic information about the target activities from an expert-defined linguistic resource, VerbNet. Our hypothesis is that activities that share similar event semantics, as defined by the semantic predicates of VerbNet, will be more likely to share some visual components. We use a deep convolutional neural network approach as a baseline and incorporate linguistic information from VerbNet through multi-task learning. We present results of experiments showing the added information has negligible impact on recognition performance. We discuss how this may be because the lexical semantic information defined by VerbNet is generally not visually salient given the video processing approach used here, and how we may handle this in future approaches.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>reale-EtAl:2018:W18-43</bibkey>
   </paper>

   <paper id="4308">
     <title>Fine-grained Structure-based News Genre Categorization</title>
     <author><first>Zeyu</first><last>Dai</last></author>
     <author><first>Himanshu</first><last>Taneja</last></author>
     <author><first>Ruihong</first><last>Huang</last></author>
     <booktitle>Proceedings of the Workshop Events and Stories in the News 2018</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, U.S.A</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>61–67</pages>
     <url>http://www.aclweb.org/anthology/W18-4308</url>
     <abstract>Journalists usually organize and present the contents of a news article following a well-defined structure. In this work, we propose a new task to categorize news articles based on their content presentation structures, which is beneficial for various NLP applications. We first define a small set of news elements considering their functions (e.g., <i>introducing the main story or event, catching the reader’s attention</i> and <i>providing details</i>) in a news story and their writing style (<i>narrative</i> or <i>expository</i>), and then formally define four commonly used news article structures based on their selections and organizations of news elements. We create an annotated dataset for structure-based news genre identification, and finally, we build a predictive model to assess the feasibility of this classification task using structure indicative features.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>dai-taneja-huang:2018:W18-43</bibkey>
   </paper>

   <paper id="4309">
     <title>On Training Classifiers for Linking Event Templates</title>
     <author><first>Jakub</first><last>Piskorski</last></author>
     <author><first>Fredi</first><last>Saric</last></author>
     <author><first>Vanni</first><last>Zavarella</last></author>
     <author><first>Martin</first><last>Atkinson</last></author>
     <booktitle>Proceedings of the Workshop Events and Stories in the News 2018</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, U.S.A</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>68–78</pages>
     <url>http://www.aclweb.org/anthology/W18-4309</url>
     <abstract>The paper reports on exploring various machine learning techniques and a range of textual and meta-data features to train classifiers for linking related event templates automatically extracted from online news. With the best model using textual features only we achieved 94.7% (92.9%) F1 score on GOLD (SILVER) dataset. These figures were further improved to 98.6% (GOLD) and 97% (SILVER) F1 score by adding meta-data features, mainly thanks to the strong discriminatory power of automatically extracted geographical information related to events.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>piskorski-EtAl:2018:W18-43</bibkey>
   </paper>

   <paper id="4310">
     <title>HEI: Hunter Events Interface A platform based on services for the detection and reasoning about events</title>
     <author><first>Antonio</first><last>Sorgente</last></author>
     <author><first>Antonio</first><last>Calabrese</last></author>
     <author><first>Gianluca</first><last>Coda</last></author>
     <author><first>Paolo</first><last>Vanacore</last></author>
     <author><first>Francesco</first><last>Mele</last></author>
     <booktitle>Proceedings of the Workshop Events and Stories in the News 2018</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, U.S.A</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>79–88</pages>
     <url>http://www.aclweb.org/anthology/W18-4310</url>
     <abstract>In this paper we present the definition and implementation of the Hunter Events Interface (HEI) System. The HEI System is a system for events annotation and temporal reasoning in Natural Language Texts and media, mainly oriented to texts of historical and cultural contents available on the Web. In this work we assume that events are defined through various components: actions, participants, locations, and occurrence intervals. The HEI system, through independent services, locates (annotates) the various components, and successively associates them to a specific event. The objective of this work is to build a system integrating services for the identification of events, the discovery of their connections, and the evaluation of their consistency. We believe this interface is useful to develop applications that use the notion of story, to integrate data of digital cultural archives, and to build systems of fruition in the same field. The HEI system has been partially developed within the TrasTest project</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>sorgente-EtAl:2018:W18-43</bibkey>
   </paper>

   <paper id="4400">
     <title>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</title>
     <editor><first>Ritesh</first><last>Kumar</last></editor>
     <editor><first>Atul Kr.</first><last>Ojha</last></editor>
     <editor><first>Marcos</first><last>Zampieri</last></editor>
     <editor><first>Shervin</first><last>Malmasi</last></editor>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-44</url>
     <bibtype>book</bibtype>
     <bibkey>W18-44:2018</bibkey>
   </paper>

   <paper id="4401">
     <title>Benchmarking Aggression Identification in Social Media</title>
     <author><first>Ritesh</first><last>Kumar</last></author>
     <author><first>Atul Kr.</first><last>Ojha</last></author>
     <author><first>Shervin</first><last>Malmasi</last></author>
     <author><first>Marcos</first><last>Zampieri</last></author>
     <booktitle>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–11</pages>
     <url>http://www.aclweb.org/anthology/W18-4401</url>
     <abstract>In this paper, we present the report and findings of the Shared Task on Aggression Identification organised as part of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC - 1) at COLING 2018. The task was to develop a classifier that could discriminate between Overtly Aggressive, Covertly Aggressive, and Non-aggressive texts. For this task, the participants were provided with a dataset of 15,000 aggression-annotated Facebook Posts and Comments each in Hindi (in both Roman and Devanagari script) and English for training and validation. For testing, two different sets - one from Facebook and another from a different social media - were provided. A total of 130 teams registered to participate in the task, 30 teams submitted their test runs, and finally 20 teams also sent their system description paper which are included in the TRAC workshop proceedings. The best system obtained a weighted F-score of 0.64 for both Hindi and English on the Facebook test sets, while the best scores on the surprise set were 0.60 and 0.50 for English and Hindi respectively. The results presented in this report depict how challenging the task is. The positive response from the community and the great levels of participation in the first edition of this shared task also highlights the interest in this topic.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>kumar-EtAl:2018:W18-441</bibkey>
   </paper>

   <paper id="4402">
     <title>RiTUAL-UH at TRAC 2018 Shared Task: Aggression Identification</title>
     <author><first>Niloofar</first><last>Safi Samghabadi</last></author>
     <author><first>Deepthi</first><last>Mave</last></author>
     <author><first>Sudipta</first><last>Kar</last></author>
     <author><first>Thamar</first><last>Solorio</last></author>
     <booktitle>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>12–18</pages>
     <url>http://www.aclweb.org/anthology/W18-4402</url>
     <abstract>This paper presents our system for “TRAC 2018 Shared Task on Aggression Identification”. Our best systems for the English dataset use a combination of lexical and semantic features. However, for Hindi data using only lexical features gave us the best results. We obtained weighted F1-measures of 0.5921 for the English Facebook task (ranked 12th), 0.5663 for the English Social Media task (ranked 6th), 0.6292 for the Hindi Facebook task (ranked 1st), and 0.4853 for the Hindi Social Media task (ranked 2nd).</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>safisamghabadi-EtAl:2018:W18-44</bibkey>
   </paper>

   <paper id="4403">
     <title>IRIT at TRAC 2018</title>
     <author><first>Faneva</first><last>Ramiandrisoa</last></author>
     <author><first>Josiane</first><last>Mothe</last></author>
     <booktitle>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>19–27</pages>
     <url>http://www.aclweb.org/anthology/W18-4403</url>
     <abstract>This paper describes the participation of the IRIT team to the TRAC 2018 shared task on Aggression Identification and more precisely to the shared task in English language. The three following methods have been used: a) a combination of machine learning techniques that relies on a set of features and document/text vectorization, b) Convolutional Neural Network (CNN) and c) a combination of Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM). Best results were obtained when using the method (a) on the English test data from Facebook which ranked our method sixteenth out of thirty teams, and the method (c) on the English test data from other social media, where we obtained the fifteenth rank out of thirty.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>ramiandrisoa-mothe:2018:W18-44</bibkey>
   </paper>

   <paper id="4404">
     <title>Fully Connected Neural Network with Advance Preprocessor to Identify Aggression over Facebook and Twitter</title>
     <author><first>Kashyap</first><last>Raiyani</last></author>
     <author><first>Teresa</first><last>Gonçalves</last></author>
     <author><first>Paulo</first><last>Quaresma</last></author>
     <author><first>Vitor Beires</first><last>Nogueira</last></author>
     <booktitle>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>28–41</pages>
     <url>http://www.aclweb.org/anthology/W18-4404</url>
     <abstract>Paper presents the different methodologies developed &amp; tested and discusses their results, with the goal of identifying the best possible method for the aggression identification problem in social media.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>raiyani-EtAl:2018:W18-44</bibkey>
   </paper>

   <paper id="4405">
     <title>Cyberbullying Intervention Based on Convolutional Neural Networks</title>
     <author><first>Qianjia</first><last>Huang</last></author>
     <author><first>Diana</first><last>Inkpen</last></author>
     <author><first>Jianhong</first><last>Zhang</last></author>
     <author><first>David</first><last>Van Bruwaene</last></author>
     <booktitle>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>42–51</pages>
     <url>http://www.aclweb.org/anthology/W18-4405</url>
     <abstract>This paper describes the process of building a cyberbullying intervention interface driven by a machine-learning based text-classification service. We make two main contributions. First, we show that cyberbullying can be identified in real-time before it takes place, with available machine learning and natural language processing tools. Second, we present a mechanism that provides individuals with early feedback about how other people would feel about wording choices in their messages before they are sent out. This interface not only gives a chance for the user to revise the text, but also provides a system-level flagging/intervention in a situation related to cyberbullying.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>huang-EtAl:2018:W18-44</bibkey>
   </paper>

   <paper id="4406">
     <title>LSTMs with Attention for Aggression Detection</title>
     <author><first>Nishant</first><last>Nikhil</last></author>
     <author><first>Ramit</first><last>Pahwa</last></author>
     <author><first>Mehul Kumar</first><last>Nirala</last></author>
     <author><first>Rohan</first><last>Khilnani</last></author>
     <booktitle>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>52–57</pages>
     <url>http://www.aclweb.org/anthology/W18-4406</url>
     <abstract>In this paper, we describe the system submitted for the shared task on Aggression Identification in Facebook posts and comments by the team Nishnik. Previous works demonstrate that LSTMs have achieved remarkable performance in natural language processing tasks. We deploy an LSTM model with an attention unit over it. Our system ranks 6th and 4th in the Hindi subtask for Facebook comments and subtask for generalized social media data respectively. And it ranks 17th and 10th in the corresponding English subtasks.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>nikhil-EtAl:2018:W18-44</bibkey>
   </paper>

   <paper id="4407">
     <title>TRAC-1 Shared Task on Aggression Identification: IIT(ISM)@COLING’18</title>
     <author><first>Ritesh</first><last>Kumar</last></author>
     <author><first>Guggilla</first><last>Bhanodai</last></author>
     <author><first>Rajendra</first><last>Pamula</last></author>
     <author><first>Maheshwar Reddy</first><last>Chennuru</last></author>
     <booktitle>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>58–65</pages>
     <url>http://www.aclweb.org/anthology/W18-4407</url>
     <abstract>This paper describes the work that our team bhanodaig did at Indian Institute of Technology (ISM) towards TRAC-1 Shared Task on Aggression Identification in Social Media for COLING 2018. In this paper we label aggression identification into three categories: Overtly Aggressive, Covertly Aggressive and Non-aggressive. We train a model to differentiate between these categories and then analyze the results in order to better understand how we can distinguish between them. We participated in two different tasks named as English (Facebook) task and English (Social Media) task. For English (Facebook) task System 05 was our best run (i.e. 0.3572) above the Random Baseline (i.e. 0.3535). For English (Social Media) task our system 02 got the value (i.e. 0.1960) below the Random Bseline (i.e. 0.3477). For all of our runs we used Long Short-Term Memory model. Overall, our performance is not satisfactory. However, as new entrant to the field, our scores are encouraging enough to work for better results in future.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>kumar-EtAl:2018:W18-442</bibkey>
   </paper>

   <paper id="4408">
     <title>An Ensemble Approach for Aggression Identification in English and Hindi Text</title>
     <author><first>Arjun</first><last>Roy</last></author>
     <author><first>Prashant</first><last>Kapil</last></author>
     <author><first>KINGSHUK</first><last>BASAK</last></author>
     <author><first>Asif</first><last>Ekbal</last></author>
     <booktitle>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>66–73</pages>
     <url>http://www.aclweb.org/anthology/W18-4408</url>
     <abstract>This paper describes our system submitted in the shared task at COLING 2018 TRAC-1: Aggression Identification. The objective of this task was to predict online aggression spread through online textual post or comment. The dataset was released in two languages, English and Hindi. We submitted a single system for Hindi and a single system for English. Both the systems are based on an ensemble architecture where the individual models are based on Convoluted Neural Network and Support Vector Machine. Evaluation shows promising results for both the languages.The total submission for English was 30 and Hindi was 15. Our system on English facebook and social media obtained F1 score of 0.5151 and 0.5099 respectively where Hindi facebook and social media obtained F1 score of 0.5599 and 0.3790 respectively.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>roy-EtAl:2018:W18-44</bibkey>
   </paper>

   <paper id="4409">
     <title>Aggression Identification and Multi Lingual Word Embeddings</title>
     <author><first>Thiago</first><last>Galery</last></author>
     <author><first>Efstathios</first><last>Charitos</last></author>
     <author><first>Ye</first><last>Tian</last></author>
     <booktitle>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>74–79</pages>
     <url>http://www.aclweb.org/anthology/W18-4409</url>
     <abstract>The system presented here took part in the 2018 Trolling, Aggression and Cyberbullying shared task (Forest and Trees team) and uses a Gated Recurrent Neural Network architecture (Cho et al., 2014) in an attempt to assess whether combining pre-trained English and Hindi fastText (Mikolov et al., 2018) word embeddings as a representation of the sequence input would improve classification performance. The motivation for this comes from the fact that the shared task data for English contained many Hindi tokens and therefore some users might be doing code-switching: the alternation between two or more languages in communication. To test this hypothesis, we also aligned Hindi and English vectors using pre-computed SVD matrices that pulls representations from different languages into a common space (Smith et al., 2017). Two conditions were tested: (i) one with standard pre-trained fastText word embeddings where each Hindi word is treated as an OOV token, and (ii) another where word embeddings for Hindi and English are loaded in a common vector space, so Hindi tokens can be assigned a meaningful representation. We submitted the second (i.e., multilingual) system and obtained the scores of 0.531 weighted F1 for the EN-FB dataset and 0.438 weighted F1 for the EN-TW dataset.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>galery-charitos-tian:2018:W18-44</bibkey>
   </paper>

   <paper id="4410">
     <title>A K-Competitive Autoencoder for Aggression Detection in Social Media Text</title>
     <author><first>Promita</first><last>Maitra</last></author>
     <author><first>Ritesh</first><last>Sarkhel</last></author>
     <booktitle>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>80–89</pages>
     <url>http://www.aclweb.org/anthology/W18-4410</url>
     <abstract>We present an approach to detect aggression from social media text in this work. A winner-takes-all autoencoder, called Emoti-KATE is proposed for this purpose. Using a log-normalized, weighted word-count vector at input dimensions, the autoencoder simulates a competition between neurons in the hidden layer to minimize the reconstruction loss between the input and final output layers. We have evaluated the performance of our system on the datasets provided by the organizers of TRAC workshop, 2018. Using the encoding generated by Emoti-KATE, a 3-way classification is performed for every social media text in the dataset. Each data point is classified as ‘Overtly Aggressive’, ‘Covertly Aggressive’ or ‘Non-aggressive’. Results show that our (team name: PMRS) proposed method is able to achieve promising results on some of these datasets. In this paper, we have described the effects of introducing an winner-takes-all autoencoder for the task of aggression detection, reported its performance on four different datasets, analyzed some of its limitations and how to improve its performance in future works.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>maitra-sarkhel:2018:W18-44</bibkey>
   </paper>

   <paper id="4411">
     <title>Aggression Detection in Social Media: Using Deep Neural Networks, Data Augmentation, and Pseudo Labeling</title>
     <author><first>Segun Taofeek</first><last>Aroyehun</last></author>
     <author><first>Alexander</first><last>Gelbukh</last></author>
     <booktitle>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>90–97</pages>
     <url>http://www.aclweb.org/anthology/W18-4411</url>
     <abstract>With the advent of the read-write web which facilitates social interactions in online spaces, the rise of anti-social behaviour in online spaces has attracted the attention of researchers. In this paper, we address the challenge of automatically identifying aggression in social media posts. Our team, saroyehun, participated in the English track of the Aggression Detection in Social Media Shared Task. On this task, we investigate the efficacy of deep neural network models of varying complexity. Our results reveal that deep neural network models require more data points to do better than an NBSVM linear baseline based on character n-grams. Our improved deep neural network models were trained on augmented data and pseudo labeled examples. Our LSTM classifier receives a weighted macro-F1 score of 0.6425 to rank first overall on the Facebook subtask of the shared task. On the social media sub-task, our CNN-LSTM model records a weighted macro-F1 score of 0.5920 to place third overall.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>aroyehun-gelbukh:2018:W18-44</bibkey>
   </paper>

   <paper id="4412">
     <title>Identifying Aggression and Toxicity in Comments using Capsule Network</title>
     <author><first>Saurabh</first><last>Srivastava</last></author>
     <author><first>Prerna</first><last>Khurana</last></author>
     <author><first>Vartika</first><last>Tewari</last></author>
     <booktitle>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>98–105</pages>
     <url>http://www.aclweb.org/anthology/W18-4412</url>
     <abstract>Aggression and related activities like trolling, hate speech etc. involve toxic comments in various forms. These are common scenarios in today’s time and websites react by shutting down their comment sections. To tackle this, an algorithmic solution is preferred to human moderation which is slow and expensive. In this paper, we propose a single model capsule network with focal loss to achieve this task which is suitable for production environment. Our model achieves competitive results over other strong baseline methods, which show its effectiveness and that focal loss exhibits significant improvement in such cases where class imbalance is a regular issue. Additionally, we show that the problem of extensive data preprocessing, data augmentation can be tackled by capsule networks implicitly. We achieve an overall ROC AUC of 98.46 on Kaggle-toxic comment dataset and show that it beats other architectures by a good margin. As comments tend to be written in more than one language, and transliteration is a common problem, we further show that our model handles this effectively by applying our model on TRAC shared task dataset which contains comments in code-mixed Hindi-English.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>srivastava-khurana-tewari:2018:W18-44</bibkey>
   </paper>

   <paper id="4413">
     <title>Degree based Classification of Harmful Speech using Twitter Data</title>
     <author><first>Sanjana</first><last>Sharma</last></author>
     <author><first>Saksham</first><last>Agrawal</last></author>
     <author><first>Manish</first><last>Shrivastava</last></author>
     <booktitle>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>106–112</pages>
     <url>http://www.aclweb.org/anthology/W18-4413</url>
     <abstract>Harmful speech has various forms and it has been plaguing the social media in different ways. If we need to crackdown different degrees of hate speech and abusive behavior amongst it, the classification needs to be based on complex ramifications which needs to be defined and hold accountable for, other than racist, sexist or against some particular group and community. This paper primarily describes how we created an ontological classification of harmful speech based on degree of hateful intent and used it to annotate twitter data accordingly. The key contribution of this paper is the new dataset of tweets we created based on ontological classes and degrees of harmful speech found in the text. We also propose supervised classification system for recognizing these respective harmful speech classes in the texts hence. This serves as a preliminary work to lay down foundation on defining different classes of harmful speech and subsequent work will be done in making it’s automatic detection more robust and efficient.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>sharma-agrawal-shrivastava:2018:W18-44</bibkey>
   </paper>

   <paper id="4414">
     <title>Aggressive Language Identification Using Word Embeddings and Sentiment Features</title>
     <author><first>Constantin</first><last>Orasan</last></author>
     <booktitle>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>113–119</pages>
     <url>http://www.aclweb.org/anthology/W18-4414</url>
     <abstract>This paper describes our participation in the First Shared Task on Aggression Identification. The method proposed relies on machine learning to identify social media texts which contain aggression. The main features employed by our method are information extracted from word embeddings and the output of a sentiment analyser. Several machine learning methods and different combinations of features were tried. The official submissions used Support Vector Machines and Random Forests. The official evaluation showed that for texts similar to the ones in the training dataset Random Forests work best, whilst for texts which are different SVMs are a better choice. The evaluation also showed that despite its simplicity the method performs well when compared with more elaborated methods.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>orasan:2018:W18-44</bibkey>
   </paper>

   <paper id="4415">
     <title>Aggression Detection in Social Media using Deep Neural Networks</title>
     <author><first>Sreekanth</first><last>Madisetty</last></author>
     <author><first>Maunendra</first><last>Sankar Desarkar</last></author>
     <booktitle>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>120–127</pages>
     <url>http://www.aclweb.org/anthology/W18-4415</url>
     <abstract>With the rise of user-generated content in social media coupled with almost non-existent moderation in many such systems, aggressive contents have been observed to rise in such forums. In this paper, we work on the problem of aggression detection in social media. Aggression can sometimes be expressed directly or overtly or it can be hidden or covert in the text. On the other hand, most of the content in social media is non-aggressive in nature. We propose an ensemble based system to classify an input post to into one of three classes, namely, Overtly Aggressive, Covertly Aggressive, and Non-aggressive. Our approach uses three deep learning methods, namely, Convolutional Neural Networks (CNN) with five layers (input, convolution, pooling, hidden, and output), Long Short Term Memory networks (LSTM), and Bi-directional Long Short Term Memory networks (Bi-LSTM). A majority voting based ensemble method is used to combine these classifiers (CNN, LSTM, and Bi-LSTM). We trained our method on Facebook comments dataset and tested on Facebook comments (in-domain) and other social media posts (cross-domain). Our system achieves the F1-score (weighted) of 0.604 for Facebook posts and 0.508 for social media posts.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>madisetty-sankardesarkar:2018:W18-44</bibkey>
   </paper>

   <paper id="4416">
     <title>Merging Datasets for Aggressive Text Identification</title>
     <author><first>Paula</first><last>Fortuna</last></author>
     <author><first>José</first><last>Ferreira</last></author>
     <author><first>Luiz</first><last>Pires</last></author>
     <author><first>Guilherme</first><last>Routar</last></author>
     <author><first>Sérgio</first><last>Nunes</last></author>
     <booktitle>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>128–139</pages>
     <url>http://www.aclweb.org/anthology/W18-4416</url>
     <abstract>This paper presents the approach of the team “groutar” to the shared task on Aggression Identification, considering the test sets in English, both from Facebook and general Social Media. This experiment aims to test the effect of merging new datasets in the performance of classification models. We followed a standard machine learning approach with training, validation, and testing phases, and considered features such as part-of-speech, frequencies of insults, punctuation, sentiment, and capitalization. In terms of algorithms, we experimented with Boosted Logistic Regression, Multi-Layer Perceptron, Parallel Random Forest and eXtreme Gradient Boosting. One question appearing was how to merge datasets using different classification systems (e.g. aggression vs. toxicity). Other issue concerns the possibility to generalize models and apply them to data from different social networks. Regarding these, we merged two datasets, and the results showed that training with similar data is an advantage in the classification of social networks data. However, adding data from different platforms, allowed slightly better results in both Facebook and Social Media, indicating that more generalized models can be an advantage.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>fortuna-EtAl:2018:W18-44</bibkey>
   </paper>

   <paper id="4417">
     <title>Cyberbullying Detection Task: the EBSI-LIA-UNAM System (ELU) at COLING’18 TRAC-1</title>
     <author><first>Ignacio</first><last>Arroyo-Fernández</last></author>
     <author><first>Dominic</first><last>Forest</last></author>
     <author><first>Juan-Manuel</first><last>Torres-Moreno</last></author>
     <author><first>Mauricio</first><last>Carrasco-Ruiz</last></author>
     <author><first>Thomas</first><last>Legeleux</last></author>
     <author><first>Karen</first><last>Joannette</last></author>
     <booktitle>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>140–149</pages>
     <url>http://www.aclweb.org/anthology/W18-4417</url>
     <abstract>The phenomenon of cyberbullying has growing in worrying proportions with the development of social networks. Forums and chat rooms are spaces where serious damage can now be done to others, while the tools for avoiding on-line spills are still limited. This study aims to as- sess the ability that both classical and state-of-the-art vector space modeling methods provide to well known learning machines to identify aggression levels in social network cyberbullying (i.e. social network posts manually labeled as Overtly Aggressive, Covertly Aggressive and Non- aggressive). To this end, an exploratory stage was performed first in order to find relevant settings to test, i.e. by using training and development samples, we trained multiple learning machines using multiple vector space modeling methods and discarded the less informative configurations. Finally, we selected the two best settings and their voting combination to form three compet- ing systems. These systems were submitted to the competition of the TRACK-1 task of the Workshop on Trolling, Aggression and Cyberbullying. Our voting combination system resulted second place in predicting Aggression levels on a test set of untagged social network posts.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>arroyofernndez-EtAl:2018:W18-44</bibkey>
   </paper>

   <paper id="4418">
     <title>Aggression Identification Using Deep Learning and Data Augmentation</title>
     <author><first>Julian</first><last>Risch</last></author>
     <author><first>Ralf</first><last>Krestel</last></author>
     <booktitle>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>150–158</pages>
     <url>http://www.aclweb.org/anthology/W18-4418</url>
     <abstract>Social media platforms allow users to share and discuss their opinions online. However, a minority of user posts is aggressive, thereby hinders respectful discussion, and — at an extreme level — is liable to prosecution. The automatic identification of such harmful posts is important, be- cause it can support the costly manual moderation of online discussions. Further, the automation allows unprecedented analyses of discussion datasets that contain millions of posts. This system description paper presents our submission to the First Shared Task on Aggression Identification. We propose to augment the provided dataset to increase the number of labeled comments from 15,000 to 60,000. Thereby, we introduce linguistic variety into the dataset. As a consequence of the larger amount of training data, we are able to train a special deep neural net, which generalizes especially well to unseen data. To further boost the performance, we combine this neural net with three logistic regression classifiers trained on character and word n-grams, and hand-picked syntactic features. This ensemble is more robust than the individual single models. Our team named “Julian” achieves an F1-score of 60% on both English datasets, 63% on the Hindi Facebook dataset, and 38% on the Hindi Twitter dataset.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>risch-krestel:2018:W18-441</bibkey>
   </paper>

   <paper id="4419">
     <title>Cyber-aggression Detection using Cross Segment-and-Concatenate Multi-Task Learning from Text</title>
     <author><first>Ahmed</first><last>Husseini Orabi</last></author>
     <author><first>Mahmoud</first><last>Husseini Orabi</last></author>
     <author><first>Qianjia</first><last>Huang</last></author>
     <author><first>Diana</first><last>Inkpen</last></author>
     <author><first>David</first><last>Van Bruwaene</last></author>
     <booktitle>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>159–165</pages>
     <url>http://www.aclweb.org/anthology/W18-4419</url>
     <abstract>In this paper, we propose a novel deep-learning architecture for text classification, named cross segment-and-concatenate multi-task learning (CSC-MTL). We use CSC-MTL to improve the performance of cyber-aggression detection from text. Our approach provides a robust shared feature representation for multi-task learning by detecting contrasts and similarities among polarity and neutral classes. We participated in the cyber-aggression shared task under the team name uOttawa. We report 59.74% F1 performance for the Facebook test set and 56.9% for the Twitter test set, for detecting aggression from text.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>husseiniorabi-EtAl:2018:W18-44</bibkey>
   </paper>

   <paper id="4420">
     <title>Delete or not Delete? Semi-Automatic Comment Moderation for the Newsroom</title>
     <author><first>Julian</first><last>Risch</last></author>
     <author><first>Ralf</first><last>Krestel</last></author>
     <booktitle>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>166–176</pages>
     <url>http://www.aclweb.org/anthology/W18-4420</url>
     <abstract>Comment sections of online news providers have enabled millions to share and discuss their opinions on news topics. Today, moderators ensure respectful and informative discussions by deleting not only insults, defamation, and hate speech, but also unverifiable facts. This process has to be transparent and comprehensive in order to keep the community engaged. Further, news providers have to make sure to not give the impression of censorship or dissemination of fake news. Yet manual moderation is very expensive and becomes more and more unfeasible with the increasing amount of comments. Hence, we propose a semi-automatic, holistic approach, which includes comment features but also their context, such as information about users and articles. For evaluation, we present experiments on a novel corpus of 3 million news comments annotated by a team of professional moderators.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>risch-krestel:2018:W18-442</bibkey>
   </paper>

   <paper id="4421">
     <title>Textual Aggression Detection through Deep Learning</title>
     <author><first>Antonela</first><last>Tommasel</last></author>
     <author><first>Juan Manuel</first><last>Rodriguez</last></author>
     <author><first>Daniela</first><last>Godoy</last></author>
     <booktitle>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>177–187</pages>
     <url>http://www.aclweb.org/anthology/W18-4421</url>
     <abstract>Cyberbullying and cyberaggression are serious and widespread issues increasingly affecting Internet users. With the widespread of social media networks, bullying, once limited to particular places, can now occur anytime and anywhere. Cyberaggression refers to aggressive online behaviour that aims at harming other individuals, and involves rude, insulting, offensive, teasing or demoralising comments through online social media. Considering the dangerous consequences that cyberaggression has on its victims and its rapid spread amongst internet users (specially kids and teens), it is crucial to understand how cyberbullying occurs to prevent it from escalating. Given the massive information overload on the Web, there is an imperious need to develop intelligent techniques to automatically detect harmful content, which would allow the large-scale social media monitoring and early detection of undesired situations. This paper presents the Isistanitos’s approach for detecting aggressive content in multiple social media sites. The approach is based on combining Support Vector Machines and Recurrent Neural Network models for analysing a wide-range of character, word, word embeddings, sentiment and irony features. Results confirmed the difficulty of the task (particularly for detecting covert aggressions), showing the limitations of traditionally used features.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>tommasel-rodriguez-godoy:2018:W18-44</bibkey>
   </paper>

   <paper id="4422">
     <title>Combining Shallow and Deep Learning for Aggressive Text Detection</title>
     <author><first>Viktor</first><last>Golem</last></author>
     <author><first>Mladen</first><last>Karan</last></author>
     <author><first>Jan</first><last>Šnajder</last></author>
     <booktitle>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>188–198</pages>
     <url>http://www.aclweb.org/anthology/W18-4422</url>
     <abstract>We describe the participation of team TakeLab in the aggression detection shared task at the TRAC1 workshop for English. Aggression manifests in a variety of ways. Unlike some forms of aggression that are impossible to prevent in day-to-day life, aggressive speech abounding on social networks could in principle be prevented or at least reduced by simply disabling users that post aggressively worded messages. The first step in achieving this is to detect such messages. The task, however, is far from being trivial, as what is considered as aggressive speech can be quite subjective, and the task is further complicated by the noisy nature of user-generated text on social networks. Our system learns to distinguish between open aggression, covert aggression, and non-aggression in social media texts. We tried different machine learning approaches, including traditional (shallow) machine learning models, deep learning models, and a combination of both. We achieved respectable results, ranking 4th and 8th out of 31 submissions on the Facebook and Twitter test sets, respectively.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>golem-karan-najder:2018:W18-44</bibkey>
   </paper>

   <paper id="4423">
     <title>Filtering Aggression from the Multilingual Social Media Feed</title>
     <author><first>sandip</first><last>modha</last></author>
     <author><first>Prasenjit</first><last>Majumder</last></author>
     <author><first>Thomas</first><last>Mandl</last></author>
     <booktitle>Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>199–207</pages>
     <url>http://www.aclweb.org/anthology/W18-4423</url>
     <abstract>This paper describes the participation of team DA-LD-Hildesheim from the Information Retrieval Lab(IRLAB) at DA-IICT Gandhinagar, India in collaboration with the University of Hildesheim, Germany and LDRP-ITR, Gandhinagar, India in a shared task on Aggression Identification workshop in COLING 2018. The objective of the shared task is to identify the level of aggression from the User-Generated contents within Social media written in English, Devnagiri Hindi and Romanized Hindi. Aggression levels are categorized into three predefined classes namely: ‘Overtly Aggressive‘, ‘Covertly Aggressive‘ and ‘Non-aggressive‘. The participating teams are required to develop a multi-class classifier which classifies User-generated content into these pre-defined classes. Instead of relying on a bag-of-words model, we have used pre-trained vectors for word embedding. We have performed experiments with standard machine learning classifiers. In addition, we have developed various deep learning models for the multi-class classification problem. Using the validation data, we found that validation accuracy of our deep learning models outperform all standard machine learning classifiers and voting based ensemble techniques and results on test data support these findings. We have also found that hyper-parameters of the deep neural network are the keys to improve the results.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>modha-majumder-mandl:2018:W18-44</bibkey>
   </paper>

   <paper id="4500">
     <title>Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</title>
     <editor><first>Beatrice</first><last>Alex</last></editor>
     <editor><first>Stefania</first><last>Degaetano-Ortlieb</last></editor>
     <editor><first>Anna</first><last>Feldman</last></editor>
     <editor><first>Anna</first><last>Kazantseva</last></editor>
     <editor><first>Nils</first><last>Reiter</last></editor>
     <editor><first>Stan</first><last>Szpakowicz</last></editor>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-45</url>
     <bibtype>book</bibtype>
     <bibkey>W18-45:2018</bibkey>
   </paper>

   <paper id="4501">
     <title>Learning Diachronic Analogies to Analyze Concept Change</title>
     <author><first>Matthias</first><last>Orlikowski</last></author>
     <author><first>Matthias</first><last>Hartung</last></author>
     <author><first>Philipp</first><last>Cimiano</last></author>
     <booktitle>Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–11</pages>
     <url>http://www.aclweb.org/anthology/W18-4501</url>
     <abstract>We propose to study the evolution of concepts by learning to complete diachronic analogies between lists of terms which relate to the same concept at different points in time. We present a number of models based on operations on word embedddings that correspond to different assumptions about the characteristics of diachronic analogies and change in concept vocabularies. These are tested in a quantitative evaluation for nine different concepts on a corpus of Dutch newspapers from the 1950s and 1980s. We show that a model which treats the concept terms as analogous and learns weights to compensate for diachronic changes (weighted linear combination) is able to more accurately predict the missing term than a learned transformation and two baselines for most of the evaluated concepts. We also find that all models tend to be coherent in relation to the represented concept, but less discriminative in regard to other concepts. Additionally, we evaluate the effect of aligning the time-specific embedding spaces using orthogonal Procrustes, finding varying effects on performance, depending on the model, concept and evaluation metric. For the weighted linear combination, however, results improve with alignment in a majority of cases. All related code is released publicly.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>orlikowski-hartung-cimiano:2018:W18-45</bibkey>
   </paper>

   <paper id="4502">
     <title>A Linked Coptic Dictionary Online</title>
     <author><first>Frank</first><last>Feder</last></author>
     <author><first>Maxim</first><last>Kupreyev</last></author>
     <author><first>Emma</first><last>Manning</last></author>
     <author><first>Caroline T.</first><last>Schroeder</last></author>
     <author><first>Amir</first><last>Zeldes</last></author>
     <booktitle>Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>12–21</pages>
     <url>http://www.aclweb.org/anthology/W18-4502</url>
     <abstract>We describe a new project publishing a freely available online dictionary for Coptic. The dictionary encompasses comprehensive cross-referencing mechanisms, including linking entries to an online scanned edition of Crum’s Coptic Dictionary, internal cross-references and etymological information, translated searchable definitions in English, French and German, and linked corpus data which provides frequencies and corpus look-up for headwords and multiword expressions. Headwords are available for linking in external projects using a REST API. We describe the challenges in encoding our dictionary using TEI XML and implementing linking mechanisms to construct a Web interface querying frequency information, which draw on NLP tools to recognize inflected forms in context. We evaluate our dictionary’s coverage using digital corpora of Coptic available online.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>feder-EtAl:2018:W18-45</bibkey>
   </paper>

   <paper id="4503">
     <title>Using relative entropy for detection and analysis of periods of diachronic linguistic change</title>
     <author><first>Stefania</first><last>Degaetano-Ortlieb</last></author>
     <author><first>Elke</first><last>Teich</last></author>
     <booktitle>Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>22–33</pages>
     <url>http://www.aclweb.org/anthology/W18-4503</url>
     <abstract>We present a data-driven approach to detect periods of linguistic change and the lexical and grammatical features contributing to change. We focus on the development of scientific English in the late modern period. Our approach is based on relative entropy (Kullback-Leibler Divergence) comparing temporally adjacent periods and sliding over the time line from past to present. Using a diachronic corpus of scientific publications of the Royal Society of London, we show how periods of change reflect the interplay between lexis and grammar, where periods of lexical expansion are typically followed by periods of grammatical consolidation resulting in a balance between expressivity and communicative efficiency. Our method is generic and can be applied to other data sets, languages and time ranges.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>degaetanoortlieb-teich:2018:W18-45</bibkey>
   </paper>

   <paper id="4504">
     <title>Cliche Expressions in Literary and Genre Novels</title>
     <author><first>Andreas</first><last>van Cranenburgh</last></author>
     <booktitle>Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>34–43</pages>
     <url>http://www.aclweb.org/anthology/W18-4504</url>
     <abstract>Should writers “avoid clichés like the plague”? Clichés are said to be a prominent characteristic of “low brow” literature, and conversely, a negative marker of “high brow” literature. Clichés may concern the storyline, the characters, or the style of writing. We focus on cliché expressions, ready-made stock phrases which can be taken as a sign of uncreative writing. We present a corpus study in which we examine to what extent cliché expressions can be attested in a corpus of various kinds of contemporary fiction, based on a large, curated lexicon of cliché expressions. The results show to what extent the negative view on clichés is supported by data: we find a significant negative correlation of -0.48 between cliché density and literary ratings of texts. We also investigate interactions with genre and characterize the language of clichés with several basic textual features. Code used for this paper is available at https://github.com/andreasvc/litcliches/</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>vancranenburgh:2018:W18-45</bibkey>
   </paper>

   <paper id="4505">
     <title>Analysis of Rhythmic Phrasing: Feature Engineering vs. Representation Learning for Classifying Readout Poetry</title>
     <author><first>Timo</first><last>Baumann</last></author>
     <author><first>Hussein</first><last>Hussein</last></author>
     <author><first>Burkhard</first><last>Meyer-Sickendiek</last></author>
     <booktitle>Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>44–49</pages>
     <url>http://www.aclweb.org/anthology/W18-4505</url>
     <abstract>We show how to classify the phrasing of readout poems with the help of machine learning algorithms that use manually engineered features or automatically learn representations. We investigate modern and postmodern poems from the webpage lyrikline, and focus on two exemplary rhythmical patterns in order to detect the rhythmic phrasing: The Parlando and the Variable Foot. These rhythmical patterns have been compared by using two important theoretical works: The Generative Theory of Tonal Music and the Rhythmic Phrasing in English Verse. Using both, we focus on a combination of four different features: The grouping structure, the metrical structure, the time-span-variation, and the prolongation in order to detect the rhythmic phrasing in the two rhythmical types. We use manually engineered features based on text-speech alignment and parsing for classification. We also train a neural network to learn its own representation based on text, speech and audio during pauses. The neural network outperforms manual feature engineering, reaching an f-measure of 0.85.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>baumann-hussein-meyersickendiek:2018:W18-45</bibkey>
   </paper>

   <paper id="4506">
     <title>Cross-Discourse and Multilingual Exploration of Textual Corpora with the DualNeighbors Algorithm</title>
     <author><first>Taylor</first><last>Arnold</last></author>
     <author><first>Lauren</first><last>Tilton</last></author>
     <booktitle>Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>50–59</pages>
     <url>http://www.aclweb.org/anthology/W18-4506</url>
     <abstract>Word choice is dependent on the cultural context of writers and their subjects. Different words are used to describe similar actions, objects, and features based on factors such as class, race, gender, geography and political affinity. Exploratory techniques based on locating and counting words may, therefore, lead to conclusions that reinforce culturally inflected boundaries. We offer a new method, the DualNeighbors algorithm, for linking thematically similar documents both within and across discursive and linguistic barriers to reveal cross-cultural connections. Qualitative and quantitative evaluations of this technique are shown as applied to two cultural datasets of interest to researchers across the humanities and social sciences. An open-source implementation of the DualNeighbors algorithm is provided to assist in its application.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>arnold-tilton:2018:W18-45</bibkey>
   </paper>

   <paper id="4507">
     <title>The Historical Significance of Textual Distances</title>
     <author><first>Ted</first><last>Underwood</last></author>
     <booktitle>Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>60–69</pages>
     <url>http://www.aclweb.org/anthology/W18-4507</url>
     <abstract>Measuring similarity is a basic task in information retrieval, and now often a building-block for more complex arguments about cultural change. But do measures of textual similarity and distance really correspond to evidence about cultural proximity and differentiation? To explore that question empirically, this paper compares textual and social measures of the similarities between genres of English-language fiction. Existing measures of textual similarity (cosine similarity on tf-idf vectors or topic vectors) are also compared to new strategies that strive to anchor textual measurement in a social context.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>underwood:2018:W18-45</bibkey>
   </paper>

   <paper id="4508">
     <title>One Size Fits All? A simple LSTM for non-literal token and construction-level classification</title>
     <author><first>Erik-Lân</first><last>Do Dinh</last></author>
     <author><first>Steffen</first><last>Eger</last></author>
     <author><first>Iryna</first><last>Gurevych</last></author>
     <booktitle>Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>70–80</pages>
     <url>http://www.aclweb.org/anthology/W18-4508</url>
     <abstract>In this paper, we tackle four different tasks of non-literal language classification: token and construction level metaphor detection, classification of idiomatic use of infinitive-verb compounds, and classification of non-literal particle verbs. One of the tasks operates on the token level, while the three other tasks classify constructions such as “hot topic” or “stehen lassen” (“to allow sth. to stand” vs. “to abandon so.”). The two metaphor detection tasks are in English, while the two non-literal language detection tasks are in German. We propose a simple context-encoding LSTM model and show that it outperforms the state-of-the-art on two tasks. Additionally, we experiment with different embeddings for the token level metaphor detection task and find that 1) their performance varies according to the genre, and 2) word2vec embeddings perform best on 3 out of 4 genres, despite being one of the simplest tested model. In summary, we present a large-scale analysis of a neural model for non-literal language classification (i) at different granularities, (ii) in different languages, (iii) over different non-literal language phenomena.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>dodinh-eger-gurevych:2018:W18-45</bibkey>
   </paper>

   <paper id="4509">
     <title>Supervised Rhyme Detection with Siamese Recurrent Networks</title>
     <author><first>Thomas</first><last>Haider</last></author>
     <author><first>Jonas</first><last>Kuhn</last></author>
     <booktitle>Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>81–86</pages>
     <url>http://www.aclweb.org/anthology/W18-4509</url>
     <abstract>We present the first supervised approach to rhyme detection with Siamese Recurrent Networks (SRN) that offer near perfect performance (97% accuracy) with a single model on rhyme pairs for German, English and French, allowing future large scale analyses. SRNs learn a similarity metric on variable length character sequences that can be used as judgement on the distance of imperfect rhyme pairs and for binary classification. For training, we construct a diachronically balanced rhyme goldstandard of New High German (NHG) poetry. For further testing, we sample a second collection of NHG poetry and set of contemporary Hip-Hop lyrics, annotated for rhyme and assonance. We train several high-performing SRN models and evaluate them qualitatively on selected sonnetts.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>haider-kuhn:2018:W18-45</bibkey>
   </paper>

   <paper id="4510">
     <title>Normalizing Early English Letters to Present-day English Spelling</title>
     <author><first>Mika</first><last>Hämäläinen</last></author>
     <author><first>Tanja</first><last>Säily</last></author>
     <author><first>Jack</first><last>Rueter</last></author>
     <author><first>Jörg</first><last>Tiedemann</last></author>
     <author><first>Eetu</first><last>Mäkelä</last></author>
     <booktitle>Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>87–96</pages>
     <url>http://www.aclweb.org/anthology/W18-4510</url>
     <abstract>This paper presents multiple methods for normalizing the most deviant and infrequent historical spellings in a corpus consisting of personal correspondence from the 15th to the 19th century. The methods include machine translation (neural and statistical), edit distance and rule-based FST. Different normalization methods are compared and evaluated. All of the methods have their own strengths in word normalization. This calls for finding ways of combining the results from these methods to leverage their individual strengths.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>hmlinen-EtAl:2018:W18-45</bibkey>
   </paper>

   <paper id="4511">
     <title>Power Networks: A Novel Neural Architecture to Predict Power Relations</title>
     <author><first>Michelle</first><last>Lam</last></author>
     <author><first>Catherina</first><last>Xu</last></author>
     <author><first>Vinodkumar</first><last>Prabhakaran</last></author>
     <booktitle>Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>97–102</pages>
     <url>http://www.aclweb.org/anthology/W18-4511</url>
     <abstract>Can language analysis reveal the underlying social power relations that exist between participants of an interaction? Prior work within NLP has shown promise in this area, but the performance of automatically predicting power relations using NLP analysis of social interactions remains wanting. In this paper, we present a novel neural architecture that captures manifestations of power within individual emails which are then aggregated in an order-preserving way in order to infer the direction of power between pairs of participants in an email thread. We obtain an accuracy of 80.4%, a 10.1% improvement over state-of-the-art methods, in this task. We further apply our model to the task of predicting power relations between individuals based on the entire set of messages exchanged between them; here also, our model significantly outperforms the 70.0% accuracy using prior state-of-the-art techniques, obtaining an accuracy of 83.0%.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>lam-xu-prabhakaran:2018:W18-45</bibkey>
   </paper>

   <paper id="4512">
     <title>Automated Acquisition of Patterns for Coding Political Event Data: Two Case Studies</title>
     <author><first>Peter</first><last>Makarov</last></author>
     <booktitle>Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>103–112</pages>
     <url>http://www.aclweb.org/anthology/W18-4512</url>
     <abstract>We present a simple approach to the generation and labeling of extraction patterns for coding political event data, an important task in computational social science. We use weak supervision to identify pattern candidates and learn distributed representations for them. Given seed extraction patterns from existing pattern dictionaries, we use label propagation to label pattern candidates. We present two case studies. i) We derive patterns of acceptable quality for a number of international relations &amp; conflicts categories using pattern candidates of O’Connor et al (2013). ii) We derive patterns for coding protest events that outperform an established set of Tabari / Petrarch hand-crafted patterns.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>makarov:2018:W18-45</bibkey>
   </paper>

   <paper id="4513">
     <title>A Method for Human-Interpretable Paraphrasticality Prediction</title>
     <author><first>Maria</first><last>Moritz</last></author>
     <author><first>Johannes</first><last>Hellrich</last></author>
     <author><first>Sven</first><last>Büchel</last></author>
     <booktitle>Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>113–118</pages>
     <url>http://www.aclweb.org/anthology/W18-4513</url>
     <abstract>The detection of reused text is important in a wide range of disciplines. However, even as research in the field of plagiarism detection is constantly improving, heavily modified or paraphrased text is still challenging for current methodologies. For historical texts, these problems are even more severe, since text sources were often subject to stronger and more frequent modifications. Despite the need for tools to automate text criticism, e.g., tracing modifications in historical text, algorithmic support is still limited. While current techniques can tell if and how frequently a text has been modified, very little work has been done on determining the degree and kind of paraphrastic modification—despite such information being of substantial interest to scholars. We present a human-interpretable, feature-based method to measure paraphrastic modification. Evaluating our technique on three data sets, we find that our approach performs competitive to text similarity scores borrowed from machine translation evaluation, being much harder to interpret.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>moritz-hellrich-bchel:2018:W18-45</bibkey>
   </paper>

   <paper id="4514">
     <title>Exploring word embeddings and phonological similarity for the unsupervised correction of language learner errors</title>
     <author><first>Ildikó</first><last>Pilán</last></author>
     <author><first>Elena</first><last>Volodina</last></author>
     <booktitle>Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>119–128</pages>
     <url>http://www.aclweb.org/anthology/W18-4514</url>
     <abstract>The presence of misspellings and other errors or non-standard word forms poses a considerable challenge for NLP systems. Although several supervised approaches have been proposed previously to normalize these, annotated training data is scarce for many languages. We investigate, therefore, an unsupervised method where correction candidates for Swedish language learners’ errors are retrieved from word embeddings. Furthermore, we compare the usefulness of combining cosine similarity with orthographic and phonological similarity based on a neural grapheme-to-phoneme conversion system we train for this purpose. Although combinations of similarity measures have been explored for finding error correction candidates, it remains unclear how these measures relate to each other and how much they contribute individually to identifying the correct alternative. We experiment with different combinations of these and find that integrating phonological information is especially useful when the majority of learner errors are related to misspellings, but less so when errors are of a variety of types including, e.g. grammatical errors.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>piln-volodina:2018:W18-45</bibkey>
   </paper>

   <paper id="4515">
     <title>Towards Coreference for Literary Text: Analyzing Domain-Specific Phenomena</title>
     <author><first>Ina</first><last>Roesiger</last></author>
     <author><first>Sarah</first><last>Schulz</last></author>
     <author><first>Nils</first><last>Reiter</last></author>
     <booktitle>Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>129–138</pages>
     <url>http://www.aclweb.org/anthology/W18-4515</url>
     <abstract>Coreference resolution is the task of grouping together references to the same discourse entity. Resolving coreference in literary texts could benefit a number of Digital Humanities (DH) tasks, such as analyzing the depiction of characters and/or their relations. Domain-dependent training data has shown to improve coreference resolution for many domains, e.g. the biomedical domain, as its properties differ significantly from news text or dialogue, on which automatic systems are typically trained. Literary texts could also benefit from corpora annotated with coreference. We therefore analyze the specific properties of coreference-related phenomena on a number of texts and give directions for the adaptation of annotation guidelines. As some of the adaptations have profound impact, we also present a new annotation tool for coreference, with a focus on enabling annotation of long texts with many discourse entities.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>roesiger-schulz-reiter:2018:W18-45</bibkey>
   </paper>

   <paper id="4516">
     <title>An Evaluation of Lexicon-based Sentiment Analysis Techniques for the Plays of Gotthold Ephraim Lessing</title>
     <author><first>Thomas</first><last>Schmidt</last></author>
     <author><first>Manuel</first><last>Burghardt</last></author>
     <booktitle>Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>139–149</pages>
     <url>http://www.aclweb.org/anthology/W18-4516</url>
     <abstract>We present results from a project in the research area of sentiment analysis of drama texts, more concretely the plays of Gotthold Ephraim Lessing. We conducted an annotation study to create a gold standard for a systematic evaluation. The gold standard consists of 200 speeches of Lessing’s plays manually annotated with sentiment information. We explore the performance of different German sentiment lexicons and processing configurations like lemmatization, the extension of lexicons with historical linguistic variants or stop words elimination to explore the influence of these parameters and find best practices for our domain of application. The best performing configuration accomplishes an accuracy of 70%. We discuss the problems and challenges for sentiment analysis in this area and describe our next steps toward further research.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>schmidt-burghardt:2018:W18-45</bibkey>
   </paper>

   <paper id="4517">
     <title>Automatic identification of unknown names with specific roles</title>
     <author><first>Samia</first><last>Touileb</last></author>
     <author><first>Truls</first><last>Pedersen</last></author>
     <author><first>Helle</first><last>Sjøvaag</last></author>
     <booktitle>Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>150–158</pages>
     <url>http://www.aclweb.org/anthology/W18-4517</url>
     <abstract>Automatically identifying persons in a particular role within a large corpus can be a difficult task, especially if you don’t know who you are actually looking for. Resources compiling names of persons can be available, but no exhaustive lists exist. However, such lists usually contain known names that are “visible” in the national public sphere, and tend to ignore the marginal and international ones. In this article we propose a method for automatically generating suggestions of names found in a corpus of Norwegian news articles, and which “naturally” belong to a given initial list of members, and that were not known (compiled in a list) beforehand. The approach is based, in part, on the assumption that surface level syntactic features reveal parts of the underlying semantic content and can help uncover the structure of the language.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>touileb-pedersen-sjvaag:2018:W18-45</bibkey>
   </paper>

   <paper id="4518">
     <title>Induction of a Large-Scale Knowledge Graph from the Regesta Imperii</title>
     <author><first>Juri</first><last>Opitz</last></author>
     <author><first>Leo</first><last>Born</last></author>
     <author><first>Vivi</first><last>Nastase</last></author>
     <booktitle>Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>159–168</pages>
     <url>http://www.aclweb.org/anthology/W18-4518</url>
     <abstract>We induce and visualize a Knowledge Graph over the Regesta Imperii (RI), an important large-scale resource for medieval history research. The RI comprise more than 150,000 digitized abstracts of medieval charters issued by the Roman-German kings and popes distributed over many European locations and a time span of more than 700 years. Our goal is to provide a resource for historians to visualize and query the RI, possibly aiding medieval history research. The resulting medieval graph and visualization tools are shared publicly.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>opitz-born-nastase:2018:W18-45</bibkey>
   </paper>

   <paper id="4600">
     <title>Proceedings of the Workshop on Linguistic Complexity and Natural Language Processing</title>
     <editor><first>Leonor</first><last>Becerra-Bonache</last></editor>
     <editor><first>M. Dolores</first><last>Jiménez-López</last></editor>
     <editor><first>Carlos</first><last>Martín-Vide</last></editor>
     <editor><first>Adrià</first><last>Torrens-Urrutia</last></editor>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New-Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-46</url>
     <bibtype>book</bibtype>
     <bibkey>W18-46:2018</bibkey>
   </paper>

   <paper id="4601">
     <title>A Gold Standard to Measure Relative Linguistic Complexity with a Grounded Language Learning Model</title>
     <author><first>Leonor</first><last>Becerra-Bonache</last></author>
     <author><first>Henning</first><last>Christiansen</last></author>
     <author><first>M. Dolores</first><last>Jiménez-López</last></author>
     <booktitle>Proceedings of the Workshop on Linguistic Complexity and Natural Language Processing</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New-Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–9</pages>
     <url>http://www.aclweb.org/anthology/W18-4601</url>
     <abstract>This paper focuses on linguistic complexity from a relative perspective. It presents a grounded language learning system that can be used to study linguistic complexity from a developmental point of view and introduces a tool for generating a gold standard in order to evaluate the performance of the learning system. In general, researchers agree that it is more feasible to approach complexity from an objective or theory-oriented viewpoint than from a subjective or user-related point of view. Studies that have adopted a relative complexity approach have showed some preferences for L2 learners. In this paper, we try to show that computational models of the process of language acquisition may be an important tool to consider children and the process of first language acquisition as suitable candidates for evaluating the complexity of languages.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>becerrabonache-christiansen-jimnezlpez:2018:W18-46</bibkey>
   </paper>

   <paper id="4602">
     <title>Computational Complexity of Natural Languages: A Reasoned Overview</title>
     <author><first>António</first><last>Branco</last></author>
     <booktitle>Proceedings of the Workshop on Linguistic Complexity and Natural Language Processing</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New-Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>10–19</pages>
     <url>http://www.aclweb.org/anthology/W18-4602</url>
     <abstract>There has been an upsurge of research interest in natural language complexity. As this interest will benefit from being informed by established contributions in this area, this paper presents a reasoned overview of central results concerning the computational complexity of natural language parsing. This overview also seeks to help to understand why, contrary to recent and widespread assumptions, it is by no means sufficient that an agent handles sequences of items under a pattern <tex-math>a^n b^n</tex-math> or under a pattern <tex-math>a^n b^m c^n d^m</tex-math> to ascertain ipso facto that this is the result of at least an underlying context-free grammar or an underlying context-sensitive grammar, respectively. In addition, it seeks to help to understand why it is also not sufficient that an agent handles sequences of items under a pattern <tex-math>a^n b^n</tex-math> for it to be deemed as having a cognitive capacity of higher computational complexity.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>branco:2018:W18-46</bibkey>
   </paper>

   <paper id="4603">
     <title>Modeling Violations of Selectional Restrictions with Distributional Semantics</title>
     <author><first>Emmanuele</first><last>Chersoni</last></author>
     <author><first>Adrià</first><last>Torrens Urrutia</last></author>
     <author><first>Philippe</first><last>Blache</last></author>
     <author><first>Alessandro</first><last>Lenci</last></author>
     <booktitle>Proceedings of the Workshop on Linguistic Complexity and Natural Language Processing</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New-Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>20–29</pages>
     <url>http://www.aclweb.org/anthology/W18-4603</url>
     <abstract>Distributional Semantic Models have been successfully used for modeling selectional preferences in a variety of scenarios, since distributional similarity naturally provides an estimate of the degree to which an argument satisfies the requirement of a given predicate. However, we argue that the performance of such models on rare verb-argument combinations has received relatively little attention: it is not clear whether they are able to distinguish the combinations that are simply atypical, or implausible, from the semantically anomalous ones, and in particular, they have never been tested on the task of modeling their differences in processing complexity. In this paper, we compare two different models of thematic fit by testing their ability of identifying violations of selectional restrictions in two datasets from the experimental studies.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>chersoni-EtAl:2018:W18-46</bibkey>
   </paper>

   <paper id="4604">
     <title>Comparing morphological complexity of Spanish, Otomi and Nahuatl</title>
     <author><first>Ximena</first><last>Gutierrez-Vasques</last></author>
     <author><first>Victor</first><last>Mijangos</last></author>
     <booktitle>Proceedings of the Workshop on Linguistic Complexity and Natural Language Processing</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New-Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>30–37</pages>
     <url>http://www.aclweb.org/anthology/W18-4604</url>
     <abstract>We use two small parallel corpora for comparing the morphological complexity of Spanish, Otomi and Nahuatl. These are languages that belong to different linguistic families, the latter are low-resourced. We take into account two quantitative criteria, on one hand the distribution of types over tokens in a corpus, on the other, perplexity and entropy as indicators of word structure predictability. We show that a language can be complex in terms of how many different morphological word forms can produce, however, it may be less complex in terms of predictability of its internal structure of words.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>gutierrezvasques-mijangos:2018:W18-46</bibkey>
   </paper>

   <paper id="4605">
     <title>Uniform Information Density Effects on Syntactic Choice in Hindi</title>
     <author><first>Ayush</first><last>Jain</last></author>
     <author><first>Vishal</first><last>Singh</last></author>
     <author><first>Sidharth</first><last>Ranjan</last></author>
     <author><first>Rajakrishnan</first><last>Rajkumar</last></author>
     <author><first>Sumeet</first><last>Agarwal</last></author>
     <booktitle>Proceedings of the Workshop on Linguistic Complexity and Natural Language Processing</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New-Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>38–48</pages>
     <url>http://www.aclweb.org/anthology/W18-4605</url>
     <abstract>According to the UNIFORM INFORMATION DENSITY (UID) hypothesis (Levy and Jaeger, 2007; Jaeger, 2010), speakers tend to distribute information density across the signal uniformly while producing language. The prior works cited above studied syntactic reduction in language production at particular choice points in a sentence. In contrast, we use a variant of the above UID hypothesis in order to investigate the extent to which word order choices in Hindi are influenced by the drive to minimize the variance of information across entire sentences. To this end, we propose multiple lexical and syntactic measures (at both word and constituent levels) to capture the uniform spread of information across a sentence. Subsequently, we incorporate these measures in machine learning models aimed to distinguish between a naturally occurring corpus sentence and its grammatical variants (expressing the same idea). Our results indicate that our UID measures are not a significant factor in predicting the corpus sentence in the presence of lexical surprisal, a competing control predictor. Finally, in the light of other recent works, we conclude with a discussion of reasons for UID not being suitable for a theory of word order.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>jain-EtAl:2018:W18-46</bibkey>
   </paper>

   <paper id="4606">
     <title>Investigating the importance of linguistic complexity features across different datasets related to language learning</title>
     <author><first>Ildikó</first><last>Pilán</last></author>
     <author><first>Elena</first><last>Volodina</last></author>
     <booktitle>Proceedings of the Workshop on Linguistic Complexity and Natural Language Processing</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New-Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>49–58</pages>
     <url>http://www.aclweb.org/anthology/W18-4606</url>
     <abstract>We present the results of our investigations aiming at identifying the most informative linguistic complexity features for classifying language learning levels in three different datasets. The datasets vary across two dimensions: the size of the instances (texts vs. sentences) and the language learning skill they involve (reading comprehension texts vs. texts written by learners themselves). We present a subset of the most predictive features for each dataset, taking into consideration significant differences in their per-class mean values and show that these subsets lead not only to simpler models, but also to an improved classification performance. Furthermore, we pinpoint fourteen central features that are good predictors regardless of the size of the linguistic unit analyzed or the skills involved, which include both morpho-syntactic and lexical dimensions.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>piln-volodina:2018:W18-46</bibkey>
   </paper>

   <paper id="4607">
     <title>An Approach to Measuring Complexity with a Fuzzy Grammar &amp; Degrees of Grammaticality</title>
     <author><first>Adrià</first><last>Torrens Urrutia</last></author>
     <booktitle>Proceedings of the Workshop on Linguistic Complexity and Natural Language Processing</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New-Mexico</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>59–67</pages>
     <url>http://www.aclweb.org/anthology/W18-4607</url>
     <abstract>This paper presents an approach to evaluate complexity of a given natural language input by means of a Fuzzy Grammar with some fuzzy logic formulations. Usually, the approaches in linguistics has described a natural language grammar by means of discrete terms. However, a grammar can be explained in terms of degrees by following the concepts of linguistic gradience &amp; fuzziness. Understanding a grammar as a fuzzy or gradient object allows us to establish degrees of grammaticality for every linguistic input. This shall be meaningful for linguistic complexity considering that the less grammatical an input is the more complex its processing will be. In this regard, the degree of complexity of a linguistic input (which is a linguistic representation of a natural language expression) depends on the chosen grammar. The bases of the fuzzy grammar are shown here. Some of these are described by Fuzzy Type Theory. The linguistic inputs are characterized by constraints through a Property Grammar.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>torrensurrutia:2018:W18-46</bibkey>
   </paper>

   <paper id="4700">
     <title>Proceedings 14th Joint ACL - ISO Workshop on Interoperable Semantic Annotation</title>
     <editor><first>Harry</first><last>Bunt</last></editor>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-47</url>
     <bibtype>book</bibtype>
     <bibkey>W18-47</bibkey>
   </paper>

   <paper id="4701">
     <title>DialEdit: Annotations for Spoken Conversational Image Editing</title>
     <author><first>Ramesh</first><last>Manuvirakurike</last></author>
     <author><first>Jacqueline</first><last>Brixey</last></author>
     <author><first>Trung</first><last>Bui</last></author>
     <author><first>Walter</first><last>Chang</last></author>
     <author><first>Ron</first><last>Artstein</last></author>
     <author><first>Kalliroi</first><last>Georgila</last></author>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–9</pages>
     <url>http://www.aclweb.org/anthology/W18-4701</url>
     <bibtype>inproceedings</bibtype>
     <bibkey>W18-4701</bibkey>
   </paper>

   <paper id="4702">
     <title>Interoperable Annotation of Events and Event Relations across Domains</title>
     <author><first>Jun</first><last>Araki</last></author>
     <author><first>Lamana</first><last>Mulaffer</last></author>
     <author><first>Arun</first><last>Pandian</last></author>
     <author><first>Yukari</first><last>Yamakawa</last></author>
     <author><first>Kemal</first><last>Oflazer</last></author>
     <author><first>Teruko</first><last>Mitamura</last></author>
     <booktitle>Proceedings 14th Joint ACL - ISO Workshop on Interoperable Semantic Annotation</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>10–20</pages>
     <url>http://www.aclweb.org/anthology/W18-4702</url>
     <bibtype>inproceedings</bibtype>
     <bibkey>W18-4702</bibkey>
   </paper>

   <paper id="4703">
     <title>Downward Compatible Revision of Dialogue Annotation</title>
     <author><first>Harry</first><last>Bunt</last></author>
     <author><first>Emer</first><last>Gilmartin</last></author>
     <author><first>Simon</first><last>Keizer</last></author>
     <author><first>Catherine</first><last>Pelachaud</last></author>
     <author><first>Volha</first><last>Petukhova</last></author>
     <author><first>Laurent</first><last>Prevot</last></author>
     <author><first>Mariet</first><last>Theune</last></author>
     <booktitle>Proceedings 14th Joint ACL - ISO Workshop on Interoperable Semantic Annotation</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>21–34</pages>
     <url>http://www.aclweb.org/anthology/W18-4703</url>
     <bibtype>inproceedings</bibtype>
     <bibkey>W18-4703</bibkey>
   </paper>

   <paper id="4704">
     <title>The Revision of ISO-Space,Focused on the Movement Link</title>
     <author><first>Kiyong</first><last>Lee</last></author>
     <author><first>James</first><last>Pustejovsky</last></author>
     <author><first>Harry</first><last>Bunt</last></author>
     <booktitle>Proceedings 14th Joint ACL - ISO Workshop on Interoperable Semantic Annotation</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>35–44</pages>
     <url>http://www.aclweb.org/anthology/W18-4704</url>
     <bibtype>inproceedings</bibtype>
     <bibkey>W18-4704</bibkey>
   </paper>

   <paper id="4705">
     <title>Chat,Chunk and Topic in Casual Conversation</title>
     <author><first>Emer</first><last>Gilmartin</last></author>
     <author><first>Carl</first><last>Vogel</last></author>
     <booktitle>Proceedings 14th Joint ACL - ISO Workshop on Interoperable Semantic Annotation</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>45–52</pages>
     <url>http://www.aclweb.org/anthology/W18-4705</url>
     <bibtype>inproceedings</bibtype>
     <bibkey>W18-4705</bibkey>
   </paper>

   <paper id="4706">
     <title>Annotation of the Syntax/Semantics interface as a Bridge between Deep Linguistic Parsing and TimeML</title>
     <author><first>Mark-Matthias</first><last>Zymla</last></author>
     <booktitle>Proceedings 14th Joint ACL - ISO Workshop on Interoperable Semantic Annotation</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>53–59</pages>
     <url>http://www.aclweb.org/anthology/W18-4706</url>
     <bibtype>inproceedings</bibtype>
     <bibkey>W18-4706</bibkey>
   </paper>

   <paper id="4707">
     <title>A Dialogue Annotation Scheme for Weight Management Chat using the Trans-Theoretical Model of Health Behavior Change</title>
     <author><first>Ramesh</first><last>Manuvirakurike</last></author>
     <author><first>Sumanth</first><last>Bharawadj</last></author>
     <author><first>Kalliroi</first><last>Georgila</last></author>
     <booktitle>Proceedings 14th Joint ACL - ISO Workshop on Interoperable Semantic Annotation</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>60–68</pages>
     <url>http://www.aclweb.org/anthology/W18-4707</url>
     <bibtype>inproceedings</bibtype>
     <bibkey>W18-4707</bibkey>
   </paper>

   <paper id="4708">
     <title>Annotating Measurable Quantitative Informationin Language: for an ISO Standard</title>
     <author><first>Tianyong</first><last>Hao</last></author>
     <author><first>Haotai</first><last>Wang</last></author>
     <author><first>Xinyu</first><last>Cao</last></author>
     <author><first>Kiyong</first><last>Lee</last></author>
     <booktitle>Proceedings 14th Joint ACL - ISO Workshop on Interoperable Semantic Annotation</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>69–75</pages>
     <url>http://www.aclweb.org/anthology/W18-4708</url>
     <bibtype>inproceedings</bibtype>
     <bibkey>W18-4708</bibkey>
   </paper>

   <paper id="4709">
     <title>Improving String Processing for Temporal Relations</title>
     <author><first>David</first><last>Woods</last></author>
     <author><first>Tim</first><last>Fernando</last></author>
     <booktitle>Proceedings 14th Joint ACL - ISO Workshop on Interoperable Semantic Annotation</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>76-86</pages>
     <url>http://www.aclweb.org/anthology/W18-4709</url>
     <bibtype>inproceedings</bibtype>
     <bibkey>W18-4709</bibkey>
   </paper>

   <paper id="4710">
     <title>Discourse Annotation in the PDTB: The Next Generation</title>
     <author><first>Rashmi</first><last>Prasad</last></author>
     <author><first>Bonnie</first><last>Webber</last></author>
     <author><first>Alan</first><last>Lee</last></author>
     <booktitle>Proceedings 14th Joint ACL - ISO Workshop on Interoperable Semantic Annotation</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>87–97</pages>
     <url>http://www.aclweb.org/anthology/W18-4710</url>
     <bibtype>inproceedings</bibtype>
     <bibkey>W18-4710</bibkey>
   </paper>

   <paper id="4711">
     <title>Towards Understanding End-of-trip Instructions in a Taxi Ride Scenario</title>
     <author><first>Deepthi</first><last>Karkada</last></author>
     <author><first>Ramesh</first><last>Manuvirakurike</last></author>
     <author><first>Kalliroi</first><last>Georgila</last></author>
     <booktitle>Proceedings 14th Joint ACL - ISO Workshop on Interoperable Semantic Annotation</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>98–107</pages>
     <url>http://www.aclweb.org/anthology/W18-4711</url>
     <bibtype>inproceedings</bibtype>
     <bibkey>W18-4711</bibkey>
   </paper>

   <paper id="4800">
     <title>Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages</title>
     <editor><first>Judith L.</first><last>Klavans</last></editor>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-48</url>
     <bibtype>book</bibtype>
     <bibkey>W18-48:2018</bibkey>
   </paper>

   <paper id="4801">
     <title>Computational Challenges for Polysynthetic Languages</title>
     <author><first>Judith L.</first><last>Klavans</last></author>
     <booktitle>Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–11</pages>
     <url>http://www.aclweb.org/anthology/W18-4801</url>
     <abstract>Given advances in computational linguistic analysis of complex languages using Machine Learning as well as standard Finite State Transducers, coupled with recent efforts in language revitalization, the time was right to organize a first workshop to bring together experts in language technology and linguists on the one hand with language practitioners and revitalization experts on the other. This one-day meeting provides a promising forum to discuss new research on polysynthetic languages in combination with the needs of linguistic communities where such languages are written and spoken.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>klavans:2018:W18-48</bibkey>
   </paper>

   <paper id="4802">
     <title>A Neural Morphological Analyzer for Arapaho Verbs Learned from a Finite State Transducer</title>
     <author><first>Sarah</first><last>Moeller</last></author>
     <author><first>Ghazaleh</first><last>Kazeminejad</last></author>
     <author><first>Andrew</first><last>Cowell</last></author>
     <author><first>Mans</first><last>Hulden</last></author>
     <booktitle>Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>12–20</pages>
     <url>http://www.aclweb.org/anthology/W18-4802</url>
     <abstract>We experiment with training an encoder-decoder neural model for mimicking the behavior of an existing hand-written finite-state morphological grammar for Arapaho verbs, a polysynthetic language with a highly complex verbal inflection system. After adjusting for ambiguous parses, we find that the system is able to generalize to unseen forms with accuracies of 98.68% (unambiguous verbs) and 92.90% (all verbs).</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>moeller-EtAl:2018:W18-48</bibkey>
   </paper>

   <paper id="4803">
     <title>Finite-state morphology for Kwak’wala: A phonological approach</title>
     <author><first>Patrick</first><last>Littell</last></author>
     <booktitle>Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>21–30</pages>
     <url>http://www.aclweb.org/anthology/W18-4803</url>
     <abstract>This paper presents the phonological layer of a Kwak’wala finite-state morphological transducer, using the phonological hypotheses of Lincoln and Rath (1986) and the lenient composition operation of Karttunen (1998) to mediate the complicated relationship between underlying and surface forms. The resulting system decomposes the wide variety of surface forms in such a way that the morphological layer can be specified using unique and largely concatenative morphemes.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>littell:2018:W18-48</bibkey>
   </paper>

   <paper id="4804">
     <title>A prototype finite-state morphological analyser for Chukchi</title>
     <author><first>Vasilisa</first><last>Andriyanets</last></author>
     <author><first>Francis</first><last>Tyers</last></author>
     <booktitle>Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>31–40</pages>
     <url>http://www.aclweb.org/anthology/W18-4804</url>
     <abstract>In this article we describe the application of finite-state transducers to the morphological and phonological systems of Chukchi, a polysynthetic language spoken in the north of the Russian Federation. The language exhibits progressive and regressive vowel harmony, productive incorporation and extensive circumfixing. To implement the analyser we use the well-known Helsinki Finite-State Toolkit (HFST). The resulting model covers the majority of the morphological and phonological processes. A brief evaluation carried out on publically-available corpora shows that the coverage of the transducer is between and 53% and 76%. An error evaluation of 100 tokens randomly selected from the corpus, which were not covered by the analyser shows that most of the morphological processes are covered and that the majority of errors are caused by a limited stem lexicon.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>andriyanets-tyers:2018:W18-48</bibkey>
   </paper>

   <paper id="4805">
     <title>Natural Language Generation for Polysynthetic Languages: Language Teaching and Learning Software for Kanyen’kéha (Mohawk)</title>
     <author><first>Greg</first><last>Lessard</last></author>
     <author><first>Nathan</first><last>Brinklow</last></author>
     <author><first>Michael</first><last>Levison</last></author>
     <booktitle>Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>41–52</pages>
     <url>http://www.aclweb.org/anthology/W18-4805</url>
     <abstract>Kanyen’kéha (in English, Mohawk) is an Iroquoian language spoken primarily in Eastern Canada (Ontario, Québec). Classified as endangered, it has only a small number of speakers and very few younger native speakers. Consequently, teachers and courses, teaching materials and software are urgently needed. In the case of software, the polysynthetic nature of Kanyen’kéha means that the number of possible combinations grows exponentially and soon surpasses attempts to capture variant forms by hand. It is in this context that we describe an attempt to produce language teaching materials based on a generative approach. A natural language generation environment (ivi/Vinci) embedded in a web environment (VinciLingua) makes it possible to produce, by rule, variant forms of indefinite complexity. These may be used as models to explore, or as materials to which learners respond. Generated materials may take the form of written text, oral utterances, or images; responses may be typed on a keyboard, gestural (using a mouse) or, to a limited extent, oral. The software also provides complex orthographic, morphological and syntactic analysis of learner productions. We describe the trajectory of development of materials for a suite of four courses on Kanyen’kéha, the first of which will be taught in the fall of 2018.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>lessard-brinklow-levison:2018:W18-48</bibkey>
   </paper>

   <paper id="4806">
     <title>Kawennón:nis: the Wordmaker for Kanyen’kéha</title>
     <author><first>Anna</first><last>Kazantseva</last></author>
     <author><first>Owennatekha Brian</first><last>Maracle</last></author>
     <author><first>Ronkwe’tiyóhstha Josiah</first><last>Maracle</last></author>
     <author><first>Aidan</first><last>Pine</last></author>
     <booktitle>Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>53–64</pages>
     <url>http://www.aclweb.org/anthology/W18-4806</url>
     <abstract>In this paper we describe preliminary work on Kawennón:nis, a verb conjugator for Kanyen’kéha (Ohsweken dialect). The project is the result of a collaboration between Onkwawenna Kentyohkwa Kanyen’kéha immersion school and the Canadian National Research Council’s Indigenous Language Technology lab. The purpose of Kawennón:nis is to build on the educational successes of the Onkwawenna Kentyohkwa school and develop a tool that assists students in learning how to conjugate verbs in Kanyen’kéha; a skill that is essential to mastering the language. Kawennón:nis is implemented with both web and mobile front-ends that communicate with an application programming interface that in turn communicates with a symbolic language model implemented as a finite state transducer. Eventually, it will serve as a foundation for several other applications for both Kanyen’kéha and other Iroquoian languages.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>kazantseva-EtAl:2018:W18-48</bibkey>
   </paper>

   <paper id="4807">
     <title>Using the Nunavut Hansard Data for Experiments in Morphological Analysis and Machine Translation</title>
     <author><first>Jeffrey</first><last>Micher</last></author>
     <booktitle>Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>65–72</pages>
     <url>http://www.aclweb.org/anthology/W18-4807</url>
     <abstract>Inuktitut is a polysynthetic language spoken in Northern Canada and is one of the official languages of the Canadian territory of Nunavut. As such, the Nunavut Legislature publishes all of its proceedings in parallel English and Inuktitut. Several parallel English-Inuktitut corpora from these proceedings have been created from these data and are publically available. The corpus used for current experiments is described. Morphological processing of one of these corpora was carried out and details about the processing are provided. Then, the processed corpus was used in morphological analysis and machine translation (MT) experiments. The morphological analysis experiments aimed to improve the coverage of morphological processing of the corpus, and compare an additional experimental condition to previously published results. The machine translation experiments made use of the additional morphologically analyzed word types in a statistical machine translation system designed to translate to and from Inuktitut morphemes. Results are reported and next steps are defined.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>micher:2018:W18-48</bibkey>
   </paper>

   <paper id="4808">
     <title>Lost in Translation: Analysis of Information Loss During Machine Translation Between Polysynthetic and Fusional Languages</title>
     <author><first>Manuel</first><last>Mager</last></author>
     <author><first>Elisabeth</first><last>Mager</last></author>
     <author><first>Alfonso</first><last>Medina-Urrea</last></author>
     <author><first>Ivan Vladimir</first><last>Meza Ruiz</last></author>
     <author><first>Katharina</first><last>Kann</last></author>
     <booktitle>Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>73–83</pages>
     <url>http://www.aclweb.org/anthology/W18-4808</url>
     <abstract>Machine translation from polysynthetic to fusional languages is a challenging task, which gets further complicated by the limited amount of parallel text available. Thus, translation performance is far from the state of the art for high-resource and more intensively studied language pairs. To shed light on the phenomena which hamper automatic translation to and from polysynthetic languages, we study translations from three low-resource, polysynthetic languages (Nahuatl, Wixarika and Yorem Nokki) into Spanish and vice versa. Doing so, we find that in a morpheme-to-morpheme alignment an important amount of information contained in polysynthetic morphemes has no Spanish counterpart, and its translation is often omitted. We further conduct a qualitative analysis and, thus, identify morpheme types that are commonly hard to align or ignored in the translation process.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>mager-EtAl:2018:W18-48</bibkey>
   </paper>

   <paper id="4809">
     <title>Automatic Glossing in a Low-Resource Setting for Language Documentation</title>
     <author><first>Sarah</first><last>Moeller</last></author>
     <author><first>Mans</first><last>Hulden</last></author>
     <booktitle>Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>84–93</pages>
     <url>http://www.aclweb.org/anthology/W18-4809</url>
     <abstract>Morphological analysis of morphologically rich and low-resource languages is important to both descriptive linguistics and natural language processing. Field documentary efforts usually procure analyzed data in cooperation with native speakers who are capable of providing some level of linguistic information. Manually annotating such data is very expensive and the traditional process is arguably too slow in the face of language endangerment and loss. We report on a case study of learning to automatically gloss a Nakh-Daghestanian language, Lezgi, from a very small amount of seed data. We compare a conditional random field based sequence labeler and a neural encoder-decoder model and show that a nearly 0.9 F1-score on labeled accuracy of morphemes can be achieved with 3,000 words of transcribed oral text. Errors are mostly limited to morphemes with high allomorphy. These results are potentially useful for developing rapid annotation and fieldwork tools to support documentation of morphologically rich, endangered languages.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>moeller-hulden:2018:W18-48</bibkey>
   </paper>

   <paper id="4900">
     <title>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</title>
     <editor><first>Agata</first><last>Savary</last></editor>
     <editor><first>Carlos</first><last>Ramisch</last></editor>
     <editor><first>Jena D.</first><last>Hwang</last></editor>
     <editor><first>Nathan</first><last>Schneider</last></editor>
     <editor><first>Melanie</first><last>Andresen</last></editor>
     <editor><first>Sameer</first><last>Pradhan</last></editor>
     <editor><first>Miriam R L</first><last>Petruck</last></editor>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-49</url>
     <bibtype>book</bibtype>
     <bibkey>W18-49:2018</bibkey>
   </paper>

   <paper id="4901">
     <title>Annotation Schemes for Surface Construction Labeling</title>
     <author><first>Lori</first><last>Levin</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1</pages>
     <url>http://www.aclweb.org/anthology/W18-4901</url>
     <abstract>In this talk I will describe the interaction of linguistics and language technologies in Surface Construction Labeling (SCL) from the perspective of corpus annotation tasks such as definiteness, modality, and causality. Linguistically, following Construction Grammar, SCL recognizes that meaning may be carried by morphemes, words, or arbitrary constellations of morpho-lexical elements. SCL is like Shallow Semantic Parsing in that it does not attempt a full compositional analysis of meaning, but rather identifies only the main elements of a semantic frame, where the frames may be invoked by constructions as well as lexical items. Computationally, SCL is different from tasks such as information extraction in that it deals only with meanings that are expressed in a conventional, grammaticalized way and does not address inferred meanings. I review the work of Dunietz (2018) on the labeling of causal frames including causal connectives and cause and effect arguments. I will describe how to design an annotation scheme for SCL, including isolating basic units of form and meaning and building a “constructicon”. I will conclude with remarks about the nature of universal categories and universal meaning representations in language technologies. This talk describes joint work with Jaime Carbonell, Jesse Dunietz, Nathan Schneider, and Miriam Petruck.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>levin:2018:W18-49</bibkey>
   </paper>

   <paper id="4902">
     <title>From Lexical Functional Grammar to Enhanced Universal Dependencies</title>
     <author><first>Adam</first><last>Przepiórkowski</last></author>
     <author><first>Agnieszka</first><last>Patejuk</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>2–4</pages>
     <url>http://www.aclweb.org/anthology/W18-4902</url>
     <abstract>This is a summary of an invited talk.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>przepirkowski-patejuk:2018:W18-49</bibkey>
   </paper>

   <paper id="4903">
     <title>Leaving no token behind: comprehensive (and delicious) annotation of MWEs and supersenses</title>
     <author><first>Nathan</first><last>Schneider</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>5</pages>
     <url>http://www.aclweb.org/anthology/W18-4903</url>
     <abstract>I will describe an unorthodox approach to lexical semantic annotation that prioritizes corpus coverage, democratizing analysis of a wide range of expression types. I argue that a lexicon-free lexical semantics—defined in terms of units and supersense tags—is an appetizing direction for NLP, as it is robust, cost-effective, easily understood, not too language-specific, and can serve as a foundation for richer semantic structure. Linguistic delicacies from the STREUSLE and DiMSUM corpora, which have been multiword- and supersense-annotated, attest to the veritable smörgåsbord of noncanonical constructions in English, including various flavors of prepositions, MWEs, and other curiosities. Bio: Nathan Schneider is an annotation schemer and computational modeler for natural language. As Assistant Professor of Linguistics and Computer Science at Georgetown University, he looks for synergies between practical language technologies and the scientific study of language. He specializes in broad-coverage semantic analysis: designing linguistic meaning representations, annotating them in corpora, and automating them with statistical natural language processing techniques. A central focus in this research is the nexus between grammar and lexicon as manifested in multiword expressions and adpositions/case markers. He has inhabited UC Berkeley (BA in Computer Science and Linguistics), Carnegie Mellon University (Ph.D. in Language Technologies), and the University of Edinburgh (postdoc). Now a Hoya and leader of NERT, he continues to play with data and algorithms for linguistic meaning.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>schneider:2018:W18-49</bibkey>
   </paper>

   <paper id="4904">
     <title>Processing MWEs: Neurocognitive Bases of Verbal MWEs and Lexical Cohesiveness within MWEs</title>
     <author><first>Shohini</first><last>Bhattasali</last></author>
     <author><first>Murielle</first><last>Fabre</last></author>
     <author><first>John</first><last>Hale</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>6–17</pages>
     <url>http://www.aclweb.org/anthology/W18-4904</url>
     <abstract>Multiword expressions have posed a challenge in the past for computational linguistics since they comprise a heterogeneous family of word clusters and are difficult to detect in natural language data. In this paper, we present a fMRI study based on language comprehension to provide neuroimaging evidence for processing MWEs. We investigate whether different MWEs have distinct neural bases, e.g. if verbal MWEs involve separate brain areas from non-verbal MWEs and if MWEs with varying levels of cohesiveness activate dissociable brain regions. Our study contributes neuroimaging evidence illustrating that different MWEs elicit spatially distinct patterns of activation. We also adapt an association measure, usually used to detect MWEs, as a cognitively plausible metric for language processing.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>bhattasali-fabre-hale:2018:W18-49</bibkey>
   </paper>

   <paper id="4905">
     <title>The Interplay of Form and Meaning in Complex Medical Terms: Evidence from a Clinical Corpus</title>
     <author><first>Leonie</first><last>Grön</last></author>
     <author><first>Ann</first><last>Bertels</last></author>
     <author><first>Heylen</first><last>Kris</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>18–29</pages>
     <url>http://www.aclweb.org/anthology/W18-4905</url>
     <abstract>We conduct a corpus study to investigate the structure of multi-word expressions (MWEs) in the clinical domain. Based on an existing medical taxonomy, we develop an annotation scheme and label a sample of MWEs from a Dutch corpus with semantic and grammatical features. The analysis of the annotated data shows that the formal structure of clinical MWEs correlates with their conceptual properties. The insights gained from this study could inform the design of Natural Language Processing (NLP) systems for clinical writing, but also for other specialized genres.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>grn-bertels-kris:2018:W18-49</bibkey>
   </paper>

   <paper id="4906">
     <title>Discourse and Lexicons: Lexemes, MWEs, Grammatical Constructions and Compositional Word Combinations to Signal Discourse Relations</title>
     <author><first>Laurence</first><last>Danlos</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>30–40</pages>
     <url>http://www.aclweb.org/anthology/W18-4906</url>
     <abstract>Lexicons generally record a list of lexemes or non-compositional multiword expressions. We propose to build lexicons for compositional word combinations, namely “secondary discourse connectives”. Secondary discourse connectives play the same function as “primary discourse connectives” but the latter are either lexemes or non-compositional multiword expressions. The paper defines primary and secondary connectives, and explains why it is possible to build a lexicon for the compositional ones and how it could be organized. It also puts forward the utility of such a lexicon in discourse annotation and parsing. Finally, it opens the discussion on the constructions that signal a discourse relation between two spans of text.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>danlos:2018:W18-49</bibkey>
   </paper>

   <paper id="4907">
     <title>From Chinese Word Segmentation to Extraction of Constructions: Two Sides of the Same Algorithmic Coin</title>
     <author><first>Jean-Pierre</first><last>Colson</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>41–50</pages>
     <url>http://www.aclweb.org/anthology/W18-4907</url>
     <abstract>This paper presents the results of two experiments carried out within the framework of computational construction grammar. Starting from the constructionist point of view that there are just constructions in language, including lexical ones, we tested the validity of a clustering algorithm that was primarily designed for MWE extraction, the cpr-score (Colson, 2017), on Chinese word segmentation. Our results indicate a striking recall rate of 75 percent without any special adaptation to Chinese or to the lexicon, which confirms that there is some similarity between extracting MWEs and CWS. Our second experiment also suggests that the same methodology might be used for extracting more schematic or abstract constructions, thereby providing evidence for the statistical foundation of construction grammar.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>colson:2018:W18-49</bibkey>
   </paper>

   <paper id="4908">
     <title>Fixed Similes: Measuring aspects of the relation between MWE idiomatic semantics and syntactic flexibility</title>
     <author><first>Stella</first><last>Markantonatou</last></author>
     <author><first>Panagiotis</first><last>Kouris</last></author>
     <author><first>Yanis</first><last>Maistros</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>51–61</pages>
     <url>http://www.aclweb.org/anthology/W18-4908</url>
     <abstract>We shed light on aspects of the relation between the semantics and the syntactic flexibility of multiword expressions by investigating fixed adjective similes (FS), a predicative multiword expression class not studied in this respect before. We find that only a subset of the syntactic structures observed in the data are related with idiomaticity. We identify and measure two aspects of idiomaticity, one of which seems to allow for predictions about FS syntactic flexibility. Our research draws on a resource developed with the semantic and detailed syntactic annotation of web-retrieved Modern Greek material, indicating frequency of use of the individual similes.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>markantonatou-kouris-maistros:2018:W18-49</bibkey>
   </paper>

   <paper id="4909">
     <title>Fine-Grained Termhood Prediction for German Compound Terms Using Neural Networks</title>
     <author><first>Anna</first><last>Hätty</last></author>
     <author><first>Sabine</first><last>Schulte im Walde</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>62–73</pages>
     <url>http://www.aclweb.org/anthology/W18-4909</url>
     <abstract>Automatic term identification and investigating the understandability of terms in a specialized domain are often treated as two separate lines of research. We propose a combined approach for this matter, by defining fine-grained classes of termhood and framing a classification task. The classes reflect tiers of a term’s association to a domain. The new setup is applied to German closed compounds as term candidates in the domain of cooking. For the prediction of the classes, we compare several neural network architectures and also take salient information about the compounds’ components into account. We show that applying a similar class distinction to the compounds’ components and propagating this information within the network improves the compound class prediction results.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>htty-schulteimwalde:2018:W18-49</bibkey>
   </paper>

   <paper id="4910">
     <title>Towards a Computational Lexicon for Moroccan Darija: Words, Idioms, and Constructions</title>
     <author><first>Jamal</first><last>Laoudi</last></author>
     <author><first>Claire</first><last>Bonial</last></author>
     <author><first>Lucia</first><last>Donatelli</last></author>
     <author><first>Stephen</first><last>Tratz</last></author>
     <author><first>Clare</first><last>Voss</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>74–85</pages>
     <url>http://www.aclweb.org/anthology/W18-4910</url>
     <abstract>In this paper, we explore the challenges of building a computational lexicon for Moroccan Darija (MD), an Arabic dialect spoken by over 32 million people worldwide but which only recently has begun appearing frequently in written form in social media. We raise the question of what belongs in such a lexicon and start by describing our work building traditional word-level lexicon entries with their English translations. We then discuss challenges in translating idiomatic MD text that led to creating multi-word expression lexicon entries whose meanings could not be fully derived from the individual words. Finally, we provide a preliminary exploration of constructions to be considered for inclusion in an MD constructicon by translating examples of English constructions and examining their MD counterparts.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>laoudi-EtAl:2018:W18-49</bibkey>
   </paper>

   <paper id="4911">
     <title>Verbal Multiword Expressions in Basque Corpora</title>
     <author><first>Uxoa</first><last>Iñurrieta</last></author>
     <author><first>Itziar</first><last>Aduriz</last></author>
     <author><first>Ainara</first><last>Estarrona</last></author>
     <author><first>Itziar</first><last>Gonzalez-Dios</last></author>
     <author><first>Antton</first><last>Gurrutxaga</last></author>
     <author><first>Ruben</first><last>Urizar</last></author>
     <author><first>Iñaki</first><last>Alegria</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>86–95</pages>
     <url>http://www.aclweb.org/anthology/W18-4911</url>
     <abstract>This paper presents a Basque corpus where Verbal Multiword Expressions (VMWEs) were annotated following universal guidelines. Information on the annotation is given, and some ideas for discussion upon the guidelines are also proposed. The corpus is useful not only for NLP-related research, but also to draw conclusions on Basque phraseology in comparison with other languages.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>iurrieta-EtAl:2018:W18-49</bibkey>
   </paper>

   <paper id="4912">
     <title>Annotation of Tense and Aspect Semantics for Sentential AMR</title>
     <author><first>Lucia</first><last>Donatelli</last></author>
     <author><first>Michael</first><last>Regan</last></author>
     <author><first>William</first><last>Croft</last></author>
     <author><first>Nathan</first><last>Schneider</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>96–108</pages>
     <url>http://www.aclweb.org/anthology/W18-4912</url>
     <abstract>Although English grammar encodes a number of semantic contrasts with tense and aspect marking, these semantics are currently ignored by Abstract Meaning Representation (AMR) annotations. This paper extends sentence-level AMR to include a coarse-grained treatment of tense and aspect semantics. The proposed framework augments the representation of finite predications to include a four-way temporal distinction (event time before, up to, at, or after speech time) and several aspectual distinctions (including static vs. dynamic, habitual vs. episodic, and telic vs. atelic). This will enable AMR to be used for NLP tasks and applications that require sophisticated reasoning about time and event structure.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>donatelli-EtAl:2018:W18-49</bibkey>
   </paper>

   <paper id="4913">
     <title>A Syntax-Based Scheme for the Annotation and Segmentation of German Spoken Language Interactions</title>
     <author><first>Swantje</first><last>Westpfahl</last></author>
     <author><first>Jan</first><last>Gorisch</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>109–120</pages>
     <url>http://www.aclweb.org/anthology/W18-4913</url>
     <abstract>Unlike corpora of written language where segmentation can mainly be derived from orthographic punctuation marks, the basis for segmenting spoken language corpora is not predetermined by the primary data, but rather has to be established by the corpus compilers. This impedes consistent querying and visualization of such data. Several ways of segmenting have been proposed, some of which are based on syntax. In this study, we developed and evaluated annotation and segmentation guidelines in reference to the topological field model for German. We can show that these guidelines are used consistently across annotators. We also investigated the influence of various interactional settings with a rather simple measure, the word-count per segment and unit-type. We observed that the word count and the distribution of each unit type differ in varying interactional settings and that our developed segmentation and annotation guidelines are used consistently across annotators. In conclusion, our syntax-based segmentations reflect interactional properties that are intrinsic to the social interactions that participants are involved in. This can be used for further analysis of social interaction and opens the possibility for automatic segmentation of transcripts.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>westpfahl-gorisch:2018:W18-49</bibkey>
   </paper>

   <paper id="4914">
     <title>An Annotated Corpus of Picture Stories Retold by Language Learners</title>
     <author><first>Christine</first><last>Köhn</last></author>
     <author><first>Arne</first><last>Köhn</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>121–132</pages>
     <url>http://www.aclweb.org/anthology/W18-4914</url>
     <abstract>Corpora with language learner writing usually consist of essays, which are difficult to annotate reliably and to process automatically due to the high degree of freedom and the nature of learner language. We develop a task which mildly constrains learner utterances to facilitate consistent annotation and reliable automatic processing but at the same time does not prime learners with textual information. In this task, learners retell a comic strip. We present the resulting task-based corpus of stories written by learners of German. We designed the corpus to be able to serve multiple purposes: The corpus was manually annotated, including target hypotheses and syntactic structures. We achieve a very high inter-annotator agreement: κ = 0.765 for the annotation of minimal target hypotheses and κ = 0.507 for the extended target hypotheses. We attribute this to the design of our task and the annotation guidelines, which are based on those for the Falko corpus (Reznicek et al., 2012).</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>khn-khn:2018:W18-49</bibkey>
   </paper>

   <paper id="4915">
     <title>Developing and Evaluating Annotation Procedures for Twitter Data during Hazard Events</title>
     <author><first>Kevin</first><last>Stowe</last></author>
     <author><first>Martha</first><last>Palmer</last></author>
     <author><first>Jennings</first><last>Anderson</last></author>
     <author><first>Marina</first><last>Kogan</last></author>
     <author><first>Leysia</first><last>Palen</last></author>
     <author><first>Kenneth M.</first><last>Anderson</last></author>
     <author><first>Rebecca</first><last>Morss</last></author>
     <author><first>Julie</first><last>Demuth</last></author>
     <author><first>Heather</first><last>Lazrus</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>133–143</pages>
     <url>http://www.aclweb.org/anthology/W18-4915</url>
     <abstract>When a hazard such as a hurricane threatens, people are forced to make a wide variety of decisions, and the information they receive and produce can influence their own and others’ actions. As social media grows more popular, an increasing number of people are using social media platforms to obtain and share information about approaching threats and discuss their interpretations of the threat and their protective decisions. This work aims to improve understanding of natural disasters through social media and provide an annotation scheme to identify themes in user’s social media behavior and facilitate efforts in supervised machine learning. To that end, this work has three contributions: (1) the creation of an annotation scheme to consistently identify hazard-related themes in Twitter, (2) an overview of agreement rates and difficulties in identifying annotation categories, and (3) a public release of both the dataset and guidelines developed from this scheme.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>stowe-EtAl:2018:W18-49</bibkey>
   </paper>

   <paper id="4916">
     <title>A Treebank for the Healthcare Domain</title>
     <author><first>Nganthoibi</first><last>Oinam</last></author>
     <author><first>Diwakar</first><last>Mishra</last></author>
     <author><first>Pinal</first><last>Patel</last></author>
     <author><first>Narayan</first><last>Choudhary</last></author>
     <author><first>Hitesh</first><last>Desai</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>144–155</pages>
     <url>http://www.aclweb.org/anthology/W18-4916</url>
     <abstract>This paper presents a treebank for the healthcare domain developed at ezDI. The treebank is created from a wide array of clinical health record documents across hospitals. The data has been de-identified and annotated for constituent syntactic structure. The treebank contains a total of 52053 sentences that have been sampled for subdomains as well as linguistic variations. The paper outlines the sampling process followed to ensure a better domain representation in the corpus, the annotation process and challenges, and corpus statistics. The Penn Treebank tagset and guidelines were largely followed, but there were many syntactic contexts that warranted adaptation of the guidelines. The treebank created was used to re-train the Berkeley parser and the Stanford parser. These parsers were also trained with the GENIA treebank for comparative quality assessment. Our treebank yielded great-er accuracy on both parsers. Berkeley parser performed better on our treebank with an average F1 measure of 91 across 5-folds. This was a significant jump from the out-of-the-box F1 score of 70 on Berkeley parser’s default grammar.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>oinam-EtAl:2018:W18-49</bibkey>
   </paper>

   <paper id="4917">
     <title>The RST Spanish-Chinese Treebank</title>
     <author><first>Shuyuan</first><last>Cao</last></author>
     <author><first>Iria</first><last>da Cunha</last></author>
     <author><first>Mikel</first><last>Iruskieta</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>156–166</pages>
     <url>http://www.aclweb.org/anthology/W18-4917</url>
     <abstract>Discourse analysis is necessary for different tasks of Natural Language Processing (NLP). As two of the most spoken languages in the world, discourse analysis between Spanish and Chinese is important for NLP research. This paper aims to present the first open Spanish-Chinese parallel corpus annotated with discourse information, whose theoretical framework is based on the Rhetorical Structure Theory (RST). We have evaluated and harmonized each annotation part to obtain a high annotated-quality corpus. The corpus is already available to the public.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>cao-dacunha-iruskieta:2018:W18-49</bibkey>
   </paper>

   <paper id="4918">
     <title>All Roads Lead to UD: Converting Stanford and Penn Parses to English Universal Dependencies with Multilayer Annotations</title>
     <author><first>Siyao</first><last>Peng</last></author>
     <author><first>Amir</first><last>Zeldes</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>167–177</pages>
     <url>http://www.aclweb.org/anthology/W18-4918</url>
     <abstract>We describe and evaluate different approaches to the conversion of gold standard corpus data from Stanford Typed Dependencies (SD) and Penn-style constituent trees to the latest English Universal Dependencies representation (UD 2.2). Our results indicate that pure SD to UD conversion is highly accurate across multiple genres, resulting in around 1.5% errors, but can be improved further to fewer than 0.5% errors given access to annotations beyond the pure syntax tree, such as entity types and coreference resolution, which are necessary for correct generation of several UD relations. We show that constituent-based conversion using CoreNLP (with automatic NER) performs substantially worse in all genres, including when using gold constituent trees, primarily due to underspecification of phrasal grammatical functions.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>peng-zeldes:2018:W18-49</bibkey>
   </paper>

   <paper id="4919">
     <title>The Other Side of the Coin: Unsupervised Disambiguation of Potentially Idiomatic Expressions by Contrasting Senses</title>
     <author><first>Hessel</first><last>Haagsma</last></author>
     <author><first>Malvina</first><last>Nissim</last></author>
     <author><first>Johan</first><last>Bos</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>178–184</pages>
     <url>http://www.aclweb.org/anthology/W18-4919</url>
     <abstract>Disambiguation of potentially idiomatic expressions involves determining the sense of a potentially idiomatic expression in a given context, e.g. determining that make hay in ‘Investment banks made hay while takeovers shone.’ is used in a figurative sense. This enables automatic interpretation of idiomatic expressions, which is important for applications like machine translation and sentiment analysis. In this work, we present an unsupervised approach for English that makes use of literalisations of idiom senses to improve disambiguation, which is based on the lexical cohesion graph-based method by Sporleder and Li (2009). Experimental results show that, while literalisation carries novel information, its performance falls short of that of state-of-the-art unsupervised methods.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>haagsma-nissim-bos:2018:W18-49</bibkey>
   </paper>

   <paper id="4920">
     <title>Do Character-Level Neural Network Language Models Capture Knowledge of Multiword Expression Compositionality?</title>
     <author><first>Ali</first><last>Hakimi Parizi</last></author>
     <author><first>Paul</first><last>Cook</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>185–192</pages>
     <url>http://www.aclweb.org/anthology/W18-4920</url>
     <abstract>In this paper, we propose the first model for multiword expression (MWE) compositionality prediction based on character-level neural network language models. Experimental results on two kinds of MWEs (noun compounds and verb-particle constructions) and two languages (English and German) suggest that character-level neural network language models capture knowledge of multiword expression compositionality, in particular for English noun compounds and the particle component of English verb-particle constructions. In contrast to many other approaches to MWE compositionality prediction, this character-level approach does not require token-level identification of MWEs in a training corpus, and can potentially predict the compositionality of out-of-vocabulary MWEs.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>hakimiparizi-cook:2018:W18-49</bibkey>
   </paper>

   <paper id="4921">
     <title>Constructing an Annotated Corpus of Verbal MWEs for English</title>
     <author><first>Abigail</first><last>Walsh</last></author>
     <author><first>Claire</first><last>Bonial</last></author>
     <author><first>Kristina</first><last>Geeraert</last></author>
     <author><first>John P.</first><last>McCrae</last></author>
     <author><first>Nathan</first><last>Schneider</last></author>
     <author><first>Clarissa</first><last>Somers</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>193–200</pages>
     <url>http://www.aclweb.org/anthology/W18-4921</url>
     <abstract>This paper describes the construction and annotation of a corpus of verbal MWEs for English, as part of the PARSEME Shared Task 1.1 on automatic identification of verbal MWEs. The criteria for corpus selection, the categories of MWEs used, and the training process are discussed, along with the particular issues that led to revisions in edition 1.1 of the annotation guidelines. Finally, an overview of the characteristics of the final annotated corpus is presented, as well as some discussion on inter-annotator agreement.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>walsh-EtAl:2018:W18-49</bibkey>
   </paper>

   <paper id="4922">
     <title>Cooperating Tools for MWE Lexicon Management and Corpus Annotation</title>
     <author><first>Yuji</first><last>Matsumoto</last></author>
     <author><first>Akihiko</first><last>Kato</last></author>
     <author><first>Hiroyuki</first><last>Shindo</last></author>
     <author><first>Toshio</first><last>Morita</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>201–206</pages>
     <url>http://www.aclweb.org/anthology/W18-4922</url>
     <abstract>We present tools for lexicon and corpus management that offer cooperating functionality in corpus annotation. The former, named Cradle, stores a set of words and expressions where multi-word expressions are defined with their own part-of-speech information and internal syntactic structures. The latter, named ChaKi, manages text corpora with part-of-speech (POS) and syntactic dependency structure annotations. Those two tools cooperate so that the words and multi-word expressions stored in Cradle are directly referred to by ChaKi in conducting corpus annotation, and the words and expressions annotated in ChaKi can be output as a list of lexical entities that are to be stored in Cradle.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>matsumoto-EtAl:2018:W18-49</bibkey>
   </paper>

   <paper id="4923">
     <title>“Fingers in the Nose”: Evaluating Speakers’ Identification of Multi-Word Expressions Using a Slightly Gamified Crowdsourcing Platform</title>
     <author><first>Karën</first><last>Fort</last></author>
     <author><first>Bruno</first><last>Guillaume</last></author>
     <author><first>Matthieu</first><last>Constant</last></author>
     <author><first>Nicolas</first><last>Lefèbvre</last></author>
     <author><first>Yann-Alan</first><last>Pilatte</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>207–213</pages>
     <url>http://www.aclweb.org/anthology/W18-4923</url>
     <abstract>This article presents the results we obtained in crowdsourcing French speakers’ intuition concerning multi-work expressions (MWEs). We developed a slightly gamified crowdsourcing platform, part of which is designed to test users’ ability to identify MWEs with no prior training. The participants perform relatively well at the task, with a recall reaching 65% for MWEs that do not behave as function words.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>fort-EtAl:2018:W18-49</bibkey>
   </paper>

   <paper id="4924">
     <title>Improving Domain Independent Question Parsing with Synthetic Treebanks</title>
     <author><first>Halim-Antoine</first><last>Boukaram</last></author>
     <author><first>Nizar</first><last>Habash</last></author>
     <author><first>Micheline</first><last>Ziadee</last></author>
     <author><first>Majd</first><last>Sakr</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>214–221</pages>
     <url>http://www.aclweb.org/anthology/W18-4924</url>
     <abstract>Automatic syntactic parsing for question constructions is a challenging task due to the paucity of training examples in most treebanks. The near absence of question constructions is due to the dominance of the news domain in treebanking efforts. In this paper, we compare two synthetic low-cost question treebank creation methods with a conventional manual high-cost annotation method in the context of three domains (news questions, political talk shows, and chatbots) for Modern Standard Arabic, a language with relatively low resources and rich morphology. Our results show that synthetic methods can be effective at significantly reducing parsing errors for a target domain without having to invest large resources on manual annotation; and the combination of manual and synthetic methods is our best domain-independent performer.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>boukaram-EtAl:2018:W18-49</bibkey>
   </paper>

   <paper id="4925">
     <title>Edition 1.1 of the PARSEME Shared Task on Automatic Identification of Verbal Multiword Expressions</title>
     <author><first>Carlos</first><last>Ramisch</last></author>
     <author><first>Silvio Ricardo</first><last>Cordeiro</last></author>
     <author><first>Agata</first><last>Savary</last></author>
     <author><first>Veronika</first><last>Vincze</last></author>
     <author><first>Verginica</first><last>Barbu Mititelu</last></author>
     <author><first>Archna</first><last>Bhatia</last></author>
     <author><first>Maja</first><last>Buljan</last></author>
     <author><first>Marie</first><last>Candito</last></author>
     <author><first>Polona</first><last>Gantar</last></author>
     <author><first>Voula</first><last>Giouli</last></author>
     <author><first>Tunga</first><last>Güngör</last></author>
     <author><first>Abdelati</first><last>Hawwari</last></author>
     <author><first>Uxoa</first><last>Iñurrieta</last></author>
     <author><first>Jolanta</first><last>Kovalevskaitė</last></author>
     <author><first>Simon</first><last>Krek</last></author>
     <author><first>Timm</first><last>Lichte</last></author>
     <author><first>Chaya</first><last>Liebeskind</last></author>
     <author><first>Johanna</first><last>Monti</last></author>
     <author><first>Carla</first><last>Parra Escartín</last></author>
     <author><first>Behrang</first><last>QasemiZadeh</last></author>
     <author><first>Renata</first><last>Ramisch</last></author>
     <author><first>Nathan</first><last>Schneider</last></author>
     <author><first>Ivelina</first><last>Stoyanova</last></author>
     <author><first>Ashwini</first><last>Vaidya</last></author>
     <author><first>Abigail</first><last>Walsh</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>222–240</pages>
     <url>http://www.aclweb.org/anthology/W18-4925</url>
     <abstract>This paper describes the PARSEME Shared Task 1.1 on automatic identification of verbal multiword expressions. We present the annotation methodology, focusing on changes from last year’s shared task. Novel aspects include enhanced annotation guidelines, additional annotated data for most languages, corpora for some new languages, and new evaluation settings. Corpora were created for 20 languages, which are also briefly discussed. We report organizational principles behind the shared task and the evaluation metrics employed for ranking. The 17 participating systems, their methods and obtained results are also presented and analysed.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>ramisch-EtAl:2018:W18-49</bibkey>
   </paper>

   <paper id="4926">
     <title>CRF-Seq and CRF-DepTree at PARSEME Shared Task 2018: Detecting Verbal MWEs using Sequential and Dependency-Based Approaches</title>
     <author><first>Erwan</first><last>Moreau</last></author>
     <author><first>Ashjan</first><last>Alsulaimani</last></author>
     <author><first>Alfredo</first><last>Maldonado</last></author>
     <author><first>Carl</first><last>Vogel</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>241–247</pages>
     <url>http://www.aclweb.org/anthology/W18-4926</url>
     <abstract>This paper describes two systems for detecting Verbal Multiword Expressions (VMWEs) which both competed in the closed track at the PARSEME VMWE Shared Task 2018. CRF-DepTree-categs implements an approach based on the dependency tree, intended to exploit the syntactic and semantic relations between tokens; CRF-Seq-nocategs implements a robust sequential method which requires only lemmas and morphosyntactic tags. Both systems ranked in the top half of the ranking, the latter ranking second for token-based evaluation. The code for both systems is published under the GNU General Public License version 3.0 and is available at http://github.com/erwanm/adapt-vmwe18.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>moreau-EtAl:2018:W18-49</bibkey>
   </paper>

   <paper id="4927">
     <title>Deep-BGT at PARSEME Shared Task 2018: Bidirectional LSTM-CRF Model for Verbal Multiword Expression Identification</title>
     <author><first>Gözde</first><last>Berk</last></author>
     <author><first>Berna</first><last>Erden</last></author>
     <author><first>Tunga</first><last>Güngör</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>248–253</pages>
     <url>http://www.aclweb.org/anthology/W18-4927</url>
     <abstract>This paper describes the Deep-BGT system that participated to the PARSEME shared task 2018 on automatic identification of verbal multiword expressions (VMWEs). Our system is language-independent and uses the bidirectional Long Short-Term Memory model with a Conditional Random Field layer on top (bidirectional LSTM-CRF). To the best of our knowledge, this paper is the first one that employs the bidirectional LSTM-CRF model for VMWE identification. Furthermore, the gappy 1-level tagging scheme is used for discontiguity and overlaps. Our system was evaluated on 10 languages in the open track and it was ranked the second in terms of the general ranking metric.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>berk-erden-gngr:2018:W18-49</bibkey>
   </paper>

   <paper id="4928">
     <title>GBD-NER at PARSEME Shared Task 2018: Multi-Word Expression Detection Using Bidirectional Long-Short-Term Memory Networks and Graph-Based Decoding</title>
     <author><first>Tiberiu</first><last>Boroș</last></author>
     <author><first>Ruxandra</first><last>Burtica</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>254–260</pages>
     <url>http://www.aclweb.org/anthology/W18-4928</url>
     <abstract>This paper addresses the issue of multi-word expression (MWE) detection by employing a new decoding strategy inspired after graph-based parsing. We show that this architecture achieves state-of-the-art results with minimum feature-engineering, just by relying on lexicalized and morphological attributes. We validate our approach in a multilingual setting, using standard MWE corpora supplied in the PARSEME Shared Task.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>boro-burtica:2018:W18-49</bibkey>
   </paper>

   <paper id="4929">
     <title>Mumpitz at PARSEME Shared Task 2018: A Bidirectional LSTM for the Identification of Verbal Multiword Expressions</title>
     <author><first>Rafael</first><last>Ehren</last></author>
     <author><first>Timm</first><last>Lichte</last></author>
     <author><first>Younes</first><last>Samih</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>261–267</pages>
     <url>http://www.aclweb.org/anthology/W18-4929</url>
     <abstract>In this paper, we describe Mumpitz, the system we submitted to the PARSEME Shared task on automatic identification of verbal multiword expressions (VMWEs). Mumpitz consists of a Bidirectional Recurrent Neural Network (BRNN) with Long Short-Term Memory (LSTM) units and a heuristic that leverages the dependency information provided in the PARSEME corpus data to differentiate VMWEs in a sentence. We submitted results for seven languages in the closed track of the task and for one language in the open track. For the open track we used the same system, but with pretrained instead of randomly initialized word embeddings to improve the system performance.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>ehren-lichte-samih:2018:W18-49</bibkey>
   </paper>

   <paper id="4930">
     <title>TRAPACC and TRAPACCS at PARSEME Shared Task 2018: Neural Transition Tagging of Verbal Multiword Expressions</title>
     <author><first>Regina</first><last>Stodden</last></author>
     <author><first>Behrang</first><last>QasemiZadeh</last></author>
     <author><first>Laura</first><last>Kallmeyer</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>268–274</pages>
     <url>http://www.aclweb.org/anthology/W18-4930</url>
     <abstract>We describe the TRAPACC system and its variant TRAPACCS that participated in the closed track of the PARSEME Shared Task 2018 on labeling verbal multiword expressions (VMWEs). TRAPACC is a modified arc-standard transition system based on Constant and Nivre’s (2016) model of joint syntactic and lexical analysis in which the oracle is approximated using a classifier. For TRAPACC, the classifier consists of a data-independent dimension reduction and a convolutional neural network (CNN) for learning and labelling transitions. TRAPACCS extends TRAPACC by replacing the softmax layer of the CNN with a support vector machine (SVM). We report the results obtained for 19 languages, for 8 of which our system yields the best results compared to other participating systems in the closed-track of the shared task.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>stodden-qasemizadeh-kallmeyer:2018:W18-49</bibkey>
   </paper>

   <paper id="4931">
     <title>TRAVERSAL at PARSEME Shared Task 2018: Identification of Verbal Multiword Expressions Using a Discriminative Tree-Structured Model</title>
     <author><first>Jakub</first><last>Waszczuk</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>275–282</pages>
     <url>http://www.aclweb.org/anthology/W18-4931</url>
     <abstract>This paper describes a system submitted to the closed track of the PARSEME shared task (edition 1.1) on automatic identification of verbal multiword expressions (VMWEs). The system represents VMWE identification as a labeling task where one of two labels (MWE or not-MWE) must be predicted for each node in the dependency tree based on local context, including adjacent nodes and their labels. The system relies on multiclass logistic regression to determine the globally optimal labeling of a tree. The system ranked 1st in the general cross-lingual ranking of the closed track systems, according to both official evaluation measures: MWE-based F1 and token-based F1.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>waszczuk:2018:W18-49</bibkey>
   </paper>

   <paper id="4932">
     <title>VarIDE at PARSEME Shared Task 2018: Are Variants Really as Alike as Two Peas in a Pod?</title>
     <author><first>Caroline</first><last>Pasquer</last></author>
     <author><first>Carlos</first><last>Ramisch</last></author>
     <author><first>Agata</first><last>Savary</last></author>
     <author><first>Jean-Yves</first><last>Antoine</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>283–289</pages>
     <url>http://www.aclweb.org/anthology/W18-4932</url>
     <abstract>We describe the VarIDE system (standing for Variant IDEntification) which participated in the edition 1.1 of the PARSEME shared task on automatic identification of verbal multiword expressions (VMWEs). Our system focuses on the task of VMWE variant identification by using morphosyntactic information in the training data to predict if candidates extracted from the test corpus could be idiomatic, thanks to a naive Bayes classifier. We report results for 19 languages.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>pasquer-EtAl:2018:W18-49</bibkey>
   </paper>

   <paper id="4933">
     <title>Veyn at PARSEME Shared Task 2018: Recurrent Neural Networks for VMWE Identification</title>
     <author><first>Nicolas</first><last>Zampieri</last></author>
     <author><first>Manon</first><last>Scholivet</last></author>
     <author><first>Carlos</first><last>Ramisch</last></author>
     <author><first>Benoit</first><last>Favre</last></author>
     <booktitle>Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018)</booktitle>
     <month>August</month>
     <year>2018</year>
     <address>Santa Fe, New Mexico, USA</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>290–296</pages>
     <url>http://www.aclweb.org/anthology/W18-4933</url>
     <abstract>This paper describes the Veyn system, submitted to the closed track of the PARSEME Shared Task 2018 on automatic identification of verbal multiword expressions (VMWEs). Veyn is based on a sequence tagger using recurrent neural networks. We represent VMWEs using a variant of the begin-inside-outside encoding scheme combined with the VMWE category tag. In addition to the system description, we present development experiments to determine the best tagging scheme. Veyn is freely available, covers 19 languages, and was ranked ninth (MWE-based) and eight (Token-based) among 13 submissions, considering macro-averaged F1 across languages.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>zampieri-EtAl:2018:W18-49</bibkey>
   </paper>
   <paper id="5000">
     <title>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</title>
     <editor><first>Kazunori</first><last>Komatani</last></editor>
     <editor><first>Diane</first><last>Litman</last></editor>
     <editor><first>Kai</first><last>Yu</last></editor>
     <editor><first>Alex</first><last>Papangelis</last></editor>
     <editor><first>Lawrence</first><last>Cavedon</last></editor>
     <editor><first>Mikio</first><last>Nakano</last></editor>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-50</url>
     <bibtype>book</bibtype>
     <bibkey>SIGdial:2018</bibkey>
   </paper>

   <paper id="5001">
     <title>Zero-Shot Dialog Generation with Cross-Domain Latent Actions</title>
     <author><first>Tiancheng</first><last>Zhao</last></author>
     <author><first>Maxine</first><last>Eskenazi</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–10</pages>
     <url>http://www.aclweb.org/anthology/W18-5001</url>
     <abstract>This paper introduces zero-shot dialog generation (ZSDG), as a step towards neural dialog systems that can instantly generalize to new situations with minimum data. ZSDG requires an end-to-end generative dialog system to generalize to a new domain for which only a domain description is provided and no training dialogs are available. Then a novel learning framework, Action Matching, is proposed. This algorithm can learn a cross-domain embedding space that models the semantics of dialog responses which in turn, enables a neural dialog generation model to generalize to new domains. We evaluate our methods on two datasets, a new synthetic dialog dataset, and an existing human-human multi-domain dialog dataset. Experimental results show that our method is able to achieve superior performance in learning dialog models that can rapidly adapt their behavior to new domains and suggests promising future research.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>zhao-eskenazi:2018:SIGdial</bibkey>
   </paper>

   <paper id="5002">
     <title>Changing the Level of Directness in Dialogue using Dialogue Vector Models and Recurrent Neural Networks</title>
     <author><first>Louisa</first><last>Pragst</last></author>
     <author><first>Stefan</first><last>Ultes</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>11–19</pages>
     <url>http://www.aclweb.org/anthology/W18-5002</url>
     <abstract>In cooperative dialogues, identifying the intent of ones conversation partner and acting accordingly is of great importance. While this endeavour is facilitated by phrasing intentions as directly as possible, we can observe in human-human communication that a number of factors such as cultural norms and politeness may result in expressing one’s intent indirectly. Therefore, in human-computer communication we have to anticipate the possibility of users being indirect and be prepared to interpret their actual meaning. Furthermore, a dialogue system should be able to conform to human expectations by adjusting the degree of directness it uses to improve the user experience. To reach those goals, we propose an approach to differentiate between direct and indirect utterances and find utterances of the opposite characteristic that express the same intent. In this endeavour, we employ dialogue vector models and recurrent neural networks.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>pragst-ultes:2018:SIGdial</bibkey>
   </paper>

   <paper id="5003">
     <title>Modeling Linguistic and Personality Adaptation for Natural Language Generation</title>
     <author><first>Zhichao</first><last>Hu</last></author>
     <author><first>Jean</first><last>Fox Tree</last></author>
     <author><first>Marilyn</first><last>Walker</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>20–31</pages>
     <url>http://www.aclweb.org/anthology/W18-5003</url>
     <abstract>Previous work has shown that conversants adapt to many aspects of their partners’ language. Other work has shown that while every person is unique, they often share general patterns of behavior. Theories of personality aim to explain these shared patterns, and studies have shown that many linguistic cues are correlated with personality traits. We propose an adaptation measure for adaptive natural language generation for dialogs that integrates the predictions of both personality theories and adaptation theories, that can be applied as a dialog unfolds, on a turn by turn basis. We show that our measure meets criteria for validity, and that adaptation varies according to corpora and task, speaker, and the set of features used to model it. We also produce fine-grained models according to the dialog segmentation or the speaker, and demonstrate the decaying trend of adaptation.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>hu-foxtree-walker:2018:SIGdial</bibkey>
   </paper>

   <paper id="5004">
     <title>Estimating User Interest from Open-Domain Dialogue</title>
     <author><first>Michimasa</first><last>Inaba</last></author>
     <author><first>Kenichi</first><last>Takahashi</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>32–40</pages>
     <url>http://www.aclweb.org/anthology/W18-5004</url>
     <abstract>Dialogue personalization is an important issue in the field of open-domain chat-oriented dialogue systems. If these systems could consider their users’ interests, user engagement and satisfaction would be greatly improved. This paper proposes a neural network-based method for estimating users’ interests from their utterances in chat dialogues to personalize dialogue systems’ responses. We introduce a method for effectively extracting topics and user interests from utterances and also propose a pre-training approach that increases learning efficiency. Our experimental results indicate that the proposed model can estimate user’s interest more accurately than baseline approaches.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>inaba-takahashi:2018:SIGdial</bibkey>
   </paper>

   <paper id="5005">
     <title>Does Ability Affect Alignment in Second Language Tutorial Dialogue?</title>
     <author><first>Arabella</first><last>Sinclair</last></author>
     <author><first>Adam</first><last>Lopez</last></author>
     <author><first>C. G.</first><last>Lucas</last></author>
     <author><first>Dragan</first><last>Gasevic</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>41–50</pages>
     <url>http://www.aclweb.org/anthology/W18-5005</url>
     <abstract>The role of alignment between interlocutors in second language learning is different to that in fluent conversational dialogue. Learners gain linguistic skill through increased alignment, yet the extent to which they can align will be constrained by their ability. Tutors may use alignment to teach and encourage the student, yet still must push the student and correct their errors, decreasing alignment. To understand how learner ability interacts with alignment, we measure the influence of ability on lexical priming, an indicator of alignment. We find that lexical priming in learner-tutor dialogues differs from that in conversational and task-based dialogues, and we find evidence that alignment increases with ability and with word complexity.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>sinclair-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5006">
     <title>Just Talking - Modelling Casual Conversation</title>
     <author><first>Emer</first><last>Gilmartin</last></author>
     <author><first>Christian</first><last>Saam</last></author>
     <author><first>Carl</first><last>Vogel</last></author>
     <author><first>Nick</first><last>Campbell</last></author>
     <author><first>Vincent</first><last>Wade</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>51–59</pages>
     <url>http://www.aclweb.org/anthology/W18-5006</url>
     <abstract>Casual conversation has become a focus for artificial dialogue applications. Such talk is ubiquitous and its structure differs from that found in the task-based interactions which have been the focus of dialogue system design for many years. It is unlikely that such conversations can be modelled as an extension of task-based talk. We review theories of casual conversation, report on our studies of the structure of casual dialogue, and outline challenges we see for the development of spoken dialog systems capable of carrying on casual friendly conversation in addition to performing well-defined tasks.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>gilmartin-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5007">
     <title>Neural User Simulation for Corpus-based Policy Optimisation of Spoken Dialogue Systems</title>
     <author><first>Florian</first><last>Kreyssig</last></author>
     <author><first>Iñigo</first><last>Casanueva</last></author>
     <author><first>Paweł</first><last>Budzianowski</last></author>
     <author><first>Milica</first><last>Gasic</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>60–69</pages>
     <url>http://www.aclweb.org/anthology/W18-5007</url>
     <abstract>User Simulators are one of the major tools that enable offline training of task-oriented dialogue systems. For this task the Agenda-Based User Simulator (ABUS) is often used. The ABUS is based on hand-crafted rules and its output is in semantic form. Issues arise from both properties such as limited diversity and the inability to interface a text-level belief tracker. This paper introduces the Neural User Simulator (NUS) whose behaviour is learned from a corpus and which generates natural language, hence needing a less labelled dataset than simulators generating a semantic output. In comparison to much of the past work on this topic, which evaluates user simulators on corpus-based metrics, we use the NUS to train the policy of a reinforcement learning based Spoken Dialogue System. The NUS is compared to the ABUS by evaluating the policies that were trained using the simulators. Cross-model evaluation is performed i.e. training on one simulator and testing on the other. Furthermore, the trained policies are tested on real users. In both evaluation tasks the NUS outperformed the ABUS.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>kreyssig-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5008">
     <title>Introduction method for argumentative dialogue using paired question-answering interchange about personality</title>
     <author><first>Kazuki</first><last>Sakai</last></author>
     <author><first>Ryuichiro</first><last>Higashinaka</last></author>
     <author><first>Yuichiro</first><last>Yoshikawa</last></author>
     <author><first>Hiroshi</first><last>Ishiguro</last></author>
     <author><first>Junji</first><last>Tomita</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>70–79</pages>
     <url>http://www.aclweb.org/anthology/W18-5008</url>
     <abstract>To provide a better discussion experience in current argumentative dialogue systems, it is necessary for the user to feel motivated to participate, even if the system already responds appropriately. In this paper, we propose a method that can smoothly introduce argumentative dialogue by inserting an initial discourse, consisting of question-answer pairs concerning personality. The system can induce interest of the users prior to agreement or disagreement during the main discourse. By disclosing their interests, the users will feel familiarity and motivation to further engage in the argumentative dialogue and understand the system’s intent. To verify the effectiveness of a question-answer dialogue inserted before the argument, a subjective experiment was conducted using a text chat interface. The results suggest that inserting the question-answer dialogue enhances familiarity and naturalness. Notably, the results suggest that women more than men regard the dialogue as more natural and the argument as deepened, following an exchange concerning personality.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>sakai-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5009">
     <title>Automatic Token and Turn Level Language Identification for Code-Switched Text Dialog: An Analysis Across Language Pairs and Corpora</title>
     <author><first>Vikram</first><last>Ramanarayanan</last></author>
     <author><first>Robert</first><last>Pugh</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>80–88</pages>
     <url>http://www.aclweb.org/anthology/W18-5009</url>
     <abstract>We examine the efficacy of various feature–learner combinations for language identification in different types of text-based code-switched interactions – human-human dialog, human-machine dialog as well as monolog – at both the token and turn levels. In order to examine the generalization of such methods across language pairs and datasets, we analyze 10 different datasets of code-switched text. We extract a variety of character- and word-based text features and pass them into multiple learners, including conditional random fields, logistic regressors and recurrent neural networks. We further examine the efficacy of novel character-level embedding and GloVe features in improving performance and observe that our best-performing text system significantly outperforms a majority vote baseline across language pairs and datasets.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>ramanarayanan-pugh:2018:SIGdial</bibkey>
   </paper>

   <paper id="5010">
     <title>A Situated Dialogue System for Learning Structural Concepts in Blocks World</title>
     <author><first>Ian</first><last>Perera</last></author>
     <author><first>James</first><last>Allen</last></author>
     <author><first>Choh Man</first><last>Teng</last></author>
     <author><first>Lucian</first><last>Galescu</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>89–98</pages>
     <url>http://www.aclweb.org/anthology/W18-5010</url>
     <abstract>We present a modular, end-to-end dialogue system for a situated agent to address a multimodal, natural language dialogue task in which the agent learns complex representations of block structure classes through assertions, demonstrations, and questioning. The concept to learn is provided to the user through a set of positive and negative visual examples, from which the user determines the underlying constraints to be provided to the system in natural language. The system in turn asks questions about demonstrated examples and simulates new examples to check its knowledge and verify the user’s description is complete. We find that this task is non-trivial for users and generates natural language that is varied yet understood by our deep language understanding architecture.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>perera-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5011">
     <title>Pardon the Interruption: Managing Turn-Taking through Overlap Resolution in Embodied Artificial Agents</title>
     <author><first>Felix</first><last>Gervits</last></author>
     <author><first>Matthias</first><last>Scheutz</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>99–109</pages>
     <url>http://www.aclweb.org/anthology/W18-5011</url>
     <attachment type="attachment">W18-5011.Attachment.mp4</attachment>
     <abstract>Speech overlap is a common phenomenon in natural conversation and in task-oriented interactions. As human-robot interaction (HRI) becomes more sophisticated, the need to effectively manage turn-taking and resolve overlap becomes more important. In this paper, we introduce a computational model for speech overlap resolution in embodied artificial agents. The model identifies when overlap has occurred and uses timing information, dialogue history, and the agent’s goals to generate context-appropriate behavior. We implement this model in a Nao robot using the DIARC cognitive robotic architecture. The model is evaluated on a corpus of task-oriented human dialogue, and we find that the robot can replicate many of the most common overlap resolution behaviors found in the human data.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>gervits-scheutz:2018:SIGdial</bibkey>
   </paper>

   <paper id="5012">
     <title>Consequences and Factors of Stylistic Differences in Human-Robot Dialogue</title>
     <author><first>Stephanie</first><last>Lukin</last></author>
     <author><first>Kimberly</first><last>Pollard</last></author>
     <author><first>Claire</first><last>Bonial</last></author>
     <author><first>Matthew</first><last>Marge</last></author>
     <author><first>Cassidy</first><last>Henry</last></author>
     <author><first>Ron</first><last>Artstein</last></author>
     <author><first>David</first><last>Traum</last></author>
     <author><first>Clare</first><last>Voss</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>110–118</pages>
     <url>http://www.aclweb.org/anthology/W18-5012</url>
     <abstract>This paper identifies stylistic differences in instruction-giving observed in a corpus of human-robot dialogue. Differences in verbosity and structure (i.e., single-intent vs. multi-intent instructions) arose naturally without restrictions or prior guidance on how users should speak with the robot. Different styles were found to produce different rates of miscommunication, and correlations were found between style differences and individual user variation, trust, and interaction experience with the robot. Understanding potential consequences and factors that influence style can inform design of dialogue systems that are robust to natural variation from human users.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>lukin-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5013">
     <title>Turn-Taking Strategies for Human-Robot Peer-Learning Dialogue</title>
     <author><first>Ranjini</first><last>Das</last></author>
     <author><first>Heather</first><last>Pon-Barry</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>119–129</pages>
     <url>http://www.aclweb.org/anthology/W18-5013</url>
     <abstract>In this paper, we apply the contribution model of grounding to a corpus of human-human peer-mentoring dialogues. From this analysis, we propose effective turn-taking strategies for human-robot interaction with a teachable robot. Specifically, we focus on (1) how robots can encourage humans to present and (2) how robots can signal that they are going to begin a new presentation. We evaluate the strategies against a corpus of human-robot dialogues and offer three guidelines for teachable robots to follow to achieve more human-like collaborative dialogue.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>das-ponbarry:2018:SIGdial</bibkey>
   </paper>

   <paper id="5014">
     <title>Predicting Perceived Age: Both Language Ability and Appearance are Important</title>
     <author><first>Sarah</first><last>Plane</last></author>
     <author><first>Ariel</first><last>Marvasti</last></author>
     <author><first>Tyler</first><last>Egan</last></author>
     <author><first>Casey</first><last>Kennington</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>130–139</pages>
     <url>http://www.aclweb.org/anthology/W18-5014</url>
     <abstract>When interacting with robots in a situated spoken dialogue setting, human dialogue partners tend to assign anthropomorphic and social characteristics to those robots. In this paper, we explore the age and educational level that human dialogue partners assign to three different robotic systems, including an un-embodied spoken dialogue system. We found that how a robot speaks is as important to human perceptions as the way the robot looks. Using the data from our experiment, we derived prosodic, emotional, and linguistic features from the participants to train and evaluate a classifier that predicts perceived intelligence, age, and education level.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>plane-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5015">
     <title>Multimodal Hierarchical Reinforcement Learning Policy for Task-Oriented Visual Dialog</title>
     <author><first>Jiaping</first><last>Zhang</last></author>
     <author><first>Tiancheng</first><last>Zhao</last></author>
     <author><first>Zhou</first><last>Yu</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>140–150</pages>
     <url>http://www.aclweb.org/anthology/W18-5015</url>
     <abstract>Creating an intelligent conversational system that understands vision and language is one of the ultimate goals in Artificial Intelligence (AI) (Winograd, 1972). Extensive research has focused on vision-to-language generation, however, limited research has touched on combining these two modalities in a goal-driven dialog context. We propose a multimodal hierarchical reinforcement learning framework that dynamically integrates vision and language for task-oriented visual dialog. The framework jointly learns the multimodal dialog state representation and the hierarchical dialog policy to improve both dialog task success and efficiency. We also propose a new technique, state adaptation, to integrate context awareness in the dialog state representation. We evaluate the proposed framework and the state adaptation technique in an image guessing game and achieve promising results.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>zhang-zhao-yu:2018:SIGdial</bibkey>
   </paper>

   <paper id="5016">
     <title>Language-Guided Adaptive Perception for Efficient Grounded Communication with Robotic Manipulators in Cluttered Environments</title>
     <author><first>Siddharth</first><last>Patki</last></author>
     <author><first>Thomas</first><last>Howard</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>151–160</pages>
     <url>http://www.aclweb.org/anthology/W18-5016</url>
     <abstract>The utility of collaborative manipulators for shared tasks is highly dependent on the speed and accuracy of communication between the human and the robot. The run-time of recently developed probabilistic inference models for situated symbol grounding of natural language instructions depends on the complexity of the representation of the environment in which they reason. As we move towards more complex bi-directional interactions, tasks, and environments, we need intelligent perception models that can selectively infer precise pose, semantics, and affordances of the objects when inferring exhaustively detailed world models is inefficient and prohibits real-time interaction with these robots. In this paper we propose a model of language and perception for the problem of adapting the configuration of the robot perception pipeline for tasks where constructing exhaustively detailed models of the environment is inefficient and inconsequential for symbol grounding. We present experimental results from a synthetic corpus of natural language instructions for robot manipulation in example environments. The results demonstrate that by adapting perception we get significant gains in terms of run-time for perception and situated symbol grounding of the language instructions without a loss in the accuracy of the latter.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>patki-howard:2018:SIGdial</bibkey>
   </paper>

   <paper id="5017">
     <title>Unsupervised Counselor Dialogue Clustering for Positive Emotion Elicitation in Neural Dialogue System</title>
     <author><first>Nurul</first><last>Lubis</last></author>
     <author><first>Sakriani</first><last>Sakti</last></author>
     <author><first>Koichiro</first><last>Yoshino</last></author>
     <author><first>Satoshi</first><last>Nakamura</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>161–170</pages>
     <url>http://www.aclweb.org/anthology/W18-5017</url>
     <abstract>Positive emotion elicitation seeks to improve user’s emotional state through dialogue system interaction, where a chat-based scenario is layered with an implicit goal to address user’s emotional needs. Standard neural dialogue system approaches still fall short in this situation as they tend to generate only short, generic responses. Learning from expert actions is critical, as these potentially differ from standard dialogue acts. In this paper, we propose using a hierarchical neural network for response generation that is conditioned on 1) expert’s action, 2) dialogue context, and 3) user emotion, encoded from user input. We construct a corpus of interactions between a counselor and 30 participants following a negative emotional exposure to learn expert actions and responses in a positive emotion elicitation scenario. Instead of relying on the expensive, labor intensive, and often ambiguous human annotations, we unsupervisedly cluster the expert’s responses and use the resulting labels to train the network. Our experiments and evaluation show that the proposed approach yields lower perplexity and generates a larger variety of responses.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>lubis-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5018">
     <title>Discovering User Groups for Natural Language Generation</title>
     <author><first>Nikos</first><last>Engonopoulos</last></author>
     <author><first>Christoph</first><last>Teichmann</last></author>
     <author><first>Alexander</first><last>Koller</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>171–179</pages>
     <url>http://www.aclweb.org/anthology/W18-5018</url>
     <abstract>We present a model which predicts how individual users of a dialog system understand and produce utterances based on user groups. In contrast to previous work, these user groups are not specified beforehand, but learned in training. We evaluate on two referring expression (RE) generation tasks; our experiments show that our model can identify user groups and learn how to most effectively talk to them, and can dynamically assign unseen users to the correct groups as they interact with the system.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>engonopoulos-teichmann-koller:2018:SIGdial</bibkey>
   </paper>

   <paper id="5019">
     <title>Controlling Personality-Based Stylistic Variation with Neural Natural Language Generators</title>
     <author><first>Shereen</first><last>Oraby</last></author>
     <author><first>Lena</first><last>Reed</last></author>
     <author><first>Shubhangi</first><last>Tandon</last></author>
     <author><first>Sharath</first><last>T.S.</last></author>
     <author><first>Stephanie</first><last>Lukin</last></author>
     <author><first>Marilyn</first><last>Walker</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>180–190</pages>
     <url>http://www.aclweb.org/anthology/W18-5019</url>
     <abstract>Natural language generators for task-oriented dialogue must effectively realize system dialogue actions and their associated semantics. In many applications, it is also desirable for generators to control the style of an utterance. To date, work on task-oriented neural generation has primarily focused on semantic fidelity rather than achieving stylistic goals, while work on style has been done in contexts where it is difficult to measure content preservation. Here we present three different sequence-to-sequence models and carefully test how well they disentangle content and style. We use a statistical generator, Personage, to synthesize a new corpus of over 88,000 restaurant domain utterances whose style varies according to models of personality, giving us total control over both the semantic content and the stylistic variation in the training data. We then vary the amount of explicit stylistic supervision given to the three models. We show that our most explicit model can simultaneously achieve high fidelity to both semantic and stylistic goals: this model adds a context vector of 36 stylistic parameters as input to the hidden state of the encoder at each time step, showing the benefits of explicit stylistic supervision, even when the amount of training data is large.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>oraby-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5020">
     <title>A Context-aware Convolutional Natural Language Generation model for Dialogue Systems</title>
     <author><first>Sourab</first><last>Mangrulkar</last></author>
     <author><first>Suhani</first><last>Shrivastava</last></author>
     <author><first>Veena</first><last>Thenkanidiyoor</last></author>
     <author><first>Dileep</first><last>Aroor Dinesh</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>191–200</pages>
     <url>http://www.aclweb.org/anthology/W18-5020</url>
     <abstract>Natural language generation (NLG) is an important component in spoken dialog systems (SDSs). A model for NLG involves sequence to sequence learning. State-of-the-art NLG models are built using recurrent neural network (RNN) based sequence to sequence models (Ondřej Dušek and Filip Jurčı́ček, 2016a). Convolutional sequence to sequence based models have been used in the domain of machine translation but their application as Natural Language Generators in dialogue systems is still unexplored. In this work, we propose a novel approach to NLG using convolutional neural network (CNN) based sequence to sequence learning. CNN-based approach allows to build a hierarchical model which encapsulates dependencies between words via shorter path unlike RNNs. In contrast to recurrent models, convolutional approach allows for efficient utilization of computational resources by parallelizing computations over all elements, and eases the learning process by applying constant number of nonlinearities. We also propose to use CNN-based reranker for obtaining responses having semantic correspondence with input dialogue acts. The proposed model is capable of entrainment. Studies using a standard dataset shows the effectiveness of the proposed CNN-based approach to NLG.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>mangrulkar-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5021">
     <title>A Unified Neural Architecture for Joint Dialog Act Segmentation and Recognition in Spoken Dialog System</title>
     <author><first>Tianyu</first><last>Zhao</last></author>
     <author><first>Tatsuya</first><last>Kawahara</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>201–208</pages>
     <url>http://www.aclweb.org/anthology/W18-5021</url>
     <abstract>In spoken dialog systems (SDSs), dialog act (DA) segmentation and recognition provide essential information for response generation. A majority of previous works assumed ground-truth segmentation of DA units, which is not available from automatic speech recognition (ASR) in SDS. We propose a unified architecture based on neural networks, which consists of a sequence tagger for segmentation and a classifier for recognition. The DA recognition model is based on hierarchical neural networks to incorporate the context of preceding sentences. We investigate sharing some layers of the two components so that they can be trained jointly and learn generalized features from both tasks. An evaluation on the Switchboard Dialog Act (SwDA) corpus shows that the jointly-trained models outperform independently-trained models, single-step models, and other reported results in DA segmentation, recognition, and joint tasks.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>zhao-kawahara:2018:SIGdial</bibkey>
   </paper>

   <paper id="5022">
     <title>Cost-Sensitive Active Learning for Dialogue State Tracking</title>
     <author><first>Kaige</first><last>Xie</last></author>
     <author><first>Cheng</first><last>Chang</last></author>
     <author><first>Liliang</first><last>Ren</last></author>
     <author><first>Lu</first><last>Chen</last></author>
     <author><first>Kai</first><last>Yu</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>209–213</pages>
     <url>http://www.aclweb.org/anthology/W18-5022</url>
     <abstract>Dialogue state tracking (DST), when formulated as a supervised learning problem, relies on labelled data. Since dialogue state annotation usually requires labelling all turns of a single dialogue and utilizing context information, it is very expensive to annotate all available unlabelled data. In this paper, a novel cost-sensitive active learning framework is proposed based on a set of new dialogue-level query strategies. This is the first attempt to apply active learning for dialogue state tracking. Experiments on DSTC2 show that active learning with mixed data query strategies can effectively achieve the same DST performance with significantly less data annotation compared to traditional training approaches.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>xie-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5023">
     <title>Discourse Coherence in the Wild: A Dataset, Evaluation and Methods</title>
     <author><first>Alice</first><last>Lai</last></author>
     <author><first>Joel</first><last>Tetreault</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>214–223</pages>
     <url>http://www.aclweb.org/anthology/W18-5023</url>
     <attachment type="attachment">W18-5023.Attachment.pdf</attachment>
     <abstract>To date there has been very little work on assessing discourse coherence methods on real-world data. To address this, we present a new corpus of real-world texts (GCDC) as well as the first large-scale evaluation of leading discourse coherence algorithms. We show that neural models, including two that we introduce here (SentAvg and ParSeq), tend to perform best. We analyze these performance differences and discuss patterns we observed in low coherence texts in four domains.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>lai-tetreault:2018:SIGdial</bibkey>
   </paper>

   <paper id="5024">
     <title>Neural Dialogue Context Online End-of-Turn Detection</title>
     <author><first>Ryo</first><last>Masumura</last></author>
     <author><first>Tomohiro</first><last>Tanaka</last></author>
     <author><first>Atsushi</first><last>Ando</last></author>
     <author><first>Ryo</first><last>Ishii</last></author>
     <author><first>Ryuichiro</first><last>Higashinaka</last></author>
     <author><first>Yushi</first><last>Aono</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>224–228</pages>
     <url>http://www.aclweb.org/anthology/W18-5024</url>
     <abstract>This paper proposes a fully neural network based dialogue-context online end-of-turn detection method that can utilize long-range interactive information extracted from both speaker’s utterances and collocutor’s utterances. The proposed method combines multiple time-asynchronous long short-term memory recurrent neural networks, which can capture speaker’s and collocutor’s multiple sequential features, and their interactions. On the assumption of applying the proposed method to spoken dialogue systems, we introduce speaker’s acoustic sequential features and collocutor’s linguistic sequential features, each of which can be extracted in an online manner. Our evaluation confirms the effectiveness of taking dialogue context formed by the speaker’s utterances and collocutor’s utterances into consideration.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>masumura-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5025">
     <title>Spoken Dialogue for Information Navigation</title>
     <author><first>Alexandros</first><last>Papangelis</last></author>
     <author><first>Panagiotis</first><last>Papadakos</last></author>
     <author><first>Yannis</first><last>Stylianou</last></author>
     <author><first>Yannis</first><last>Tzitzikas</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>229–234</pages>
     <url>http://www.aclweb.org/anthology/W18-5025</url>
     <abstract>Aiming to expand the current research paradigm for training conversational AI agents that can address real-world challenges, we take a step away from traditional slot-filling goal-oriented spoken dialogue systems (SDS) and model the dialogue in a way that allows users to be more expressive in describing their needs. The goal is to help users make informed decisions rather than being fed matching items. To this end, we describe the Linked-Data SDS (LD-SDS), a system that exploits semantic knowledge bases that connect to linked data, and supports complex constraints and preferences. We describe the required changes in language understanding and state tracking, and the need for mined features, and we report the promising results (in terms of semantic errors, effort, etc) of a preliminary evaluation after training two statistical dialogue managers in various conditions.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>papangelis-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5026">
     <title>Improving User Impression in Spoken Dialog System with Gradual Speech Form Control</title>
     <author><first>Yukiko</first><last>Kageyama</last></author>
     <author><first>Yuya</first><last>Chiba</last></author>
     <author><first>Takashi</first><last>Nose</last></author>
     <author><first>Akinori</first><last>Ito</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>235–240</pages>
     <url>http://www.aclweb.org/anthology/W18-5026</url>
     <abstract>This paper examines a method to improve the user impression of a spoken dialog system by introducing a mechanism that gradually changes form of utterances every time the user uses the system. In some languages, including Japanese, the form of utterances changes corresponding to social relationship between the talker and the listener. Thus, this mechanism can be effective to express the system’s intention to make social distance to the user closer; however, an actual effect of this method is not investigated enough when introduced to the dialog system. In this paper, we conduct dialog experiments and show that controlling the form of system utterances can improve the users’ impression.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>kageyama-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5027">
     <title>A Bilingual Interactive Human Avatar Dialogue System</title>
     <author><first>Dana</first><last>Abu Ali</last></author>
     <author><first>Muaz</first><last>Ahmad</last></author>
     <author><first>Hayat</first><last>Al Hassan</last></author>
     <author><first>Paula</first><last>Dozsa</last></author>
     <author><first>Ming</first><last>Hu</last></author>
     <author><first>Jose</first><last>Varias</last></author>
     <author><first>Nizar</first><last>Habash</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>241–244</pages>
     <url>http://www.aclweb.org/anthology/W18-5027</url>
     <abstract>This demonstration paper presents a bilingual (Arabic-English) interactive human avatar dialogue system. The system is named TOIA (time-offset interaction application), as it simulates face-to-face conversations between humans using digital human avatars recorded in the past. TOIA is a conversational agent, similar to a chat bot, except that it is based on an actual human being and can be used to preserve and tell stories. The system is designed to allow anybody, simply using a laptop, to create an avatar of themselves, thus facilitating cross-cultural and cross-generational sharing of narratives to wider audiences. The system currently supports monolingual and cross-lingual dialogues in Arabic and English, but can be extended to other languages.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>abuali-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5028">
     <title>DialCrowd: A toolkit for easy dialog system assessment</title>
     <author><first>Kyusong</first><last>Lee</last></author>
     <author><first>Tiancheng</first><last>Zhao</last></author>
     <author><first>Alan W</first><last>Black</last></author>
     <author><first>Maxine</first><last>Eskenazi</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>245–248</pages>
     <url>http://www.aclweb.org/anthology/W18-5028</url>
     <abstract>When creating a dialog system, developers need to test each version to ensure that it is performing correctly. Recently the trend has been to test on large datasets or to ask many users to try out a system. Crowdsourcing has solved the issue of finding users, but it presents new challenges such as how to use a crowdsourcing platform and what type of test is appropriate. DialCrowd has been designed to make system assessment easier and to ensure the quality of the result. This paper describes DialCrowd, what specific needs it fulfills and how it works. It then relates a test of DialCrowd by a group of dialog system developer.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>lee-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5029">
     <title>Leveraging Multimodal Dialog Technology for the Design of Automated and Interactive Student Agents for Teacher Training</title>
     <author><first>David</first><last>Pautler</last></author>
     <author><first>Vikram</first><last>Ramanarayanan</last></author>
     <author><first>Kirby</first><last>Cofino</last></author>
     <author><first>Patrick</first><last>Lange</last></author>
     <author><first>David</first><last>Suendermann-Oeft</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>249–252</pages>
     <url>http://www.aclweb.org/anthology/W18-5029</url>
     <abstract>We present a paradigm for interactive teacher training that leverages multimodal dialog technology to puppeteer custom-designed embodied conversational agents (ECAs) in student roles. We used the open-source multimodal dialog system HALEF to implement a small-group classroom math discussion involving Venn diagrams where a human teacher candidate has to interact with two student ECAs whose actions are controlled by the dialog system. Such an automated paradigm has the potential to be extended and scaled to a wide range of interactive simulation scenarios in education, medicine, and business where group interaction training is essential.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>pautler-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5030">
     <title>An Empirical Study of Self-Disclosure in Spoken Dialogue Systems</title>
     <author><first>Abhilasha</first><last>Ravichander</last></author>
     <author><first>Alan W</first><last>Black</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>253–263</pages>
     <url>http://www.aclweb.org/anthology/W18-5030</url>
     <abstract>Self-disclosure is a key social strategy employed in conversation to build relations and increase conversational depth. It has been heavily studied in psychology and linguistic literature, particularly for its ability to induce self-disclosure from the recipient, a phenomena known as reciprocity. However, we know little about how self-disclosure manifests in conversation with automated dialog systems, especially as any self-disclosure on the part of a dialog system is patently disingenuous. In this work, we run a large-scale quantitative analysis on the effect of self-disclosure by analyzing interactions between real-world users and a spoken dialog system in the context of social conversation. We find that indicators of reciprocity occur even in human-machine dialog, with far-reaching implications for chatbots in a variety of domains including education, negotiation and social dialog.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>ravichander-black:2018:SIGdial</bibkey>
   </paper>

   <paper id="5031">
     <title>Role play-based question-answering by real users for building chatbots with consistent personalities</title>
     <author><first>Ryuichiro</first><last>Higashinaka</last></author>
     <author><first>Masahiro</first><last>Mizukami</last></author>
     <author><first>Hidetoshi</first><last>Kawabata</last></author>
     <author><first>Emi</first><last>Yamaguchi</last></author>
     <author><first>Noritake</first><last>Adachi</last></author>
     <author><first>Junji</first><last>Tomita</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>264–272</pages>
     <url>http://www.aclweb.org/anthology/W18-5031</url>
     <abstract>Having consistent personalities is important for chatbots if we want them to be believable. Typically, many question-answer pairs are prepared by hand for achieving consistent responses; however, the creation of such pairs is costly. In this study, our goal is to collect a large number of question-answer pairs for a particular character by using role play-based question-answering in which multiple users play the roles of certain characters and respond to questions by online users. Focusing on two famous characters, we conducted a large-scale experiment to collect question-answer pairs by using real users. We evaluated the effectiveness of role play-based question-answering and found that, by using our proposed method, the collected pairs lead to good-quality chatbots that exhibit consistent personalities.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>higashinaka-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5032">
     <title>Addressing Objects and Their Relations: The Conversational Entity Dialogue Model</title>
     <author><first>Stefan</first><last>Ultes</last></author>
     <author><first>Paweł</first><last>Budzianowski</last></author>
     <author><first>Iñigo</first><last>Casanueva</last></author>
     <author><first>Lina M.</first><last>Rojas Barahona</last></author>
     <author><first>Bo-Hsiang</first><last>Tseng</last></author>
     <author><first>Yen-chen</first><last>Wu</last></author>
     <author><first>Steve</first><last>Young</last></author>
     <author><first>Milica</first><last>Gasic</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>273–283</pages>
     <url>http://www.aclweb.org/anthology/W18-5032</url>
     <attachment type="attachment">W18-5032.Attachment.pdf</attachment>
     <abstract>Statistical spoken dialogue systems usually rely on a single- or multi-domain dialogue model that is restricted in its capabilities of modelling complex dialogue structures, e.g., relations. In this work, we propose a novel dialogue model that is centred around entities and is able to model relations as well as multiple entities of the same type. We demonstrate in a prototype implementation benefits of relation modelling on the dialogue level and show that a trained policy using these relations outperforms the multi-domain baseline. Furthermore, we show that by modelling the relations on the dialogue level, the system is capable of processing relations present in the user input and even learns to address them in the system response.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>ultes-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5033">
     <title>Conversational Image Editing: Incremental Intent Identification in a New Dialogue Task</title>
     <author><first>Ramesh</first><last>Manuvinakurike</last></author>
     <author><first>Trung</first><last>Bui</last></author>
     <author><first>Walter</first><last>Chang</last></author>
     <author><first>Kallirroi</first><last>Georgila</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>284–295</pages>
     <url>http://www.aclweb.org/anthology/W18-5033</url>
     <abstract>We present “conversational image editing”, a novel real-world application domain combining dialogue, visual information, and the use of computer vision. We discuss the importance of dialogue incrementality in this task, and build various models for incremental intent identification based on deep learning and traditional classification algorithms. We show how our model based on convolutional neural networks outperforms models based on random forests, long short term memory networks, and conditional random fields. By training embeddings based on image-related dialogue corpora, we outperform pre-trained out-of-the-box embeddings, for intention identification tasks. Our experiments also provide evidence that incremental intent processing may be more efficient for the user and could save time in accomplishing tasks.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>manuvinakurike-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5034">
     <title>Fine-Grained Discourse Structures in Continuation Semantics</title>
     <author><first>Timothée</first><last>Bernard</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>296–305</pages>
     <url>http://www.aclweb.org/anthology/W18-5034</url>
     <abstract>In this work, we are interested in the computation of logical representations of discourse. We argue that all discourse connectives are anaphors obeying different sets of constraints and show how this view allows one to account for the semantically parenthetical use of attitude verbs and verbs of report (e.g., think, say) and for sequences of conjunctions (A CONJ_1 B CONJ_2 C). We implement this proposal in event semantics using de Groote (2006)’s dynamic framework.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>bernard:2018:SIGdial</bibkey>
   </paper>

   <paper id="5035">
     <title>Automatic Extraction of Causal Relations from Text using Linguistically Informed Deep Neural Networks</title>
     <author><first>Tirthankar</first><last>Dasgupta</last></author>
     <author><first>Rupsa</first><last>Saha</last></author>
     <author><first>Lipika</first><last>Dey</last></author>
     <author><first>Abir</first><last>Naskar</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>306–316</pages>
     <url>http://www.aclweb.org/anthology/W18-5035</url>
     <abstract>In this paper we have proposed a linguistically informed recursive neural network architecture for automatic extraction of cause-effect relations from text. These relations can be expressed in arbitrarily complex ways. The architecture uses word level embeddings and other linguistic features to detect causal events and their effects mentioned within a sentence. The extracted events and their relations are used to build a causal-graph after clustering and appropriate generalization, which is then used for predictive purposes. We have evaluated the performance of the proposed extraction model with respect to two baseline systems,one a rule-based classifier, and the other a conditional random field (CRF) based supervised model. We have also compared our results with related work reported in the past by other authors on SEMEVAL data set, and found that the proposed bi-directional LSTM model enhanced with an additional linguistic layer performs better. We have also worked extensively on creating new annotated datasets from publicly available data, which we are willing to share with the community.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>dasgupta-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5036">
     <title>Toward zero-shot Entity Recognition in Task-oriented Conversational Agents</title>
     <author><first>Marco</first><last>Guerini</last></author>
     <author><first>Simone</first><last>Magnolini</last></author>
     <author><first>Vevake</first><last>Balaraman</last></author>
     <author><first>Bernardo</first><last>Magnini</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>317–326</pages>
     <url>http://www.aclweb.org/anthology/W18-5036</url>
     <abstract>We present a domain portable zero-shot learning approach for entity recognition in task-oriented conversational agents, which does not assume any annotated sentences at training time. Rather, we derive a neural model of the entity names based only on available gazetteers, and then apply the model to recognize new entities in the context of user utterances. In order to evaluate our working hypothesis we focus on nominal entities that are largely used in e-commerce to name products. Through a set of experiments in two languages (English and Italian) and three different domains (furniture, food, clothing), we show that the neural gazetteer-based approach outperforms several competitive baselines, with minimal requirements of linguistic features.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>guerini-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5037">
     <title>Identifying Explicit Discourse Connectives in German</title>
     <author><first>Peter</first><last>Bourgonje</last></author>
     <author><first>Manfred</first><last>Stede</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>327–331</pages>
     <url>http://www.aclweb.org/anthology/W18-5037</url>
     <abstract>We are working on an end-to-end Shallow Discourse Parsing system for German and in this paper focus on the first subtask: the identification of explicit connectives. Starting with the feature set from an English system and a Random Forest classifier, we evaluate our approach on a (relatively small) German annotated corpus, the Potsdam Commentary Corpus. We introduce new features and experiment with including additional training data obtained through annotation projection and achieve an f-score of 83.89.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>bourgonje-stede:2018:SIGdial</bibkey>
   </paper>

   <paper id="5038">
     <title>Feudal Dialogue Management with Jointly Learned Feature Extractors</title>
     <author><first>Iñigo</first><last>Casanueva</last></author>
     <author><first>Paweł</first><last>Budzianowski</last></author>
     <author><first>Stefan</first><last>Ultes</last></author>
     <author><first>Florian</first><last>Kreyssig</last></author>
     <author><first>Bo-Hsiang</first><last>Tseng</last></author>
     <author><first>Yen-chen</first><last>Wu</last></author>
     <author><first>Milica</first><last>Gasic</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>332–337</pages>
     <url>http://www.aclweb.org/anthology/W18-5038</url>
     <abstract>Reinforcement learning (RL) is a promising dialogue policy optimisation approach, but traditional RL algorithms fail to scale to large domains. Recently, Feudal Dialogue Management (FDM), has shown to increase the scalability to large domains by decomposing the dialogue management decision into two steps, making use of the domain ontology to abstract the dialogue state in each step. In order to abstract the state space, however, previous work on FDM relies on handcrafted feature functions. In this work, we show that these feature functions can be learned jointly with the policy model while obtaining similar performance, even outperforming the handcrafted features in several environments and domains.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>casanueva-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5039">
     <title>Variational Cross-domain Natural Language Generation for Spoken Dialogue Systems</title>
     <author><first>Bo-Hsiang</first><last>Tseng</last></author>
     <author><first>Florian</first><last>Kreyssig</last></author>
     <author><first>Paweł</first><last>Budzianowski</last></author>
     <author><first>Iñigo</first><last>Casanueva</last></author>
     <author><first>Yen-chen</first><last>Wu</last></author>
     <author><first>Stefan</first><last>Ultes</last></author>
     <author><first>Milica</first><last>Gasic</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>338–343</pages>
     <url>http://www.aclweb.org/anthology/W18-5039</url>
     <abstract>Cross-domain natural language generation (NLG) is still a difficult task within spoken dialogue modelling. Given a semantic representation provided by the dialogue manager, the language generator should generate sentences that convey desired information. Traditional template-based generators can produce sentences with all necessary information, but these sentences are not sufficiently diverse. With RNN-based models, the diversity of the generated sentences can be high, however, in the process some information is lost. In this work, we improve an RNN-based generator by considering latent information at the sentence level during generation using conditional variational auto-encoder architecture. We demonstrate that our model outperforms the original RNN-based generator, while yielding highly diverse sentences. In addition, our model performs better when the training data is limited.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>tseng-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5040">
     <title>Coherence Modeling Improves Implicit Discourse Relation Recognition</title>
     <author><first>Noriki</first><last>Nishida</last></author>
     <author><first>Hideki</first><last>Nakayama</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>344–349</pages>
     <url>http://www.aclweb.org/anthology/W18-5040</url>
     <abstract>The research described in this paper examines how to learn linguistic knowledge associated with discourse relations from unlabeled corpora. We introduce an unsupervised learning method on text coherence that could produce numerical representations that improve implicit discourse relation recognition in a semi-supervised manner. We also empirically examine two variants of coherence modeling: order-oriented and topic-oriented negative sampling, showing that, of the two, topic-oriented negative sampling tends to be more effective.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>nishida-nakayama:2018:SIGdial</bibkey>
   </paper>

   <paper id="5041">
     <title>Adversarial Learning of Task-Oriented Neural Dialog Models</title>
     <author><first>Bing</first><last>Liu</last></author>
     <author><first>Ian</first><last>Lane</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>350–359</pages>
     <url>http://www.aclweb.org/anthology/W18-5041</url>
     <abstract>In this work, we propose an adversarial learning method for reward estimation in reinforcement learning (RL) based task-oriented dialog models. Most of the current RL based task-oriented dialog systems require the access to a reward signal from either user feedback or user ratings. Such user ratings, however, may not always be consistent or available in practice. Furthermore, online dialog policy learning with RL typically requires a large number of queries to users, suffering from sample efficiency problem. To address these challenges, we propose an adversarial learning method to learn dialog rewards directly from dialog samples. Such rewards are further used to optimize the dialog policy with policy gradient based RL. In the evaluation in a restaurant search domain, we show that the proposed adversarial dialog learning method achieves advanced dialog success rate comparing to strong baseline methods. We further discuss the covariate shift problem in online adversarial dialog learning and show how we can address that with partial access to user feedback.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>liu-lane:2018:SIGdial</bibkey>
   </paper>

   <paper id="5042">
     <title>Constructing a Lexicon of English Discourse Connectives</title>
     <author><first>Debopam</first><last>Das</last></author>
     <author><first>Tatjana</first><last>Scheffler</last></author>
     <author><first>Peter</first><last>Bourgonje</last></author>
     <author><first>Manfred</first><last>Stede</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>360–365</pages>
     <url>http://www.aclweb.org/anthology/W18-5042</url>
     <abstract>We present a new lexicon of English discourse connectives called DiMLex-Eng, built by merging information from two annotated corpora and an additional list of relation signals from the literature. The format follows the German connective lexicon DiMLex, which provides a cross-linguistically applicable XML schema. DiMLex-Eng contains 149 English connectives, and gives information on syntactic categories, discourse semantics and non-connective uses (if any). We report on the development steps and discuss design decisions encountered in the lexicon expansion phase. The resource is freely available for use in studies of discourse structure and computational applications.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>das-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5043">
     <title>Maximizing SLU Performance with Minimal Training Data Using Hybrid RNN Plus Rule-based Approach</title>
     <author><first>Takeshi</first><last>Homma</last></author>
     <author><first>Adriano S.</first><last>Arantes</last></author>
     <author><first>Maria Teresa</first><last>Gonzalez Diaz</last></author>
     <author><first>Masahito</first><last>Togami</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>366–370</pages>
     <url>http://www.aclweb.org/anthology/W18-5043</url>
     <abstract>Spoken language understanding (SLU) by using recurrent neural networks (RNN) achieves good performances for large training data sets, but collecting large training datasets is a challenge, especially for new voice applications. Therefore, the purpose of this study is to maximize SLU performances, especially for small training data sets. To this aim, we propose a novel CRF-based dialog act selector which chooses suitable dialog acts from outputs of RNN SLU and rule-based SLU. We evaluate the selector by using DSTC2 corpus when RNN SLU is trained by less than 1,000 training sentences. The evaluation demonstrates the selector achieves Micro F1 better than both RNN and rule-based SLUs. In addition, it shows the selector achieves better Macro F1 than RNN SLU and the same Macro F1 as rule-based SLU. Thus, we confirmed our method offers advantages in SLU performances for small training data sets.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>homma-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5044">
     <title>An Analysis of the Effect of Emotional Speech Synthesis on Non-Task-Oriented Dialogue System</title>
     <author><first>Yuya</first><last>Chiba</last></author>
     <author><first>Takashi</first><last>Nose</last></author>
     <author><first>Taketo</first><last>Kase</last></author>
     <author><first>Mai</first><last>Yamanaka</last></author>
     <author><first>Akinori</first><last>Ito</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>371–375</pages>
     <url>http://www.aclweb.org/anthology/W18-5044</url>
     <attachment type="attachment">W18-5044.Attachment.pdf</attachment>
     <abstract>This paper explores the effect of emotional speech synthesis on a spoken dialogue system when the dialogue is non-task-oriented. Although the use of emotional speech responses have been shown to be effective in a limited domain, e.g., scenario-based and counseling dialogue, the effect is still not clear in the non-task-oriented dialogue such as voice chatting. For this purpose, we constructed a simple dialogue system with example- and rule-based dialogue management. In the system, two types of emotion labeling with emotion estimation are adopted, i.e., system-driven and user-cooperative emotion labeling. We conducted a dialogue experiment where subjects evaluate the subjective quality of the system and the dialogue from the multiple aspects such as richness of the dialogue and impression of the agent. We then analyze and discuss the results and show the advantage of using appropriate emotions for the expressive speech responses in the non-task-oriented system.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>chiba-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5045">
     <title>Multi-task Learning for Joint Language Understanding and Dialogue State Tracking</title>
     <author><first>Abhinav</first><last>Rastogi</last></author>
     <author><first>Raghav</first><last>Gupta</last></author>
     <author><first>Dilek</first><last>Hakkani-Tur</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>376–384</pages>
     <url>http://www.aclweb.org/anthology/W18-5045</url>
     <abstract>This paper presents a novel approach for multi-task learning of language understanding (LU) and dialogue state tracking (DST) in task-oriented dialogue systems. Multi-task training enables the sharing of the neural network layers responsible for encoding the user utterance for both LU and DST and improves performance while reducing the number of network parameters. In our proposed framework, DST operates on a set of candidate values for each slot that has been mentioned so far. These candidate sets are generated using LU slot annotations for the current user utterance, dialogue acts corresponding to the preceding system utterance and the dialogue state estimated for the previous turn, enabling DST to handle slots with a large or unbounded set of possible values and deal with slot values not seen during training. Furthermore, to bridge the gap between training and inference, we investigate the use of scheduled sampling on LU output for the current user utterance as well as the DST output for the preceding turn.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>rastogi-gupta-hakkanitur:2018:SIGdial</bibkey>
   </paper>

   <paper id="5046">
     <title>Weighting Model Based on Group Dynamics to Measure Convergence in Multi-party Dialogue</title>
     <author><first>Zahra</first><last>Rahimi</last></author>
     <author><first>Diane</first><last>Litman</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>385–390</pages>
     <url>http://www.aclweb.org/anthology/W18-5046</url>
     <abstract>This paper proposes a new weighting method for extending a dyad-level measure of convergence to multi-party dialogues by considering group dynamics instead of simply averaging. Experiments indicate the usefulness of the proposed weighted measure and also show that in general a proper weighting of the dyad-level measures performs better than non-weighted averaging in multiple tasks.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>rahimi-litman:2018:SIGdial</bibkey>
   </paper>

   <paper id="5047">
     <title>Concept Transfer Learning for Adaptive Language Understanding</title>
     <author><first>Su</first><last>Zhu</last></author>
     <author><first>Kai</first><last>Yu</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>391–399</pages>
     <url>http://www.aclweb.org/anthology/W18-5047</url>
     <abstract>Concept definition is important in language understanding (LU) adaptation since literal definition difference can easily lead to data sparsity even if different data sets are actually semantically correlated. To address this issue, in this paper, a novel concept transfer learning approach is proposed. Here, substructures within literal concept definition are investigated to reveal the relationship between concepts. A hierarchical semantic representation for concepts is proposed, where a semantic slot is represented as a composition of <i>atomic concepts</i>. Based on this new hierarchical representation, transfer learning approaches are developed for adaptive LU. The approaches are applied to two tasks: value set mismatch and domain adaptation, and evaluated on two LU benchmarks: ATIS and DSTC 2&amp;3. Thorough empirical studies validate both the efficiency and effectiveness of the proposed method. In particular, we achieve state-of-the-art performance (F₁-score 96.08%) on ATIS by only using lexicon features.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>zhu-yu:2018:SIGdial</bibkey>
   </paper>

   <paper id="5048">
     <title>Cogent: A Generic Dialogue System Shell Based on a Collaborative Problem Solving Model</title>
     <author><first>Lucian</first><last>Galescu</last></author>
     <author><first>Choh Man</first><last>Teng</last></author>
     <author><first>James</first><last>Allen</last></author>
     <author><first>Ian</first><last>Perera</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>400–409</pages>
     <url>http://www.aclweb.org/anthology/W18-5048</url>
     <abstract>The bulk of current research in dialogue systems is focused on fairly simple task models, primarily state-based. Progress on developing dialogue systems for more complex tasks has been limited by the lack generic toolkits to build from. In this paper we report on our development from the ground up of a new dialogue model based on collaborative problem solving. We implemented the model in a dialogue system shell (Cogent) that al-lows developers to plug in problem-solving agents to create dialogue systems in new domains. The Cogent shell has now been used by several independent teams of researchers to develop dialogue systems in different domains, with varied lexicons and interaction style, each with their own problem-solving back-end. We believe this to be the first practical demonstration of the feasibility of a CPS-based dialogue system shell.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>galescu-EtAl:2018:SIGdial</bibkey>
   </paper>

   <paper id="5049">
     <title>Identifying Domain Independent Update Intents in Task Based Dialogs</title>
     <author><first>Prakhar</first><last>Biyani</last></author>
     <author><first>Cem</first><last>Akkaya</last></author>
     <author><first>Kostas</first><last>Tsioutsiouliklis</last></author>
     <booktitle>Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue</booktitle>
     <month>July</month>
     <year>2018</year>
     <address>Melbourne, Australia</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>410–419</pages>
     <url>http://www.aclweb.org/anthology/W18-5049</url>
     <abstract>One important problem in task-based conversations is that of effectively updating the belief estimates of user-mentioned slot-value pairs. Given a user utterance, the intent of a slot-value pair is captured using dialog acts (DA) expressed in that utterance. However, in certain cases, DA’s fail to capture the actual update intent of the user. In this paper, we describe such cases and propose a new type of semantic class for user intents. This new type, Update Intents (UI), is directly related to the type of update a user intends to perform for a slot-value pair. We define five types of UI’s, which are independent of the domain of the conversation. We build a multi-class classification model using LSTM’s to identify the type of UI in user utterances in the Restaurant and Shopping domains. Experimental results show that our models achieve strong classification performance in terms of F-1 score.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>biyani-akkaya-tsioutsiouliklis:2018:SIGdial</bibkey>
   </paper>

   <paper id="5100">
     <title>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</title>
     <editor><first>Darja</first><last>Fišer</last></editor>
     <editor><first>Ruihong</first><last>Huang</last></editor>
     <editor><first>Vinodkumar</first><last>Prabhakaran</last></editor>
     <editor><first>Rob</first><last>Voigt</last></editor>
     <editor><first>Zeerak</first><last>Waseem</last></editor>
     <editor><first>Jacqueline</first><last>Wernimont</last></editor>
     <month>October</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-51</url>
     <bibtype>book</bibtype>
     <bibkey>ALW2:2018</bibkey>
   </paper>

   <paper id="5101">
     <title>Neural Character-based Composition Models for Abuse Detection</title>
     <author><first>Pushkar</first><last>Mishra</last></author>
     <author><first>Helen</first><last>Yannakoudakis</last></author>
     <author><first>Ekaterina</first><last>Shutova</last></author>
     <booktitle>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</booktitle>
     <month>October</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–10</pages>
     <url>http://www.aclweb.org/anthology/W18-5101</url>
     <abstract>The advent of social media in recent years has fed into some highly undesirable phenomena such as proliferation of offensive language, hate speech, sexist remarks, etc. on the Internet. In light of this, there have been several efforts to automate the detection and moderation of such abusive content. However, deliberate obfuscation of words by users to evade detection poses a serious challenge to the effectiveness of these efforts. The current state of the art approaches to abusive language detection, based on recurrent neural networks, do not explicitly address this problem and resort to a generic OOV (out of vocabulary) embedding for unseen words. However, in using a single embedding for all unseen words we lose the ability to distinguish between obfuscated and non-obfuscated or rare words. In this paper, we address this problem by designing a model that can compose embeddings for unseen words. We experimentally demonstrate that our approach significantly advances the current state of the art in abuse detection on datasets from two different domains, namely Twitter and Wikipedia talk page.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>mishra-yannakoudakis-shutova:2018:ALW2</bibkey>
   </paper>

   <paper id="5102">
     <title>Hate Speech Dataset from a White Supremacy Forum</title>
     <author><first>Ona</first><last>de Gibert</last></author>
     <author><first>Naiara</first><last>Perez</last></author>
     <author><first>Aitor</first><last>García Pablos</last></author>
     <author><first>Montse</first><last>Cuadros</last></author>
     <booktitle>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</booktitle>
     <month>October</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>11–20</pages>
     <url>http://www.aclweb.org/anthology/W18-5102</url>
     <abstract>Hate speech is commonly defined as any communication that disparages a target group of people based on some characteristic such as race, colour, ethnicity, gender, sexual orientation, nationality, religion, or other characteristic.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>degibert-EtAl:2018:ALW2</bibkey>
   </paper>

   <paper id="5103">
     <title>A Review of Standard Text Classification Practices for Multi-label Toxicity Identification of Online Content</title>
     <author><first>Isuru</first><last>Gunasekara</last></author>
     <author><first>Isar</first><last>Nejadgholi</last></author>
     <booktitle>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</booktitle>
     <month>October</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>21–25</pages>
     <url>http://www.aclweb.org/anthology/W18-5103</url>
     <abstract>Language toxicity identification presents a gray area in the ethical debate surrounding freedom of speech and censorship. Today’s social media landscape is littered with unfiltered content that can be anywhere from slightly abusive to hate inducing. In response, we focused on training a multi-label classifier to detect both the type and level of toxicity in online content. This content is typically colloquial and conversational in style. Its classification therefore requires huge amounts of annotated data due to its variability and inconsistency. We compare standard methods of text classification in this task. A conventional one-vs-rest SVM classifier with character and word level frequency-based representation of text reaches 0.9763 ROC AUC score. We demonstrated that leveraging more advanced technologies such as word embeddings, recurrent neural networks, attention mechanism, stacking of classifiers and semi-supervised training can improve the ROC AUC score of classification to 0.9862. We suggest that in order to choose the right model one has to consider the accuracy of models as well as inference complexity based on the application.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>gunasekara-nejadgholi:2018:ALW2</bibkey>
   </paper>

   <paper id="5104">
     <title>Predictive Embeddings for Hate Speech Detection on Twitter</title>
     <author><first>Rohan</first><last>Kshirsagar</last></author>
     <author><first>Tyrus</first><last>Cukuvac</last></author>
     <author><first>Kathy</first><last>McKeown</last></author>
     <author><first>Susan</first><last>McGregor</last></author>
     <booktitle>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</booktitle>
     <month>October</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>26–32</pages>
     <url>http://www.aclweb.org/anthology/W18-5104</url>
     <abstract>We present a neural-network based approach to classifying online hate speech in general,</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>kshirsagar-EtAl:2018:ALW2</bibkey>
   </paper>

   <paper id="5105">
     <title>Challenges for Toxic Comment Classification: An In-Depth Error Analysis</title>
     <author><first>Betty</first><last>van Aken</last></author>
     <author><first>Julian</first><last>Risch</last></author>
     <author><first>Ralf</first><last>Krestel</last></author>
     <author><first>Alexander</first><last>Löser</last></author>
     <booktitle>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</booktitle>
     <month>October</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>33–42</pages>
     <url>http://www.aclweb.org/anthology/W18-5105</url>
     <abstract>Toxic comment classification has become an active research field with many recently proposed approaches. However, while these approaches address some of the task’s challenges others still remain unsolved and directions for further research are needed. To this end, we compare different deep learning and shallow approaches on a new, large comment dataset and propose an ensemble that outperforms all individual models. Further, we validate our findings on a second dataset. The results of the ensemble enable us to perform an extensive error analysis, which reveals open challenges for state-of-the-art methods and directions towards pending future research. These challenges include missing paradigmatic context and inconsistent dataset labels.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>vanaken-EtAl:2018:ALW2</bibkey>
   </paper>

   <paper id="5106">
     <title>Aggression Detection on Social Media Text Using Deep Neural Networks</title>
     <author><first>Vinay</first><last>Singh</last></author>
     <author><first>Aman</first><last>Varshney</last></author>
     <author><first>Syed Sarfaraz</first><last>Akhtar</last></author>
     <author><first>Deepanshu</first><last>Vijay</last></author>
     <author><first>Manish</first><last>Shrivastava</last></author>
     <booktitle>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</booktitle>
     <month>October</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>43–50</pages>
     <url>http://www.aclweb.org/anthology/W18-5106</url>
     <abstract>In the past few years, bully and aggressive posts on social media have grown significantly, causing serious consequences for victims/users of all demographics. Majority of the work in this field has been done for English only. In this paper, we introduce a deep learning based classification system for Facebook posts and comments of Hindi-English Code-Mixed text to detect the aggressive behaviour of/towards users. Our work focuses on text from users majorly in the Indian Subcontinent. </abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>singh-EtAl:2018:ALW2</bibkey>
   </paper>

   <paper id="5107">
     <title>Creating a WhatsApp Dataset to Study Pre-teen Cyberbullying</title>
     <author><first>Rachele</first><last>Sprugnoli</last></author>
     <author><first>Stefano</first><last>Menini</last></author>
     <author><first>Sara</first><last>Tonelli</last></author>
     <author><first>Filippo</first><last>Oncini</last></author>
     <author><first>Enrico</first><last>Piras</last></author>
     <booktitle>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</booktitle>
     <month>October</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>51–59</pages>
     <url>http://www.aclweb.org/anthology/W18-5107</url>
     <abstract>Although WhatsApp is used by teenagers as one major channel of cyberbullying, such interactions remain invisible due to the app privacy policies that do not allow ex-post data collection. Indeed, most of the information on these phenomena rely on surveys regarding self-reported data.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>sprugnoli-EtAl:2018:ALW2</bibkey>
   </paper>

   <paper id="5108">
     <title>Improving Moderation of Online Discussions via Interpretable Neural Models</title>
     <author><first>Andrej</first><last>Švec</last></author>
     <author><first>Matúš</first><last>Pikuliak</last></author>
     <author><first>Marian</first><last>Simko</last></author>
     <author><first>Maria</first><last>Bielikova</last></author>
     <booktitle>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</booktitle>
     <month>October</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>60–65</pages>
     <url>http://www.aclweb.org/anthology/W18-5108</url>
     <abstract>Growing amount of comments make online discussions difficult to moderate by human moderators only. Antisocial behavior is a common occurrence that often discourages other users from participating in discussion. We propose a neural network based method that partially automates the moderation process. It consists of two steps. First, we detect inappropriate comments for moderators to see. Second, we highlight inappropriate parts within these comments to make the moderation faster. We evaluated our method on data from a major Slovak news discussion platform.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>vec-EtAl:2018:ALW2</bibkey>
   </paper>

   <paper id="5109">
     <title>Aggressive language in an online hacking forum</title>
     <author><first>Andrew</first><last>Caines</last></author>
     <author><first>Sergio</first><last>Pastrana</last></author>
     <author><first>Alice</first><last>Hutchings</last></author>
     <author><first>Paula</first><last>Buttery</last></author>
     <booktitle>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</booktitle>
     <month>October</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>66–74</pages>
     <url>http://www.aclweb.org/anthology/W18-5109</url>
     <abstract>We probe the heterogeneity in levels of abusive language in different sections of the Internet, using an annotated corpus of Wikipedia page edit comments to train a binary classifier for abuse detection. Our test data come from the CrimeBB Corpus of hacking-related forum posts and we find that (a) forum interactions are rarely abusive, (b) the abusive language which does exist tends to be relatively mild compared to that found in the Wikipedia comments domain, and tends to involve aggressive posturing rather than hate speech or threats of violence. We observe that the purpose of conversations in online forums tend to be more constructive and informative than those in Wikipedia page edit comments which are geared more towards adversarial interactions, and that this may explain the lower levels of abuse found in our forum data than in Wikipedia comments. Further work remains to be done to compare these results with other inter-domain classification experiments, and to understand the impact of aggressive language in forum conversations.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>caines-EtAl:2018:ALW2</bibkey>
   </paper>

   <paper id="5110">
     <title>The Effects of User Features on Twitter Hate Speech Detection</title>
     <author><first>Elise</first><last>Fehn Unsvåg</last></author>
     <author><first>Björn</first><last>Gambäck</last></author>
     <booktitle>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</booktitle>
     <month>October</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>75–85</pages>
     <url>http://www.aclweb.org/anthology/W18-5110</url>
     <abstract>The paper investigates the potential effects user features have on hate speech classification. A quantitative analysis of Twitter data was conducted to better understand user characteristics, but no correlations were found between hateful text and the characteristics of the users who had posted it. However, experiments with a hate speech classifier based on datasets from three different languages showed that combining certain user features with textual features gave slight improvements of classification performance. While the incorporation of user features resulted in varying impact on performance for the different datasets used, user network-related features provided the most consistent improvements.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>fehnunsvg-gambck:2018:ALW2</bibkey>
   </paper>

   <paper id="5111">
     <title>Interpreting Neural Network Hate Speech Classifiers</title>
     <author><first>Cindy</first><last>Wang</last></author>
     <booktitle>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</booktitle>
     <month>October</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>86–92</pages>
     <url>http://www.aclweb.org/anthology/W18-5111</url>
     <abstract>Neural network hate speech classifiers outperform other methods, but the prevalence of hate speech necessitates better interpretability for automated detection systems. We propose several techniques to visualize and understand the domain-specific semantic meaning of a network’s internal structures.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>wang:2018:ALW2</bibkey>
   </paper>

   <paper id="5112">
     <title>Determining Code Words in Euphemistic Hate Speech Using Word Embedding Networks</title>
     <author><first>Rijul</first><last>Magu</last></author>
     <author><first>Jiebo</first><last>Luo</last></author>
     <booktitle>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</booktitle>
     <month>October</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>93–100</pages>
     <url>http://www.aclweb.org/anthology/W18-5112</url>
     <abstract>While analysis of online explicit abusive language detection has lately seen an ever-increasing focus, implicit abuse detection remains a largely unexplored space. We carry out a study on a subcategory of implicit hate: euphemistic hate speech. We propose a method to assist in identifying unknown euphemisms (or code words) given a set of hateful tweets containing a known code word. Our approach leverages word embeddings and network analysis (through centrality measures and community detection) in a manner that can be generalized to identify euphemisms across contexts- not just hate speech.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>magu-luo:2018:ALW2</bibkey>
   </paper>

   <paper id="5113">
     <title>Comparative Studies of Detecting Abusive Language on Twitter</title>
     <author><first>Younghun</first><last>Lee</last></author>
     <author><first>Seunghyun</first><last>Yoon</last></author>
     <author><first>Kyomin</first><last>Jung</last></author>
     <booktitle>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</booktitle>
     <month>October</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>101–106</pages>
     <url>http://www.aclweb.org/anthology/W18-5113</url>
     <abstract>The context-dependent nature of online aggression makes annotating large collections of data extremely difficult. Previously studied datasets in abusive language detection have been insufficient in size to efficiently train deep learning models. Recently, Hate and Abusive Speech on Twitter, a dataset much greater in size and reliability, has been released. However, this dataset has not been comprehensively studied to its potential. In this paper, we conduct the first comparative study of various learning models on Hate and Abusive Speech on Twitter, and discuss the possibility of using additional features and context data for improvements. Experimental results show that bidirectional GRU networks trained on word-level features, with Latent Topic Clustering modules, is the most accurate model scoring 0.805 F1.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>lee-yoon-jung:2018:ALW2</bibkey>
   </paper>

   <paper id="5114">
     <title>Boosting Text Classification Performance on Sexist Tweets by Text Augmentation and Text Generation Using a Combination of Knowledge Graphs</title>
     <author><first>sima</first><last>sharifirad</last></author>
     <author><first>Borna</first><last>Jafarpour</last></author>
     <author><first>Stan</first><last>Matwin</last></author>
     <booktitle>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</booktitle>
     <month>October</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>107–114</pages>
     <url>http://www.aclweb.org/anthology/W18-5114</url>
     <abstract>Text classification models have been heavily</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>sharifirad-jafarpour-matwin:2018:ALW2</bibkey>
   </paper>

   <paper id="5115">
     <title>Learning Representations for Detecting Abusive Language</title>
     <author><first>Magnus</first><last>Sahlgren</last></author>
     <author><first>Tim</first><last>Isbister</last></author>
     <author><first>Fredrik</first><last>Olsson</last></author>
     <booktitle>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</booktitle>
     <month>October</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>115–123</pages>
     <url>http://www.aclweb.org/anthology/W18-5115</url>
     <abstract>This paper discusses the question whether it is possible to learn a generic representation that is useful for detecting various types of abusive language. The approach is inspired by recent advances in transfer learning and word embeddings, and we learn representations from two different datasets containing various degrees of abusive language. We compare the learned representation with two standard approaches; one based on lexica, and one based on data-specific <tex-math>n</tex-math>-grams. Our experiments show that learned representations do contain useful information that can be used to improve detection performance when training data is limited.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>sahlgren-isbister-olsson:2018:ALW2</bibkey>
   </paper>

   <paper id="5116">
     <title>Datasets of Slovene and Croatian Moderated News Comments</title>
     <author><first>Nikola</first><last>Ljubešić</last></author>
     <author><first>Tomaž</first><last>Erjavec</last></author>
     <author><first>Darja</first><last>Fišer</last></author>
     <booktitle>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</booktitle>
     <month>October</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>124–131</pages>
     <url>http://www.aclweb.org/anthology/W18-5116</url>
     <abstract>This paper presents two large newly constructed datasets of moderated news comments from two highly popular online news portals in the respective countries: the Slovene RTV MCC and the Croatian 24sata. The datasets are analyzed by performing manual annotation of the types of the content which have been deleted by moderators and by investigating deletion trends among users and threads. Next, initial experiments on automatically detecting the deleted content in the datasets are presented. Both datasets are published in encrypted form, to enable others to perform experiments on detecting content to be deleted without revealing potentially inappropriate content. Finally, the baseline classification models trained on the non-encrypted datasets are disseminated as well to enable real-world use.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>ljubei-erjavec-fier:2018:ALW2</bibkey>
   </paper>

   <paper id="5117">
     <title>Cross-Domain Detection of Abusive Language Online</title>
     <author><first>Mladen</first><last>Karan</last></author>
     <author><first>Jan</first><last>Šnajder</last></author>
     <booktitle>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</booktitle>
     <month>October</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>132–137</pages>
     <url>http://www.aclweb.org/anthology/W18-5117</url>
     <abstract>We investigate to what extent the models trained to detect general abusive language generalize between different datasets labeled with different abusive language types. To this end, we compare the cross-domain performance of simple classification models on nine different datasets, finding that the models fail to generalize to out-domain datasets and that having at least some in-domain data is important. We also show that using the frustratingly simple domain adaptation (Daume III, 2007) in most cases improves the results over in-domain training, specially when used to augment a smaller dataset with a larger one.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>karan-najder:2018:ALW2</bibkey>
   </paper>

   <paper id="5118">
     <title>Did you offend me? Classification of Offensive Tweets in Hinglish Language</title>
     <author><first>Puneet</first><last>Mathur</last></author>
     <author><first>Ramit</first><last>Sawhney</last></author>
     <author><first>Meghna</first><last>Ayyar</last></author>
     <author><first>Rajiv</first><last>Shah</last></author>
     <booktitle>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</booktitle>
     <month>October</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>138–148</pages>
     <url>http://www.aclweb.org/anthology/W18-5118</url>
     <abstract>The use of code-switched languages e.g, Hinglish, which is derived by the blending of Hindi with the English language) is getting much popular on Twitter due to their ease of communication in native languages. However, spelling variations and absence of grammar rules introduce ambiguity and make it difficult to understand the text automatically. This paper presents the Multi-Input Multi-Channel Transfer Learning based model (MIMCT) to detect offensive (hate speech or abusive) Hinglish tweets from the proposed Hinglish Offensive Tweet (HOT) dataset using transfer learning coupled with multiple feature inputs. Specifically, it takes multiple primary word embedding along with secondary extracted features as inputs to train a multi-channel CNN-LSTM architecture that has been pre-trained on English tweets through transfer learning. The proposed MIMCT model outperforms the baseline supervised classification models, transfer learning based CNN and LSTM models to establish itself as the state of the art in the unexplored domain of Hinglish offensive text classification.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>mathur-EtAl:2018:ALW2</bibkey>
   </paper>

   <paper id="5119">
     <title>Decipherment for Adversarial Offensive Language Detection</title>
     <author><first>Zhelun</first><last>Wu</last></author>
     <author><first>Nishant</first><last>Kambhatla</last></author>
     <author><first>Anoop</first><last>Sarkar</last></author>
     <booktitle>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</booktitle>
     <month>October</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>149–159</pages>
     <url>http://www.aclweb.org/anthology/W18-5119</url>
     <abstract>Automated filters are commonly used by online services to stop users from sending age-inappropriate, bullying messages, or asking others to expose personal information. Previous work has focused on rules or classifiers to detect and filter offensive messages, but these are vulnerable to cleverly disguised plaintext and unseen expressions especially in an adversarial setting where the users can repeatedly try to bypass the filter. In this paper, we model the disguised messages as if they are produced by encrypting the original message using an invented cipher. We apply automatic decipherment techniques to decode the disguised malicious text, which can be then filtered using rules or classifiers. We provide experimental results on three different datasets and show that decipherment is an effective tool for this task.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>wu-kambhatla-sarkar:2018:ALW2</bibkey>
   </paper>

   <paper id="5120">
     <title>The Linguistic Ideologies of Deep Abusive Language Classification</title>
     <author><first>Michael</first><last>Castelle</last></author>
     <booktitle>Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</booktitle>
     <month>October</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>160–170</pages>
     <url>http://www.aclweb.org/anthology/W18-5120</url>
     <revision id="2">W18-5120v2</revision>
     <abstract>This paper brings together theories from sociolinguistics and linguistic anthropology to critically evaluate the so-called “language ideologies” — the set of beliefs and ways of speaking about language — in the practices of abusive language classification in modern machine learning-based NLP. This argument is made at both a conceptual and empirical level, as we review approaches to abusive language from different fields, and use two neural network methods to analyze three datasets developed for abusive language classification tasks (drawn from Wikipedia, Facebook, and StackOverflow). By evaluating and comparing these results, we argue for the importance of incorporating theories of pragmatics and metapragmatics into both the design of classification tasks as well as in ML architectures.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>castelle:2018:ALW2</bibkey>
   </paper>

   <paper id="5200">
     <title>Proceedings of the 5th Workshop on Argument Mining</title>
     <editor><first>Noam</first><last>Slonim</last></editor>
     <editor><first>Ranit</first><last>Aharonov</last></editor>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-52</url>
     <bibtype>book</bibtype>
     <bibkey>W18-52:2018</bibkey>
   </paper>

   <paper id="5201">
     <title>Argumentative Link Prediction using Residual Networks and Multi-Objective Learning</title>
     <author><first>Andrea</first><last>Galassi</last></author>
     <author><first>Marco</first><last>Lippi</last></author>
     <author><first>Paolo</first><last>Torroni</last></author>
     <booktitle>Proceedings of the 5th Workshop on Argument Mining</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–10</pages>
     <url>http://www.aclweb.org/anthology/W18-5201</url>
     <abstract>We explore the use of residual networks for argumentation mining, with an emphasis on link prediction. We propose a domain-agnostic method that makes no assumptions on document or argument structure. We evaluate our method on a challenging dataset consisting of user-generated comments collected from an online platform. Results show that our model outperforms an equivalent deep network and offers results comparable with state-of-the-art methods that rely on domain knowledge.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>galassi-lippi-torroni:2018:W18-52</bibkey>
   </paper>

   <paper id="5202">
     <title>End-to-End Argument Mining for Discussion Threads Based on Parallel Constrained Pointer Architecture</title>
     <author><first>Gaku</first><last>Morio</last></author>
     <author><first>Katsuhide</first><last>Fujita</last></author>
     <booktitle>Proceedings of the 5th Workshop on Argument Mining</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>11–21</pages>
     <url>http://www.aclweb.org/anthology/W18-5202</url>
     <abstract>Argument Mining (AM) is a relatively recent discipline, which concentrates on extracting claims or premises from discourses, and inferring their structures. However, many existing works do not consider micro-level AM studies on discussion threads sufficiently.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>morio-fujita:2018:W18-52</bibkey>
   </paper>

   <paper id="5203">
     <title>ArguminSci: A Tool for Analyzing Argumentation and Rhetorical Aspects in Scientific Writing</title>
     <author><first>Anne</first><last>Lauscher</last></author>
     <author><first>Goran</first><last>Glavaš</last></author>
     <author><first>Kai</first><last>Eckert</last></author>
     <booktitle>Proceedings of the 5th Workshop on Argument Mining</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>22–28</pages>
     <url>http://www.aclweb.org/anthology/W18-5203</url>
     <abstract>Argumentation is arguably one of the central features of scientific language. We present <i>ArguminSci</i>, an easy-to-use tool that analyzes argumentation and other rhetorical aspects of scientific writing, which we collectively dub <i>scitorics</i>. The main aspect we focus on is the fine-grained argumentative analysis of scientific text through identification of argument components. The functionality of <i>ArguminSci</i> is accessible via three interfaces: as a command line tool, via a RESTful application programming interface, and as a web application.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>lauscher-glava-eckert:2018:W18-52</bibkey>
   </paper>

   <paper id="5204">
     <title>Evidence Type Classification in Randomized Controlled Trials</title>
     <author><first>Tobias</first><last>Mayer</last></author>
     <author><first>Elena</first><last>Cabrio</last></author>
     <author><first>Serena</first><last>Villata</last></author>
     <booktitle>Proceedings of the 5th Workshop on Argument Mining</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>29–34</pages>
     <url>http://www.aclweb.org/anthology/W18-5204</url>
     <abstract>Randomized Controlled Trials (RCT) are a common type of experimental studies in the medical domain for evidence-based decision making. The ability to automatically extract the <i>arguments</i> proposed therein can be of valuable support for clinicians and practitioners in their daily evidence-based decision making activities. Given the peculiarity of the medical domain and the required level of detail, standard approaches to argument component detection in <i>argument(ation) mining</i> are not fine-grained enough to support such activities. In this paper, we introduce a new sub-task of the argument component identification task: <i>evidence type classification</i>. To address it, we propose a supervised approach and we test it on a set of RCT abstracts on different medical topics.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>mayer-cabrio-villata:2018:W18-52</bibkey>
   </paper>

   <paper id="5205">
     <title>Predicting the Usefulness of Amazon Reviews Using Off-The-Shelf Argumentation Mining</title>
     <author><first>Marco</first><last>Passon</last></author>
     <author><first>Marco</first><last>Lippi</last></author>
     <author><first>Giuseppe</first><last>Serra</last></author>
     <author><first>Carlo</first><last>Tasso</last></author>
     <booktitle>Proceedings of the 5th Workshop on Argument Mining</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>35–39</pages>
     <url>http://www.aclweb.org/anthology/W18-5205</url>
     <abstract>Internet users generate content at unprecedented rates. Building intelligent systems capable of discriminating useful content within this ocean of information is thus becoming a urgent need. In this paper, we aim to predict the usefulness of Amazon reviews, and to do this we exploit features coming from an off-the-shelf argumentation mining system. We argue that the usefulness of a review, in fact, is strictly related to its argumentative content, whereas the use of an already trained system avoids the costly need of relabeling a novel dataset. Results obtained on a large publicly available corpus support this hypothesis.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>passon-EtAl:2018:W18-52</bibkey>
   </paper>

   <paper id="5206">
     <title>An Argument-Annotated Corpus of Scientific Publications</title>
     <author><first>Anne</first><last>Lauscher</last></author>
     <author><first>Goran</first><last>Glavaš</last></author>
     <author><first>Simone Paolo</first><last>Ponzetto</last></author>
     <booktitle>Proceedings of the 5th Workshop on Argument Mining</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>40–46</pages>
     <url>http://www.aclweb.org/anthology/W18-5206</url>
     <abstract>Argumentation is an essential feature of scientific language. We present an annotation study resulting in a corpus of scientific publications annotated with argumentative components and relations. The argumentative annotations have been added to the existing Dr. Inventor Corpus, already annotated for four other rhetorical aspects. We analyze the annotated argumentative structures and investigate the relations between argumentation and other rhetorical aspects of scientific writing, such as discourse roles and citation contexts.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>lauscher-glava-ponzetto:2018:W18-52</bibkey>
   </paper>

   <paper id="5207">
     <title>Annotating Claims in the Vaccination Debate</title>
     <author><first>Benedetta</first><last>Torsi</last></author>
     <author><first>Roser</first><last>Morante</last></author>
     <booktitle>Proceedings of the 5th Workshop on Argument Mining</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>47–56</pages>
     <url>http://www.aclweb.org/anthology/W18-5207</url>
     <abstract>In this paper we present annotation experiments with three different annotation schemes for the identification of argument components in texts related to the vaccination debate. Identifying claims about vaccinations made by participants in the debate is of great societal interest, as the decision to vaccinate or not has impact in public health and safety. Since most corpora that have been annotated with argumentation information contain texts that belong to a specific genre and have a well defined argumentation structure, we needed to adjust the annotation schemes to our corpus, which contains heterogeneous texts from the Web. We started with a complex annotation scheme that had to be simplified due to low IAA. In our final experiment, which focused on annotating claims, annotators reached 57.3% IAA.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>torsi-morante:2018:W18-52</bibkey>
   </paper>

   <paper id="5208">
     <title>Argument Component Classification for Classroom Discussions</title>
     <author><first>Luca</first><last>Lugini</last></author>
     <author><first>Diane</first><last>Litman</last></author>
     <booktitle>Proceedings of the 5th Workshop on Argument Mining</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>57–67</pages>
     <url>http://www.aclweb.org/anthology/W18-5208</url>
     <abstract>This paper focuses on argument component classification for transcribed spoken classroom discussions, with the goal of automatically classifying student utterances into claims, evidence, and warrants.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>lugini-litman:2018:W18-52</bibkey>
   </paper>

   <paper id="5209">
     <title>Evidence Types, Credibility Factors, and Patterns or Soft Rules for Weighing Conflicting Evidence: Argument Mining in the Context of Legal Rules Governing Evidence Assessment</title>
     <author><first>Vern R.</first><last>Walker</last></author>
     <author><first>Dina</first><last>Foerster</last></author>
     <author><first>Julia Monica</first><last>Ponce</last></author>
     <author><first>Matthew</first><last>Rosen</last></author>
     <booktitle>Proceedings of the 5th Workshop on Argument Mining</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>68–78</pages>
     <url>http://www.aclweb.org/anthology/W18-5209</url>
     <abstract>This paper reports on the results of an empirical study of adjudicatory decisions about veterans’ claims for disability benefits in the United States. It develops a typology of kinds of relevant evidence (argument premises) employed in cases, and it identifies factors that the tribunal considers when assessing the credibility or trustworthiness of individual items of evidence. It also reports on patterns or “soft rules” that the tribunal uses to comparatively weigh the probative value of conflicting evidence. These evidence types, credibility factors, and comparison patterns are developed to be inter-operable with legal rules governing the evidence assessment process in the U.S. This approach should be transferable to other legal and non-legal domains.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>walker-EtAl:2018:W18-52</bibkey>
   </paper>

   <paper id="5210">
     <title>Feasible Annotation Scheme for Capturing Policy Argument Reasoning using Argument Templates</title>
     <author><first>Paul</first><last>Reisert</last></author>
     <author><first>Naoya</first><last>Inoue</last></author>
     <author><first>Tatsuki</first><last>Kuribayashi</last></author>
     <author><first>Kentaro</first><last>Inui</last></author>
     <booktitle>Proceedings of the 5th Workshop on Argument Mining</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>79–89</pages>
     <url>http://www.aclweb.org/anthology/W18-5210</url>
     <abstract>Most of the existing works on argument mining cast the problem of argumentative structure identification as classification tasks (e.g. attack-support relations, stance, explicit premise/claim).</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>reisert-EtAl:2018:W18-52</bibkey>
   </paper>

   <paper id="5211">
     <title>Frame- and Entity-Based Knowledge for Common-Sense Argumentative Reasoning</title>
     <author><first>Teresa</first><last>Botschen</last></author>
     <author><first>Daniil</first><last>Sorokin</last></author>
     <author><first>Iryna</first><last>Gurevych</last></author>
     <booktitle>Proceedings of the 5th Workshop on Argument Mining</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>90–96</pages>
     <url>http://www.aclweb.org/anthology/W18-5211</url>
     <abstract>Common-sense argumentative reasoning is a challenging task that requires holistic understanding of the argumentation where external knowledge about the world is hypothesized to play a key role. We explore the idea of using event knowledge about prototypical situations from FrameNet and fact knowledge about concrete entities from Wikidata to solve the task. We find that both resources can contribute to an improvement over the non-enriched approach and point out two persisting challenges: first, integration of many annotations of the same type, and second, fusion of complementary annotations. After our explorations, we question the key role of external world knowledge with respect to the argumentative reasoning task and rather point towards a logic-based analysis of the chain of reasoning.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>botschen-sorokin-gurevych:2018:W18-52</bibkey>
   </paper>

   <paper id="5212">
     <title>Incorporating Topic Aspects for Online Comment Convincingness Evaluation</title>
     <author><first>Yunfan</first><last>Gu</last></author>
     <author><first>Zhongyu</first><last>Wei</last></author>
     <author><first>Maoran</first><last>Xu</last></author>
     <author><first>Hao</first><last>Fu</last></author>
     <author><first>Yang</first><last>Liu</last></author>
     <author><first>Xuanjing</first><last>Huang</last></author>
     <booktitle>Proceedings of the 5th Workshop on Argument Mining</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>97–104</pages>
     <url>http://www.aclweb.org/anthology/W18-5212</url>
     <abstract>In this paper, we propose to incorporate topic aspects information for online comments convincingness evaluation. Our model makes use of graph convolutional network to utilize implicit topic information within a discussion thread to assist the evaluation of convincingness of each single comment. In order to test the effectiveness of our proposed model, we annotate topic information on top of a public dataset for argument convincingness evaluation. Experimental results show that topic information is able to improve the performance for convincingness evaluation. We also make a move to detect topic aspects automatically.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>gu-EtAl:2018:W18-52</bibkey>
   </paper>

   <paper id="5213">
     <title>Proposed Method for Annotation of Scientific Arguments in Terms of Semantic Relations and Argument Schemes</title>
     <author><first>Nancy</first><last>Green</last></author>
     <booktitle>Proceedings of the 5th Workshop on Argument Mining</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>105–110</pages>
     <url>http://www.aclweb.org/anthology/W18-5213</url>
     <abstract>This paper presents a proposed method for annotation of scientific arguments in biological/biomedical journal articles. Semantic entities and relations are used to represent the propositional content of arguments in instances of argument schemes. We describe an experiment in which we encoded the arguments in a journal article to identify issues in this approach. Our catalogue of argument schemes and a copy of the annotated article are now publically available.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>green:2018:W18-52</bibkey>
   </paper>

   <paper id="5214">
     <title>Using context to identify the language of face-saving</title>
     <author><first>Nona</first><last>Naderi</last></author>
     <author><first>Graeme</first><last>Hirst</last></author>
     <booktitle>Proceedings of the 5th Workshop on Argument Mining</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>111–120</pages>
     <url>http://www.aclweb.org/anthology/W18-5214</url>
     <abstract>We created a corpus of utterances that attempt to save face from parliamentary debates and use it to automatically analyze the language of reputation defence. Our proposed model that incorporates information regarding threats to reputation can predict reputation defence language with high confidence. Further experiments and evaluations on different datasets show that the model is able to generalize to new utterances and can predict the language of reputation defence in a new dataset.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>naderi-hirst:2018:W18-52</bibkey>
   </paper>

   <paper id="5215">
     <title>Dave the debater: a retrieval-based and generative argumentative dialogue agent</title>
     <author><first>Dieu-Thu</first><last>Le</last></author>
     <author><first>Cam Tu</first><last>Nguyen</last></author>
     <author><first>Kim Anh</first><last>Nguyen</last></author>
     <booktitle>Proceedings of the 5th Workshop on Argument Mining</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>121–130</pages>
     <url>http://www.aclweb.org/anthology/W18-5215</url>
     <abstract>In this paper, we explore the problem of developing an argumentative dialogue agent that can be able to discuss with human users on controversial topics. We describe two systems that use retrieval-based and generative models to make argumentative responses to the users. The experiments show promising results although they have been trained on a small dataset.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>le-nguyen-nguyen:2018:W18-52</bibkey>
   </paper>

   <paper id="5216">
     <title>PD3: Better Low-Resource Cross-Lingual Transfer By Combining Direct Transfer and Annotation Projection</title>
     <author><first>Steffen</first><last>Eger</last></author>
     <author><first>Andreas</first><last>Rücklé</last></author>
     <author><first>Iryna</first><last>Gurevych</last></author>
     <booktitle>Proceedings of the 5th Workshop on Argument Mining</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>131–143</pages>
     <url>http://www.aclweb.org/anthology/W18-5216</url>
     <abstract>We consider unsupervised cross-lingual transfer on two tasks, viz., sentence-level argumentation mining and standard POS tagging. We combine direct transfer using bilingual embeddings with annotation projection, which projects labels across unlabeled parallel data. We do so by either merging respective source and target language datasets or alternatively by using multi-task learning. Our combination strategy considerably improves upon both direct transfer and projection with few available parallel sentences, the most realistic scenario for many low-resource target languages.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>eger-rckl-gurevych:2018:W18-52</bibkey>
   </paper>

   <paper id="5217">
     <title>Cross-Lingual Argumentative Relation Identification: from English to Portuguese</title>
     <author><first>Gil</first><last>Rocha</last></author>
     <author><first>Christian</first><last>Stab</last></author>
     <author><first>Henrique</first><last>Lopes Cardoso</last></author>
     <author><first>Iryna</first><last>Gurevych</last></author>
     <booktitle>Proceedings of the 5th Workshop on Argument Mining</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>144–154</pages>
     <url>http://www.aclweb.org/anthology/W18-5217</url>
     <abstract>Argument mining aims to detect and identify argument structures from textual resources. In this paper, we aim to address the task of argumentative relation identification, a subtask of argument mining, for which several approaches have been recently proposed in a monolingual setting. To overcome the lack of annotated resources in less-resourced languages, we present the first attempt to address</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>rocha-EtAl:2018:W18-52</bibkey>
   </paper>

   <paper id="5218">
     <title>More or less controlled elicitation of argumentative text: Enlarging a microtext corpus via crowdsourcing</title>
     <author><first>Maria</first><last>Skeppstedt</last></author>
     <author><first>Andreas</first><last>Peldszus</last></author>
     <author><first>Manfred</first><last>Stede</last></author>
     <booktitle>Proceedings of the 5th Workshop on Argument Mining</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>155–163</pages>
     <url>http://www.aclweb.org/anthology/W18-5218</url>
     <abstract>We present an extension of an annotated corpus of short argumentative texts that had originally been built in a controlled text production experiment. Our extension more than doubles the size of the corpus by means of crowdsourcing. We report on the setup of this experiment and on the consequences that crowdsourcing had for assembling the data, and in particular for annotation. We labeled the argumentative structure by marking claims, premises, and relations between them, following the scheme used in the original corpus, but had to make a few modifications in response to interesting phenomena in the data. Finally, we report on an experiment with the automatic prediction of this argumentation structure: We first replicated the approach of an earlier study on the original corpus, and compare the performance to various settings involving the extension.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>skeppstedt-peldszus-stede:2018:W18-52</bibkey>
   </paper>

   <paper id="5300">
     <title>Proceedings of the 6th BioASQ Workshop A challenge on large-scale biomedical semantic indexing and question answering</title>
     <editor><first>Ioannis A.</first><last>Kakadiaris</last></editor>
     <editor><first>George</first><last>Paliouras</last></editor>
     <editor><first>Anastasia</first><last>Krithara</last></editor>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-53</url>
     <bibtype>book</bibtype>
     <bibkey>BioASQ:2018</bibkey>
   </paper>

   <paper id="5301">
     <title>Results of the sixth edition of the BioASQ Challenge</title>
     <author><first>Anastasios</first><last>Nentidis</last></author>
     <author><first>Anastasia</first><last>Krithara</last></author>
     <author><first>Konstantinos</first><last>Bougiatiotis</last></author>
     <author><first>Georgios</first><last>Paliouras</last></author>
     <author><first>Ioannis</first><last>Kakadiaris</last></author>
     <booktitle>Proceedings of the 6th BioASQ Workshop A challenge on large-scale biomedical semantic indexing and question answering</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–10</pages>
     <url>http://www.aclweb.org/anthology/W18-5301</url>
     <abstract>This paper presents the results of the sixth edition</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>nentidis-EtAl:2018:BioASQ</bibkey>
   </paper>

   <paper id="5302">
     <title>Semantic role labeling tools for biomedical question answering: a study of selected tools on the BioASQ datasets</title>
     <author><first>Fabian</first><last>Eckert</last></author>
     <author><first>Mariana</first><last>Neves</last></author>
     <booktitle>Proceedings of the 6th BioASQ Workshop A challenge on large-scale biomedical semantic indexing and question answering</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>11–21</pages>
     <url>http://www.aclweb.org/anthology/W18-5302</url>
     <abstract>Question answering (QA) systems usually rely on advanced natural language processing components to precisely understand the questions and extract the answers. Semantic role labeling (SRL) is known to boost performance for QA, but its use for biomedical texts has not yet been fully studied. We analyzed the performance of three SRL tools (BioKIT, BIOSMILE and PathLSTM) on 1776 questions</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>eckert-neves:2018:BioASQ</bibkey>
   </paper>

   <paper id="5303">
     <title>Macquarie University at BioASQ 6b: Deep learning and deep reinforcement learning for query-based summarisation</title>
     <author><first>Diego</first><last>Molla</last></author>
     <booktitle>Proceedings of the 6th BioASQ Workshop A challenge on large-scale biomedical semantic indexing and question answering</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>22–29</pages>
     <url>http://www.aclweb.org/anthology/W18-5303</url>
     <abstract>This paper describes Macquarie University’s contribution to the BioASQ Challenge (BioASQ 6b, Phase B). We focused on the extraction of the ideal answers, and the task was approached as an instance of query-based multi-document summarisation. In particular, this paper focuses on the experiments related to the deep learning and reinforcement learning approaches used in the submitted runs. The best run used a deep learning model under a regression-based framework. The deep learning architecture used features derived from the output of LSTM chains on word embeddings, plus features based on similarity with the query, and sentence position. The reinforcement learning approach was a proof-of-concept prototype that trained a global policy using REINFORCE. The global policy was implemented as a neural network that used tf.idf features encoding the candidate sentence, question, and context.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>molla:2018:BioASQ</bibkey>
   </paper>

   <paper id="5304">
     <title>AUEB at BioASQ 6: Document and Snippet Retrieval</title>
     <author><first>George</first><last>Brokos</last></author>
     <author><first>Polyvios</first><last>Liosis</last></author>
     <author><first>Ryan</first><last>McDonald</last></author>
     <author><first>Dimitris</first><last>Pappas</last></author>
     <author><first>Ion</first><last>Androutsopoulos</last></author>
     <booktitle>Proceedings of the 6th BioASQ Workshop A challenge on large-scale biomedical semantic indexing and question answering</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>30–39</pages>
     <url>http://www.aclweb.org/anthology/W18-5304</url>
     <abstract>We present system details for AUEB’s submissions to the BioASQ 6 Document and Snippet Retrieval challenge (Task 6b Phase A). Our models use novel extensions to deep learning architectures that operate solely over the text of the query and candidate document/snippets. Overall, our systems scored at the top or near the top for all batches and metrics of the challenge, highlighting the effectiveness of deep learning for the task.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>brokos-EtAl:2018:BioASQ</bibkey>
   </paper>

   <paper id="5305">
     <title>MindLab Neural Network Approach at BioASQ 6B</title>
     <author><first>Andrés</first><last>Rosso-Mateus</last></author>
     <author><first>Fabio A.</first><last>González</last></author>
     <author><first>Manuel</first><last>Montes-y-Gómez</last></author>
     <booktitle>Proceedings of the 6th BioASQ Workshop A challenge on large-scale biomedical semantic indexing and question answering</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>40–46</pages>
     <url>http://www.aclweb.org/anthology/W18-5305</url>
     <abstract>Biomedical Question Answering is concerned with the development of methods and systems that automatically find answers to natural language posed questions. In this work, we describe the system used in the BioASQ Challenge task 6b for document retrieval and snippet retrieval (with particular emphasis in this subtask). The proposed model makes use of semantic similarity patterns that are evaluated and measured by a convolutional neural network architecture. Subsequently, the snippet ranking performance is improved with a pseudo-relevance feedback approach in a later step. Based on the preliminary results, we reached the second position in snippet retrieval sub-task.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>rossomateus-gonzlez-montesygmez:2018:BioASQ</bibkey>
   </paper>

   <paper id="5306">
     <title>AttentionMeSH: Simple, Effective and Interpretable Automatic MeSH Indexer</title>
     <author><first>Qiao</first><last>Jin</last></author>
     <author><first>Bhuwan</first><last>Dhingra</last></author>
     <author><first>William</first><last>Cohen</last></author>
     <author><first>Xinghua</first><last>Lu</last></author>
     <booktitle>Proceedings of the 6th BioASQ Workshop A challenge on large-scale biomedical semantic indexing and question answering</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>47–56</pages>
     <url>http://www.aclweb.org/anthology/W18-5306</url>
     <abstract>There are millions of articles in PubMed database. To facilitate information retrieval, curators in the National Library of Medicine (NLM) assign a set of Medical Subject Headings (MeSH) to each article. MeSH is a hierarchically-organized vocabulary, containing about 28K different concepts, covering the fields from clinical medicine to information sciences. Several automatic MeSH indexing models have been developed to improve the time-consuming and financially expensive manual annotation, including the NLM official tool – Medical Text Indexer, and the winner of BioASQ Task5a challenge – DeepMeSH. However, these models are complex and not interpretable. We propose a novel end-to-end model, AttentionMeSH, which utilizes deep learning and attention mechanism to index MeSH terms to biomedical text. The attention mechanism enables the model to associate textual evidence with annotations, thus providing interpretability at the word level. The model also uses a novel masking mechanism to enhance accuracy and speed. In the final week of BioASQ Chanllenge Task6a, we ranked 2nd by average MiF using an on-construction model. After the contest, we achieve close to state-of-the-art MiF performance of ~0.684 using our final model. Human evaluations show AttentionMeSH also provides high level of interpretability, achieving 0.90@20 recall of all expert-labeled relevant words given an MeSH-article pair.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>jin-EtAl:2018:BioASQ</bibkey>
   </paper>

   <paper id="5307">
     <title>Extraction Meets Abstraction: Ideal Answer Generation for Biomedical Questions</title>
     <author><first>Yutong</first><last>Li</last></author>
     <author><first>Nicholas</first><last>Gekakis</last></author>
     <author><first>Qiuze</first><last>Wu</last></author>
     <author><first>Boyue</first><last>Li</last></author>
     <author><first>Khyathi</first><last>Chandu</last></author>
     <author><first>Eric</first><last>Nyberg</last></author>
     <booktitle>Proceedings of the 6th BioASQ Workshop A challenge on large-scale biomedical semantic indexing and question answering</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>57–65</pages>
     <url>http://www.aclweb.org/anthology/W18-5307</url>
     <abstract>The growing number of biomedical publications is a challenge for human researchers, who invest considerable effort to search for relevant documents and pinpointed answers. Biomedical Question Answering can automatically generate answers for a user’s topic or question, significantly reducing the effort required to locate the most relevant information in a large document corpus. Extractive summarization techniques, which concatenate the most relevant text units drawn from multiple documents, perform well on automatic evaluation metrics like ROUGE, but score poorly on human readability, due to the presence of redundant text and grammatical errors in the answer. </abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>li-EtAl:2018:BioASQ</bibkey>
   </paper>

   <paper id="5308">
     <title>UNCC QA: Biomedical Question Answering system</title>
     <author><first>Abhishek</first><last>Bhandwaldar</last></author>
     <author><first>Wlodek</first><last>Zadrozny</last></author>
     <booktitle>Proceedings of the 6th BioASQ Workshop A challenge on large-scale biomedical semantic indexing and question answering</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>66–71</pages>
     <url>http://www.aclweb.org/anthology/W18-5308</url>
     <abstract>A question answering system is a system capable of answering natural language questions. The aim of the system is to tap into the vast unstructured information and aid the researchers by providing a natural language search interface. BioaASQ is a research competition that aims to progress the research in biomedical question answering systems. In this paper, we detail our submission to the BioASQ competition’s Biomedical Semantic Question and Answering task. Our system uses extractive summarization techniques to generate answers and has scored highest ROUGE-2 and Rogue-SU4 in all test batch sets.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>bhandwaldar-zadrozny:2018:BioASQ</bibkey>
   </paper>

   <paper id="5309">
     <title>An Adaption of BIOASQ Question Answering dataset for Machine Reading systems by Manual Annotations of Answer Spans.</title>
     <author><first>Sanjay</first><last>Kamath</last></author>
     <author><first>Brigitte</first><last>Grau</last></author>
     <author><first>Yue</first><last>Ma</last></author>
     <booktitle>Proceedings of the 6th BioASQ Workshop A challenge on large-scale biomedical semantic indexing and question answering</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>72–78</pages>
     <url>http://www.aclweb.org/anthology/W18-5309</url>
     <abstract>BIOASQ Task B Phase B challenge focuses on extracting answers from snippets for a given question. The dataset provided by the organizers contains answers, but not all their variants. Henceforth a manual annotation was performed to extract all forms of correct answers. This article shows the impact of using all occurrences of correct answers for training on the evaluation scores which are improved significantly.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>kamath-grau-ma:2018:BioASQ</bibkey>
   </paper>

   <paper id="5310">
     <title>Ontology-Based Retrieval &amp; Neural Approaches for BioASQ Ideal Answer Generation</title>
     <author><first>Ashwin</first><last>Naresh Kumar</last></author>
     <author><first>Harini</first><last>Kesavamoorthy</last></author>
     <author><first>Madhura</first><last>Das</last></author>
     <author><first>Pramati</first><last>Kalwad</last></author>
     <author><first>Khyathi</first><last>Chandu</last></author>
     <author><first>Teruko</first><last>Mitamura</last></author>
     <author><first>Eric</first><last>Nyberg</last></author>
     <booktitle>Proceedings of the 6th BioASQ Workshop A challenge on large-scale biomedical semantic indexing and question answering</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>79–89</pages>
     <url>http://www.aclweb.org/anthology/W18-5310</url>
     <abstract>The ever-increasing magnitude of biomedical information sources makes it difficult and time-consuming for a human researcher to find the most relevant documents and pinpointed answers for a specific question or topic when using only a traditional search engine. Biomedical Question Answering systems automatically identify the most relevant documents and pinpointed answers, given an information need expressed as a natural language question. Generating a non-redundant, human-readable summary that satisfies the information need of a given biomedical question is focus of the Ideal Answer Generation task, part of the BioASQ challenge. This paper presents a system for ideal answer generation (using ontology-based retrieval and a neural learning-to-rank approach, combined with extractive and abstractive summarization techniques) which achieved the highest ROUGE score of 0.659 on the BioASQ 5b batch 2 test.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>nareshkumar-EtAl:2018:BioASQ</bibkey>
   </paper>

   <paper id="5400">
     <title>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
     <editor><first>Johns Hopkins University</first><last>Tal Linzen</last></editor>
     <editor><first>Tilburg University</first><last>Grzegorz Chrupała</last></editor>
     <editor><first>Tilburg University</first><last>Afra Alishahi</last></editor>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-54</url>
     <bibtype>book</bibtype>
     <bibkey>BlackboxNLP:2018</bibkey>
   </paper>

   <paper id="5401">
     <title>When does deep multi-task learning work for loosely related document classification tasks?</title>
     <author><first>Emma</first><last>Kerinec</last></author>
     <author><first>Chloé</first><last>Braud</last></author>
     <author><first>Anders</first><last>Søgaard</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–8</pages>
     <url>http://www.aclweb.org/anthology/W18-5401</url>
     <abstract>This work aims to contribute to our understanding of when multi-task learning through parameter sharing in deep neural networks leads to improvements over single-task learning. We focus on the setting of learning from <i>loosely related</i> tasks, for which no theoretical guarantees exist. We therefore approach the question empirically, studying which properties of datasets and single-task learning characteristics correlate with improvements from multi-task learning. We are the first to study this in a text classification setting and across more than 500 different task pairs.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>kerinec-braud-sgaard:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5402">
     <title>Analyzing Learned Representations of a Deep ASR Performance Prediction Model</title>
     <author><first>Zied</first><last>Elloumi</last></author>
     <author><first>Laurent</first><last>Besacier</last></author>
     <author><first>Olivier</first><last>Galibert</last></author>
     <author><first>Benjamin</first><last>Lecouteux</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>9–15</pages>
     <url>http://www.aclweb.org/anthology/W18-5402</url>
     <abstract>This paper addresses a relatively new task: prediction of ASR performance on unseen broadcast programs. </abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>elloumi-EtAl:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5403">
     <title>Explaining non-linear Classifier Decisions within Kernel-based Deep Architectures</title>
     <author><first>Danilo</first><last>Croce</last></author>
     <author><first>Daniele</first><last>Rossini</last></author>
     <author><first>Roberto</first><last>Basili</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>16–24</pages>
     <url>http://www.aclweb.org/anthology/W18-5403</url>
     <abstract>Nonlinear methods such as deep neural networks achieve state-of-the-art performances in several semantic NLP tasks. </abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>croce-rossini-basili:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5404">
     <title>Nightmare at test time: How punctuation prevents parsers from generalizing</title>
     <author><first>Anders</first><last>Søgaard</last></author>
     <author><first>Miryam</first><last>de Lhoneux</last></author>
     <author><first>Isabelle</first><last>Augenstein</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>25–29</pages>
     <url>http://www.aclweb.org/anthology/W18-5404</url>
     <abstract>Punctuation is a strong indicator of syntactic structure, and parsers trained on text with punctuation often rely heavily on this signal. Punctuation is a diversion, however, since human language processing does not rely on punctuation to the same extent, and in informal texts, we therefore often leave out punctuation. We also use punctuation ungrammatically for emphatic or creative purposes, or simply by mistake. We show that (a) dependency parsers are sensitive to <i>both</i> absence of punctuation and to alternative uses; (b) neural parsers tend to be more sensitive than vintage parsers; (c) training neural parsers <i>without</i> punctuation outperforms all out-of-the-box parsers across all scenarios where punctuation departs from standard punctuation. Our main experiments are on synthetically corrupted data to study the effect of punctuation in isolation and avoid potential confounds, but we also show effects on out-of-domain data.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>sgaard-delhoneux-augenstein:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5405">
     <title>Evaluating Textual Representations through Image Generation</title>
     <author><first>Graham</first><last>Spinks</last></author>
     <author><first>Marie-Francine</first><last>Moens</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>30–39</pages>
     <url>http://www.aclweb.org/anthology/W18-5405</url>
     <abstract>We present a methodology for determining the quality of textual representations through the ability to generate images from them. Continuous representations of textual input are ubiquitous in modern Natural Language Processing techniques either at the core of machine learning algorithms or as the by-product at any given layer of a neural network. While current techniques to evaluate such representations focus on their performance on particular tasks, they don’t provide a clear understanding of the level of informational detail that is stored within them, especially their ability to represent spatial information. The central premise of this paper is that visual inspection or analysis is the most convenient method to quickly and accurately determine information content. Through the use of text-to-image neural networks, we propose a new technique to compare the quality of textual representations by visualizing their information content. The method is illustrated on a medical dataset where the correct representation of spatial information and shorthands are of particular importance. For four different well-known textual representations, we show with a quantitative analysis that some representations are consistently able to deliver higher quality visualizations of the information content. Additionally, we show that the quantitative analysis technique correlates with the judgment of a human expert evaluator in terms of alignment.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>spinks-moens:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5406">
     <title>On the Role of Text Preprocessing in Neural Network Architectures: An Evaluation Study on Text Categorization and Sentiment Analysis</title>
     <author><first>Jose</first><last>Camacho-Collados</last></author>
     <author><first>Mohammad Taher</first><last>Pilehvar</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>40–46</pages>
     <url>http://www.aclweb.org/anthology/W18-5406</url>
     <abstract>Text preprocessing is often the first step in the pipeline of a Natural Language Processing (NLP) system, with potential impact in its final performance. Despite its importance, text preprocessing has not received much attention in the deep learning literature. In this paper we investigate the impact of simple text preprocessing decisions (particularly tokenizing, lemmatizing, lowercasing and multiword grouping) on the performance of a standard neural text classifier. We perform an extensive evaluation on standard benchmarks from text categorization and sentiment analysis. While our experiments show that a simple tokenization of input text is generally adequate, they also highlight significant degrees of variability across preprocessing techniques. This reveals the importance of paying attention to this usually-overlooked step in the pipeline, particularly when comparing different models. Finally, our evaluation provides insights into the best preprocessing practices for training word embeddings.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>camachocollados-pilehvar:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5407">
     <title>Jump to better conclusions: SCAN both left and right</title>
     <author><first>Joost</first><last>Bastings</last></author>
     <author><first>Marco</first><last>Baroni</last></author>
     <author><first>Jason</first><last>Weston</last></author>
     <author><first>Kyunghyun</first><last>Cho</last></author>
     <author><first>Douwe</first><last>Kiela</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>47–55</pages>
     <url>http://www.aclweb.org/anthology/W18-5407</url>
     <abstract>Lake &amp; Baroni (2018) recently introduced the SCAN data set, which consists of simple commands paired with action sequences and is intended to test the </abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>bastings-EtAl:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5408">
     <title>Understanding Convolutional Neural Networks for Text Classification</title>
     <author><first>Alon</first><last>Jacovi</last></author>
     <author><first>Oren</first><last>Sar Shalom</last></author>
     <author><first>Yoav</first><last>Goldberg</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>56–65</pages>
     <url>http://www.aclweb.org/anthology/W18-5408</url>
     <abstract>We present an analysis into the inner workings of Convolutional Neural Networks (CNNs) for processing text. CNNs used for computer vision can be interpreted by projecting filters into image space, but for discrete sequence inputs CNNs remain a mystery. We aim to understand the method by which the networks process and classify text. We examine common hypotheses to this problem: that filters, accompanied by global max-pooling, serve as ngram detectors. We show that filters may capture several different semantic classes of ngrams by using different activation patterns, and that global max-pooling induces behavior which separates important ngrams from the rest. Finally, we show practical use cases derived from our findings in the form of model interpretability (explaining a trained model by deriving a concrete identity for each filter, bridging the gap between visualization tools in vision tasks and NLP) and prediction interpretability (explaining predictions).</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>jacovi-sarshalom-goldberg:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5409">
     <title>Linguistic representations in multi-task neural networks for ellipsis resolution</title>
     <author><first>Ola</first><last>Rønning</last></author>
     <author><first>Daniel</first><last>Hardt</last></author>
     <author><first>Anders</first><last>Søgaard</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>66–73</pages>
     <url>http://www.aclweb.org/anthology/W18-5409</url>
     <abstract>Sluicing resolution is the task of identifying the antecedent to a question ellipsis. Antecedents are often sentential constituents, and previous work has therefore relied on syntactic parsing, together with complex linguistic features. A recent model instead used partial parsing as an auxiliary task in sequential neural network architectures to inject syntactic information. We explore the linguistic information being brought to bear by such networks, both by defining subsets of the data exhibiting relevant linguistic characteristics, and by examining the internal representations of the network. Both perspectives provide evidence for substantial linguistic knowledge being deployed by the neural networks.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>rnning-hardt-sgaard:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5410">
     <title>Unsupervised Token-wise Alignment to Improve Interpretation of Encoder-Decoder Models</title>
     <author><first>Shun</first><last>Kiyono</last></author>
     <author><first>Sho</first><last>Takase</last></author>
     <author><first>Jun</first><last>Suzuki</last></author>
     <author><first>Naoaki</first><last>Okazaki</last></author>
     <author><first>Kentaro</first><last>Inui</last></author>
     <author><first>Masaaki</first><last>Nagata</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>74–81</pages>
     <url>http://www.aclweb.org/anthology/W18-5410</url>
     <abstract>Developing a method for understanding the inner workings of black-box neural methods is an important research endeavor.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>kiyono-EtAl:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5411">
     <title>Rule induction for global explanation of trained models</title>
     <author><first>Madhumita</first><last>Sushil</last></author>
     <author><first>Simon</first><last>Suster</last></author>
     <author><first>Walter</first><last>Daelemans</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>82–97</pages>
     <url>http://www.aclweb.org/anthology/W18-5411</url>
     <abstract>Understanding the behavior of a trained network and finding explanations for its outputs is important for improving the network’s performance and generalization ability, and for ensuring trust in automated systems. Several approaches have previously been proposed to identify and visualize the most important features by analyzing a trained network. However, the relations between different features and classes are lost in most cases. We propose a technique to induce sets of if-then-else rules that capture these relations to globally explain the predictions of a network. We first calculate the importance of the features in the trained network. We then weigh the original inputs with these feature importance scores, simplify the transformed input space, and finally fit a rule induction model to explain the model predictions. We find that the output rule-sets can explain the predictions of a neural network trained for 4-class text classification from the 20 newsgroups dataset to a macro-averaged F-score of 0.80. We make the code available at https://github.com/clips/interpret_with_rules.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>sushil-suster-daelemans:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5412">
     <title>Can LSTM Learn to Capture Agreement? The Case of Basque</title>
     <author><first>Shauli</first><last>Ravfogel</last></author>
     <author><first>Yoav</first><last>Goldberg</last></author>
     <author><first>Francis</first><last>Tyers</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>98–107</pages>
     <url>http://www.aclweb.org/anthology/W18-5412</url>
     <abstract>We focus on the task of agreement prediction in Basque, as a case study for a task that requires implicit understanding of sentence structure and the acquisition of a complex but consistent morphological system. In a series of controlled experiments, we probe the ability of sequential models to learn agreement patterns and asses different aspects of the problem. Analyzing experimental results from two syntactic prediction tasks – verb number prediction and suffix recovery – we find that sequential models perform worse on agreement prediction in Basque than one might expect on the basis of a previous agreement prediction work in English. Tentative findings based on diagnostic classifiers suggest the network makes use of local heuristics as a proxy for the hierarchical structure of the sentence. We propose the Basque agreement prediction task as challenging benchmark for models that attempt to learn regularities in human language.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>ravfogel-goldberg-tyers:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5413">
     <title>Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks</title>
     <author><first>Joao</first><last>Loula</last></author>
     <author><first>Marco</first><last>Baroni</last></author>
     <author><first>Brenden</first><last>Lake</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>108–114</pages>
     <url>http://www.aclweb.org/anthology/W18-5413</url>
     <abstract>Systematic compositionality is the ability to recombine meaningful units with regular and predictable outcomes, and it’s seen as key to humans’ capacity for generalization in language. Recent work (Lake and Baroni, 2018) has studied systematic compositionality in modern seq2seq models using generalization to novel navigation instructions in a grounded environment as a probing tool. Lake and Baroni’s main experiment required the models to quickly bootstrap the meaning of new words. We extend this framework here to settings where the model needs only to recombine well-trained functional words (such as “around” and “right”) in novel contexts. Our findings confirm and strengthen the earlier ones: seq2seq models can be impressively good at generalizing to novel combinations of previously-seen input, but only when they receive extensive training on the specific pattern to be generalized (e.g., generalizing from many examples of “X around right” to “jump around right”), while failing when generalization requires novel application of compositional rules (e.g., inferring the meaning of “around right” from those of “right” and “around”).</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>loula-baroni-lake:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5414">
     <title>Evaluating the Ability of LSTMs to Learn Context-Free Grammars</title>
     <author><first>Luzi</first><last>Sennhauser</last></author>
     <author><first>Robert</first><last>Berwick</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>115–124</pages>
     <url>http://www.aclweb.org/anthology/W18-5414</url>
     <abstract>While long short-term memory (LSTM) neural net architectures are designed to capture sequence information, human language is generally composed of hierarchical structures. This raises the question as to whether LSTMs can learn hierarchical structures. We explore this question with a well-formed bracket prediction task using two types of brackets modeled by an LSTM.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>sennhauser-berwick:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5415">
     <title>Interpretable Neural Architectures for Attributing an Ad’s Performance to its Writing Style</title>
     <author><first>Reid</first><last>Pryzant</last></author>
     <author><first>Sugato</first><last>Basu</last></author>
     <author><first>Kazoo</first><last>Sone</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>125–135</pages>
     <url>http://www.aclweb.org/anthology/W18-5415</url>
     <abstract>How much does “free shipping!” help an advertisement’s ability to persuade?</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>pryzant-basu-sone:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5416">
     <title>Interpreting Neural Networks with Nearest Neighbors</title>
     <author><first>Eric</first><last>Wallace</last></author>
     <author><first>Shi</first><last>Feng</last></author>
     <author><first>Jordan</first><last>Boyd-Graber</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>136–144</pages>
     <url>http://www.aclweb.org/anthology/W18-5416</url>
     <abstract>Local model interpretation methods explain individual predictions by</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>wallace-feng-boydgraber:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5417">
     <title>‘Indicatements’ that character language models learn English morpho-syntactic units and regularities</title>
     <author><first>Yova</first><last>Kementchedjhieva</last></author>
     <author><first>Adam</first><last>Lopez</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>145–153</pages>
     <url>http://www.aclweb.org/anthology/W18-5417</url>
     <abstract>Character language models have access to surface morphological patterns, but it is not clear whether or how they learn abstract morphological regularities. We instrument a character language model with several probes, finding that it can develop a specific unit to identify word boundaries and, by extension, morpheme boundaries, which allows it to capture linguistic properties and regularities of these units. Our language model proves surprisingly good at identifying the selectional restrictions of English derivational morphemes, a task that requires both morphological and syntactic awareness. Thus we conclude that, when morphemes overlap extensively with the words of a language, a character language model can perform morphological abstraction.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>kementchedjhieva-lopez:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5418">
     <title>LISA: Explaining Recurrent Neural Network Judgments via Layer-wIse Semantic Accumulation and Example to Pattern Transformation</title>
     <author><first>Pankaj</first><last>Gupta</last></author>
     <author><first>Hinrich</first><last>Schütze</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>154–164</pages>
     <url>http://www.aclweb.org/anthology/W18-5418</url>
     <abstract>Recurrent neural networks (RNNs) are temporal</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>gupta-schtze:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5419">
     <title>Analysing the potential of seq-to-seq models for incremental interpretation in task-oriented dialogue</title>
     <author><first>Dieuwke</first><last>Hupkes</last></author>
     <author><first>Sanne</first><last>Bouwmeester</last></author>
     <author><first>Raquel</first><last>Fernández</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>165–174</pages>
     <url>http://www.aclweb.org/anthology/W18-5419</url>
     <abstract>We investigate how encoder-decoder models trained on a synthetic dataset of task-oriented dialogues process disfluencies, such as hesitations and self-corrections. We find that, contrary to earlier results, disfluencies have very little impact on the task success of seq-to-seq models with attention. Using visualisations and diagnostic classifiers, we analyse the representations that are incrementally built by the model, and discover that models develop little to no awareness of the structure of disfluencies. However, adding disfluencies to the data appears to help the model create clearer representations overall, as evidenced by the attention patterns the different models exhibit.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>hupkes-bouwmeester-fernndez:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5420">
     <title>An Operation Sequence Model for Explainable Neural Machine Translation</title>
     <author><first>Felix</first><last>Stahlberg</last></author>
     <author><first>Danielle</first><last>Saunders</last></author>
     <author><first>Bill</first><last>Byrne</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>175–186</pages>
     <url>http://www.aclweb.org/anthology/W18-5420</url>
     <abstract>We propose to achieve explainable neural machine translation (NMT) by changing the output representation to explain itself. We present a novel approach to NMT which generates the target sentence by monotonically walking through the source sentence. Word reordering is modeled by operations which allow setting markers in the target sentence and move a target-side write head between those markers. In contrast to many modern neural models, our system emits explicit word alignment information which is often crucial to practical machine translation as it improves explainability. Our technique can outperform a plain text system in terms of BLEU score under the recent Transformer architecture on Japanese-English and Portuguese-English, and is within 0.5 BLEU difference on Spanish-English.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>stahlberg-saunders-byrne:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5421">
     <title>Introspection for convolutional automatic speech recognition</title>
     <author><first>Andreas</first><last>Krug</last></author>
     <author><first>Sebastian</first><last>Stober</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>187–199</pages>
     <url>http://www.aclweb.org/anthology/W18-5421</url>
     <abstract>Artificial Neural Networks (ANNs) have experienced great success in the past few years. The increasing complexity of these models leads to less understanding about their decision processes. Therefore, introspection techniques have been proposed, mostly for images as input data. Patterns or relevant regions in images can be intuitively interpreted by a human observer. This is not the case for more complex data like speech recordings. In this work, we investigate the application of common introspection techniques from computer vision to an Automatic Speech Recognition (ASR) task. To this end, we use a model similar to image classification, which predicts letters from spectrograms. We show difficulties in applying image introspection to ASR. To tackle these problems, we propose normalized averaging of aligned inputs (NAvAI): a data-driven method to reveal learned patterns for prediction of specific classes. Our method integrates information from many data examples through local introspection techniques for Convolutional Neural Networks (CNNs). We demonstrate that our method provides better interpretability of letter-specific patterns than existing methods.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>krug-stober:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5422">
     <title>Learning and Evaluating Sparse Interpretable Sentence Embeddings</title>
     <author><first>Valentin</first><last>Trifonov</last></author>
     <author><first>Octavian-Eugen</first><last>Ganea</last></author>
     <author><first>Anna</first><last>Potapenko</last></author>
     <author><first>Thomas</first><last>Hofmann</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>200–210</pages>
     <url>http://www.aclweb.org/anthology/W18-5422</url>
     <abstract>Previous research on word embeddings has shown that sparse representations, which can be either learned on top of existing dense embeddings or obtained through model constraints during training time, have the benefit of increased interpretability properties: to some degree, each dimension can be understood by a human and associated with a recognizable feature in the data. In this paper, we transfer this idea to sentence embeddings and explore several approaches to obtain a sparse representation. We further introduce a novel, quantitative and automated evaluation metric for sentence embedding interpretability, based on topic coherence methods. We observe an increase in interpretability compared to dense models, on a dataset of movie dialogs and on the scene descriptions from the MS COCO dataset.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>trifonov-EtAl:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5423">
     <title>What do RNN Language Models Learn about Filler–Gap Dependencies?</title>
     <author><first>Ethan</first><last>Wilcox</last></author>
     <author><first>Roger</first><last>Levy</last></author>
     <author><first>Takashi</first><last>Morita</last></author>
     <author><first>Richard</first><last>Futrell</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>211–221</pages>
     <url>http://www.aclweb.org/anthology/W18-5423</url>
     <abstract>RNN language models have achieved state-of-the-art perplexity results and have proven useful in a suite of NLP tasks, but it is as yet unclear what syntactic generalizations they learn. Here we investigate whether state-of-the-art RNN language models represent long-distance filler–gap dependencies and constraints on them. Examining RNN behavior on experimentally controlled sentences designed to expose filler–gap dependencies, we show that RNNs can represent the relationship in multiple syntactic positions and over large spans of text. Furthermore, we show that RNNs learn a subset of the known restrictions on filler–gap dependencies, known as island constraints: RNNs show evidence for wh-islands, adjunct islands, and complex NP islands. These studies demonstrates that state-of-the-art RNN models are able to learn and generalize about empty syntactic positions.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>wilcox-EtAl:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5424">
     <title>Do Language Models Understand Anything? On the Ability of LSTMs to Understand Negative Polarity Items</title>
     <author><first>Jaap</first><last>Jumelet</last></author>
     <author><first>Dieuwke</first><last>Hupkes</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>222–231</pages>
     <url>http://www.aclweb.org/anthology/W18-5424</url>
     <abstract>In this paper, we attempt to link the inner workings of a neural language model to linguistic theory, focusing on a complex phenomenon well discussed in formal linguistics: (negative) polarity items.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>jumelet-hupkes:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5425">
     <title>Closing Brackets with Recurrent Neural Networks</title>
     <author><first>Natalia</first><last>Skachkova</last></author>
     <author><first>Thomas</first><last>Trost</last></author>
     <author><first>Dietrich</first><last>Klakow</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>232–239</pages>
     <url>http://www.aclweb.org/anthology/W18-5425</url>
     <abstract>Many natural and formal languages contain words or symbols that require a matching counterpart for making an expression well-formed. The combination of opening and closing brackets is a typical example of such a construction. Due to their commonness, the ability to follow such rules is important for language modeling. Currently, recurrent neural networks (RNNs) are extensively used for this task. We investigate whether they are capable of learning the rules of opening and closing brackets by applying them to synthetic Dyck languages that consist of different types of brackets. We provide an analysis of the statistical properties of these languages as a baseline and show strengths and limits of Elman-RNNs, GRUs and LSTMs in experiments on random samples of these languages. In terms of perplexity and prediction accuracy, the RNNs get close to the theoretical baseline in most cases.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>skachkova-trost-klakow:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5426">
     <title>Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information</title>
     <author><first>Mario</first><last>Giulianelli</last></author>
     <author><first>Jack</first><last>Harding</last></author>
     <author><first>Florian</first><last>Mohnert</last></author>
     <author><first>Dieuwke</first><last>Hupkes</last></author>
     <author><first>Willem</first><last>Zuidema</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>240–248</pages>
     <url>http://www.aclweb.org/anthology/W18-5426</url>
     <abstract>How do neural language models keep track of number agreement between subject and verb? We show that ‘diagnostic classifiers’, trained to predict number from the internal states of the language model, provide a detailed understanding of how, when, and where this information is represented. Moreover, they give us insight in when and where this information is corrupted in cases where the language model ends up making agreement errors. To demonstrate the causal role that the representations we find play, we then use this information to influence the course of the LSTM during the processing of difficult sentences. Results from such an intervention show a large increase in the language model’s accuracy. Together, these results show that diagnostic classifiers give us an unrivalled detailed look into the representation of linguistic information in neural models, and moreover demonstrate that this knowledge can be use to improve their</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>giulianelli-EtAl:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5427">
     <title>Iterative Recursive Attention Model for Interpretable Sequence Classification</title>
     <author><first>Martin</first><last>Tutek</last></author>
     <author><first>Jan</first><last>Šnajder</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>249–257</pages>
     <url>http://www.aclweb.org/anthology/W18-5427</url>
     <abstract>Natural language processing has greatly benefited from the introduction of the attention mechanism. However, standard attention models are of limited interpretability for tasks that involve a series of inference steps. We describe an iterative recursive attention model, which constructs incremental representations of input data through reusing results of previously computed queries. We train our model on sentiment classification datasets and demonstrate its capacity to identify and combine different aspects of the input in an easily interpretable manner, while obtaining performance close to the state of the art.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>tutek-najder:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5428">
     <title>Interpreting Word-Level Hidden State Behaviour of Character-Level LSTM Language Models</title>
     <author><first>Avery</first><last>Hiebert</last></author>
     <author><first>Cole</first><last>Peterson</last></author>
     <author><first>Alona</first><last>Fyshe</last></author>
     <author><first>Nishant</first><last>Mehta</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>258–266</pages>
     <url>http://www.aclweb.org/anthology/W18-5428</url>
     <abstract>While Long Short-Term Memory networks (LSTMs) and other forms of recurrent neural network have been successfully applied to language modeling on a character level, the hidden state dynamics of these models can be difficult to interpret. We investigate the hidden states of such a model by using the HDBSCAN clustering algorithm to identify points in the text at which the hidden state is similar. Focusing on whitespace characters prior to the beginning of a word reveals interpretable clusters that offer insight into how the LSTM may combine contextual and character-level information to identify parts of speech. We also introduce a method for deriving word vectors from the hidden state representation in order to investigate the word-level knowledge of the model. These word vectors encode meaningful semantic information even for words that appear only once in the training text.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>hiebert-EtAl:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5429">
     <title>Importance of Self-Attention for Sentiment Analysis</title>
     <author><first>Gaël</first><last>Letarte</last></author>
     <author><first>Frédérik</first><last>Paradis</last></author>
     <author><first>Philippe</first><last>Giguère</last></author>
     <author><first>François</first><last>Laviolette</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>267–275</pages>
     <url>http://www.aclweb.org/anthology/W18-5429</url>
     <abstract>Despite their superior performance, deep learning models often lack interpretability. In this paper, we explore the modeling of insightful relations between words, in order to understand and enhance predictions. To this effect, we propose the Self-Attention Network (SANet), a flexible and interpretable architecture for text classification. Experiments indicate that gains obtained by self-attention is task-dependent. For instance, experiments on sentiment analysis tasks showed an improvement of around 2% when using self-attention compared to a baseline without attention, while topic classification showed no gain. Interpretability brought forward by our architecture highlighted the importance of neighboring word interactions to extract sentiment.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>letarte-EtAl:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5430">
     <title>Firearms and Tigers are Dangerous, Kitchen Knives and Zebras are Not: Testing whether Word Embeddings Can Tell</title>
     <author><first>Pia</first><last>Sommerauer</last></author>
     <author><first>Antske</first><last>Fokkens</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>276–286</pages>
     <url>http://www.aclweb.org/anthology/W18-5430</url>
     <abstract>This paper presents an approach for investigating the nature of semantic information captured by word embeddings. We propose a method that extends an existing human- elicited semantic property dataset with gold negative examples using crowd judgments. Our experimental approach tests the ability of supervised classifiers to identify semantic features in word embedding vectors and compares this to a feature-identification method based on full vector cosine similarity. The idea behind this method is that properties identified by classifiers, but not through full vector comparison are captured by embeddings. Properties that cannot be identified by either method are not. Our results provide an initial indication that semantic properties relevant for the way entities interact (e.g. dangerous) are captured, while perceptual information (e.g. colors) is not represented. We conclude that, though preliminary, these results show that our method is suitable for identifying which properties are captured by embeddings.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>sommerauer-fokkens:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5431">
     <title>An Analysis of Encoder Representations in Transformer-Based Machine Translation</title>
     <author><first>Alessandro</first><last>Raganato</last></author>
     <author><first>Jörg</first><last>Tiedemann</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>287–297</pages>
     <url>http://www.aclweb.org/anthology/W18-5431</url>
     <abstract>The attention mechanism is a successful technique in modern NLP, especially in tasks like machine translation. The recently proposed network architecture of the Transformer is based entirely on attention mechanisms and achieves new state of the art results in neural machine translation, outperforming other sequence-to-sequence models.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>raganato-tiedemann:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5432">
     <title>Evaluating Grammaticality in Seq2seq Models with a Broad Coverage HPSG Grammar: A Case Study on Machine Translation</title>
     <author><first>Johnny</first><last>Wei</last></author>
     <author><first>Khiem</first><last>Pham</last></author>
     <author><first>Brendan</first><last>O’Connor</last></author>
     <author><first>Brian</first><last>Dillon</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>298–305</pages>
     <url>http://www.aclweb.org/anthology/W18-5432</url>
     <abstract>Sequence to sequence (seq2seq) models are often employed in settings where the target output is natural language. However, the syntactic properties of the language generated from these models are not well understood. We explore whether such output belongs to a formal and realistic grammar, by employing the English Resource Grammar (ERG), a broad coverage, linguistically precise HPSG-based grammar of English. From a French to English parallel corpus, we analyze the parseability and grammatical constructions occurring in output from a seq2seq translation model. Over 93% of the model translations are parseable, suggesting that it learns to generate conforming to a grammar. The model has trouble learning the distribution of rarer syntactic rules, and we pinpoint several constructions that differentiate translations between the references and our model.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>wei-EtAl:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5433">
     <title>Context-Free Transductions with Neural Stacks</title>
     <author><first>Yiding</first><last>Hao</last></author>
     <author><first>William</first><last>Merrill</last></author>
     <author><first>Dana</first><last>Angluin</last></author>
     <author><first>Robert</first><last>Frank</last></author>
     <author><first>Noah</first><last>Amsel</last></author>
     <author><first>Andrew</first><last>Benz</last></author>
     <author><first>Simon</first><last>Mendelsohn</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>306–315</pages>
     <url>http://www.aclweb.org/anthology/W18-5433</url>
     <abstract>This paper analyzes the behavior of stack-augmented recurrent neural network (RNN) models. Due to the architectural similarity between stack RNNs and pushdown transducers, we train stack RNN models on a number of tasks, including string reversal, context-free language modelling, and cumulative XOR evaluation. Examining the behavior of our networks, we show that stack-augmented RNNs can discover intuitive stack-based strategies for solving our tasks. However, stack RNNs are more difficult to train than classical architectures such as LSTMs. Rather than employ stack-based strategies, more complex stack-augmented networks often find approximate solutions by using the stack as unstructured memory.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>hao-EtAl:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5434">
     <title>Learning Explanations from Language Data</title>
     <author><first>David</first><last>Harbecke</last></author>
     <author><first>Robert</first><last>Schwarzenberg</last></author>
     <author><first>Christoph</first><last>Alt</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>316–318</pages>
     <url>http://www.aclweb.org/anthology/W18-5434</url>
     <abstract>PatternAttribution is a recent method, introduced in the vision domain, that explains classifications of deep neural networks. We demonstrate that it also generates meaningful interpretations in the language domain.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>harbecke-schwarzenberg-alt:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5435">
     <title>How much should you ask? On the question structure in QA systems.</title>
     <author><first>Barbara</first><last>Rychalska</last></author>
     <author><first>Dominika</first><last>Basaj</last></author>
     <author><first>Anna</first><last>Wróblewska</last></author>
     <author><first>Przemyslaw</first><last>Biecek</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>319–321</pages>
     <url>http://www.aclweb.org/anthology/W18-5435</url>
     <abstract>Datasets that boosted state-of-the-art solutions for Question Answering (QA) systems prove that it is possible to ask questions in natural language manner. However, users are still used to query-like systems where they type in keywords to search for answer. In this study we validate which parts of questions are essential for obtaining valid answer. In order to conclude that, we take advantage of LIME - a framework that explains prediction by local approximation. We find that grammar and natural language is disregarded by QA. State-of-the-art model can answer properly even if ’asked’ only with a few words with high coefficients calculated with LIME. According to our knowledge, it is the first time that QA model is being explained by LIME.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>rychalska-EtAl:2018:BlackboxNLP1</bibkey>
   </paper>

   <paper id="5436">
     <title>Does it care what you asked? Understanding Importance of Verbs in Deep Learning QA System</title>
     <author><first>Barbara</first><last>Rychalska</last></author>
     <author><first>Dominika</first><last>Basaj</last></author>
     <author><first>Anna</first><last>Wróblewska</last></author>
     <author><first>Przemyslaw</first><last>Biecek</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>322–324</pages>
     <url>http://www.aclweb.org/anthology/W18-5436</url>
     <abstract>In this paper we present the results of an investigation of the importance of verbs in a deep learning QA system trained on SQuAD dataset. We show that main verbs in questions carry little influence on the decisions made by the system - in over 90% of researched cases swapping verbs for their antonyms did not change system decision. We track this phenomenon down to the insides of the net, analyzing the mechanism of self-attention and values contained in hidden layers of RNN. Finally, we recognize the characteristics of the SQuAD dataset as the source of the problem. Our work refers to the recently popular topic of adversarial examples in NLP, combined with investigating deep net structure.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>rychalska-EtAl:2018:BlackboxNLP2</bibkey>
   </paper>

   <paper id="5437">
     <title>Interpretable Textual Neuron Representations for NLP</title>
     <author><first>Nina</first><last>Poerner</last></author>
     <author><first>Benjamin</first><last>Roth</last></author>
     <author><first>Hinrich</first><last>Schütze</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>325–327</pages>
     <url>http://www.aclweb.org/anthology/W18-5437</url>
     <abstract>Input optimization methods, such as Google Deep Dream, create interpretable representations of neurons for computer vision DNNs. We propose and evaluate ways of transferring this technology to NLP. Our results suggest that gradient ascent with a gumbel softmax layer produces n-gram representations that outperform naive corpus search in terms of target neuron activation. The representations highlight differences in syntax awareness between the language and visual models of the Imaginet architecture.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>poerner-roth-schtze:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5438">
     <title>Language Models Learn POS First</title>
     <author><first>Naomi</first><last>Saphra</last></author>
     <author><first>Adam</first><last>Lopez</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>328–330</pages>
     <url>http://www.aclweb.org/anthology/W18-5438</url>
     <abstract>A glut of recent research shows that language models capture linguistic structure. Such work answers the question of whether a model represents linguistic structure. But how and when are these structures acquired? Rather than treating the training process itself as a black box, we investigate how representations of linguistic structure are learned over time. In particular, we demonstrate that different aspects of linguistic structure are learned at different rates, with part of speech tagging acquired early and global topic information learned continuously.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>saphra-lopez:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5439">
     <title>Predicting and interpreting embeddings for out of vocabulary words in downstream tasks</title>
     <author><first>Nicolas</first><last>Garneau</last></author>
     <author><first>Jean-Samuel</first><last>Leboeuf</last></author>
     <author><first>Luc</first><last>Lamontagne</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>331–333</pages>
     <url>http://www.aclweb.org/anthology/W18-5439</url>
     <abstract>We propose a novel way to handle out of vocabulary (OOV) words in downstream natural language processing (NLP) tasks. We implement a network that predicts useful embeddings for OOV words based on their morphology and on the context in which they appear. Our model also incorporates an attention mechanism indicating the focus allocated to the left context words, the right context words or the word’s characters, hence making the prediction more interpretable. The model is a</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>garneau-leboeuf-lamontagne:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5440">
     <title>Probing sentence embeddings for structure-dependent tense</title>
     <author><first>Geoff</first><last>Bacon</last></author>
     <author><first>Terry</first><last>Regier</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>334–336</pages>
     <url>http://www.aclweb.org/anthology/W18-5440</url>
     <abstract>Learning universal sentence representations which accurately model sentential semantic content is a current goal of natural language processing research. A prominent and successful approach is to train recurrent neural networks (RNNs) to encode sentences into fixed length vectors. Many core linguistic phenomena that one would like to model in universal sentence representations depend on syntactic structure. Despite the fact that RNNs do not have explicit syntactic structural representations, there is some evidence that RNNs can approximate such structure-dependent phenomena under certain conditions, in addition to their widespread success in practical tasks. In this work, we assess RNNs’ ability to learn the structure-dependent phenomenon of main clause tense.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>bacon-regier:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5441">
     <title>Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation</title>
     <author><first>Adam</first><last>Poliak</last></author>
     <author><first>Aparajita</first><last>Haldar</last></author>
     <author><first>Rachel</first><last>Rudinger</last></author>
     <author><first>J. Edward</first><last>Hu</last></author>
     <author><first>Ellie</first><last>Pavlick</last></author>
     <author><first>Aaron Steven</first><last>White</last></author>
     <author><first>Benjamin</first><last>Van Durme</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>337–340</pages>
     <url>http://www.aclweb.org/anthology/W18-5441</url>
     <abstract>We present a large-scale collection of diverse natural language inference (NLI) datasets that help provide insight into how well a sentence representation encoded by a neural network captures distinct types of reasoning. The collection results from recasting 13 existing datasets from 7 semantic phenomena into a common NLI structure, resulting in over half a million labeled context-hypothesis pairs in total. Our collection of diverse datasets is available at http://www.decomp.net/ and will grow over time as additional resources are recast and added from novel sources.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>poliak-EtAl:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5442">
     <title>Interpretable Word Embedding Contextualization</title>
     <author><first>Kyoung-Rok</first><last>Jang</last></author>
     <author><first>Sung-Hyon</first><last>Myaeng</last></author>
     <author><first>Sang-Bum</first><last>Kim</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>341–343</pages>
     <url>http://www.aclweb.org/anthology/W18-5442</url>
     <abstract>In this paper, we propose a method of calibrating a word embedding, so that the semantic it conveys becomes more relevant to the context. Our method is novel because the output shows clearly which senses that were originally presented in a target word embedding become stronger or weaker. This is possible by utilizing the technique of using sparse coding to recover senses that comprises a word embedding.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>jang-myaeng-kim:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5443">
     <title>State Gradients for RNN Memory Analysis</title>
     <author><first>Lyan</first><last>Verwimp</last></author>
     <author><first>Hugo</first><last>Van hamme</last></author>
     <author><first>Vincent</first><last>Renkens</last></author>
     <author><first>Patrick</first><last>Wambacq</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>344–346</pages>
     <url>http://www.aclweb.org/anthology/W18-5443</url>
     <abstract>We present a framework for analyzing what the state in RNNs remembers from its input embeddings. We compute the gradients of the states with respect to the input embeddings and decompose the gradient matrix with Singular Value Decomposition to analyze which directions in the embedding space are best transferred to the hidden state space, characterized by the largest singular values. We apply our approach to LSTM language models and investigate to what extent and for how long certain classes of words are remembered on average for a certain corpus. Additionally, the extent to which a specific property or relationship is remembered by the RNN can be tracked by comparing a vector characterizing that property with the direction(s) in embedding space that are best preserved in hidden state space.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>verwimp-EtAl:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5444">
     <title>Extracting Syntactic Trees from Transformer Encoder Self-Attentions</title>
     <author><first>David</first><last>Mareček</last></author>
     <author><first>Rudolf</first><last>Rosa</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>347–349</pages>
     <url>http://www.aclweb.org/anthology/W18-5444</url>
     <abstract>This is a work in progress about extracting the sentence tree structures from the encoder’s self-attention weights, when translating into another language using the Transformer neural network architecture. We visualize the structures and discuss their characteristics with respect to the existing syntactic theories and annotations.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>mareek-rosa:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5445">
     <title>Portable, layer-wise task performance monitoring for NLP models</title>
     <author><first>Tom</first><last>Lippincott</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>350–352</pages>
     <url>http://www.aclweb.org/anthology/W18-5445</url>
     <abstract>There is a long-standing interest in understanding the internal behavior of neural networks. Deep neural architectures for natural language processing (NLP) are often accompanied by explanations for their effectiveness, from general observations (e.g. RNNs can represent unbounded dependencies in a sequence) to specific arguments about linguistic phenomena (early layers encode lexical information, deeper layers syntactic). The recent ascendancy of DNNs is fueling efforts in the NLP community to explore these claims. Previous work has tended to focus on easily-accessible representations like word or sentence embeddings, with deeper structure requiring more ad hoc methods to extract and examine. In this work, we introduce Vivisect, a toolkit that aims at a general solution for broad and fine-grained monitoring in the major DNN frameworks, with minimal change to research patterns.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>lippincott:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5446">
     <title>GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</title>
     <author><first>Alex</first><last>Wang</last></author>
     <author><first>Amanpreet</first><last>Singh</last></author>
     <author><first>Julian</first><last>Michael</last></author>
     <author><first>Felix</first><last>Hill</last></author>
     <author><first>Omer</first><last>Levy</last></author>
     <author><first>Samuel</first><last>Bowman</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>353–355</pages>
     <url>http://www.aclweb.org/anthology/W18-5446</url>
     <abstract>For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusively tailored to a specific task, genre, or dataset.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>wang-EtAl:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5447">
     <title>Explicitly modeling case improves neural dependency parsing</title>
     <author><first>Clara</first><last>Vania</last></author>
     <author><first>Adam</first><last>Lopez</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>356–358</pages>
     <url>http://www.aclweb.org/anthology/W18-5447</url>
     <abstract>Neural dependency parsing models that compose word representations from characters can presumably exploit morphosyntax when making attachment decisions. How much do they know about morphology? We investigate how well they handle morphological case, which is important for parsing. Our experiments on Czech, German and Russian suggest that adding explicit morphological case—either oracle or predicted—improves neural dependency parsing, indicating that the learned representations in these models do not fully encode the morphological knowledge that they need, and can still benefit from targeted forms of explicit linguistic modeling.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>vania-lopez:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5448">
     <title>Language Modeling Teaches You More than Translation Does: Lessons Learned Through Auxiliary Syntactic Task Analysis</title>
     <author><first>Kelly</first><last>Zhang</last></author>
     <author><first>Samuel</first><last>Bowman</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>359–361</pages>
     <url>http://www.aclweb.org/anthology/W18-5448</url>
     <abstract>Recently, researchers have found that deep LSTMs trained on tasks like machine translation learn substantial syntactic and semantic information about their input sentences, including part-of-speech. These findings begin to shed light on why pretrained representations, like ELMo and CoVe, are so beneficial for neural language understanding models. We still, though, do not yet have a clear understanding of how the choice of pretraining objective affects the type of linguistic information that models learn. With this in mind, we compare four objectives—language modeling, translation, skip-thought, and autoencoding—on their ability to induce syntactic and part-of-speech information, holding constant the quantity and genre of the training data, as well as the LSTM architecture.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>zhang-bowman:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5449">
     <title>Representation of Word Meaning in the Intermediate Projection Layer of a Neural Language Model</title>
     <author><first>Steven</first><last>Derby</last></author>
     <author><first>Paul</first><last>Miller</last></author>
     <author><first>Brian</first><last>Murphy</last></author>
     <author><first>Barry</first><last>Devereux</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>362–364</pages>
     <url>http://www.aclweb.org/anthology/W18-5449</url>
     <abstract>In this work, we evaluate latent semantic knowledge present in the LSTM activation patterns produced before and after the word of interest. We evaluate whether these activations predict human similarity ratings, human-derived property knowledge, and brain imaging data. In this way, we test the model’s ability to encode important semantic information relevant to word prediction, and it’s relationship with human cognitive semantic representations.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>derby-EtAl:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5450">
     <title>Interpretable Structure Induction via Sparse Attention</title>
     <author><first>Ben</first><last>Peters</last></author>
     <author><first>Vlad</first><last>Niculae</last></author>
     <author><first>André F. T.</first><last>Martins</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>365–367</pages>
     <url>http://www.aclweb.org/anthology/W18-5450</url>
     <abstract>Neural network methods are experiencing wide adoption in NLP, thanks to their empirical performance on many tasks. Modern neural architectures go way beyond simple feedforward and recurrent models: they are complex pipelines that perform soft, differentiable computation instead of discrete logic. The price of such soft computing is the introduction of dense dependencies, which make it hard to disentangle the patterns that trigger a prediction. Our recent work on sparse and structured latent computation presents a promising avenue for enhancing interpretability of such neural pipelines. Through this extended abstract, we aim to discuss and explore the potential and impact of our methods.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>peters-niculae-martins:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5451">
     <title>Debugging Sequence-to-Sequence Models with Seq2Seq-Vis</title>
     <author><first>Hendrik</first><last>Strobelt</last></author>
     <author><first>Sebastian</first><last>Gehrmann</last></author>
     <author><first>Michael</first><last>Behrisch</last></author>
     <author><first>Adam</first><last>Perer</last></author>
     <author><first>Hanspeter</first><last>Pfister</last></author>
     <author><first>Alexander</first><last>Rush</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>368–370</pages>
     <url>http://www.aclweb.org/anthology/W18-5451</url>
     <abstract>Neural sequence-to-sequence models have proven to be</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>strobelt-EtAl:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5452">
     <title>Grammar Induction with Neural Language Models: An Unusual Replication</title>
     <author><first>Phu Mon</first><last>Htut</last></author>
     <author><first>Kyunghyun</first><last>Cho</last></author>
     <author><first>Samuel</first><last>Bowman</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>371–373</pages>
     <url>http://www.aclweb.org/anthology/W18-5452</url>
     <abstract>Grammar induction is the task of learning syntactic structure without the expert-labeled treebanks (Charniak and Carroll, 1992; Klein and Manning, 2002). Recent work on latent tree learning offers a new family of approaches to this problem by inducing syntactic structure using the supervision from a downstream NLP task (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018). In a recent paper published at ICLR, Shen et al. (2018) introduce such a model and report near state-of-the-art results on the target task of language modeling, and the first strong latent tree learning result on constituency parsing. During the analysis of this model, we discover issues that make the original results hard to trust, including tuning and even training on what is effectively the test set. Here, we analyze the model under different configurations to understand what it learns and to identify the conditions under which it succeeds. We find that this model represents the first empirical success for neural network latent tree learning, and that neural language modeling warrants further study as a setting for grammar induction.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>htut-cho-bowman:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5453">
     <title>Does Syntactic Knowledge in Multilingual Language Models Transfer Across Languages?</title>
     <author><first>Prajit</first><last>Dhar</last></author>
     <author><first>Arianna</first><last>Bisazza</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>374–377</pages>
     <url>http://www.aclweb.org/anthology/W18-5453</url>
     <abstract>Recent work has shown that neural models can be successfully trained on multiple languages simultaneously.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>dhar-bisazza:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5454">
     <title>Exploiting Attention to Reveal Shortcomings in Memory Models</title>
     <author><first>Kaylee</first><last>Burns</last></author>
     <author><first>Aida</first><last>Nematzadeh</last></author>
     <author><first>Erin</first><last>Grant</last></author>
     <author><first>Alison</first><last>Gopnik</last></author>
     <author><first>Tom</first><last>Griffiths</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>378–380</pages>
     <url>http://www.aclweb.org/anthology/W18-5454</url>
     <abstract>The decision making processes of deep networks are difficult to understand and while their accuracy often improves with increased architectural complexity, so too does their opacity. Practical use of machine learning models, especially for question and answering applications, demands a system that is interpretable. We analyze the attention of a memory network model to reconcile contradictory performance on a challenging question-answering dataset that is inspired by theory-of-mind experiments. We equate success on questions to task classification, which explains not only test-time failures but also how well the model generalizes to new training conditions.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>burns-EtAl:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5455">
     <title>End-to-end Image Captioning Exploits Distributional Similarity in Multimodal Space</title>
     <author><first>Pranava Swaroop</first><last>Madhyastha</last></author>
     <author><first>Josiah</first><last>Wang</last></author>
     <author><first>Lucia</first><last>Specia</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>381–383</pages>
     <url>http://www.aclweb.org/anthology/W18-5455</url>
     <abstract>We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn ‘distributional similarity’ in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the ‘image’ side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. Our analysis indicates that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>madhyastha-wang-specia:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5456">
     <title>Limitations in learning an interpreted language with recurrent models</title>
     <author><first>Denis</first><last>Paperno</last></author>
     <booktitle>Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>384–386</pages>
     <url>http://www.aclweb.org/anthology/W18-5456</url>
     <abstract>In this submission I report work in progress on learning simplified interpreted languages by means of recurrent models. The data is constructed to reflect core properties of natural language as modeled in formal syntax and semantics. Preliminary results suggest that LSTM networks do generalise to compositional interpretation, albeit only in the most favorable learning setting.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>paperno:2018:BlackboxNLP</bibkey>
   </paper>

   <paper id="5500">
     <title>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</title>
     <editor><first>James</first><last>Thorne</last></editor>
     <editor><first>Andreas</first><last>Vlachos</last></editor>
     <editor><first>Oana</first><last>Cocarascu</last></editor>
     <editor><first>Christos</first><last>Christodoulopoulos</last></editor>
     <editor><first>Arpit</first><last>Mittal</last></editor>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <url>http://www.aclweb.org/anthology/W18-55</url>
     <bibtype>book</bibtype>
     <bibkey>FEVER:2018</bibkey>
   </paper>

   <paper id="5501">
     <title>The Fact Extraction and VERification (FEVER) Shared Task</title>
     <author><first>James</first><last>Thorne</last></author>
     <author><first>Andreas</first><last>Vlachos</last></author>
     <author><first>Oana</first><last>Cocarascu</last></author>
     <author><first>Christos</first><last>Christodoulopoulos</last></author>
     <author><first>Arpit</first><last>Mittal</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>1–9</pages>
     <url>http://www.aclweb.org/anthology/W18-5501</url>
     <abstract>We present the results of the first Fact Extraction and VERification (FEVER) Shared Task. The task challenged participants to classify whether human-written factoid claims could be Supported or Refuted using evidence retrieved from Wikipedia. We received entries from 23 competing teams, 19 of which scored higher than the previously published baseline. The best performing system achieved a FEVER score of 64.21%. In this paper, we present the results of the shared task and a summary of the systems, highlighting commonalities and innovations among participating systems.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>thorne-EtAl:2018:FEVER</bibkey>
   </paper>

   <paper id="5502">
     <title>The Data Challenge in Misinformation Detection: Source Reputation vs. Content Veracity</title>
     <author><first>Fatemeh</first><last>Torabi Asr</last></author>
     <author><first>Maite</first><last>Taboada</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>10–15</pages>
     <url>http://www.aclweb.org/anthology/W18-5502</url>
     <abstract>Misinformation detection at the level of full news articles is a text classification problem. Reliably labeled data in this domain is rare. Previous work relied on news articles collected from so-called “reputable” and “suspicious” websites and labeled accordingly. We leverage fact-checking websites to collect individually-labeled news articles with regard to the veracity of their content and use this data to test the cross-domain generalization of a classifier trained on bigger text collections but labeled according to source reputation. Our results suggest that reputation-based classification is not sufficient for predicting the veracity level of the majority of news articles, and that the system performance on different test datasets depends on topic distribution. Therefore collecting well-balanced and carefully-assessed training data is a priority for developing robust misinformation detection systems.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>torabiasr-taboada:2018:FEVER</bibkey>
   </paper>

   <paper id="5503">
     <title>Crowdsourcing Semantic Label Propagation in Relation Classification</title>
     <author><first>Anca</first><last>Dumitrache</last></author>
     <author><first>Lora</first><last>Aroyo</last></author>
     <author><first>Chris</first><last>Welty</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>16–21</pages>
     <url>http://www.aclweb.org/anthology/W18-5503</url>
     <abstract>Distant supervision is a popular method for performing relation extraction from text that is known to produce noisy labels. Most progress in relation extraction and classification has been made with crowdsourced corrections to distant-supervised labels, and there is evidence that indicates still more would be better. In this paper, we explore the problem of propagating human annotation signals gathered for open-domain relation classification through the CrowdTruth methodology for crowdsourcing, that captures ambiguity in annotations by measuring inter-annotator disagreement. Our approach propagates annotations to sentences that are similar in a low dimensional embedding space, expanding the number of labels by two orders of magnitude. Our experiments show significant improvement in a sentence-level multi-class relation classifier.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>dumitrache-aroyo-welty:2018:FEVER</bibkey>
   </paper>

   <paper id="5504">
     <title>Retrieve and Re-rank: A Simple and Effective IR Approach to Simple Question Answering over Knowledge Graphs</title>
     <author><first>Vishal</first><last>Gupta</last></author>
     <author><first>Manoj</first><last>Chinnakotla</last></author>
     <author><first>Manish</first><last>Shrivastava</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>22–27</pages>
     <url>http://www.aclweb.org/anthology/W18-5504</url>
     <abstract>SimpleQuestions is a commonly used benchmark</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>gupta-chinnakotla-shrivastava:2018:FEVER</bibkey>
   </paper>

   <paper id="5505">
     <title>Information Nutrition Labels: A Plugin for Online News Evaluation</title>
     <author><first>Vincentius</first><last>Kevin</last></author>
     <author><first>Birte</first><last>Högden</last></author>
     <author><first>Claudia</first><last>Schwenger</last></author>
     <author><first>Ali</first><last>Sahan</last></author>
     <author><first>Neelu</first><last>Madan</last></author>
     <author><first>Piush</first><last>Aggarwal</last></author>
     <author><first>Anusha</first><last>Bangaru</last></author>
     <author><first>Farid</first><last>Muradov</last></author>
     <author><first>Ahmet</first><last>Aker</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>28–33</pages>
     <url>http://www.aclweb.org/anthology/W18-5505</url>
     <abstract>In this paper we present a browser plugin NewsScan that assists online news readers in evaluating the quality of online content they read by providing information nutrition labels for online news articles. In analogy to groceries, where nutrition labels help consumers make choices that they consider best for themselves, information nutrition labels tag online news articles with data that help readers judge the articles they engage with. This paper discusses the choice of the labels, their implementation and visualization.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>kevin-EtAl:2018:FEVER</bibkey>
   </paper>

   <paper id="5506">
     <title>Joint Modeling for Query Expansion and Information Extraction with Reinforcement Learning</title>
     <author><first>Motoki</first><last>Taniguchi</last></author>
     <author><first>Yasuhide</first><last>Miura</last></author>
     <author><first>Tomoko</first><last>Ohkuma</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>34–39</pages>
     <url>http://www.aclweb.org/anthology/W18-5506</url>
     <abstract>Information extraction about an event can be improved by incorporating external evidence.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>taniguchi-miura-ohkuma:2018:FEVER</bibkey>
   </paper>

   <paper id="5507">
     <title>Towards Automatic Fake News Detection: Cross-Level Stance Detection in News Articles</title>
     <author><first>Costanza</first><last>Conforti</last></author>
     <author><first>Mohammad Taher</first><last>Pilehvar</last></author>
     <author><first>Nigel</first><last>Collier</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>40–49</pages>
     <url>http://www.aclweb.org/anthology/W18-5507</url>
     <abstract>In this paper, we propose to adapt the four-staged pipeline proposed by Zubiaga et al. (2018) for the Rumor Verification task to the problem of Fake News Detection. We show that the recently released Fnc-1 corpus covers two of its steps, namely the Tracking and the Stance Detection task. We identify asymmetry in length to be a key characteristic of the latter step, when adapted to the framework of Fake News Detection and propose to handle it as a specific type of Cross-Level Stance Detection. Inspired by theories from the field of Journalism Studies, we implement and test two architectures to successfully model the internal structure of an article and its interactions with a claim.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>conforti-pilehvar-collier:2018:FEVER</bibkey>
   </paper>

   <paper id="5508">
     <title>Belittling the Source: Trustworthiness Indicators to Obfuscate Fake News on the Web</title>
     <author><first>Diego</first><last>Esteves</last></author>
     <author><first>Aniketh Janardhan</first><last>Reddy</last></author>
     <author><first>Piyush</first><last>Chawla</last></author>
     <author><first>Jens</first><last>Lehmann</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>50–59</pages>
     <url>http://www.aclweb.org/anthology/W18-5508</url>
     <abstract>With the growth of the internet, the number of fake-news online has been proliferating every year. The consequences of such phenomena are manifold, ranging from lousy decision-making process to bullying and violence episodes. Therefore, fact-checking algorithms became a valuable asset. To this aim, an important step to detect fake-news is to have access to a credibility score for a given information source. However, most of the widely used Web indicators have either been shut-down to the public (e.g., Google PageRank) or are not free for use (Alexa Rank). Further existing databases are short-manually curated lists of online sources, which do not scale. Finally, most of the research on the topic is theoretical-based or explore confidential data in a restricted simulation environment. In this paper we explore current research, highlight the challenges and propose solutions to tackle the problem of classifying websites into a credibility scale. The proposed model automatically extracts source reputation cues and computes a credibility factor, providing valuable insights which can help in belittling dubious and confirming trustful unknown websites. Experimental results outperform state of the art in the 2-classes and 5-classes setting.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>esteves-EtAl:2018:FEVER</bibkey>
   </paper>

   <paper id="5509">
     <title>Automated Fact-Checking of Claims in Argumentative Parliamentary Debates</title>
     <author><first>Nona</first><last>Naderi</last></author>
     <author><first>Graeme</first><last>Hirst</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>60–65</pages>
     <url>http://www.aclweb.org/anthology/W18-5509</url>
     <abstract>We present an automated approach to distinguish true, false, stretch, and dodge statements in questions and answers in the Canadian Parliament. We leverage the truthfulness annotations of a U.S. fact-checking corpus by training a neural net model and incorporating the prediction probabilities into our models.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>naderi-hirst:2018:FEVER</bibkey>
   </paper>

   <paper id="5510">
     <title>Stance Detection in Fake News A Combined Feature Representation</title>
     <author><first>Bilal</first><last>Ghanem</last></author>
     <author><first>Paolo</first><last>Rosso</last></author>
     <author><first>Francisco</first><last>Rangel</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>66–71</pages>
     <url>http://www.aclweb.org/anthology/W18-5510</url>
     <abstract>With the uncontrolled increasing of fake news and rumors over the Web, different approaches have been proposed to address the problem. In this paper, we present an approach that combines lexical, word embeddings and n-gram features to detect the stance in fake news. Our approach has been tested on the Fake News Challenge (FNC-1) dataset. Given a news title-article pair, the FNC-1 task aims at determining the relevance of the article and the title. Our proposed approach has achieved an accurate result (59.6% Macro F1) that is close to the state-of-the-art result with 0.013 difference using a simple feature representation. Furthermore, we have investigated the importance of different lexicons in the detection of the classification labels.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>ghanem-rosso-rangel:2018:FEVER</bibkey>
   </paper>

   <paper id="5511">
     <title>Zero-shot Relation Classification as Textual Entailment</title>
     <author><first>Abiola</first><last>Obamuyide</last></author>
     <author><first>Andreas</first><last>Vlachos</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>72–78</pages>
     <url>http://www.aclweb.org/anthology/W18-5511</url>
     <abstract>We consider the task of relation classification, and pose this task as one of textual entailment. We show that this formulation leads to several advantages, including the ability to (i) perform zero-shot relation classification by exploiting relation descriptions, (ii) utilize existing textual entailment models, and (iii) leverage readily available textual entailment datasets, to enhance the performance of relation classification systems. Our experiments show that the proposed approach achieves 20.16% and 61.32% in F1 zero-shot classification performance on two datasets, which further improved to 22.80% and 64.78% respectively with the use of conditional encoding.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>obamuyide-vlachos:2018:FEVER</bibkey>
   </paper>

   <paper id="5512">
     <title>Teaching Syntax by Adversarial Distraction</title>
     <author><first>Juho</first><last>Kim</last></author>
     <author><first>Christopher</first><last>Malon</last></author>
     <author><first>Asim</first><last>Kadav</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>79–84</pages>
     <url>http://www.aclweb.org/anthology/W18-5512</url>
     <abstract>Existing entailment datasets mainly pose problems which can be answered without attention to grammar or word order. Learning syntax requires comparing examples where different grammar and word order change the desired classification. We introduce several datasets based on synthetic transformations of natural entailment examples in SNLI or FEVER, to teach aspects of grammar and word order. We show that without retraining, popular entailment models are unaware that these syntactic differences change meaning. With retraining, some but not all popular entailment models can learn to compare the syntax properly.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>kim-malon-kadav:2018:FEVER</bibkey>
   </paper>

   <paper id="5513">
     <title>Where is Your Evidence: Improving Fact-checking by Justification Modeling</title>
     <author><first>Tariq</first><last>Alhindi</last></author>
     <author><first>Savvas</first><last>Petridis</last></author>
     <author><first>Smaranda</first><last>Muresan</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>85–90</pages>
     <url>http://www.aclweb.org/anthology/W18-5513</url>
     <abstract>Fact-checking is a journalistic practice that compares a claim made publicly against trusted sources of facts. Wang (2017) introduced a large dataset of validated claims from the POLITIFACT .com website (LIAR dataset), enabling the development of machine learning approaches for fact-checking. However, approaches based on this dataset have focused primarily on modeling the claim and speaker-related metadata, without considering the evidence used by humans in labeling the claims. We extend the LIAR dataset by automatically extracting the justification from the fact-checking article used by humans to label a given claim. We show that modeling the extracted justification in conjunction with the claim (and metadata) provides a significant improvement regardless of the machine learning model used (feature-based or deep learning) both in a binary classification task (true, false) and in a six-way classification task (pants on fire, false, mostly false, half true, mostly true, true).</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>alhindi-petridis-muresan:2018:FEVER</bibkey>
   </paper>

   <paper id="5514">
     <title>Affordance Extraction and Inference based on Semantic Role Labeling</title>
     <author><first>Daniel</first><last>Loureiro</last></author>
     <author><first>Alípio</first><last>Jorge</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>91–96</pages>
     <url>http://www.aclweb.org/anthology/W18-5514</url>
     <abstract>Common-sense reasoning is becoming increasingly important for the advancement of Natural Language Processing. While word embeddings have been very successful, they cannot explain which aspects of ‘coffee’ and ‘tea’ make them similar, or how they could be related to ‘shop’. In this paper, we propose an explicit word representation that builds upon the Distributional Hypothesis to represent meaning from semantic roles, and allow inference of relations from their meshing, as supported by the affordance-based Indexical Hypothesis. We find that our model improves the state-of-the-art on unsupervised word similarity tasks while allowing for direct inference of new relations from the same vector space.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>loureiro-jorge:2018:FEVER</bibkey>
   </paper>

   <paper id="5515">
     <title>UCL Machine Reading Group: Four Factor Framework For Fact Finding (HexaF)</title>
     <author><first>Takuma</first><last>Yoneda</last></author>
     <author><first>Jeff</first><last>Mitchell</last></author>
     <author><first>Johannes</first><last>Welbl</last></author>
     <author><first>Pontus</first><last>Stenetorp</last></author>
     <author><first>Sebastian</first><last>Riedel</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>97–102</pages>
     <url>http://www.aclweb.org/anthology/W18-5515</url>
     <abstract>Our system is a four stage model consisting of document retrieval, sentence retrieval, natural language inference and aggregation. Document retrieval attempts to find the name of a Wikipedia article in the claim, and then ranks each article based on capitalisation, sentence position and token match features. A set of sentences are then retrieved from the top ranked articles, based on token matches with the claim and position in the article. A natural language inference model is then applied to each of these sentences paired with the claim, giving a prediction for each potential evidence. These predictions are then aggregated using a simple MLP, and the sentences are reranked to keep only the evidence consistent with the final prediction.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>yoneda-EtAl:2018:FEVER</bibkey>
   </paper>

   <paper id="5516">
     <title>UKP-Athene: Multi-Sentence Textual Entailment for Claim Verification</title>
     <author><first>Andreas</first><last>Hanselowski</last></author>
     <author><first>Hao</first><last>Zhang</last></author>
     <author><first>Zile</first><last>Li</last></author>
     <author><first>Daniil</first><last>Sorokin</last></author>
     <author><first>Benjamin</first><last>Schiller</last></author>
     <author><first>Claudia</first><last>Schulz</last></author>
     <author><first>Iryna</first><last>Gurevych</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>103–108</pages>
     <url>http://www.aclweb.org/anthology/W18-5516</url>
     <abstract>The Fact Extraction and VERification</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>hanselowski-EtAl:2018:FEVER</bibkey>
   </paper>

   <paper id="5517">
     <title>Team Papelo: Transformer Networks at FEVER</title>
     <author><first>Christopher</first><last>Malon</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>109–113</pages>
     <url>http://www.aclweb.org/anthology/W18-5517</url>
     <abstract>We develop a system for the FEVER fact extraction and verification challenge that uses a high precision entailment classifier based on transformer networks pretrained with language modeling, to classify a broad set of potential evidence. The precision of the entailment classifier allows us to enhance recall by considering every statement from several articles to decide upon each claim. We include not only the articles best matching the claim text by TFIDF score, but read additional articles whose titles match named entities and capitalized expressions occurring in the claim text. The entailment module evaluates potential evidence one statement at a time, together with the title of the page the evidence came from (providing a hint about possible pronoun antecedents). In preliminary evaluation, the system achieves .5736 FEVER score, .6108 label accuracy, and .6485 evidence F1 on the FEVER shared task test set.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>malon:2018:FEVER</bibkey>
   </paper>

   <paper id="5518">
     <title>Uni-DUE Student Team: Tackling fact checking through decomposable attention neural network</title>
     <author><first>Jan</first><last>Kowollik</last></author>
     <author><first>Ahmet</first><last>Aker</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>114–118</pages>
     <url>http://www.aclweb.org/anthology/W18-5518</url>
     <abstract>In this paper we present our system for the FEVER Challenge.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>kowollik-aker:2018:FEVER</bibkey>
   </paper>

   <paper id="5519">
     <title>SIRIUS-LTG: An Entity Linking Approach to Fact Extraction and Verification</title>
     <author><first>Farhad</first><last>Nooralahzadeh</last></author>
     <author><first>Lilja</first><last>Øvrelid</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>119–123</pages>
     <url>http://www.aclweb.org/anthology/W18-5519</url>
     <abstract>This article presents the SIRIUS-LTG system for the Fact Extraction and VERification (FEVER) SharedTask. It consists of three components: 1)Wikipedia Page Retrieval: First we extract the entities in the claim, then we find potential Wikipedia URI candidates for each of the entities using a SPARQL query over DBpedia 2)Sentence selection: We investigate various techniques i.e. Smooth Inverse Frequency(SIF), Word Mover’s Distance (WMD), Soft-Cosine Similarity, Cosine similarity with uni-gram Term Frequency Inverse Document Frequency (TF-IDF) to rank sentences by their similarity to the claim. 3)Textual Entailment: We compare three models for the task of claim classification. We apply a Decomposable Attention (DA) model (Parikh et al., 2016), a Decomposed Graph Entailment (DGE) model (Khot et al., 2018) and a Gradient-Boosted Decision Trees (TalosTree) model (Sean et al.,2017) for this task. The experiments show that the pipeline with simple Cosine Similarity using TFIDF in sentence selection along with DA model as labelling model achieves the best results on the development set (F1evidence: 32.17, label accuracy: 59.61 andFEVER score: 0.3778). Furthermore, it obtains 30.19, 48.87 and 36.55 in terms of F1 evidence, label accuracy and FEVER score, respectively, on the test set. Our system ranks 15th among 23 participants in the shared task prior to any human-evaluation of the evidence.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>nooralahzadeh-vrelid:2018:FEVER</bibkey>
   </paper>

   <paper id="5520">
     <title>Integrating Entity Linking and Evidence Ranking for Fact Extraction and Verification</title>
     <author><first>Motoki</first><last>Taniguchi</last></author>
     <author><first>Tomoki</first><last>Taniguchi</last></author>
     <author><first>Takumi</first><last>Takahashi</last></author>
     <author><first>Yasuhide</first><last>Miura</last></author>
     <author><first>Tomoko</first><last>Ohkuma</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>124–126</pages>
     <url>http://www.aclweb.org/anthology/W18-5520</url>
     <abstract>We describe here our system and results on the FEVER shared task.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>taniguchi-EtAl:2018:FEVER</bibkey>
   </paper>

   <paper id="5521">
     <title>Robust Document Retrieval and Individual Evidence Modeling for Fact Extraction and Verification.</title>
     <author><first>Tuhin</first><last>Chakrabarty</last></author>
     <author><first>Tariq</first><last>Alhindi</last></author>
     <author><first>Smaranda</first><last>Muresan</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>127–131</pages>
     <url>http://www.aclweb.org/anthology/W18-5521</url>
     <abstract>This paper presents the ColumbiaNLP submission for the FEVER Workshop Shared Task. Our system is an end-to-end pipeline that extracts factual evidence from Wikipedia and infers a decision about the truthfulness of the claim based on the extracted evidence. Our pipeline achieves significant improvement over the baseline for all the components (Document Retrieval, Sentence Selection and Textual Entailment) both on the development set and the test set. Our team finished 6th out of 24 teams on the leader-board based on the preliminary results with a FEVER score of 49.06 on the blind test set compared to 27.45 of the baseline system.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>chakrabarty-alhindi-muresan:2018:FEVER</bibkey>
   </paper>

   <paper id="5522">
     <title>DeFactoNLP: Fact Verification using Entity Recognition, TFIDF Vector Comparison and Decomposable Attention</title>
     <author><first>Aniketh Janardhan</first><last>Reddy</last></author>
     <author><first>Gil</first><last>Rocha</last></author>
     <author><first>Diego</first><last>Esteves</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>132–137</pages>
     <url>http://www.aclweb.org/anthology/W18-5522</url>
     <abstract>In this paper, we describe DeFactoNLP, the system we designed for the FEVER 2018 Shared Task. The aim of this task was to conceive a system that can not only automatically assess the veracity of a claim but also retrieve evidence supporting this assessment from Wikipedia. In our approach, the Wikipedia documents whose Term Frequency-Inverse Document Frequency (TFIDF) vectors are most similar to the vector of the claim and those documents whose names are similar to those of the named entities (NEs) mentioned in the claim are identified as the documents which might contain evidence. The sentences in these documents are then supplied to a textual entailment recognition module. This module calculates the probability of each sentence supporting the claim, contradicting the claim or not providing any relevant information to assess the veracity of the claim. Various features computed using these probabilities are finally used by a Random Forest classifier to determine the overall truthfulness of the claim. The sentences which support this classification are returned as evidence. Our approach achieved a 0.4277 evidence F1-score, a 0.5136 label accuracy and a 0.3833 FEVER score.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>reddy-rocha-esteves:2018:FEVER</bibkey>
   </paper>

   <paper id="5523">
     <title>An End-to-End Multi-task Learning Model for Fact Checking</title>
     <author><first>sizhen</first><last>li</last></author>
     <author><first>Shuai</first><last>Zhao</last></author>
     <author><first>Bo</first><last>Cheng</last></author>
     <author><first>Hao</first><last>Yang</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>138–144</pages>
     <url>http://www.aclweb.org/anthology/W18-5523</url>
     <abstract>With huge amount of information generated every day on the web, fact checking is an im- portant and challenging task which can help people identify the authenticity of most claims as well as providing evidences selected from knowledge source like Wikipedia. Here we decompose this problem into two parts: an en- tity linking task (retrieving relative Wikipedia pages) and recognizing textual entailment be- tween the claim and selected pages. In this pa- per, we present an end-to-end multi-task learn- ing with bi-direction attention (EMBA) model to classify the claim as “supports”, “refutes” or “not enough info” with respect to the pages retrieved and detect sentences as evidence at the same time. We conduct experiments on the FEVER (Fact Extraction and VERification) paper test dataset and shared task test dataset, a new public dataset for verification against tex- tual sources. Experimental results show that our method achieves comparable performance compared with the baseline system.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>li-EtAl:2018:FEVER</bibkey>
   </paper>

   <paper id="5524">
     <title>Team GESIS Cologne: An all in all sentence-based approach for FEVER</title>
     <author><first>Wolfgang</first><last>Otto</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>145–149</pages>
     <url>http://www.aclweb.org/anthology/W18-5524</url>
     <abstract>In this system description of our pipeline to participate at the Fever Shared Task, we describe our sentence-based approach. Throughout all steps of our pipeline, we regarded single sentences as our processing unit. In our IR-</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>otto:2018:FEVER</bibkey>
   </paper>

   <paper id="5525">
     <title>Team SWEEPer: Joint Sentence Extraction and Fact Checking with Pointer Networks</title>
     <author><first>Christopher</first><last>Hidey</last></author>
     <author><first>Mona</first><last>Diab</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>150–155</pages>
     <url>http://www.aclweb.org/anthology/W18-5525</url>
     <abstract>Our model for fact checking and verification consists of two stages: 1) identifying relevant documents using lexical and syntactic features from the claim and first two sentences in the Wikipedia article and 2) jointly modeling sentence extraction and verification. As the tasks of fact checking and finding evidence are dependent on each other, an ideal model would consider the veracity of the claim when finding evidence and also find only the evidence that supports/refutes the position of the claim. We thus jointly model the second stage by using a pointer network with the claim and evidence sentence represented using the ESIM module. For stage 2, we first train both components using multi-task learning over a larger memory of extracted sentences, then tune parameters to first extract sentences and predict the relation from only the extracted sentences.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>hidey-diab:2018:FEVER</bibkey>
   </paper>

   <paper id="5526">
     <title>QED: A fact verification system for the FEVER shared task</title>
     <author><first>Jackson</first><last>Luken</last></author>
     <author><first>Nanjiang</first><last>Jiang</last></author>
     <author><first>Marie-Catherine</first><last>de Marneffe</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>156–160</pages>
     <url>http://www.aclweb.org/anthology/W18-5526</url>
     <abstract>This paper describes our system submission to the 2018 Fact Extraction and VERification (FEVER) shared task. The system uses a heuristics-based approach for evidence extraction and a modified version of the inference model by Parikh et al. (2016) for classification. Our process is broken down into three modules: potentially relevant documents are gathered based on key phrases in the claim, then any possible evidence sentences inside those documents are extracted, and finally our classifier discards any evidence deemed irrelevant and uses the remaining to classify the claim’s veracity. Our system beats the shared task baseline by 12% and is successful at finding correct evidence (evidence retrieval F1 of 62.5% on the development set).</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>luken-jiang-demarneffe:2018:FEVER</bibkey>
   </paper>

   <paper id="5527">
     <title>Team UMBC-FEVER : Claim verification using Semantic Lexical Resources</title>
     <author><first>Ankur</first><last>Padia</last></author>
     <author><first>Francis</first><last>Ferraro</last></author>
     <author><first>Tim</first><last>Finin</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>161–165</pages>
     <url>http://www.aclweb.org/anthology/W18-5527</url>
     <abstract>We describe our system used in the 2018</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>padia-ferraro-finin:2018:FEVER</bibkey>
   </paper>

   <paper id="5528">
     <title>A mostly unlexicalized model for recognizing textual entailment</title>
     <author><first>Mithun</first><last>Paul</last></author>
     <author><first>Rebecca</first><last>Sharp</last></author>
     <author><first>Mihai</first><last>Surdeanu</last></author>
     <booktitle>Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)</booktitle>
     <month>November</month>
     <year>2018</year>
     <address>Brussels, Belgium</address>
     <publisher>Association for Computational Linguistics</publisher>
     <pages>166–171</pages>
     <url>http://www.aclweb.org/anthology/W18-5528</url>
     <abstract>Many approaches to automatically recognizing entailment relations have employed classifiers over hand engineered lexicalized features, or deep learning models that implicitly capture lexicalization through word embeddings. This reliance on lexicalization may complicate the adaptation of these tools between domains. For example, such a system trained in the news domain may learn that a sentence like “Palestinians recognize Texas as part of Mexico” tends to be unsupported, but this fact (and its corresponding lexicalized cues) have no value in, say, a scientific domain. To mitigate this dependence on lexicalized information, in this paper we propose a model that reads two sentences, from any given domain, to determine entailment without using lexicalized features. Instead our model relies on features that are either unlexicalized or are domain independent such as proportion of negated verbs, antonyms, or noun overlap. In its current implementation, this model does not perform well on the FEVER dataset, due to two reasons. First, for the information retrieval portion of the task we used the baseline system provided, since this was not the aim of our project. Second, this is work in progress and we still are in the process of identifying more features and gradually increasing the accuracy of our model. In the end, we hope to build a generic end-to-end classifier, which can be used in a domain outside the one in which it was trained, with no or minimal re-training.</abstract>
     <bibtype>inproceedings</bibtype>
     <bibkey>paul-sharp-surdeanu:2018:FEVER</bibkey>
   </paper>

  <paper id="5600">
    <title>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</title>
    <editor><first>Alberto</first><last>Lavelli</last></editor>
    <editor><first>Anne-Lyse</first><last>Minard</last></editor>
    <editor><first>Fabio</first><last>Rinaldi</last></editor>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-56</url>
    <bibtype>book</bibtype>
    <bibkey>LOUHI:2018</bibkey>
  </paper>

  <paper id="5601">
    <title>Detecting Diabetes Risk from Social Media Activity</title>
    <author><first>Dane</first><last>Bell</last></author>
    <author><first>Egoitz</first><last>Laparra</last></author>
    <author><first>Aditya</first><last>Kousik</last></author>
    <author><first>Terron</first><last>Ishihara</last></author>
    <author><first>Mihai</first><last>Surdeanu</last></author>
    <author><first>Stephen</first><last>Kobourov</last></author>
    <booktitle>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–11</pages>
    <url>http://www.aclweb.org/anthology/W18-5601</url>
    <abstract>This work is the first to explore the detection of individuals’ risk of type 2 diabetes mellitus (T2DM) directly from their social media (Twitter) activity. Our approach extends a deep learning architecture with several contributions: following previous observations that language use differs by gender, it captures and uses gender information through domain adaptation; it captures recency of posts under the hypothesis that more recent posts are more representative of an individual’s current risk status; and, lastly, it demonstrates that in this scenario where activity factors are sparsely represented in the data, a bag-of-word neural network model using custom dictionaries of food and activity words performs better than other neural sequence models. Our best model, which incorporates all these contributions, achieves a risk-detection F1 of 41.9, considerably higher than the baseline rate (36.9).</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>bell-EtAl:2018:LOUHI</bibkey>
  </paper>

  <paper id="5602">
    <title>Treatment Side Effect Prediction from Online User-generated Content</title>
    <author><first>Hoang</first><last>Nguyen</last></author>
    <author><first>Kazunari</first><last>Sugiyama</last></author>
    <author><first>Min-Yen</first><last>Kan</last></author>
    <author><first>Kishaloy</first><last>Halder</last></author>
    <booktitle>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>12–21</pages>
    <url>http://www.aclweb.org/anthology/W18-5602</url>
    <abstract>With Health 2.0, patients and caregivers increasingly seek information regarding possible drug side effects during their medical treatments in online health communities. These online communities are helpful platforms for non-professional medical opinions, yet pose risk of being unreliable in quality and insufficient in quantity to cover the wide range of potential drug reactions. Current approaches to analysing such user-generated content in online forums heavily rely on feature engineering of both documents and users, and often overlook the relationships between posts within a common discussion thread. Inspired by recent advancements, we propose a neural architecture that models the textual content of user-generated documents and user experiences in online communities to predict side effects during treatment. Experimental results show that our proposed architecture outperforms baseline models.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>nguyen-EtAl:2018:LOUHI</bibkey>
  </paper>

  <paper id="5603">
    <title>Revisiting neural relation classification in clinical notes with external information</title>
    <author><first>Simon</first><last>Suster</last></author>
    <author><first>Madhumita</first><last>Sushil</last></author>
    <author><first>Walter</first><last>Daelemans</last></author>
    <booktitle>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>22–28</pages>
    <url>http://www.aclweb.org/anthology/W18-5603</url>
    <abstract>Recently, segment convolutional neural networks have been proposed for end-to-end relation extraction in the clinical domain, achieving results comparable to or outperforming the approaches with heavy manual feature engineering. In this paper, we analyze the errors made by the neural classifier based on confusion matrices, and then investigate three simple extensions to overcome its limitations. We find that including ontological association between drugs and problems, and data-induced association between medical concepts does not reliably improve the performance, but that large gains are obtained by the incorporation of semantic classes to capture relation triggers.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>suster-sushil-daelemans:2018:LOUHI</bibkey>
  </paper>

  <paper id="5604">
    <title>Supervised Machine Learning for Extractive Query Based Summarisation of Biomedical Data</title>
    <author><first>Mandeep</first><last>Kaur</last></author>
    <author><first>Diego</first><last>Molla</last></author>
    <booktitle>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>29–37</pages>
    <url>http://www.aclweb.org/anthology/W18-5604</url>
    <abstract>The automation of text summarisation of biomedical publications is a pressing need due to the plethora of information available on-line. This paper explores the impact of several supervised machine learning approaches for extracting multi-document summaries for given queries. In particular, we compare classification and regression approaches for query-based extractive summarisation using data provided by the BioASQ Challenge. We tackled the problem of annotating sentences for training classification systems and show that a simple annotation approach outperforms regression-based summarisation.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kaur-molla:2018:LOUHI</bibkey>
  </paper>

  <paper id="5605">
    <title>Comparing CNN and LSTM character-level embeddings in BiLSTM-CRF models for chemical and disease named entity recognition</title>
    <author><first>Zenan</first><last>Zhai</last></author>
    <author><first>Dat Quoc</first><last>Nguyen</last></author>
    <author><first>Karin</first><last>Verspoor</last></author>
    <booktitle>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>38–43</pages>
    <url>http://www.aclweb.org/anthology/W18-5605</url>
    <abstract>We compare the use of LSTM-based and CNN-based character-level word embeddings in BiLSTM-CRF models to approach chemical and disease named entity recognition (NER) tasks. Empirical results over the BioCreative V CDR corpus show that the use of either type of character-level word embeddings in conjunction with the BiLSTM-CRF models leads to comparable state-of-the-art performance. However, the models using CNN-based character-level word embeddings have a computational performance advantage, increasing training time over word-based models by 25% while the LSTM-based character-level word embeddings more than double the required training time.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>zhai-nguyen-verspoor:2018:LOUHI</bibkey>
  </paper>

  <paper id="5606">
    <title>Deep learning for language understanding of mental health concepts derived from Cognitive Behavioural Therapy</title>
    <author><first>Lina M.</first><last>Rojas Barahona</last></author>
    <author><first>Bo-Hsiang</first><last>Tseng</last></author>
    <author><first>Yinpei</first><last>Dai</last></author>
    <author><first>Clare</first><last>Mansfield</last></author>
    <author><first>Osman</first><last>Ramadan</last></author>
    <author><first>Stefan</first><last>Ultes</last></author>
    <author><first>Michael</first><last>Crawford</last></author>
    <author><first>Milica</first><last>Gasic</last></author>
    <booktitle>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>44–54</pages>
    <url>http://www.aclweb.org/anthology/W18-5606</url>
    <abstract>In recent years, we have seen deep learning and distributed representations of words and sentences make impact on a number of natural language processing tasks, such as similarity, entailment and sentiment analysis. Here we introduce a new task: understanding of mental health concepts derived from Cognitive Behavioural Therapy (CBT). We define a mental health ontology based on the CBT principles, annotate a large corpus where this phenomena is exhibited and perform understanding using deep learning and distributed representations. Our results show that the performance of deep learning models combined with word embeddings or sentence embeddings significantly outperform non-deep-learning models in this difficult task. This understanding module will be an essential component of a statistical dialogue system delivering therapy.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rojasbarahona-EtAl:2018:LOUHI</bibkey>
  </paper>

  <paper id="5607">
    <title>Investigating the Challenges of Temporal Relation Extraction from Clinical Text</title>
    <author><first>Diana</first><last>Galvan</last></author>
    <author><first>Naoaki</first><last>Okazaki</last></author>
    <author><first>Koji</first><last>Matsuda</last></author>
    <author><first>Kentaro</first><last>Inui</last></author>
    <booktitle>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>55–64</pages>
    <url>http://www.aclweb.org/anthology/W18-5607</url>
    <abstract>Temporal reasoning remains as an unsolved task for Natural Language Processing (NLP), particularly demonstrated in the clinical domain. The complexity of temporal representation in language is evident as results of the 2016 Clinical TempEval challenge show: the current state-of-the-art systems perform well in solving mention-identification tasks of event and time expression, but poorly in temporal relation extraction, showing a gap of around 0.25 point below human performance. We explore to adapt the tree-based LSTM-RNN model proposed by Miwa and Bansal (2016) to temporal relation extraction from clinical text, obtaining a five point improvement over the best 2016 Clinical TempEval system and two points over the state-of-the-art. We deliver a deep analysis of the results and discuss the next step towards human-like temporal reasoning.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>galvan-EtAl:2018:LOUHI</bibkey>
  </paper>

  <paper id="5608">
    <title>De-identifying Free Text of Japanese Dummy Electronic Health Records</title>
    <author><first>Kohei</first><last>Kajiyama</last></author>
    <author><first>Hiromasa</first><last>Horiguchi</last></author>
    <author><first>Takashi</first><last>Okumura</last></author>
    <author><first>Mizuki</first><last>Morita</last></author>
    <author><first>Yoshinobu</first><last>Kano</last></author>
    <booktitle>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>65–70</pages>
    <url>http://www.aclweb.org/anthology/W18-5608</url>
    <abstract>A new law was established in Japan to promote utilization of EHRs for research and developments, while de-identification is required to use EHRs. However, studies of automatic anonymization in the healthcare domain is not active for Japanese language, no de-identification tool available in practical performance for Japanese medical domains, as far as we know. Previous works show that rule-based methods are still effective, while deep learning methods are reported to be better recently. In order to implement and evaluate an de-identification tool in a practical level, we implemented three methods, rule-based, CRF, and LSTM. We prepared three datasets of pseudo EHRs with de-identification tags manually annoated. These datasets are derived from shared task data to compare with previous works, and our new data to increase training data. Our result shows that our LSTM-based method is better and robust, which leads to our future work that plans to apply our system to actual de-identification tasks in hospitals.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kajiyama-EtAl:2018:LOUHI</bibkey>
  </paper>

  <paper id="5609">
    <title>Unsupervised Identification of Study Descriptors in Toxicology Research: An Experimental Study</title>
    <author><first>Drahomira</first><last>Herrmannova</last></author>
    <author><first>Steven</first><last>Young</last></author>
    <author><first>Robert</first><last>Patton</last></author>
    <author><first>Christopher</first><last>Stahl</last></author>
    <author><first>Nicole</first><last>Kleinstreuer</last></author>
    <author><first>Mary</first><last>Wolfe</last></author>
    <booktitle>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>71–82</pages>
    <url>http://www.aclweb.org/anthology/W18-5609</url>
    <abstract>Identifying and extracting data elements such as study descriptors in publication full texts is a critical yet manual and labor-intensive step required in a number of tasks. In this paper we address the question of identifying data elements in an unsupervised manner. Specifically, provided a set of criteria describing specific study parameters, such as species, route of administration, and dosing regimen, we develop an unsupervised approach to identify text segments relevant to the criteria. A binary classifier trained to identify publications that met the criteria performs better when trained on the candidate sentences than when trained on sentences randomly picked from the text, supporting the intuition that our method is able to accurately identify study descriptors.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>herrmannova-EtAl:2018:LOUHI</bibkey>
  </paper>

  <paper id="5610">
    <title>Identification of Parallel Sentences in Comparable Monolingual Corpora from Different Registers</title>
    <author><first>Rémi</first><last>Cardon</last></author>
    <author><first>Natalia</first><last>Grabar</last></author>
    <booktitle>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>83–93</pages>
    <url>http://www.aclweb.org/anthology/W18-5610</url>
    <abstract>Parallel aligned sentences provide useful information for different NLP applications. </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>cardon-grabar:2018:LOUHI</bibkey>
  </paper>

  <paper id="5611">
    <title>Evaluation of a Prototype System that Automatically Assigns Subject Headings to Nursing Narratives Using Recurrent Neural Network</title>
    <author><first>Hans</first><last>Moen</last></author>
    <author><first>Kai</first><last>Hakala</last></author>
    <author><first>Laura-Maria</first><last>Peltonen</last></author>
    <author><first>Henry</first><last>Suhonen</last></author>
    <author><first>Petri</first><last>Loukasmäki</last></author>
    <author><first>Tapio</first><last>Salakoski</last></author>
    <author><first>Filip</first><last>Ginter</last></author>
    <author><first>Sanna</first><last>Salanterä</last></author>
    <booktitle>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>94–100</pages>
    <url>http://www.aclweb.org/anthology/W18-5611</url>
    <abstract>We present our initial evaluation of a prototype system designed to assist nurses in assigning subject headings to nursing narratives - written in the context of documenting patient care in hospitals. Currently nurses may need to memorize several hundred subject headings from standardized nursing terminologies when structuring and assigning the right section/subject headings to their text. Our aim is to allow nurses to write in a narrative manner without having to plan and structure the text with respect to sections and subject headings, instead the system should assist with the assignment of subject headings and restructuring afterwards. We hypothesize that this could reduce the time and effort needed for nursing documentation in hospitals. A central component of the system is a text classification model based on a long short-term memory (LSTM) recurrent neural network architecture, trained on a large data set of nursing notes. A simple Web-based interface has been implemented for user interaction. To evaluate the system, three nurses write a set of artificial nursing shift notes in a fully unstructured narrative manner, without planning for or consider the use of sections and subject headings. These are then fed to the system which assigns subject headings to each sentence and then groups them into paragraphs. Manual evaluation is conducted by a group of nurses. The results show that about 70% of the sentences are assigned to correct subject headings. The nurses believe that such a system can be of great help in making nursing documentation in hospitals easier and less time consuming. Finally, various measures and approaches for improving the system are discussed.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>moen-EtAl:2018:LOUHI</bibkey>
  </paper>

  <paper id="5612">
    <title>Automatically Detecting the Position and Type of Psychiatric Evaluation Report Sections</title>
    <author><first>Deya</first><last>Banisakher</last></author>
    <author><first>Naphtali</first><last>Rishe</last></author>
    <author><first>Mark A.</first><last>Finlayson</last></author>
    <booktitle>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>101–110</pages>
    <url>http://www.aclweb.org/anthology/W18-5612</url>
    <abstract>Psychiatric evaluation reports represent a rich and still mostly-untapped</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>banisakher-rishe-finlayson:2018:LOUHI</bibkey>
  </paper>

  <paper id="5613">
    <title>Iterative development of family history annotation guidelines using a synthetic corpus of clinical text</title>
    <author><first>Taraka</first><last>Rama</last></author>
    <author><first>Pål</first><last>Brekke</last></author>
    <author><first>Øystein</first><last>Nytrø</last></author>
    <author><first>Lilja</first><last>Øvrelid</last></author>
    <booktitle>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>111–121</pages>
    <url>http://www.aclweb.org/anthology/W18-5613</url>
    <abstract>In this article, we describe the development of annotation</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rama-EtAl:2018:LOUHI</bibkey>
  </paper>

  <paper id="5614">
    <title>CAS: French Corpus with Clinical Cases</title>
    <author><first>Natalia</first><last>Grabar</last></author>
    <author><first>Vincent</first><last>Claveau</last></author>
    <author><first>Clément</first><last>Dalloux</last></author>
    <booktitle>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>122–128</pages>
    <url>http://www.aclweb.org/anthology/W18-5614</url>
    <abstract>Textual corpora are extremely important for various NLP applications as they provide information necessary for creating, setting and testing these applications and the corresponding tools. They are also crucial for designing reliable methods and reproducible results. Yet, in some areas, such as the medical area, due to confidentiality or to ethical reasons, it is complicated and even impossible to access textual data representative of those produced in these areas. We propose the CAS corpus built with clinical cases, such as they are reported in the published scientific literature in French. We describe this corpus, currently containing over 397,000 word occurrences, and the existing linguistic and semantic annotations.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>grabar-claveau-dalloux:2018:LOUHI</bibkey>
  </paper>

  <paper id="5615">
    <title>Analysis of Risk Factor Domains in Psychosis Patient Health Records</title>
    <author><first>Eben</first><last>Holderness</last></author>
    <author><first>Nicholas</first><last>Miller</last></author>
    <author><first>Kirsten</first><last>Bolton</last></author>
    <author><first>Philip</first><last>Cawkwell</last></author>
    <author><first>Marie</first><last>Meteer</last></author>
    <author><first>James</first><last>Pustejovsky</last></author>
    <author><first>Mei</first><last>Hua-Hall</last></author>
    <booktitle>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>129–138</pages>
    <url>http://www.aclweb.org/anthology/W18-5615</url>
    <abstract>Readmission after discharge from a hospital is disruptive and costly, regardless of the reason. However, it can be particularly problematic for psychiatric patients, so predicting which patients may be readmitted is critically important but also very difficult. Clinical narratives in psychiatric electronic health records (EHRs) span a wide range of topics and vocabulary; therefore, a psychiatric readmission prediction model must begin with a robust and interpretable topic extraction component. We created a data pipeline for using document vector similarity metrics to perform topic extraction on psychiatric EHR data in service of our long-term goal of creating a readmission risk classifier. We show initial results for our topic extraction model and identify additional features we will be incorporating in the future.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>holderness-EtAl:2018:LOUHI</bibkey>
  </paper>

  <paper id="5616">
    <title>Patient Risk Assessment and Warning Symptom Detection Using Deep Attention-Based Neural Networks</title>
    <author><first>Ivan</first><last>Girardi</last></author>
    <author><first>Pengfei</first><last>Ji</last></author>
    <author><first>An-phi</first><last>Nguyen</last></author>
    <author><first>Nora</first><last>Hollenstein</last></author>
    <author><first>Adam</first><last>Ivankay</last></author>
    <author><first>Lorenz</first><last>Kuhn</last></author>
    <author><first>Chiara</first><last>Marchiori</last></author>
    <author><first>Ce</first><last>Zhang</last></author>
    <booktitle>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>139–148</pages>
    <url>http://www.aclweb.org/anthology/W18-5616</url>
    <abstract>We present an operational component of a real-world patient triage system. Given a specific patient presentation, the system is able to assess the level of medical urgency and issue the most appropriate recommendation in terms of best point of care and time to treat. We use an attention-based convolutional neural network</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>girardi-EtAl:2018:LOUHI</bibkey>
  </paper>

  <paper id="5617">
    <title>Syntax-based Transfer Learning for the Task of Biomedical Relation Extraction</title>
    <author><first>Joël</first><last>Legrand</last></author>
    <author><first>Yannick</first><last>Toussaint</last></author>
    <author><first>Chedy</first><last>Raïssi</last></author>
    <author><first>Adrien</first><last>Coulet</last></author>
    <booktitle>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>149–159</pages>
    <url>http://www.aclweb.org/anthology/W18-5617</url>
    <abstract>Transfer learning (TL) proposes to enhance machine learning performance on a problem, by reusing labeled data originally designed for a related problem. In particular, domain adaptation consists, for a specific task, in reusing training data developed for the same task but a distinct domain. This is particularly relevant to the applications of deep learning in Natural Language Processing, because those usually require large annotated corpora that may not exist for the targeted domain, but exist for side domains.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>legrand-EtAl:2018:LOUHI</bibkey>
  </paper>

  <paper id="5618">
    <title>In-domain Context-aware Token Embeddings Improve Biomedical Named Entity Recognition</title>
    <author><first>Golnar</first><last>Sheikhshabbafghi</last></author>
    <author><first>Inanc</first><last>Birol</last></author>
    <author><first>Anoop</first><last>Sarkar</last></author>
    <booktitle>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>160–164</pages>
    <url>http://www.aclweb.org/anthology/W18-5618</url>
    <abstract>Rapidly expanding volume of publications in the biomedical domain makes it increasingly difficult for a timely evaluation of the latest literature. That, along with a push for automated evaluation of clinical reports, present opportunities for effective natural language processing methods. In this study we target the problem of named entity recognition, where texts are processed to annotate terms that are relevant for biomedical studies. Terms of interest in the domain include gene and protein names, and cell lines and types. Here we report on a pipeline built on Embeddings from Language Models (ELMo) and a deep learning package for natural language processing (AllenNLP). We trained context-aware token embeddings on a dataset of biomedical papers using ELMo, and incorporated these embeddings in the LSTM-CRF model used by AllenNLP for named entity recognition. We show these representations improve named entity recognition for different types of biomedical named entities. We also achieve a new state of the art in gene mention detection on the BioCreative II gene mention shared task.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>sheikhshabbafghi-birol-sarkar:2018:LOUHI</bibkey>
  </paper>

  <paper id="5619">
    <title>Self-training improves Recurrent Neural Networks performance for Temporal Relation Extraction</title>
    <author><first>Chen</first><last>Lin</last></author>
    <author><first>Timothy</first><last>Miller</last></author>
    <author><first>Dmitriy</first><last>Dligach</last></author>
    <author><first>Hadi</first><last>Amiri</last></author>
    <author><first>Steven</first><last>Bethard</last></author>
    <author><first>Guergana</first><last>Savova</last></author>
    <booktitle>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>165–176</pages>
    <url>http://www.aclweb.org/anthology/W18-5619</url>
    <abstract>Neural network models are oftentimes restricted by limited labeled instances and resort to advanced architectures and features for cutting edge performance. We propose to build a recurrent neural network with multiple semantically heterogeneous embeddings within a self-training framework. Our framework makes use of labeled, unlabeled, and social media data, operates on basic features, and is scalable and generalizable. With this method, we establish the state-of-the-art result for both in- and cross-domain for a clinical temporal relation extraction task.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>lin-EtAl:2018:LOUHI</bibkey>
  </paper>

  <paper id="5620">
    <title>Listwise temporal ordering of events in clinical notes</title>
    <author><first>Serena</first><last>Jeblee</last></author>
    <author><first>Graeme</first><last>Hirst</last></author>
    <booktitle>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>177–182</pages>
    <url>http://www.aclweb.org/anthology/W18-5620</url>
    <abstract>We present metrics for listwise temporal ordering of events in clinical notes, as well as a baseline listwise temporal ranking model that generates a timeline of events that can be used in downstream medical natural language processing tasks.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>jeblee-hirst:2018:LOUHI</bibkey>
  </paper>

  <paper id="5621">
    <title>Time Expressions in Mental Health Records for Symptom Onset Extraction</title>
    <author><first>Natalia</first><last>Viani</last></author>
    <author><first>Lucia</first><last>Yin</last></author>
    <author><first>Joyce</first><last>Kam</last></author>
    <author><first>Ayunni</first><last>Alawi</last></author>
    <author><first>André</first><last>Bittar</last></author>
    <author><first>Rina</first><last>Dutta</last></author>
    <author><first>Rashmi</first><last>Patel</last></author>
    <author><first>Robert</first><last>Stewart</last></author>
    <author><first>Sumithra</first><last>Velupillai</last></author>
    <booktitle>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>183–192</pages>
    <url>http://www.aclweb.org/anthology/W18-5621</url>
    <abstract>For psychiatric disorders such as schizophrenia, longer durations of untreated psychosis are associated with worse intervention outcomes. Data included in electronic health records (EHRs) can be useful for retrospective clinical studies, but much of this is stored as unstructured text which cannot be directly used in computation. Natural Language Processing (NLP) methods can be used to extract this data, in order to identify symptoms and treatments from mental health records, and temporally anchor the first emergence of these. We are developing an EHR corpus annotated with time expressions, clinical entities and their relations, to be used for NLP development.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>viani-EtAl:2018:LOUHI</bibkey>
  </paper>

  <paper id="5622">
    <title>Evaluation of a Sequence Tagging Tool for Biomedical Texts</title>
    <author><first>Julien</first><last>Tourille</last></author>
    <author><first>Matthieu</first><last>Doutreligne</last></author>
    <author><first>Olivier</first><last>Ferret</last></author>
    <author><first>Aurélie</first><last>Névéol</last></author>
    <author><first>Nicolas</first><last>Paris</last></author>
    <author><first>Xavier</first><last>Tannier</last></author>
    <booktitle>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>193–203</pages>
    <url>http://www.aclweb.org/anthology/W18-5622</url>
    <abstract>Many applications in biomedical natural language processing rely on sequence tagging as an initial step to perform more complex analysis. To support text analysis in the biomedical domain, we introduce Yet Another SEquence Tagger (YASET), an open-source multi purpose sequence tagger that implements state-of-the-art deep learning algorithms for sequence tagging. Herein, we evaluate YASET on part-of-speech tagging and named entity recognition in a variety of text genres including articles from the biomedical literature in English and clinical narratives in French. To further characterize performance, we report distributions over 30 runs and different sizes of training datasets. YASET provides state-of-the-art performance on the CoNLL 2003 NER dataset (F1=0.87), MEDPOST corpus (F1=0.97), MERLoT corpus (F1=0.99) and NCBI disease corpus (F1=0.81). We believe that YASET is a versatile and efficient tool that can be used for sequence tagging in biomedical and clinical texts.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>tourille-EtAl:2018:LOUHI</bibkey>
  </paper>

  <paper id="5623">
    <title>Learning to Summarize Radiology Findings</title>
    <author><first>Yuhao</first><last>Zhang</last></author>
    <author><first>Daisy Yi</first><last>Ding</last></author>
    <author><first>Tianpei</first><last>Qian</last></author>
    <author><first>Christopher D.</first><last>Manning</last></author>
    <author><first>Curtis P.</first><last>Langlotz</last></author>
    <booktitle>Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>204–213</pages>
    <url>http://www.aclweb.org/anthology/W18-5623</url>
    <abstract>The Impression section of a radiology report summarizes crucial radiology findings in natural language and plays a central role in communicating these findings to physicians. However, the process of generating impressions by summarizing findings is time-consuming for radiologists and prone to errors. We propose to automate the generation of radiology impressions with neural sequence-to-sequence learning. We further propose a customized neural model for this task which learns to encode the study background information and use this information to guide the decoding process. On a large dataset of radiology reports collected from actual hospital studies, our model outperforms existing non-neural and neural baselines under the ROUGE metrics. In a blind experiment, a board-certified radiologist indicated that 67% of sampled system summaries are at least as good as the corresponding human-written summaries, suggesting significant clinical validity. To our knowledge our work represents the first attempt in this direction.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>zhang-EtAl:2018:LOUHI</bibkey>
  </paper>

  <paper id="5700">
    <title>Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI</title>
    <editor><first>Aleksandr</first><last>Chuklin</last></editor>
    <editor><first>Jeff</first><last>Dalton</last></editor>
    <editor><first>Julia</first><last>Kiseleva</last></editor>
    <editor><first>Alexey</first><last>Borisov</last></editor>
    <editor><first>Mikhail</first><last>Burtsev</last></editor>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-57</url>
    <bibtype>book</bibtype>
    <bibkey>SCAI:2018</bibkey>
  </paper>

  <paper id="5701">
    <title>Neural Response Ranking for Social Conversation: A Data-Efficient Approach</title>
    <author><first>Igor</first><last>Shalyminov</last></author>
    <author><first>Ondřej</first><last>Dušek</last></author>
    <author><first>Oliver</first><last>Lemon</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–8</pages>
    <url>http://www.aclweb.org/anthology/W18-5701</url>
    <abstract>The overall objective of ‘social’ dialogue systems is to support engaging, entertaining, and lengthy conversations on a wide variety of topics, including social chit-chat.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>shalyminov-duek-lemon:2018:SCAI</bibkey>
  </paper>

  <paper id="5702">
    <title>Autonomous Sub-domain Modeling for Dialogue Policy with Hierarchical Deep Reinforcement Learning</title>
    <author><first>Giovanni Yoko</first><last>Kristianto</last></author>
    <author><first>Huiwen</first><last>Zhang</last></author>
    <author><first>Bin</first><last>Tong</last></author>
    <author><first>Makoto</first><last>Iwayama</last></author>
    <author><first>Yoshiyuki</first><last>Kobayashi</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>9–16</pages>
    <url>http://www.aclweb.org/anthology/W18-5702</url>
    <abstract>Solving composites tasks, which consist of several inherent sub-tasks, remains a challenge in the research area of dialogue. Current studies have tackled this issue by manually decomposing the composite tasks into several sub-domains. However, much human effort is inevitable. This paper proposes a dialogue framework that autonomously models meaningful sub-domains and learns the policy over them. Our experiments show that our framework outperforms the baseline without sub-domains by 11% in terms of success rate, and is competitive with that with manually defined sub-domains.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kristianto-EtAl:2018:SCAI</bibkey>
  </paper>

  <paper id="5703">
    <title>Building Dialogue Structure from Discourse Tree of a Question</title>
    <author><first>Boris</first><last>Galitsky</last></author>
    <author><first>Dmitry</first><last>Ilvovsky</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>17–23</pages>
    <url>http://www.aclweb.org/anthology/W18-5703</url>
    <abstract>In this section we propose a reasoning-based approach to a dialogue management for a customer support chat bot. To build a dialogue scenario, we analyze the discourse tree (DT) of an initial query of a customer support dialogue that is frequently complex and multi-sentence. We then enforce rhetorical agreement between DT of the initial query and that of the answers, requests and responses. The chat bot finds answers, which are not only relevant by topic but also suitable for a given step of a conversation and match the question by style, communication means, experience level and other domain-independent attributes. We evaluate a performance of proposed algorithm in car repair domain and observe a 5 to 10% improvement for sin-gle and three-step dialogues respectively, in comparison with baseline approaches to dialogue management.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>galitsky-ilvovsky:2018:SCAI</bibkey>
  </paper>

  <paper id="5704">
    <title>A Methodology for Evaluating Interaction Strategies of Task-Oriented Conversational Agents</title>
    <author><first>Marco</first><last>Guerini</last></author>
    <author><first>Sara</first><last>Falcone</last></author>
    <author><first>Bernardo</first><last>Magnini</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>24–32</pages>
    <url>http://www.aclweb.org/anthology/W18-5704</url>
    <abstract>In task-oriented conversational agents, more attention has been usually devoted to assessing task effectiveness (i.e. quality of service), rather than to how the task is achieved (i.e. quality of experience).</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>guerini-falcone-magnini:2018:SCAI</bibkey>
  </paper>

  <paper id="5705">
    <title>A Reinforcement Learning-driven Translation Model for Search-Oriented Conversational Systems</title>
    <author><first>Wafa</first><last>Aissa</last></author>
    <author><first>Laure</first><last>Soulier</last></author>
    <author><first>Ludovic</first><last>Denoyer</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>33–39</pages>
    <url>http://www.aclweb.org/anthology/W18-5705</url>
    <abstract>Search-oriented conversational systems rely on information needs expressed in natural language (NL). We focus here on the understanding of NL expressions for building keyword-based queries. We propose a reinforcement-learning-driven translation model framework able to 1) learn the translation from NL expressions to queries in a supervised way, and, 2) to overcome the lack of large-scale dataset by framing the translation model as a word selection approach and injecting relevance feedback as a reward in the learning process. Experiments are carried out on two TREC datasets. We outline the effectiveness of our approach in a retrieval task.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>aissa-soulier-denoyer:2018:SCAI</bibkey>
  </paper>

  <paper id="5706">
    <title>Research Challenges in Building a Voice-based Artificial Personal Shopper - Position Paper</title>
    <author><first>Nut</first><last>Limsopatham</last></author>
    <author><first>Oleg</first><last>Rokhlenko</last></author>
    <author><first>David</first><last>Carmel</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>40–45</pages>
    <url>http://www.aclweb.org/anthology/W18-5706</url>
    <abstract>Recent advances in automatic speech recognition lead toward enabling a voice conversation between a human user and an intelligent virtual assistant. This provides a potential foundation for developing artificial personal shoppers for e-commerce websites, such as Alibaba, Amazon, and eBay. Personal shoppers are valuable to the on-line shops as they enhance user engagement and trust by promptly dealing with customers’ questions and concerns. Developing an artificial personal shopper requires the agent to leverage knowledge about the customer and products, while interacting with the customer in a human-like conversation. In this position paper, we motivate and describe the artificial personal shopper task, and then address a research agenda for this task by adapting and advancing existing information retrieval and natural language processing technologies.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>limsopatham-rokhlenko-carmel:2018:SCAI</bibkey>
  </paper>

  <paper id="5707">
    <title>Curriculum Learning Based on Reward Sparseness for Deep Reinforcement Learning of Task Completion Dialogue Management</title>
    <author><first>Atsushi</first><last>Saito</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>46–51</pages>
    <url>http://www.aclweb.org/anthology/W18-5707</url>
    <abstract>Learning from sparse and delayed reward</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>saito:2018:SCAI</bibkey>
  </paper>

  <paper id="5708">
    <title>Data Augmentation for Neural Online Chats Response Selection</title>
    <author><first>Wenchao</first><last>Du</last></author>
    <author><first>Alan</first><last>Black</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>52–58</pages>
    <url>http://www.aclweb.org/anthology/W18-5708</url>
    <abstract>Data augmentation seeks to manipulate the available data for training to improve the generalization ability of models. We investigate two data augmentation proxies, permutation and flipping, for neural dialog response selection task on various models over multiple datasets, including both Chinese and English languages. Different from standard data augmentation techniques, our method combines the original and synthesized data for prediction. Empirical results show that our approach can gain 1 to 3 recall-at-1 points over baseline models in both full-scale and small-scale settings.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>du-black:2018:SCAI</bibkey>
  </paper>

  <paper id="5709">
    <title>A Knowledge-Grounded Multimodal Search-Based Conversational Agent</title>
    <author><first>Shubham</first><last>Agarwal</last></author>
    <author><first>Ondřej</first><last>Dušek</last></author>
    <author><first>Ioannis</first><last>Konstas</last></author>
    <author><first>Verena</first><last>Rieser</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>59–66</pages>
    <url>http://www.aclweb.org/anthology/W18-5709</url>
    <abstract>Multimodal search-based dialogue is a challenging new task: It extends visually</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>agarwal-EtAl:2018:SCAI</bibkey>
  </paper>

  <paper id="5710">
    <title>Embedding Individual Table Columns for Resilient SQL Chatbots</title>
    <author><first>Bojan</first><last>Petrovski</last></author>
    <author><first>Ignacio</first><last>Aguado</last></author>
    <author><first>Andreea</first><last>Hossmann</last></author>
    <author><first>Michael</first><last>Baeriswyl</last></author>
    <author><first>Claudiu</first><last>Musat</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>67–73</pages>
    <url>http://www.aclweb.org/anthology/W18-5710</url>
    <abstract>Most of the world’s data is stored in relational databases. Accessing these requires specialized knowledge of the Structured Query Language (SQL), putting them out of the reach of many people. A recent research thread in Natural Language Processing (NLP) aims to alleviate this problem, by automatically translating natural language questions into SQL queries. While the proposed solutions are a great start, they lack robustness and do not easily generalize: the methods require high quality descriptions of the database table columns, and the most widely used training dataset, WikiSQL, is heavily biased towards using those descriptions as part of the questions.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>petrovski-EtAl:2018:SCAI</bibkey>
  </paper>

  <paper id="5711">
    <title>Exploring Named Entity Recognition As an Auxiliary Task for Slot Filling in Conversational Language Understanding</title>
    <author><first>Samuel</first><last>Louvan</last></author>
    <author><first>Bernardo</first><last>Magnini</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>74–80</pages>
    <url>http://www.aclweb.org/anthology/W18-5711</url>
    <abstract>Slot filling is a crucial task in the Natural Language Understanding (NLU) component of a dialogue system. Most approaches for this task rely solely on the domain-specific datasets for training. We propose a joint model of slot filling and Named Entity Recognition (NER) in a multi-task learning (MTL) setup. Our experiments on three slot filling datasets show that using NER as an auxiliary task improves slot filling performance and achieve competitive performance compared with state-of-the-art. In particular, NER is effective when supervised at the lower layer of the model. For low-resource scenarios, we found that MTL is effective for one dataset.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>louvan-magnini:2018:SCAI</bibkey>
  </paper>

  <paper id="5712">
    <title>Why are Sequence-to-Sequence Models So Dull? Understanding the Low-Diversity Problem of Chatbots</title>
    <author><first>Shaojie</first><last>Jiang</last></author>
    <author><first>Maarten</first><last>de Rijke</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>81–86</pages>
    <url>http://www.aclweb.org/anthology/W18-5712</url>
    <abstract>Diversity is a long-studied topic in information retrieval that usually refers to the requirement that retrieved results should be non-repetitive and cover different aspects. In a conversational setting, an additional dimension of diversity matters: an engaging response generation system should be able to output responses that are diverse and interesting. Sequence-to-sequence (Seq2Seq) models have been shown to be very effective for response generation. However, dialogue responses generated by Seq2Seq models tend to have low diversity. In this paper, we review known sources and existing approaches to this low-diversity problem. We also identify a source of low diversity that has been little studied so far, namely model over-confidence. We sketch several directions for tackling model over-confidence and, hence, the low-diversity problem, including confidence penalties and label smoothing.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>jiang-derijke:2018:SCAI</bibkey>
  </paper>

  <paper id="5713">
    <title>Retrieve and Refine: Improved Sequence Generation Models For Dialogue</title>
    <author><first>Jason</first><last>Weston</last></author>
    <author><first>Emily</first><last>Dinan</last></author>
    <author><first>Alexander</first><last>Miller</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>87–92</pages>
    <url>http://www.aclweb.org/anthology/W18-5713</url>
    <abstract>Sequence generation models for dialogue are known to have several problems: they tend to produce short, generic sentences that are uninformative and unengaging. Retrieval models on the other hand can surface interesting responses, but are restricted to the given retrieval set leading to erroneous replies that cannot be tuned to the specific context. In this work we develop a model that combines the two approaches to avoid both their deficiencies: first retrieve a response and then refine it – the final sequence generator treating the retrieval as additional context. We show on the recent ConvAI2 challenge task our approach produces responses superior to both standard retrieval and generation models in human evaluations.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>weston-dinan-miller:2018:SCAI</bibkey>
  </paper>

  <paper id="5800">
    <title>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</title>
    <editor><first>Sandra</first><last>Kuebler</last></editor>
    <editor><first>Garrett</first><last>Nicolai</last></editor>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-58</url>
    <bibtype>book</bibtype>
    <bibkey>SIGMORPHON:2018</bibkey>
  </paper>

  <paper id="5801">
    <title>Efficient Computation of Implicational Universals in Constraint-Based Phonology Through the Hyperplane Separation Theorem</title>
    <author><first>Giorgio</first><last>Magri</last></author>
    <booktitle>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–10</pages>
    <url>http://www.aclweb.org/anthology/W18-5801</url>
    <abstract>This paper focuses on the most basic implicational universals in phonological theory, called T-orders after Anttila and Andrus (2006). It develops necessary and sufficient constraint characterizations of T-orders within Harmonic Grammar and Optimality Theory. These conditions rest on the rich convex geometry underlying these frameworks. They are phonologically intuitive and have significant algorithmic implications.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>magri:2018:SIGMORPHON</bibkey>
  </paper>

  <paper id="5802">
    <title>Lexical Networks in !Xung</title>
    <author><first>Syed-Amad</first><last>Hussain</last></author>
    <author><first>Micha</first><last>Elsner</last></author>
    <author><first>Amanda</first><last>Miller</last></author>
    <booktitle>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>11–20</pages>
    <url>http://www.aclweb.org/anthology/W18-5802</url>
    <abstract>We investigate the lexical network properties of the large phoneme inventory Southern African language Mangetti Dune !Xung as it compares to English and other commonly-studied languages. Lexical networks are graphs in which nodes (words) are linked to their minimal pairs; global properties of these networks are believed to mediate lexical access in the minds of speakers. We show that the network properties of !Xung are within the range found in previously-studied languages. By simulating data (“pseudolexicons”) with varying levels of phonotactic structure, we find that the lexical network properties of !Xung diverge from previously-studied languages when fewer phonotactic constraints are retained. We conclude that lexical network properties are representative of an underlying cognitive structure which is necessary for efficient word retrieval and that the phonotactics of !Xung may be shaped by a selective pressure which preserves network properties within this cognitively useful range.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>hussain-elsner-miller:2018:SIGMORPHON</bibkey>
  </paper>

  <paper id="5803">
    <title>Acoustic Word Disambiguation with Phonogical Features in Danish ASR</title>
    <author><first>Andreas Søeborg</first><last>Kirkedal</last></author>
    <booktitle>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>21–31</pages>
    <url>http://www.aclweb.org/anthology/W18-5803</url>
    <abstract>Phonological features can indicate word class and we can use word class information to disambiguate both homophones and homographs in automatic speech recognition (ASR). </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kirkedal:2018:SIGMORPHON</bibkey>
  </paper>

  <paper id="5804">
    <title>Adaptor Grammars for the Linguist: Word Segmentation Experiments for Very Low-Resource Languages</title>
    <author><first>Pierre</first><last>Godard</last></author>
    <author><first>Laurent</first><last>Besacier</last></author>
    <author><first>François</first><last>Yvon</last></author>
    <author><first>Martine</first><last>Adda-Decker</last></author>
    <author><first>Gilles</first><last>Adda</last></author>
    <author><first>Hélène</first><last>Maynard</last></author>
    <author><first>Annie</first><last>Rialland</last></author>
    <booktitle>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>32–42</pages>
    <url>http://www.aclweb.org/anthology/W18-5804</url>
    <abstract>Computational Language Documentation attempts to make the most recent research in speech and language technologies available to linguists working on language preservation and documentation. In this paper, we pursue two main goals along these lines. The first is to improve upon a strong baseline for the unsupervised word discovery task on two very low-resource Bantu languages, taking advantage of the expertise of linguists on these particular languages. The second consists in exploring the Adaptor Grammar framework as a decision and prediction tool for linguists studying a new language. We experiment 162 grammar configurations for each language and show that using Adaptor Grammars for word segmentation enables us to test hypotheses about a language. Specializing a generic grammar with language specific knowledge leads to great improvements for the word discovery task, ultimately achieving a leap of about 30% token F-score from the results of a strong baseline.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>godard-EtAl:2018:SIGMORPHON</bibkey>
  </paper>

  <paper id="5805">
    <title>String Transduction with Target Language Models and Insertion Handling</title>
    <author><first>Garrett</first><last>Nicolai</last></author>
    <author><first>Saeed</first><last>Najafi</last></author>
    <author><first>Grzegorz</first><last>Kondrak</last></author>
    <booktitle>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>43–53</pages>
    <url>http://www.aclweb.org/anthology/W18-5805</url>
    <abstract>Many character-level tasks can be framed as</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>nicolai-najafi-kondrak:2018:SIGMORPHON</bibkey>
  </paper>

  <paper id="5806">
    <title>Complementary Strategies for Low Resourced Morphological Modeling</title>
    <author><first>Alexander</first><last>Erdmann</last></author>
    <author><first>Nizar</first><last>Habash</last></author>
    <booktitle>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>54–65</pages>
    <url>http://www.aclweb.org/anthology/W18-5806</url>
    <abstract>Morphologically rich languages are challenging for natural language processing tasks due to data sparsity. This can be addressed either by introducing out-of-context morphological knowledge, or by developing machine learning architectures that specifically target data sparsity and/or morphological information. We find these approaches to complement each other in a morphological paradigm modeling task in Modern Standard Arabic, which, in addition to being morphologically complex, features ubiquitous ambiguity, exacerbating sparsity with noise. Given a small number of out-of-context rules describing closed class morphology, we combine them with word embeddings leveraging subword strings and noise reduction techniques. The combination outperforms both approaches individually by about 20% absolute. While morphological resources already exist for Modern Standard Arabic, our results inform how comparable resources might be constructed for non-standard dialects or any morphologically rich, low resourced language, given scarcity of time and</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>erdmann-habash:2018:SIGMORPHON</bibkey>
  </paper>

  <paper id="5807">
    <title>Modeling Reduplication with 2-way Finite-State Transducers</title>
    <author><first>Hossep</first><last>Dolatian</last></author>
    <author><first>Jeffrey</first><last>Heinz</last></author>
    <booktitle>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>66–77</pages>
    <url>http://www.aclweb.org/anthology/W18-5807</url>
    <abstract>This article describes a novel approach to the computational modeling of reduplication. Reduplication is a well-studied linguistic phenomenon. However, it is often treated as a stumbling block within finite-state treatments of morphology. Most finite-state implementations of computational morphology cannot adequately capture the productivity of unbounded copying in reduplication, nor can they adequately capture bounded copying. We show that an understudied type of finite-state machines, two-way finite-state transducers (2-way FSTs), captures virtually all reduplicative processes, including total reduplication. 2-way FSTs can model reduplicative typology in a way which is convenient, easy to design and debug in practice, and linguistically-motivated. By virtue of being finite-state, 2-way FSTs are likewise incorporable into existing finite-state systems and programs. A small but representative typology of reduplicative processes is described in this article, alongside their corresponding 2-way FST models.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>dolatian-heinz:2018:SIGMORPHON</bibkey>
  </paper>

  <paper id="5808">
    <title>Automatically Tailoring Unsupervised Morphological Segmentation to the Language</title>
    <author><first>Ramy</first><last>Eskander</last></author>
    <author><first>Owen</first><last>Rambow</last></author>
    <author><first>Smaranda</first><last>Muresan</last></author>
    <booktitle>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>78–83</pages>
    <url>http://www.aclweb.org/anthology/W18-5808</url>
    <abstract>Morphological segmentation is beneficial for several natural language processing tasks dealing with large vocabularies. Unsupervised methods for morphological segmentation are essential for handling a diverse set of languages, including low-resource languages. Eskander et al. (2016) introduced a Language Independent Morphological Segmenter (LIMS) using Adaptor Grammars (AG) based on the best-on-average performing AG configuration. However, while LIMS worked best on average and outperforms other state-of-the-art unsupervised morphological segmentation approaches, it did not provide the optimal AG configuration for five out of the six languages. We propose two language-independent classifiers that enable the selection of the optimal or nearly-optimal configuration for the morphological segmentation of unseen languages.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>eskander-rambow-muresan:2018:SIGMORPHON</bibkey>
  </paper>

  <paper id="5809">
    <title>A Comparison of Entity Matching Methods between English and Japanese Katakana</title>
    <author><first>Michiharu</first><last>Yamashita</last></author>
    <author><first>Hideki</first><last>Awashima</last></author>
    <author><first>Hidekazu</first><last>Oiwa</last></author>
    <booktitle>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>84–92</pages>
    <url>http://www.aclweb.org/anthology/W18-5809</url>
    <abstract>Japanese Katakana is one component of the Japanese writing system and is used to express English terms, loanwords, and onomatopoeia in Japanese characters based on the phonemes. The main purpose of this research is to find the best entity matching methods between English and Katakana. We built two research questions to clarify which types of entity matching systems works better than others. The first question is what transliteration should be used for conversion. We need to transliterate English or Katakana terms into the same form in order to compute the string similarity. We consider five conversions that transliterate English to Katakana directly, Katakana to English directly, English to Katakana via phoneme, Katakana to English via phoneme, and both English and Katakana to phoneme. The second question is what should be used for the similarity measure at entity matching. To investigate the problem, we choose six methods, which are Overlap Coefficient, Cosine, Jaccard, Jaro-Winkler, Levenshtein, and the similarity of the phoneme probability predicted by RNN. Our results show that 1) matching using phonemes and conversion of Katakana to English works better than other methods, and 2) the similarity of phonemes outperforms other methods while other similarity score is changed depending on data and models.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>yamashita-awashima-oiwa:2018:SIGMORPHON</bibkey>
  </paper>

  <paper id="5810">
    <title>Seq2Seq Models with Dropout can Learn Generalizable Reduplication</title>
    <author><first>Brandon</first><last>Prickett</last></author>
    <author><first>Aaron</first><last>Traylor</last></author>
    <author><first>Joe</first><last>Pater</last></author>
    <booktitle>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>93–100</pages>
    <url>http://www.aclweb.org/anthology/W18-5810</url>
    <abstract>Natural language reduplication can pose a challenge to neural models of language, and has been argued to require variables (Marcus et al., 1999). Sequence-to-sequence neural networks have been shown to perform well at a number of other morphological tasks (Cotterell et al., 2016), and produce results that highly correlate with human behavior (Kirov, 2017; Kirov &amp; Cotterell, 2018) but do not include any explicit variables in their architecture. We find that they can learn a reduplicative pattern that generalizes to novel segments if they are trained with dropout (Srivastava et al., 2014). We argue that this matches the scope of generalization observed in human reduplication.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>prickett-traylor-pater:2018:SIGMORPHON</bibkey>
  </paper>

  <paper id="5811">
    <title>A Characterwise Windowed Approach to Hebrew Morphological Segmentation</title>
    <author><first>Amir</first><last>Zeldes</last></author>
    <booktitle>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>101–110</pages>
    <url>http://www.aclweb.org/anthology/W18-5811</url>
    <abstract>This paper presents a novel approach to the segmentation of orthographic word forms in contemporary Hebrew, focusing purely on splitting without carrying out morphological analysis or disambiguation. Casting the analysis task as character-wise binary classification and using adjacent character and word-based lexicon-lookup features, this approach achieves over 98% accuracy on the benchmark SPMRL shared task data for Hebrew, and 97% accuracy on a new out of domain Wikipedia dataset, an improvement of ~4% and 5% over previous state of the art performance.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>zeldes:2018:SIGMORPHON</bibkey>
  </paper>

  <paper id="5812">
    <title>Phonetic Vector Representations for Sound Sequence Alignment</title>
    <author><first>Pavel</first><last>Sofroniev</last></author>
    <author><first>Çağrı</first><last>Çöltekin</last></author>
    <booktitle>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>111–116</pages>
    <url>http://www.aclweb.org/anthology/W18-5812</url>
    <abstract>This study explores a number of data-driven vector representations of the</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>sofroniev-ltekin:2018:SIGMORPHON</bibkey>
  </paper>

  <paper id="5813">
    <title>Sounds Wilde. Phonetically Extended Embeddings for Author-Stylized Poetry Generation</title>
    <author><first>Aleksey</first><last>Tikhonov</last></author>
    <author><first>Ivan</first><last>Yamshchikov</last></author>
    <booktitle>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>117–124</pages>
    <url>http://www.aclweb.org/anthology/W18-5813</url>
    <abstract>This paper addresses author-stylized text generation. Using a version of a language model with extended phonetic and semantic embeddings for poetry generation we show that phonetics has comparable contribution to the overall model performance as the information on the target author. Phonetic information is shown to be important for English and Russian language. Humans tend to attribute machine generated texts to the target author.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>tikhonov-yamshchikov:2018:SIGMORPHON</bibkey>
  </paper>

  <paper id="5814">
    <title>On Hapax Legomena and Morphological Productivity</title>
    <author><first>Janet</first><last>Pierrehumbert</last></author>
    <author><first>Ramon</first><last>Granell</last></author>
    <booktitle>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>125–130</pages>
    <url>http://www.aclweb.org/anthology/W18-5814</url>
    <abstract>Quantifying and predicting morphological productivity is a long-standing challenge in corpus linguistics and psycholinguistics. The same challenge reappears in natural language processing in the context of handling words that were not seen in the training set (out-of-vocabulary, or OOV, words). Prior research showed that a good indicator of the productivity of a morpheme is the number of words involving it that occur exactly once (the hapax legomena). A technical connection was adduced between this result and Good-Turing smoothing, which assigns probability mass to unseen events on the basis of the simplifying assumption that word frequencies are stationary. In a large-scale study of 133 affixes in Wikipedia, we develop evidence that success in fact depends on tapping the frequency range in which the assumptions of Good-Turing are violated.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>pierrehumbert-granell:2018:SIGMORPHON</bibkey>
  </paper>

  <paper id="5815">
    <title>A Morphological Analyzer for Shipibo-Konibo</title>
    <author><first>Ronald</first><last>Cardenas</last></author>
    <author><first>Daniel</first><last>Zeman</last></author>
    <booktitle>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>131–139</pages>
    <url>http://www.aclweb.org/anthology/W18-5815</url>
    <abstract>We present a morphological analyzer for Shipibo-Konibo, a low-resourced native language spoken in the Amazonian region of Peru.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>cardenas-zeman:2018:SIGMORPHON</bibkey>
  </paper>

  <paper id="5816">
    <title>An Arabic Morphological Analyzer and Generator with Copious Features</title>
    <author><first>Dima</first><last>Taji</last></author>
    <author><first>Salam</first><last>Khalifa</last></author>
    <author><first>Ossama</first><last>Obeid</last></author>
    <author><first>Fadhl</first><last>Eryani</last></author>
    <author><first>Nizar</first><last>Habash</last></author>
    <booktitle>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>140–150</pages>
    <url>http://www.aclweb.org/anthology/W18-5816</url>
    <abstract>We introduce CALIMA-Star, a very rich Arabic morphological analyzer and generator that provides functional and form-based morphological features as well as built-in tokenization, phonological representation, lexical rationality and much more. This tool includes a fast engine that can be easily integrated into other systems, as well as an easy-to-use API and a web interface. CALIMA-Star also supports morphological reinflection. We evaluate CALIMA-Star against four commonly used analyzers for Arabic in terms of speed and morphological content.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>taji-EtAl:2018:SIGMORPHON</bibkey>
  </paper>

  <paper id="5817">
    <title>Sanskrit n-Retroflexion is Input-Output Tier-Based Strictly Local</title>
    <author><first>Thomas</first><last>Graf</last></author>
    <author><first>Connor</first><last>Mayer</last></author>
    <booktitle>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>151–160</pages>
    <url>http://www.aclweb.org/anthology/W18-5817</url>
    <abstract>Sanskrit /n/-retroflexion (nati) is one of the most complex segmental processes in phonology. While it is still star-free, it does not fit in any of the subregular classes that are commonly entertained in the literature. We show that when construed as a phonotactic dependency, the process fits into a class we call input-output tier-based strictly local (IO-TSL), a natural extension of the familiar class TSL. IO-TSL increases the power of TSL’s tier projection function by making it an input-output strictly local transduction. Assuming that /n/-retroflexion represents the upper bound on the complexity of segmental phonology, this shows that all of segmental phonology can be captured by combining the intuitive notion of tiers with the independently motivated machinery of strictly local mappings.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>graf-mayer:2018:SIGMORPHON</bibkey>
  </paper>

  <paper id="5818">
    <title>Phonological Features for Morphological Inflection</title>
    <author><first>Adam</first><last>Wiemerslage</last></author>
    <author><first>Miikka</first><last>Silfverberg</last></author>
    <author><first>Mans</first><last>Hulden</last></author>
    <booktitle>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>161–166</pages>
    <url>http://www.aclweb.org/anthology/W18-5818</url>
    <abstract>Modeling morphological inflection is an important task in Natural Language Processing. In contrast to earlier work that has largely used orthographic representations, we experiment with this task in a phonetic character space, representing inputs as either IPA segments or bundles of phonological distinctive features. We show that both of these inputs, somewhat counterintuitively, achieve similar accuracies on morphological inflection, slightly lower than orthographic models. We conclude that providing detailed phonological representations is largely redundant when compared to IPA segments, and that articulatory distinctions relevant for word inflection are already latently present in the distributional properties of many graphemic writing systems.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>wiemerslage-silfverberg-hulden:2018:SIGMORPHON</bibkey>
  </paper>

  <paper id="5819">
    <title>Extracting Morphophonology from Small Corpora</title>
    <author><first>Marina</first><last>Ermolaeva</last></author>
    <booktitle>Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>167–175</pages>
    <url>http://www.aclweb.org/anthology/W18-5819</url>
    <abstract>Probabilistic approaches have proven themselves well in learning phonological structure. In contrast, theoretical linguistics usually works with deterministic generalizations. The goal of this paper is to explore possible interactions between information-theoretic methods and deterministic linguistic knowledge and to examine some ways in which both can be used in tandem to extract phonological and morphophonological patterns from a small annotated dataset. Local and nonlocal processes in Mishar Tatar (Turkic/Kipchak) are examined as a case study.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ermolaeva:2018:SIGMORPHON</bibkey>
  </paper>

  <paper id="5900">
    <title>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop &amp; Shared Task</title>
    <editor><first>Graciela</first><last>Gonzalez-Hernandez</last></editor>
    <editor><first>Davy</first><last>Weissenbacher</last></editor>
    <editor><first>Abeed</first><last>Sarker</last></editor>
    <editor><first>Michael</first><last>Paul</last></editor>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-5900</url>
    <bibtype>book</bibtype>
    <bibkey>SMM4H:2018</bibkey>
  </paper>

  <paper id="5901">
    <title>Football and Beer - a Social Media Analysis on Twitter in Context of the FIFA Football World Cup 2018</title>
    <author><first>Roland</first><last>Roller</last></author>
    <author><first>Philippe</first><last>Thomas</last></author>
    <author><first>Sven</first><last>Schmeier</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop and Shared Task</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–4</pages>
    <url>http://www.aclweb.org/anthology/W18-5901</url>
    <abstract>In many societies alcohol is a legal and common recreational substance and socially accepted. Alcohol consumption often comes along with social events as it helps people to increase their sociability and to overcome their inhibitions. On the other hand we know that increased alcohol consumption can lead to serious health issues, such as cancer, cardiovascular diseases and diseases of the digestive system, to mention a few. This work examines alcohol consumption during the FIFA Football World Cup 2018, particularly the usage of alcohol related information on Twitter. For this we analyse the tweeting behaviour and show that the tournament strongly increases the interest in beer. Furthermore we show that countries who had to leave the tournament at early stage might have done something good to their fans as the interest in beer decreased again.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>roller-thomas-schmeier:2018:SMM4H</bibkey>
  </paper>

  <paper id="5902">
    <title>Stance-Taking in Topics Extracted from Vaccine-Related Tweets and Discussion Forum Posts</title>
    <author><first>Maria</first><last>Skeppstedt</last></author>
    <author><first>Manfred</first><last>Stede</last></author>
    <author><first>Andreas</first><last>Kerren</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop and Shared Task</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>5–8</pages>
    <url>http://www.aclweb.org/anthology/W18-5902</url>
    <abstract>The occurrence of stance-taking towards vaccination was measured in documents extracted by topic modelling from two different corpora, one discussion forum corpus and one tweet corpus. For some of the topics extracted, their most closely associated documents contained a proportion of vaccine stance-taking texts that exceeded the corpus average by a large margin. These extracted document sets would, therefore, form a useful resource in a process for computer-assisted analysis of argumentation on the subject of vaccination.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>skeppstedt-stede-kerren:2018:SMM4H</bibkey>
  </paper>

  <paper id="5903">
    <title>Identifying Depression on Reddit: The Effect of Training Data</title>
    <author><first>Inna</first><last>Pirina</last></author>
    <author><first>Çağrı</first><last>Çöltekin</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop and Shared Task</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>9–12</pages>
    <url>http://www.aclweb.org/anthology/W18-5903</url>
    <abstract>This paper presents a set of classification experiments for identifying depression in posts gathered from social media platforms. In addition to the data gathered previously by other researchers, we collect additional data from the social media platform Reddit. Our experiments show promising results for identifying depression from social media texts. More importantly, however, we show that the choice of corpora is crucial in identifying depression and can lead to misleading conclusions in case of poor choice of data.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>pirina-ltekin:2018:SMM4H</bibkey>
  </paper>

  <paper id="5904">
    <title>Overview of the Third Social Media Mining for Health (SMM4H) Shared Tasks at EMNLP 2018</title>
    <author><first>Davy</first><last>Weissenbacher</last></author>
    <author><first>Abeed</first><last>Sarker</last></author>
    <author><first>Michael J.</first><last>Paul</last></author>
    <author><first>Graciela</first><last>Gonzalez-Hernandez</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop and Shared Task</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>13–16</pages>
    <url>http://www.aclweb.org/anthology/W18-5904</url>
    <abstract>The goals of the SMM4H shared tasks are to release annotated social media based health related datasets to the research community, and to compare the performances of natural language processing and machine learning systems on tasks involving these datasets. The third execution of the SMM4H shared tasks, co-hosted with EMNLP-2018, comprised of four subtasks. These subtasks involve annotated user posts from Twitter (tweets) and focus on the (i) automatic classification of tweets mentioning a drug name, (ii) automatic classification of tweets containing reports of first-person medication intake, (iii) automatic classification of tweets presenting self-reports of adverse drug reaction (ADR) detection, and (iv) automatic classification of vaccine behavior mentions in tweets. A total of 14 teams participated and 78 system runs were submitted (23 for task 1, 20 for task 2, 18 for task 3, 17 for task 4).</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>weissenbacher-EtAl:2018:SMM4H</bibkey>
  </paper>

  <paper id="5905">
    <title>Changes in Psycholinguistic Attributes of Social Media Users Before, During, and After Self-Reported Influenza Symptoms</title>
    <author><first>Lucie</first><last>Flekova</last></author>
    <author><first>Vasileios</first><last>Lampos</last></author>
    <author><first>Ingemar</first><last>Cox</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop and Shared Task</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>17–21</pages>
    <url>http://www.aclweb.org/anthology/W18-5905</url>
    <abstract>Previous research has linked psychological and social variables to physical health. At the same time, psychological and social variables have been successfully predicted from the language used by individuals in social media. In this paper, we conduct an initial exploratory study linking these two areas. Using the social media platform of Twitter, we identify users self-reporting symptoms that are descriptive of influenza-like illness (ILI). We analyze the tweets of those users in the periods before, during, and after the reported symptoms, exploring emotional, cognitive, and structural components of language. We observe a post-ILI increase in social activity and cognitive processes, possibly supporting previous offline findings linking more active social activities and stronger cognitive coping skills to a better immune status.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>flekova-lampos-cox:2018:SMM4H</bibkey>
  </paper>

  <paper id="5906">
    <title>Thumbs Up and Down: Sentiment Analysis of Medical Online Forums</title>
    <author><first>Victoria</first><last>Bobicev</last></author>
    <author><first>Marina</first><last>Sokolova</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop and Shared Task</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>22–26</pages>
    <url>http://www.aclweb.org/anthology/W18-5906</url>
    <abstract>In the current study, we apply multi-class and multi-label sentence classification to sentiment analysis of online medical forums. We aim to identify major health issues discussed in online social media and the types of sentiments those issues evoke. We use ontology of personal health information for Information Extraction and apply Machine Learning methods in automated recognition of the expressed sentiments.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>bobicev-sokolova:2018:SMM4H</bibkey>
  </paper>

  <paper id="5907">
    <title>Identification of Emergency Blood Donation Request on Twitter</title>
    <author><first>Puneet</first><last>Mathur</last></author>
    <author><first>Meghna</first><last>Ayyar</last></author>
    <author><first>Sahil</first><last>Chopra</last></author>
    <author><first>Simra</first><last>Shahid</last></author>
    <author><first>Laiba</first><last>Mehnaz</last></author>
    <author><first>Rajiv</first><last>Shah</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop and Shared Task</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>27–31</pages>
    <url>http://www.aclweb.org/anthology/W18-5907</url>
    <abstract>Social media-based text mining in healthcare has received special attention in recent times due to the enhanced accessibility of social media sites like Twitter. The increasing trend of spreading important information in distress can help patients reach out to prospective blood donors in a time bound manner. However such manual efforts are mostly inefficient due to the limited network of a user. In a novel step to solve this problem, to classify tweets referring to the necessity of urgent blood donation requirement. Additionally, we also present an automated feature-based SVM classification technique that can help selective EBDR tweets reach relevant personals as well as medical authorities. Our experiments also present a quantitative evidence that linguistic along with handcrafted heuristics can act as the most representative set of signals this task with an accuracy of 97.89%.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mathur-EtAl:2018:SMM4H</bibkey>
  </paper>

  <paper id="5908">
    <title>Dealing with Medication Non-Adherence Expressions in Twitter</title>
    <author><first>Takeshi</first><last>Onishi</last></author>
    <author><first>Davy</first><last>Weissenbacher</last></author>
    <author><first>Ari</first><last>Klein</last></author>
    <author><first>Karen</first><last>O’Connor</last></author>
    <author><first>Graciela</first><last>Gonzalez-Hernandez</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop and Shared Task</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>32–33</pages>
    <url>http://www.aclweb.org/anthology/W18-5908</url>
    <abstract>Through a semi-automatic analysis of tweets, we show that Twitter users not only express Medication Non-Adherence (MNA) in social media but also their reasons for not complying; further research is necessary to fully extract automatically and analyze this information, in order to facilitate the use of this data in epidemiological studies.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>onishi-EtAl:2018:SMM4H</bibkey>
  </paper>

  <paper id="5909">
    <title>Detecting Tweets Mentioning Drug Name and Adverse Drug Reaction with Hierarchical Tweet Representation and Multi-Head Self-Attention</title>
    <author><first>Chuhan</first><last>Wu</last></author>
    <author><first>Fangzhao</first><last>Wu</last></author>
    <author><first>Junxin</first><last>Liu</last></author>
    <author><first>Sixing</first><last>Wu</last></author>
    <author><first>Yongfeng</first><last>Huang</last></author>
    <author><first>Xing</first><last>Xie</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop and Shared Task</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>34–37</pages>
    <url>http://www.aclweb.org/anthology/W18-5909</url>
    <abstract>This paper describes our system for the first and third shared tasks of the third Social Media Mining for Health Applications (SMM4H) workshop, which aims to detect the tweets mentioning drug names and adverse drug reactions.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>wu-EtAl:2018:SMM4H</bibkey>
  </paper>

  <paper id="5910">
    <title>Classification of Medication-Related Tweets Using Stacked Bidirectional LSTMs with Context-Aware Attention</title>
    <author><first>Orest</first><last>Xherija</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop and Shared Task</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>38–42</pages>
    <url>http://www.aclweb.org/anthology/W18-5910</url>
    <abstract>This paper describes the system that team UChicagoCompLx developed for the 2018 Social Media Mining for Health Applications (SMM4H) Shared Task. We use a variant of the Message-level Sentiment Analysis (MSA) model of Baziotis et al. (2017), a word-level stacked bidirectional Long Short-Term Memory (LSTM) network equipped with attention, to classify medication-related tweets in the four subtasks of the SMM4H Shared Task. Without any subtask-specific tuning, the model is able to achieve competitive results across all subtasks. We make the datasets, model weights, and code publicly available.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>xherija:2018:SMM4H</bibkey>
  </paper>

  <paper id="5911">
    <title>Shot Or Not: Comparison of NLP Approaches for Vaccination Behaviour Detection</title>
    <author><first>Aditya</first><last>Joshi</last></author>
    <author><first>Xiang</first><last>Dai</last></author>
    <author><first>Sarvnaz</first><last>Karimi</last></author>
    <author><first>Ross</first><last>Sparks</last></author>
    <author><first>Cecile</first><last>Paris</last></author>
    <author><first>C Raina</first><last>MacIntyre</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop and Shared Task</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>43–47</pages>
    <url>http://www.aclweb.org/anthology/W18-5911</url>
    <abstract>Vaccination behaviour detection deals with predicting whether or not a person received/was about to receive a vaccine. We present our submission for vaccination behaviour detection shared task at the SMM4H workshop. Our findings are based on three prevalent text classification approaches: rule-based, statistical and deep learning-based. Our final submissions are: (1) an ensemble of statistical classifiers with task-specific features derived using lexicons, language processing tools and word embeddings; and, (2) a LSTM classifier with pre-trained language models.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>joshi-EtAl:2018:SMM4H</bibkey>
  </paper>

  <paper id="5912">
    <title>Neural DrugNet</title>
    <author><first>Nishant</first><last>Nikhil</last></author>
    <author><first>Shivansh</first><last>Mundra</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop and Shared Task</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>48–49</pages>
    <url>http://www.aclweb.org/anthology/W18-5912</url>
    <abstract>In this paper, we describe the system submitted for the shared task on Social Media Mining for Health Applications by the team Light. Previous works demonstrate that LSTMs have achieved remarkable performance in natural language processing tasks. We deploy an ensemble of two LSTM models. The first one is a pretrained language model appended with a classifier and takes words as input, while the second one is a LSTM model with an attention unit over it which takes character tri-gram as input. We call the ensemble of these two models: Neural-DrugNet. Our system ranks 2nd in the second shared task: Automatic classification of posts describing medication intake.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>nikhil-mundra:2018:SMM4H</bibkey>
  </paper>

  <paper id="5913">
    <title>IRISA at SMM4H 2018: Neural Network and Bagging for Tweet Classification</title>
    <author><first>Anne-Lyse</first><last>Minard</last></author>
    <author><first>Christian</first><last>Raymond</last></author>
    <author><first>Vincent</first><last>Claveau</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop and Shared Task</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>50–51</pages>
    <url>http://www.aclweb.org/anthology/W18-5913</url>
    <abstract>This paper describes the systems developed by IRISA to participate to the four tasks of the SMM4H 2018 challenge. For these tweet classification tasks, we adopt a common approach based on recurrent neural networks (BiLSTM). Our main contributions are the use of certain features, the use of Bagging in order to deal with unbalanced datasets, and on the automatic selection of difficult examples.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>minard-raymond-claveau:2018:SMM4H</bibkey>
  </paper>

  <paper id="5914">
    <title>Drug-Use Identification from Tweets with Word and Character N-Grams</title>
    <author><first>Çağrı</first><last>Çöltekin</last></author>
    <author><first>Taraka</first><last>Rama</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop and Shared Task</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>52–53</pages>
    <url>http://www.aclweb.org/anthology/W18-5914</url>
    <abstract>This paper describes our systems in social media mining for health applications (SMM4H) shared task. We participated in all four tracks of the shared task using linear models with a combination of character and word n-gram features. We did not use any external data or domain specific information. The resulting systems achieved above-average scores among other participating systems, with F1-scores of</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ltekin-rama:2018:SMM4H</bibkey>
  </paper>

  <paper id="5915">
    <title>Automatic Identification of Drugs and Adverse Drug Reaction Related Tweets</title>
    <author><first>Segun Taofeek</first><last>Aroyehun</last></author>
    <author><first>Alexander</first><last>Gelbukh</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop and Shared Task</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>54–55</pages>
    <url>http://www.aclweb.org/anthology/W18-5915</url>
    <abstract>We describe our submissions to the Third Social Media Mining for Health Applications Shared Task. We participated in two tasks (tasks 1 and 3). For both tasks, we experimented with a traditional machine learning model (Naive Bayes Support Vector Machine (NBSVM)), deep learning models (Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM), and Bidirectional LSTM (BiLSTM)), and the combination of deep learning model with SVM. We observed that the NBSVM reaches superior performance on both tasks on our development split of the training data sets. Official result for task 1 based on the blind evaluation data shows that the predictions of the NBSVM achieved our team’s best F-score of 0.910 which is above the average score received by all submissions to the task. On task 3, the cobination of of BiLSTM and SVM gives our best F-score for the positive class of 0.394.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>aroyehun-gelbukh:2018:SMM4H</bibkey>
  </paper>

  <paper id="5916">
    <title>UZH@SMM4H: System Descriptions</title>
    <author><first>Tilia</first><last>Ellendorff</last></author>
    <author><first>Joseph</first><last>Cornelius</last></author>
    <author><first>Heath</first><last>Gordon</last></author>
    <author><first>Nicola</first><last>Colic</last></author>
    <author><first>Fabio</first><last>Rinaldi</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop and Shared Task</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>56–60</pages>
    <url>http://www.aclweb.org/anthology/W18-5916</url>
    <abstract>Our team at the University of Zürich participated in the first 3 of the 4 sub-tasks at the Social Media Mining for Health Applications (SMM4H) shared task. We experimented with different approaches for text classification, namely traditional feature-based classifiers (Logistic Regression and Support Vector</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ellendorff-EtAl:2018:SMM4H</bibkey>
  </paper>

  <paper id="5917">
    <title>Deep Learning for Social Media Health Text Classification</title>
    <author><first>Santosh</first><last>Tokala</last></author>
    <author><first>Vaibhav</first><last>Gambhir</last></author>
    <author><first>Animesh</first><last>Mukherjee</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop and Shared Task</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>61–64</pages>
    <url>http://www.aclweb.org/anthology/W18-5917</url>
    <abstract>This paper describes the systems developed for 1st and 2nd tasks of the 3rd Social Media Mining for Health Applications Shared Task at EMNLP 2018. The first task focuses on automatic detection of posts mentioning a drug name or dietary supplement, a binary classification. The second task is about distinguishing the tweets that present personal medication intake, possible medication intake</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>tokala-gambhir-mukherjee:2018:SMM4H</bibkey>
  </paper>

  <paper id="5918">
    <title>Using PPM for Health Related Text Detection</title>
    <author><first>Victoria</first><last>Bobicev</last></author>
    <author><first>Victoria</first><last>Lazu</last></author>
    <author><first>Daniela</first><last>Istrati</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop and Shared Task</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>65–66</pages>
    <url>http://www.aclweb.org/anthology/W18-5918</url>
    <abstract>This paper describes the participation of the LILU team in SMM4H challenge on social media mining for health related events description such as drug intakes or vaccinations.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>bobicev-lazu-istrati:2018:SMM4H</bibkey>
  </paper>

  <paper id="5919">
    <title>Leveraging Web Based Evidence Gathering for Drug Information Identification from Tweets</title>
    <author><first>Rupsa</first><last>Saha</last></author>
    <author><first>Abir</first><last>Naskar</last></author>
    <author><first>Tirthankar</first><last>Dasgupta</last></author>
    <author><first>Lipika</first><last>Dey</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop and Shared Task</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>67–69</pages>
    <url>http://www.aclweb.org/anthology/W18-5919</url>
    <abstract>In this paper, we have explored web-based evidence gathering and different linguistic features to automatically extract drug names from tweets and further classify such tweets into Adverse Drug Events or not. We have evaluated our proposed models with the dataset as released by the SMM4H workshop shared Task-1 and Task-3 respectively. Our evaluation results shows that the proposed model achieved good results, with Precision, Recall and F-scores of 78.5%, 88% and 82.9% respectively for Task1 and 33.2%, 54.7% and</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>saha-EtAl:2018:SMM4H</bibkey>
  </paper>

  <paper id="5920">
    <title>CLaC at SMM4H Task 1, 2, and 4</title>
    <author><first>Parsa</first><last>Bagherzadeh</last></author>
    <author><first>Nadia</first><last>Sheikh</last></author>
    <author><first>Sabine</first><last>Bergler</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop and Shared Task</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>70–73</pages>
    <url>http://www.aclweb.org/anthology/W18-5920</url>
    <abstract>CLaC Labs participated in Tasks 1, 2, and 4 using the same base architecture for all tasks with various parameter variations. This was our first exploration of this data and the SMM4H Tasks, thus a unified system was useful to compare the behavior of our architecture over the different datasets and how they interact with different linguistic features.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>bagherzadeh-sheikh-bergler:2018:SMM4H</bibkey>
  </paper>

  <paper id="6000">
    <title>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</title>
    <editor><first>Marie-Catherine</first><last>de Marneffe</last></editor>
    <editor><first>Teresa</first><last>Lynn</last></editor>
    <editor><first>Sebastian</first><last>Schuster</last></editor>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-60</url>
    <bibtype>book</bibtype>
    <bibkey>UDW2018:2018</bibkey>
  </paper>

  <paper id="6001">
    <title>Assessing the Impact of Incremental Error Detection and Correction. A Case Study on the Italian Universal Dependency Treebank</title>
    <author><first>Chiara</first><last>Alzetta</last></author>
    <author><first>Felice</first><last>Dell’Orletta</last></author>
    <author><first>Simonetta</first><last>Montemagni</last></author>
    <author><first>Maria</first><last>Simi</last></author>
    <author><first>Giulia</first><last>Venturi</last></author>
    <booktitle>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–7</pages>
    <url>http://www.aclweb.org/anthology/W18-6001</url>
    <abstract>Detection and correction of errors and inconsistencies in “gold treebanks” are becoming more and more central topics of corpus annotation. The paper illustrates a new incremental method for enhancing treebanks, with particular emphasis on the extension of error patterns across different textual genres and registers. Impact and role of corrections have been assessed in a dependency parsing experiment carried out with four different parsers, whose results are promising. For both evaluation datasets, the performance of parsers increases, in terms of the standard LAS and UAS measures and of a more focused measure taking into account only relations involved in error patterns, as well as at the level of individual dependencies.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>alzetta-EtAl:2018:UDW2018</bibkey>
  </paper>

  <paper id="6002">
    <title>Using Universal Dependencies in cross-linguistic complexity research</title>
    <author><first>Aleksandrs</first><last>Berdicevskis</last></author>
    <author><first>Çağrı</first><last>Çöltekin</last></author>
    <author><first>Katharina</first><last>Ehret</last></author>
    <author><first>Kilu</first><last>von Prince</last></author>
    <author><first>Daniel</first><last>Ross</last></author>
    <author><first>Bill</first><last>Thompson</last></author>
    <author><first>Chunxiao</first><last>Yan</last></author>
    <author><first>Vera</first><last>Demberg</last></author>
    <author><first>Gary</first><last>Lupyan</last></author>
    <author><first>Taraka</first><last>Rama</last></author>
    <author><first>Christian</first><last>Bentz</last></author>
    <booktitle>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>8–17</pages>
    <url>http://www.aclweb.org/anthology/W18-6002</url>
    <abstract>We evaluate corpus-based measures of linguistic complexity obtained using Universal Dependencies (UD) treebanks. We propose a method of estimating robustness of the complexity values obtained using a given measure and a given treebank. The results indicate that measures of syntactic complexity might be on average less robust than those of morphological complexity. We also estimate the validity of complexity measures by comparing the results for very similar languages and checking for unexpected differences. We show that some of those differences that arise can be diminished by using parallel treebanks and, more importantly from the practical point of view, by harmonizing the language-specific solutions in the UD annotation.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>berdicevskis-EtAl:2018:UDW2018</bibkey>
  </paper>

  <paper id="6003">
    <title>Expletives in Universal Dependency Treebanks</title>
    <author><first>Gosse</first><last>Bouma</last></author>
    <author><first>Jan</first><last>Hajic</last></author>
    <author><first>Dag</first><last>Haug</last></author>
    <author><first>Joakim</first><last>Nivre</last></author>
    <author><first>Per Erik</first><last>Solberg</last></author>
    <author><first>Lilja</first><last>Øvrelid</last></author>
    <booktitle>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>18–26</pages>
    <url>http://www.aclweb.org/anthology/W18-6003</url>
    <abstract>Although treebanks annotated according to the guidelines of Universal Dependencies (UD) now exist for many languages, the goal of annotating the same phenomena in a cross-linguistically consistent fashion is not always met.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>bouma-EtAl:2018:UDW2018</bibkey>
  </paper>

  <paper id="6004">
    <title>Challenges in Converting the Index Thomisticus Treebank into Universal Dependencies</title>
    <author><first>Flavio Massimiliano</first><last>Cecchini</last></author>
    <author><first>Marco</first><last>Passarotti</last></author>
    <author><first>Paola</first><last>Marongiu</last></author>
    <author><first>Daniel</first><last>Zeman</last></author>
    <booktitle>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>27–36</pages>
    <url>http://www.aclweb.org/anthology/W18-6004</url>
    <abstract>This paper describes the changes applied to the original process used to convert the Index Thomisticus Treebank, a corpus including texts in Medieval Latin by Thomas Aquinas, into the annotation style of Universal Dependencies. The changes are made both to harmonise the Universal Dependencies version of the Index Thomisticus Treebank with the two other available Latin treebanks and to fix errors and inconsistencies resulting from the original process. The paper details the treatment of different issues in PoS tagging, lemmatisation and assignment of dependency relations. Finally, it assesses the quality of the new conversion process by providing an evaluation against a gold standard.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>cecchini-EtAl:2018:UDW2018</bibkey>
  </paper>

  <paper id="6005">
    <title>Er ... well, it matters, right? On the role of data representations in spoken language dependency parsing</title>
    <author><first>Kaja</first><last>Dobrovoljc</last></author>
    <author><first>Matej</first><last>Martinc</last></author>
    <booktitle>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>37–46</pages>
    <url>http://www.aclweb.org/anthology/W18-6005</url>
    <abstract>Despite the significant improvement of data-driven dependency parsing systems in recent years, they still achieve a considerably lower performance in parsing spoken language data in comparison to written data. On the example of Spoken Slovenian Treebank, the first spoken data treebank using the UD annotation scheme, we investigate which speech-specific phenomena undermine parsing performance, through a series of training data and treebank modification experiments using two distinct state-of-the-art parsing systems. Our results show that segmentation is the most prominent cause of low parsing performance, both in parsing raw and pre-segmented transcriptions. In addition to shorter utterances, both parsers perform better on normalized transcriptions including basic markers of prosody and excluding disfluencies, discourse markers and fillers. On the other hand, the effects of written training data addition and speech-specific dependency representations largely depend on the parsing system selected.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>dobrovoljc-martinc:2018:UDW2018</bibkey>
  </paper>

  <paper id="6006">
    <title>Mind the Gap: Data Enrichment in Dependency Parsing of Elliptical Constructions</title>
    <author><first>Kira</first><last>Droganova</last></author>
    <author><first>Filip</first><last>Ginter</last></author>
    <author><first>Jenna</first><last>Kanerva</last></author>
    <author><first>Daniel</first><last>Zeman</last></author>
    <booktitle>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>47–54</pages>
    <url>http://www.aclweb.org/anthology/W18-6006</url>
    <abstract>In this paper, we focus on parsing rare and non-trivial constructions, in particular ellipsis. We report on several experiments in enrichment of training data for this specific construction, evaluated on five languages: Czech, English, Finnish, Russian and Slovak.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>droganova-EtAl:2018:UDW2018</bibkey>
  </paper>

  <paper id="6007">
    <title>Integration complexity and the order of cosisters</title>
    <author><first>William</first><last>Dyer</last></author>
    <booktitle>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>55–65</pages>
    <url>http://www.aclweb.org/anthology/W18-6007</url>
    <abstract>The cost of integrating dependent constituents to their heads is thought to involve the distance between dependent and head and the complexity of the integration (Gibson, 1998). The former has been convincingly addressed by Dependency Distance Minimization (DDM) (cf. Liu et al., 2017). The current study addresses the latter by proposing a novel theory of integration complexity derived from the entropy of the probability distribution of a dependent’s heads. An analysis of Universal Dependency corpora provides empirical evidence regarding the preferred order of isomorphic cosisters—sister constituents of the same syntactic form on the same side of their head—such as the adjectives in “pretty blue fish.” Integration complexity, alongside DDM, allows for a general theory of constituent order based on integration cost.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>dyer:2018:UDW2018</bibkey>
  </paper>

  <paper id="6008">
    <title>SUD or Surface-Syntactic Universal Dependencies: An annotation scheme near-isomorphic to UD</title>
    <author><first>Kim</first><last>Gerdes</last></author>
    <author><first>Bruno</first><last>Guillaume</last></author>
    <author><first>Sylvain</first><last>Kahane</last></author>
    <author><first>Guy</first><last>Perrier</last></author>
    <booktitle>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>66–74</pages>
    <url>http://www.aclweb.org/anthology/W18-6008</url>
    <abstract>This article proposes a surface-syntactic annotation scheme called SUD that is near-isomorphic to the Universal Dependencies (UD) annotation scheme while following distributional criteria for defining the dependency tree structure and the naming of the syntactic functions. Rule-based graph transformation grammars allow for a bi-directional transformation of UD into SUD. The back-and-forth transformation can also be seen as a powerful error-mining tool to assure the intra-language and inter-language coherence of the UD treebanks.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>gerdes-EtAl:2018:UDW2018</bibkey>
  </paper>

  <paper id="6009">
    <title>Coordinate Structures in Universal Dependencies for Head-final Languages</title>
    <author><first>Hiroshi</first><last>Kanayama</last></author>
    <author><first>Na-Rae</first><last>Han</last></author>
    <author><first>Masayuki</first><last>Asahara</last></author>
    <author><first>Jena D.</first><last>Hwang</last></author>
    <author><first>Yusuke</first><last>Miyao</last></author>
    <author><first>Jinho D.</first><last>Choi</last></author>
    <author><first>Yuji</first><last>Matsumoto</last></author>
    <booktitle>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>75–84</pages>
    <url>http://www.aclweb.org/anthology/W18-6009</url>
    <abstract>This paper discusses the representation of coordinate structures in the Universal Dependencies framework for two head-final languages,Japanese and Korean. UD applies a strict principle that makes the head of coordination the left-most conjunct. However, the guideline may produce syntactic trees which are difficult to accept in head-final languages. This paper describes the status in the current corpora and proposes alternative designs suitable for these languages.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kanayama-EtAl:2018:UDW2018</bibkey>
  </paper>

  <paper id="6010">
    <title>Investigating NP-Chunking with Universal Dependencies for English</title>
    <author><first>Ophélie</first><last>Lacroix</last></author>
    <booktitle>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>85–90</pages>
    <url>http://www.aclweb.org/anthology/W18-6010</url>
    <abstract>Chunking is a pre-processing task generally dedicated to improving constituency parsing. In this paper, we want to show that universal dependency (UD) parsing can also leverage the information provided by the task of chunking even though annotated chunks are not provided with universal dependency trees. In particular, we introduce the possibility of deducing noun-phrase (NP) chunks from universal dependencies, focusing on English as a first example. We then demonstrate how the task of NP-chunking can benefit PoS-tagging in a multi-task learning setting - comparing two different strategies - and how it can be used as a feature for dependency parsing in order to learn enriched models.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>lacroix:2018:UDW2018</bibkey>
  </paper>

  <paper id="6011">
    <title>Marrying Universal Dependencies and Universal Morphology</title>
    <author><first>Arya D.</first><last>McCarthy</last></author>
    <author><first>Miikka</first><last>Silfverberg</last></author>
    <author><first>Ryan</first><last>Cotterell</last></author>
    <author><first>Mans</first><last>Hulden</last></author>
    <author><first>David</first><last>Yarowsky</last></author>
    <booktitle>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>91–101</pages>
    <url>http://www.aclweb.org/anthology/W18-6011</url>
    <abstract>The Universal Dependencies (UD) and Universal Morphology (UniMorph) projects each present schemata for annotating the morphosyntactic details of language. Each project also provides corpora of annotated text in many languages—UD at the token level and UniMorph at the type level. As each corpus is built by different annotators, language-specific decisions hinder the goal of universal schemata. With compatibility of tags, each project’s annotations could be used to validate the other’s. Additionally, the availability of both type- and token-level resources would be a boon to tasks such as parsing and homograph disambiguation. To ease this interoperability, we present a deterministic mapping from Universal Dependencies v2 features into the UniMorph schema. We validate our approach by lookup in the UniMorph corpora and find a macro-average of 64.13% recall. We also note incompatibilities due to paucity of data on either side. Finally, we present a critical evaluation of the foundations, strengths, and weaknesses of the two annotation projects.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mccarthy-EtAl:2018:UDW2018</bibkey>
  </paper>

  <paper id="6012">
    <title>Enhancing Universal Dependency Treebanks: A Case Study</title>
    <author><first>Joakim</first><last>Nivre</last></author>
    <author><first>Paola</first><last>Marongiu</last></author>
    <author><first>Filip</first><last>Ginter</last></author>
    <author><first>Jenna</first><last>Kanerva</last></author>
    <author><first>Simonetta</first><last>Montemagni</last></author>
    <author><first>Sebastian</first><last>Schuster</last></author>
    <author><first>Maria</first><last>Simi</last></author>
    <booktitle>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>102–107</pages>
    <url>http://www.aclweb.org/anthology/W18-6012</url>
    <abstract>We evaluate cross-lingual techniques for adding enhanced dependencies to existing treebanks in Universal Dependencies. We apply a rule-based system for English and a data-driven system trained on Finnish to Swedish and Italian. We find that both systems are accurate enough to bootstrap enhanced dependencies in existing UD treebanks. For Italian, results are even on par with those of a language-specific system.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>nivre-EtAl:2018:UDW2018</bibkey>
  </paper>

  <paper id="6013">
    <title>Enhancing Universal Dependencies for Korean</title>
    <author><first>Youngbin</first><last>Noh</last></author>
    <author><first>Jiyoon</first><last>Han</last></author>
    <author><first>Tae Hwan</first><last>Oh</last></author>
    <author><first>Hansaem</first><last>Kim</last></author>
    <booktitle>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>108–116</pages>
    <url>http://www.aclweb.org/anthology/W18-6013</url>
    <abstract>In this paper, for the purpose of enhancing Universal Dependencies for the Korean language, we propose a modified method for mapping Korean Part-of-Speech(POS) tagset in relation to Universal Part-of-Speech (UPOS) tagset in order to enhance the Universal Dependencies for the Korean Language. Previous studies suggest that UPOS reflects several issues that influence dependency annotation by using the POS of Korean predicates, particularly the distinctiveness in using verb, adjective, and copula.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>noh-EtAl:2018:UDW2018</bibkey>
  </paper>

  <paper id="6014">
    <title>UD-Japanese BCCWJ: Universal Dependencies Annotation for the Balanced Corpus of Contemporary Written Japanese</title>
    <author><first>Mai</first><last>Omura</last></author>
    <author><first>Masayuki</first><last>Asahara</last></author>
    <booktitle>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>117–125</pages>
    <url>http://www.aclweb.org/anthology/W18-6014</url>
    <abstract>In this paper, we describe a corpus UD Japanese-BCCWJ that was created by converting the Balanced Corpus of Contemporary Written Japanese (BCCWJ), a Japanese language corpus, to adhere to the UD annotation schema. The BCCWJ already assigns dependency information at the level of the bunsetsu (a Japanese syntactic unit comparable to the phrase). We developed a program to convert the BCCWJ to UD based on this dependency structure, and this corpus is the result of completely automatic conversion using the program. UD Japanese-BCCWJ is the largest-scale UD Japanese corpus and the second-largest of all UD corpora, including 1,980 documents, 57,109 sentences, and 1,273k words across six distinct domains.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>omura-asahara:2018:UDW2018</bibkey>
  </paper>

  <paper id="6015">
    <title>The First Komi-Zyrian Universal Dependencies Treebanks</title>
    <author><first>Niko</first><last>Partanen</last></author>
    <author><first>Rogier</first><last>Blokland</last></author>
    <author><first>KyungTae</first><last>Lim</last></author>
    <author><first>Thierry</first><last>Poibeau</last></author>
    <author><first>Michael</first><last>Rießler</last></author>
    <booktitle>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>126–132</pages>
    <url>http://www.aclweb.org/anthology/W18-6015</url>
    <abstract>Two Komi-Zyrian treebanks were included in the Universal Dependencies 2.2 release. This article contextualizes the treebanks, discusses the process through which they were created, and outlines the future plans and timeline for the next improvements. Special attention is paid to the possibilities of using UD in the documentation and description of endangered languages.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>partanen-EtAl:2018:UDW2018</bibkey>
  </paper>

  <paper id="6016">
    <title>The Hebrew Universal Dependency Treebank: Past Present and Future</title>
    <author><first>Shoval</first><last>Sade</last></author>
    <author><first>Amit</first><last>Seker</last></author>
    <author><first>Reut</first><last>Tsarfaty</last></author>
    <booktitle>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>133–143</pages>
    <url>http://www.aclweb.org/anthology/W18-6016</url>
    <abstract>The Hebrew treebank (HTB), consisting of 6221 morpho-syntactically annotated newspaper sentences, has been the only resource for training and validating Hebrew statistical parsers for almost two decades now. During these decades, the HTB has gone through a trajectory of automatic and semi-automatic conversions, until arriving at its current UDv2 form. In this work we set out to manually validate the UDv2 version and, accordingly, we apply scheme changes to bring the UD HTB into the same theoretical ground as the rest of UD. Our experimental results show that improving the linguistic coherence and internal consistency of the UD HTB has indeed led to improved syntactic parsing performance. At the same time, there is more to be done at the points of intersection with other linguistic processing layers, in particular, at the interface of UD with external morphological and lexical resources.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>sade-seker-tsarfaty:2018:UDW2018</bibkey>
  </paper>

  <paper id="6017">
    <title>Multi-source synthetic treebank creation for improved cross-lingual dependency parsing</title>
    <author><first>Francis</first><last>Tyers</last></author>
    <author><first>Mariya</first><last>Sheyanova</last></author>
    <author><first>Aleksandra</first><last>Martynova</last></author>
    <author><first>Pavel</first><last>Stepachev</last></author>
    <author><first>Konstantin</first><last>Vinogorodskiy</last></author>
    <booktitle>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>144–150</pages>
    <url>http://www.aclweb.org/anthology/W18-6017</url>
    <abstract>This paper describes a method of creating synthetic treebanks for </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>tyers-EtAl:2018:UDW2018</bibkey>
  </paper>

  <paper id="6018">
    <title>Toward Universal Dependencies for Shipibo-Konibo</title>
    <author><first>Alonso</first><last>Vásquez</last></author>
    <author><first>Renzo</first><last>Ego Aguirre</last></author>
    <author><first>Candy</first><last>Angulo</last></author>
    <author><first>John</first><last>Miller</last></author>
    <author><first>Claudia</first><last>Villanueva</last></author>
    <author><first>Željko</first><last>Agić</last></author>
    <author><first>Roberto</first><last>Zariquiey</last></author>
    <author><first>Arturo</first><last>Oncevay</last></author>
    <booktitle>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>151–161</pages>
    <url>http://www.aclweb.org/anthology/W18-6018</url>
    <abstract>We present an initial version of the Universal Dependencies (UD) treebank for Shipibo-Konibo, the first South American, Amazonian, Panoan and Peruvian language with a resource built under UD. We describe the linguistic aspects of how the tagset was defined and the treebank was annotated; in addition we present our specific treatment of linguistic units called clitics. Although the treebank is still under development, it allowed us to perform a typological comparison against Spanish, the predominant language in Peru, and dependency syntax parsing experiments in both monolingual and cross-lingual approaches.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>vsquez-EtAl:2018:UDW2018</bibkey>
  </paper>

  <paper id="6019">
    <title>Transition-based Parsing with Lighter Feed-Forward Networks</title>
    <author><first>David</first><last>Vilares</last></author>
    <author><first>Carlos</first><last>Gómez-Rodríguez</last></author>
    <booktitle>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>162–172</pages>
    <url>http://www.aclweb.org/anthology/W18-6019</url>
    <abstract>We explore whether it is possible to build lighter parsers, that are statistically equivalent to their corresponding standard version, for a wide set of languages showing different structures and morphologies. As testbed, we use the Universal Dependencies and transition-based dependency parsers trained on feed-forward networks. For these, most existing research assumes de facto standard embedded features and relies on pre-computation tricks to obtain speedups. We explore how these features and their size can be reduced and whether this translates into speed-ups with a negligible impact on accuracy. The experiments show that grand-daughter features can be removed for the majority of treebanks without a significant (negative or positive) LAS difference. They also show how the size of the embeddings can be notably reduced.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>vilares-gmezrodrguez:2018:UDW2018</bibkey>
  </paper>

  <paper id="6020">
    <title>Extended and Enhanced Polish Dependency Bank in Universal Dependencies Format</title>
    <author><first>Alina</first><last>Wróblewska</last></author>
    <booktitle>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>173–182</pages>
    <url>http://www.aclweb.org/anthology/W18-6020</url>
    <abstract>The paper presents the largest Polish Dependency Bank in Universal Dependencies format – PDBUD – with 22K trees and 352K tokens. PDBUD builds on its previous version, i.e. the Polish UD treebank (PL-SZ), and contains all 8K PL-SZ trees. The PL-SZ trees are checked and possibly corrected in the current edition of PDBUD. Further 14K trees are automatically converted from a new version of Polish Dependency Bank. The PDBUD trees are expanded with the enhanced edges encoding the shared dependents and the shared governors of the coordinated conjuncts and with the semantic roles of some dependents. The conducted evaluation experiments show that PDBUD is large enough for training a high-quality graph-based dependency parser for Polish.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>wrblewska:2018:UDW2018</bibkey>
  </paper>

  <paper id="6021">
    <title>Approximate Dynamic Oracle for Dependency Parsing with Reinforcement Learning</title>
    <author><first>Xiang</first><last>Yu</last></author>
    <author><first>Ngoc Thang</first><last>Vu</last></author>
    <author><first>Jonas</first><last>Kuhn</last></author>
    <booktitle>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>183–191</pages>
    <url>http://www.aclweb.org/anthology/W18-6021</url>
    <abstract>We present a general approach with reinforcement learning (RL) to approximate dynamic oracles for transition systems where exact dynamic oracles are difficult to derive. We treat oracle parsing as a reinforcement learning problem, design the reward function inspired by the classical dynamic oracle, and use Deep Q-Learning (DQN) techniques to train the oracle with gold trees as features. The combination of a priori knowledge and data-driven methods enables an efficient dynamic oracle, which improves the parser performance over static oracles in several transition systems.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>yu-vu-kuhn:2018:UDW2018</bibkey>
  </paper>

  <paper id="6022">
    <title>The Coptic Universal Dependency Treebank</title>
    <author><first>Amir</first><last>Zeldes</last></author>
    <author><first>Mitchell</first><last>Abrams</last></author>
    <booktitle>Proceedings of the Second Workshop on Universal Dependencies (UDW 2018)</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>192–201</pages>
    <url>http://www.aclweb.org/anthology/W18-6022</url>
    <abstract>This paper presents the Coptic Universal Dependency Treebank, the first dependency treebank within the Egyptian subfamily of the Afro-Asiatic languages. We discuss the composition of the corpus, challenges in adapting the UD annotation scheme to existing conventions for annotating Coptic, and evaluate inter-annotator agreement on UD annotation for the language. Some specific constructions are taken as a starting point for discussing several more general UD annotation guidelines, in particular for appositions, ambiguous passivization, incorporation and object-doubling.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>zeldes-abrams:2018:UDW2018</bibkey>
  </paper>

  <paper id="6100">
    <title>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</title>
    <editor><first>Wei</first><last>Xu</last></editor>
    <editor><first>Alan</first><last>Ritter</last></editor>
    <editor><first>Tim</first><last>Baldwin</last></editor>
    <editor><first>Afshin</first><last>Rahimi</last></editor>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-61</url>
    <bibtype>book</bibtype>
    <bibkey>W-NUT2018:2018</bibkey>
  </paper>

  <paper id="6101">
    <title>Inducing a lexicon of sociolinguistic variables from code-mixed text</title>
    <author><first>Philippa</first><last>Shoemark</last></author>
    <author><first>James</first><last>Kirby</last></author>
    <author><first>Sharon</first><last>Goldwater</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–6</pages>
    <url>http://www.aclweb.org/anthology/W18-6101</url>
    <abstract>Sociolinguistics is often concerned with how variants of a linguistic item (e.g., nothing vs. nothin’) are used by different groups or in different situations. We introduce the task of inducing lexical variables from code-mixed text: that is, identifying equivalence pairs such as (football, fitba) along with their linguistic code (football→British, fitba→Scottish). We adapt a framework for identifying gender-biased word pairs to this new task, and present results on three different pairs of English dialects, using tweets as the code-mixed text. Our system achieves precision of over 70% for two of these three datasets, and produces useful results even without extensive parameter tuning. Our success in adapting this framework from gender to language variety suggests that it could be used to discover other types of analogous pairs as well.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>shoemark-kirby-goldwater:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6102">
    <title>Twitter Geolocation using Knowledge-Based Methods</title>
    <author><first>Taro</first><last>Miyazaki</last></author>
    <author><first>Afshin</first><last>Rahimi</last></author>
    <author><first>Trevor</first><last>Cohn</last></author>
    <author><first>Timothy</first><last>Baldwin</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>7–16</pages>
    <url>http://www.aclweb.org/anthology/W18-6102</url>
    <abstract>Geolocation of user posts on Twitter is useful for many applications, including disaster monitoring and news material gathering. However, the vast majority of tweets have no explicit geotag, motivating the need for automatic geolocation prediction methods. We propose the use of named entity linking in geolocation prediction, modelled using graph convolutional networks over a knowledge base of entity relations, which is combined with text-based models in an end-to-end deep learning framework. We show that our method improves on text-based models, and learns effective representations for named entities that do not appear in the training data.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>miyazaki-EtAl:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6103">
    <title>Geocoding Without Geotags: A Text-based Approach for reddit</title>
    <author><first>Keith</first><last>Harrigian</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>17–27</pages>
    <url>http://www.aclweb.org/anthology/W18-6103</url>
    <abstract>In this paper, we introduce the first geolocation inference approach for reddit, a social media platform where user pseudonymity has thus far made supervised demographic inference difficult to implement and validate. In particular, we design a text-based heuristic schema to generate ground truth location labels for reddit users in the absence of explicitly geotagged data. After evaluating the accuracy of our labeling procedure, we train and test several geolocation inference models across our reddit data set and three benchmark Twitter geolocation data sets. Ultimately, we show that geolocation models trained and applied on the same domain substantially outperform models attempting to transfer training data across domains, even more so on reddit where platform-specific interest-group metadata can be used to improve inferences.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>harrigian:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6104">
    <title>Assigning people to tasks identified in email: The EPA dataset for addressee tagging for detected task intent</title>
    <author><first>Revanth</first><last>Rameshkumar</last></author>
    <author><first>Peter</first><last>Bailey</last></author>
    <author><first>Abhishek</first><last>Jha</last></author>
    <author><first>Chris</first><last>Quirk</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>28–32</pages>
    <url>http://www.aclweb.org/anthology/W18-6104</url>
    <abstract>We describe the Enron People Assignment (EPA) dataset, in which tasks that are described in emails are associated with the person(s) responsible for carrying out these tasks. We identify tasks and the responsible people in the Enron email dataset. We define evaluation methods for this challenge and report scores for our model and naïve baselines. The resulting model enables a user experience operating within a commercial email service: given a person and a task, it determines if the person should be notified of the task.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rameshkumar-EtAl:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6105">
    <title>How do you correct run-on sentences it’s not as easy as it seems</title>
    <author><first>Junchao</first><last>Zheng</last></author>
    <author><first>Courtney</first><last>Napoles</last></author>
    <author><first>Joel</first><last>Tetreault</last></author>
    <author><first>Kostiantyn</first><last>Omelianchuk</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>33–38</pages>
    <url>http://www.aclweb.org/anthology/W18-6105</url>
    <abstract>Run-on sentences are common grammatical mistakes but little research has tackled this problem to date. This work introduces two machine learning models to correct run-on sentences that outperform leading methods for related tasks, punctuation restoration and whole-sentence grammatical error correction. Due to the limited annotated data for this error, we experiment with artificially generating training data from clean newswire text. Our findings suggest artificial training data is viable for this task. We discuss implications for correcting run-ons and other types of mistakes that have low coverage in error-annotated corpora.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>zheng-napoles-tetreault:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6106">
    <title>A POS Tagging Model Adapted to Learner English</title>
    <author><first>Ryo</first><last>Nagata</last></author>
    <author><first>Tomoya</first><last>Mizumoto</last></author>
    <author><first>Yuta</first><last>Kikuchi</last></author>
    <author><first>Yoshifumi</first><last>Kawasaki</last></author>
    <author><first>Kotaro</first><last>Funakoshi</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>39–48</pages>
    <url>http://www.aclweb.org/anthology/W18-6106</url>
    <abstract>There has been very limited work on the adaptation of Part-Of-Speech (POS) tagging to learner English despite the fact that POS tagging is widely used in related tasks. In this paper, we explore how we can adapt POS tagging to learner English efficiently and effectively. Based on the discussion of possible causes of POS tagging errors in learner English, we show that deep neural models are particularly suitable for this. Considering the previous findings and the discussion, we introduce the design of our model based on bidirectional Long Short-Term Memory. In addition, we describe how to adapt it to a wide variety of native languages (potentially, hundreds of them). In the evaluation section, we empirically show that it is effective for POS tagging in learner English, achieving an accuracy of 0.964, which significantly outperforms the state-of-the-art POS-tagger. We further investigate the tagging results in detail, revealing which part of the model design does or does not improve the performance.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>nagata-EtAl:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6107">
    <title>Normalization of Transliterated Words in Code-Mixed Data Using Seq2Seq Model &amp; Levenshtein Distance</title>
    <author><first>Soumil</first><last>Mandal</last></author>
    <author><first>Karthick</first><last>Nanmaran</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>49–53</pages>
    <url>http://www.aclweb.org/anthology/W18-6107</url>
    <abstract>Building tools for code-mixed data is rapidly gaining popularity in the NLP research community as such data is exponentially rising on social media. Working with code-mixed data contains several challenges, especially due to grammatical inconsistencies and spelling variations in addition to all the previous known challenges for social media scenarios. In this article, we present a novel architecture focusing on normalizing phonetic typing variations, which is commonly seen in code-mixed data. One of the main features of our architecture is that in addition to normalizing, it can also be utilized for back-transliteration and word identification in some cases. Our model achieved an accuracy of 90.27% on the test data.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mandal-nanmaran:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6108">
    <title>Robust Word Vectors: Context-Informed Embeddings for Noisy Texts</title>
    <author><first>Valentin</first><last>Malykh</last></author>
    <author><first>Varvara</first><last>Logacheva</last></author>
    <author><first>Taras</first><last>Khakhulin</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>54–63</pages>
    <url>http://www.aclweb.org/anthology/W18-6108</url>
    <abstract>We suggest a new language-independent architecture of robust word vectors (RoVe). It is designed to alleviate the issue of typos, which are common in almost any user-generated content, and hinder automatic text processing. Our model is morphologically motivated, which allows it to deal with unseen word forms in morphologically rich languages. </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>malykh-logacheva-khakhulin:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6109">
    <title>Paraphrase Detection on Noisy Subtitles in Six Languages</title>
    <author><first>Eetu</first><last>Sjöblom</last></author>
    <author><first>Mathias</first><last>Creutz</last></author>
    <author><first>Mikko</first><last>Aulamo</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>64–73</pages>
    <url>http://www.aclweb.org/anthology/W18-6109</url>
    <abstract>We perform automatic paraphrase detection on subtitle data from the Opusparcus corpus comprising six European languages: German, English, Finnish, French, Russian, and Swedish. We train two types of supervised sentence embedding models: a word-averaging (WA) model and a gated recurrent averaging network (GRAN) model. We find out that GRAN outperforms WA and is more robust to noisy training data. Better results are obtained with more and noisier data than less and cleaner data. Additionally, we experiment on other datasets, without reaching the same level of performance, because of domain mismatch between training and test data.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>sjblom-creutz-aulamo:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6110">
    <title>Distantly Supervised Attribute Detection from Reviews</title>
    <author><first>Lisheng</first><last>Fu</last></author>
    <author><first>Pablo</first><last>Barrio</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>74–78</pages>
    <url>http://www.aclweb.org/anthology/W18-6110</url>
    <abstract>This paper aims to detect specific attributes of a place (e.g., if it has a romantic atmosphere, or if it offers outdoor seating) from its user reviews via distant supervision: without direct annotation of review text, we use the crowdsourced attribute labels of a place as labels of the review text.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>fu-barrio:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6111">
    <title>Using Wikipedia Edits in Low Resource Grammatical Error Correction</title>
    <author><first>Adriane</first><last>Boyd</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>79–84</pages>
    <url>http://www.aclweb.org/anthology/W18-6111</url>
    <abstract>We develop a grammatical error correction (GEC) system for German using a small gold GEC corpus augmented with edits extracted from Wikipedia revision history. We extend the automatic error annotation tool ERRANT (Bryant et al., 2017) for German and use it to analyze both gold GEC corrections and Wikipedia edits (Grundkiewicz and Junczys-Dowmunt, 2014) in order to select as additional training data Wikipedia edits containing grammatical corrections similar to those in the gold corpus. Using a multilayer convolutional encoder-decoder neural network GEC approach (Chollampatt and Ng, 2018), we evaluate the contribution of Wikipedia edits and find that carefully selected Wikipedia edits increase performance by over 5%.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>boyd:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6112">
    <title>Empirical Evaluation of Character-Based Model on Neural Named-Entity Recognition in Indonesian Conversational Texts</title>
    <author><first>Kemal</first><last>Kurniawan</last></author>
    <author><first>Samuel</first><last>Louvan</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>85–92</pages>
    <url>http://www.aclweb.org/anthology/W18-6112</url>
    <abstract>Despite the long history of named-entity recognition (NER) task in the natural language processing community, previous work rarely studied the task on conversational texts. Such texts are challenging because they contain a lot of word variations which increase the number of out-of-vocabulary (OOV) words. The high number of OOV words poses a difficulty for word-based neural models. Meanwhile, there is plenty of evidence to the effectiveness of character-based neural models in mitigating this OOV problem. We report an empirical evaluation of neural sequence labeling models with character embedding to tackle NER task in Indonesian conversational texts. Our experiments show that (1) character models outperform word embedding-only models by up to 4 F1 points, (2) character models perform better in OOV cases with an improvement of as high as 15 F1 points, and (3) character models are robust against a very high OOV rate.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kurniawan-louvan:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6113">
    <title>Orthogonal Matching Pursuit for Text Classification</title>
    <author><first>Konstantinos</first><last>Skianis</last></author>
    <author><first>Nikolaos</first><last>Tziortziotis</last></author>
    <author><first>Michalis</first><last>Vazirgiannis</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>93–103</pages>
    <url>http://www.aclweb.org/anthology/W18-6113</url>
    <abstract>In text classification, the problem of overfitting arises due to the high dimensionality, making regularization essential.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>skianis-tziortziotis-vazirgiannis:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6114">
    <title>Training and Prediction Data Discrepancies: Challenges of Text Classification with Noisy, Historical Data</title>
    <author><first>R. Andrew</first><last>Kreek</last></author>
    <author><first>Emilia</first><last>Apostolova</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>104–109</pages>
    <url>http://www.aclweb.org/anthology/W18-6114</url>
    <abstract>Industry datasets used for text classification are rarely created for that purpose. In most cases, the data and target predictions are a by-product of accumulated historical data, typically fraught with noise, present in both the text-based document, as well as in the targeted labels. In this work, we address the question of how well performance metrics computed on noisy, historical data reflect the performance on the intended future machine learning model input. The results demonstrate the utility of dirty training datasets used to build prediction models for cleaner (and different) prediction inputs.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kreek-apostolova:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6115">
    <title>Detecting Code-Switching between Turkish-English Language Pair</title>
    <author><first>Zeynep</first><last>Yirmibeşoğlu</last></author>
    <author><first>Gülşen</first><last>Eryiğit</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>110–115</pages>
    <url>http://www.aclweb.org/anthology/W18-6115</url>
    <abstract>Code-switching (usage of different languages within a single conversation context in an alternative manner) is a highly increasing phenomenon in social media and colloquial usage which poses different challenges for natural language processing. This paper introduces the first study for the detection of Turkish-English code-switching and also a small test data collected from social media in order to smooth the way for further studies.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>yirmibeolu-eryiit:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6116">
    <title>Language Identification in Code-Mixed Data using Multichannel Neural Networks and Context Capture</title>
    <author><first>Soumil</first><last>Mandal</last></author>
    <author><first>Anil Kumar</first><last>Singh</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>116–120</pages>
    <url>http://www.aclweb.org/anthology/W18-6116</url>
    <abstract>An accurate language identification tool is an absolute necessity for building complex NLP systems to be used on code-mixed data. Lot of work has been recently done on the same, but there’s still room for improvement. Inspired from the recent advancements in neural network architectures for computer vision tasks, we have implemented multichannel neural networks combining CNN and LSTM for word level language identification of code-mixed data. Combining this with a Bi-LSTM-CRF context capture module, accuracies of 93.28% and 93.32% is achieved on out two testing sets.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mandal-singh:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6117">
    <title>Modeling Student Response Times: Towards Efficient One-on-one Tutoring Dialogues</title>
    <author><first>Luciana</first><last>Benotti</last></author>
    <author><first>Jayadev</first><last>Bhaskaran</last></author>
    <author><first>Sigtryggur</first><last>Kjartansson</last></author>
    <author><first>David</first><last>Lang</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>121–131</pages>
    <url>http://www.aclweb.org/anthology/W18-6117</url>
    <abstract>In this paper we investigate the task of modeling how long it would take a student to respond to a tutor question during a tutoring dialogue.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>benotti-EtAl:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6118">
    <title>Content Extraction and Lexical Analysis from Customer-Agent Interactions</title>
    <author><first>Sergiu</first><last>Nisioi</last></author>
    <author><first>Anca</first><last>Bucur</last></author>
    <author><first>Liviu P.</first><last>Dinu</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>132–136</pages>
    <url>http://www.aclweb.org/anthology/W18-6118</url>
    <abstract>In this paper, we provide a lexical comparative analysis of the vocabulary used by customers and agents in an </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>nisioi-bucur-dinu:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6119">
    <title>Preferred Answer Selection in Stack Overflow: Better Text Representations ... and Metadata, Metadata, Metadata</title>
    <author><first>Steven</first><last>Xu</last></author>
    <author><first>Andrew</first><last>Bennett</last></author>
    <author><first>Doris</first><last>Hoogeveen</last></author>
    <author><first>Jey Han</first><last>Lau</last></author>
    <author><first>Timothy</first><last>Baldwin</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>137–147</pages>
    <url>http://www.aclweb.org/anthology/W18-6119</url>
    <abstract>Community question answering (cQA) forums provide a rich source of</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>xu-EtAl:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6120">
    <title>Word-like character n-gram embedding</title>
    <author><first>Geewook</first><last>Kim</last></author>
    <author><first>Kazuki</first><last>Fukui</last></author>
    <author><first>Hidetoshi</first><last>Shimodaira</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>148–152</pages>
    <url>http://www.aclweb.org/anthology/W18-6120</url>
    <abstract>We propose a new word embedding method called “word-like character n-gram embedding”, which learns distributed representations of words by embedding word-like character n-grams.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kim-fukui-shimodaira:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6121">
    <title>Classification of Tweets about Reported Events using Neural Networks</title>
    <author><first>Kiminobu</first><last>Makino</last></author>
    <author><first>Yuka</first><last>Takei</last></author>
    <author><first>Taro</first><last>Miyazaki</last></author>
    <author><first>Jun</first><last>Goto</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>153–163</pages>
    <url>http://www.aclweb.org/anthology/W18-6121</url>
    <abstract>We developed a system that automatically extracts “Event-describing Tweets” which include incidents or accidents information for creating news reports. Event-describing Tweets can be classified into “Reported- event Tweets” and “New-information Tweets.” Reported-event Tweets cite news agencies or user generated content sites, and New- information Tweets are other Event-describing Tweets. A system is needed to classify them so that creators of factual TV programs can use them in their productions. Proposing this Tweet classification task is one of the contributions of this paper, because no prior papers have used the same task even though program creators and other events information collectors have to do it to extract required information from social networking sites. To classify Tweets in this task, this paper proposes a method to input and concatenate character and word sequences in Japanese Tweets by using convolutional neural networks. This proposed method is another contribution of this paper. For comparison, character or word input methods and other neural networks are also used. Results show that a system using the proposed method and architectures can classify Tweets with an F1 score of 88 %.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>makino-EtAl:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6122">
    <title>Learning to Define Terms in the Software Domain</title>
    <author><first>Vidhisha</first><last>Balachandran</last></author>
    <author><first>Dheeraj</first><last>Rajagopal</last></author>
    <author><first>Rose Catherine</first><last>Kanjirathinkal</last></author>
    <author><first>William</first><last>Cohen</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>164–172</pages>
    <url>http://www.aclweb.org/anthology/W18-6122</url>
    <abstract>One way to test a person’s knowledge of a domain is to ask them to define domain-specific terms. Here, we investigate the task of automatically generating definitions of technical terms by reading text from the technical domain. Specifically, we learn definitions of software entities from a large corpus built from the user forum Stack Overflow. To model definitions, we train a language model and incorporate additional domain-specific information like word-word co-occurrence, and ontological category information. Our approach improves previous baselines by 2 BLEU points for the definition generation task. Our experiments also show the additional challenges associated with the task and the short-comings of language-model based architectures for definition generation.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>balachandran-EtAl:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6123">
    <title>FrameIt: Ontology Discovery for Noisy User-Generated Text</title>
    <author><first>Dan</first><last>Iter</last></author>
    <author><first>Alon</first><last>Halevy</last></author>
    <author><first>Wang-Chiew</first><last>Tan</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>173–183</pages>
    <url>http://www.aclweb.org/anthology/W18-6123</url>
    <abstract>A common need of NLP applications is to extract structured data from text corpora in order to perform analytics or trigger an appropriate action. The ontology defining the structure is typically application dependent and in many cases it is not known a priori. We describe the FrameIt System that provides a workflow for (1) quickly discovering an ontology to model a text corpus and (2) learning an SRL model that extracts the instances of the ontology from sentences in the corpus. FrameIt exploits data that is obtained in the ontology discovery phase as weak supervision data to bootstrap the SRL model and then enables the user to refine the model with active learning. We present empirical results and qualitative analysis of the performance of FrameIt on three corpora of noisy user-generated text.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>iter-halevy-tan:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6124">
    <title>Using Author Embeddings to Improve Tweet Stance Classification</title>
    <author><first>Adrian</first><last>Benton</last></author>
    <author><first>Mark</first><last>Dredze</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>184–194</pages>
    <url>http://www.aclweb.org/anthology/W18-6124</url>
    <abstract>Many social media classification tasks analyze the content of a message, but do not consider</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>benton-dredze:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6125">
    <title>Low-resource named entity recognition via multi-source projection: Not quite there yet?</title>
    <author><first>Jan Vium</first><last>Enghoff</last></author>
    <author><first>Søren</first><last>Harrison</last></author>
    <author><first>Željko</first><last>Agić</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>195–201</pages>
    <url>http://www.aclweb.org/anthology/W18-6125</url>
    <abstract>Projecting linguistic annotations through word alignments is one of the most prevalent approaches to cross-lingual transfer learning. Conventional wisdom suggests that annotation projection “just works” regardless of the task at hand. We carefully consider multi-source projection for named entity recognition. Our experiment with 17 languages shows that to detect named entities in true low-resource languages, annotation projection may not be the right way to move forward. On a more positive note, we also uncover the conditions that do favor named entity projection from multiple sources. We argue these are infeasible under noisy low-resource constraints.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>enghoff-harrison-agi:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6126">
    <title>A Case Study on Learning a Unified Encoder of Relations</title>
    <author><first>Lisheng</first><last>Fu</last></author>
    <author><first>Bonan</first><last>Min</last></author>
    <author><first>Thien Huu</first><last>Nguyen</last></author>
    <author><first>Ralph</first><last>Grishman</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>202–207</pages>
    <url>http://www.aclweb.org/anthology/W18-6126</url>
    <abstract>Typical relation extraction models are trained on a single corpus annotated with a pre-defined relation schema. </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>fu-EtAl:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6127">
    <title>Convolutions Are All You Need (For Classifying Character Sequences)</title>
    <author><first>Zach</first><last>Wood-Doughty</last></author>
    <author><first>Nicholas</first><last>Andrews</last></author>
    <author><first>Mark</first><last>Dredze</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>208–213</pages>
    <url>http://www.aclweb.org/anthology/W18-6127</url>
    <abstract>While recurrent neural networks (RNNs)</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>wooddoughty-andrews-dredze:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6128">
    <title>Step or Not: Discriminator for The Real Instructions in User-generated Recipes</title>
    <author><first>Shintaro</first><last>Inuzuka</last></author>
    <author><first>Takahiko</first><last>Ito</last></author>
    <author><first>Jun</first><last>Harashima</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>214</pages>
    <url>http://www.aclweb.org/anthology/W18-6128</url>
    <abstract>In a recipe sharing service, users publish recipe instructions in the form of a series of steps. However, some of the “steps” are not actually part of the cooking process. Specifically, advertisements of recipes themselves (eg “introduced on TV”) and comments (eg “Thanks for many messages”) may often be included in the step section of the recipe, like the recipe author’s communication tool. However, such fake steps can cause problems when using recipe search indexing or when being spoken by devices such as smart speakers. As presented in this talk, we have constructed a discriminator that distinguishes between such a fake step and the step actually used for cooking. This project includes, but is not limited to, the creation of annotation data by classifying and analyzing recipe steps and the construction of identification models. Our models use only text information to identify the step. In our test, machine learning models achieved higher accuracy than rule-based methods that use manually chosen words.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>inuzuka-ito-harashima:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6129">
    <title>Combining Human and Machine Transcriptions on the Zooniverse Platform</title>
    <author><first>Daniel</first><last>Hanson</last></author>
    <author><first>Andrea</first><last>Simenstad</last></author>
    <booktitle>Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>215–216</pages>
    <url>http://www.aclweb.org/anthology/W18-6129</url>
    <abstract>This is a 1-page abstract on a work-in-progress for the Workshop on Noisy User-generated Text.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>hanson-simenstad:2018:W-NUT2018</bibkey>
  </paper>

  <paper id="6200">
    <title>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
    <editor><first>Alexandra</first><last>Balahur</last></editor>
    <editor><first>Saif M.</first><last>Mohammad</last></editor>
    <editor><first>Veronique</first><last>Hoste</last></editor>
    <editor><first>Roman</first><last>Klinger</last></editor>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://aclweb.org/anthology/W18-62</url>
    <bibtype>book</bibtype>
    <bibkey>WASSA2018:2018</bibkey>
  </paper>

  <paper id="6201">
    <title>Identifying Affective Events and the Reasons for their Polarity</title>
    <author><first>Ellen</first><last>Riloff</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1</pages>
    <url>http://aclweb.org/anthology/W18-6201</url>
    <abstract>Many events have a positive or negative impact on our lives (e.g., “I</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>riloff:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6202">
    <title>Deep contextualized word representations for detecting sarcasm and irony</title>
    <author><first>Suzana</first><last>Ilić</last></author>
    <author><first>Edison</first><last>Marrese-Taylor</last></author>
    <author><first>Jorge</first><last>Balazs</last></author>
    <author><first>Yutaka</first><last>Matsuo</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>2–7</pages>
    <url>http://aclweb.org/anthology/W18-6202</url>
    <abstract>Predicting context-dependent and non-literal utterances like sarcastic and ironic expressions still remains a challenging task in NLP, as it goes beyond linguistic patterns, encompassing common sense and shared knowledge as crucial components. To capture complex morpho-syntactic features that can usually serve as indicators for irony or sarcasm across dynamic contexts, we propose a model that uses character-level vector representations of words, based on ELMo. We test our model on 7 different datasets derived from 3 different data sources, providing state-of-the-art performance in 6 of them, and otherwise offering competitive results.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ili-EtAl:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6203">
    <title>Implicit Subjective and Sentimental Usages in Multi-sense Word Embeddings</title>
    <author><first>Yuqi</first><last>Sun</last></author>
    <author><first>Haoyue</first><last>Shi</last></author>
    <author><first>Junfeng</first><last>Hu</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>8–13</pages>
    <url>http://aclweb.org/anthology/W18-6203</url>
    <abstract>In multi-sense word embeddings, contextual variations in corpus may cause a univocal word to be embedded into different sense vectors.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>sun-shi-hu:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6204">
    <title>Language Independent Sentiment Analysis with Sentiment-Specific Word Embeddings</title>
    <author><first>Carl</first><last>Saroufim</last></author>
    <author><first>Akram</first><last>Almatarky</last></author>
    <author><first>Mohammad</first><last>AbdelHady</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>14–23</pages>
    <url>http://aclweb.org/anthology/W18-6204</url>
    <abstract>Data annotation is a critical step to train a text model but it is tedious, expensive and time-consuming. We present a language independent method to train a sentiment polarity model with limited amount of manually-labeled data. Word embeddings such as Word2Vec are efficient at incorporating semantic and syntactic properties of words, yielding good results for document classification. However, these embeddings might map words with opposite polarities, to vectors close to each other. We train Sentiment Specific Word Embeddings (SSWE) on top of an unsupervised Word2Vec model, using either Recurrent Neural Networks (RNN) or Convolutional Neural Networks (CNN) on data auto-labeled as “Positive” or “Negative”. For this task, we rely on the universality of emojis to auto-label a large number of French tweets using a small set of positive and negative emojis. Finally, we apply a transfer learning approach to refine the network weights with a small-size manually-labeled training data set. Experiments are conducted to evaluate the performance of this approach on French sentiment classification using benchmark data sets from SemEval 2016 competition. We were able to achieve a performance improvement by using SSWE over Word2Vec. We also used a graph-based approach for label propagation to auto-generate a sentiment lexicon.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>saroufim-almatarky-abdelhady:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6205">
    <title>Creating a Dataset for Multilingual Fine-grained Emotion-detection Using Gamification-based Annotation</title>
    <author><first>Emily</first><last>Öhman</last></author>
    <author><first>Kaisla</first><last>Kajava</last></author>
    <author><first>Jörg</first><last>Tiedemann</last></author>
    <author><first>Timo</first><last>Honkela</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>24–30</pages>
    <url>http://aclweb.org/anthology/W18-6205</url>
    <abstract>This paper introduces a gamified framework for fine-grained sentiment analysis and emotion detection. We present a flexible tool that can be used for efficient annotation based on crowd sourcing and a self-perpetuating gold standard. We also present a novel dataset with multi-dimensional annotations of emotions and sentiments in movie subtitles that enables research on sentiment preservation across languages and the creation of robust multilingual emotion detection tools. The tools and datasets are public and open-source and can easily be extended and applied for various purposes.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>hman-EtAl:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6206">
    <title>IEST: WASSA-2018 Implicit Emotions Shared Task</title>
    <author><first>Roman</first><last>Klinger</last></author>
    <author><first>Orphee</first><last>De Clercq</last></author>
    <author><first>Saif</first><last>Mohammad</last></author>
    <author><first>Alexandra</first><last>Balahur</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>31–42</pages>
    <url>http://aclweb.org/anthology/W18-6206</url>
    <abstract>Past shared tasks on emotions use data with both overt expressions of emotions (I am so happy to see you!) as well as subtle expressions where the emotions have to be inferred, for instance from event descriptions. Further, most datasets do not focus on the cause or the stimulus of the emotion. Here, for the first time, we propose a shared task where systems have to predict the emotions in a large automatically labeled dataset of tweets without access to words denoting emotions. Based on this intention, we call this the Implicit Emotion Shared Task (IEST) because the systems have to infer the emotion mostly from the context. Every tweet has an occurrence of an explicit emotion word that is masked. The tweets are collected in a manner such that they are likely to include a description of the cause of the emotion – the stimulus. Altogether, 30 teams submitted results which range from macro F1 scores of 21 % to 71 %. The baseline (Max- Ent bag of words and bigrams) obtains an F1 score of 60 % which was available to the participants during the development phase. A study with human annotators suggests that automatic methods outperform human predictions, possibly by honing into subtle textual clues not used by humans. Corpora, resources, and results are available at the shared task website at http://implicitemotions.wassa2018.com.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>klinger-EtAl:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6207">
    <title>Amobee at IEST 2018: Transfer Learning from Language Models</title>
    <author><first>Alon</first><last>Rozental</last></author>
    <author><first>Daniel</first><last>Fleischer</last></author>
    <author><first>Zohar</first><last>Kelrich</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>43–49</pages>
    <url>http://aclweb.org/anthology/W18-6207</url>
    <abstract>This paper describes the system developed at Amobee for the WASSA 2018 implicit emotions shared task (IEST). The goal of this task was to predict the emotion expressed by missing words in tweets without an explicit mention of those words. We developed an ensemble system consisting of language models together with LSTM-based networks containing a CNN attention mechanism. Our approach represents a novel use of language models—specifically trained on a large Twitter dataset—to predict and classify emotions. Our system reached 1st place with a macro F1 score of 0.7145.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rozental-fleischer-kelrich:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6208">
    <title>IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations</title>
    <author><first>Jorge</first><last>Balazs</last></author>
    <author><first>Edison</first><last>Marrese-Taylor</last></author>
    <author><first>Yutaka</first><last>Matsuo</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>50–56</pages>
    <url>http://aclweb.org/anthology/W18-6208</url>
    <abstract>In this paper we describe our system designed for the WASSA 2018 Implicit Emotion Shared Task (IEST), which obtained second place out of 30 teams with a test macro F1 score of 0.710. The system is composed of a single pre-trained ELMo layer for encoding words, a Bidirectional Long-Short Memory Network BiLSTM for enriching word representations with context, a max-pooling operation for creating sentence representations from them, and a Dense Layer for projecting the sentence representations into label space. Our</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>balazs-marresetaylor-matsuo:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6209">
    <title>NTUA-SLP at IEST 2018: Ensemble of Neural Transfer Methods for Implicit Emotion Classification</title>
    <author><first>Alexandra</first><last>Chronopoulou</last></author>
    <author><first>Aikaterini</first><last>Margatina</last></author>
    <author><first>Christos</first><last>Baziotis</last></author>
    <author><first>Alexandros</first><last>Potamianos</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>57–64</pages>
    <url>http://aclweb.org/anthology/W18-6209</url>
    <abstract>In this paper we present our approach to tackle</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>chronopoulou-EtAl:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6210">
    <title>Sentiment analysis under temporal shift</title>
    <author><first>Jan</first><last>Lukeš</last></author>
    <author><first>Anders</first><last>Søgaard</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>65–71</pages>
    <url>http://aclweb.org/anthology/W18-6210</url>
    <abstract>Sentiment analysis models often rely on training data that is several years old. In this paper, we show that lexical features change polarity over time, leading to degrading performance. This effect is particularly strong in sparse models relying only on highly predictive features. Using predictive feature selection, we are able to significantly improve the accuracy of such models over time.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>luke-sgaard:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6211">
    <title>Not Just Depressed: Bipolar Disorder Prediction on Reddit</title>
    <author><first>Ivan</first><last>Sekulic</last></author>
    <author><first>Matej</first><last>Gjurković</last></author>
    <author><first>Jan</first><last>Šnajder</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>72–78</pages>
    <url>http://aclweb.org/anthology/W18-6211</url>
    <abstract>Bipolar disorder, an illness characterized by manic and depressive episodes, affects more than 60 million people worldwide. We present a preliminary study on bipolar disorder prediction from user-generated text on Reddit, which relies on users’ self-reported labels. Our benchmark classifiers for bipolar disorder prediction outperform the baselines and reach accuracy and F1-scores of above 86%. Feature analysis shows interesting differences in language use between users with bipolar disorders and the control group, including differences in the use of emotion-expressive words.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>sekulic-gjurkovi-najder:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6212">
    <title>Topic-Specific Sentiment Analysis Can Help Identify Political Ideology</title>
    <author><first>Sumit</first><last>Bhatia</last></author>
    <author><first>Deepak</first><last>P</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>79–84</pages>
    <url>http://aclweb.org/anthology/W18-6212</url>
    <abstract>Ideological leanings of an individual can often</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>bhatia-p:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6213">
    <title>Saying no but meaning yes: negation and sentiment analysis in Basque</title>
    <author><first>Jon</first><last>Alkorta</last></author>
    <author><first>Koldo</first><last>Gojenola</last></author>
    <author><first>Mikel</first><last>Iruskieta</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>85–90</pages>
    <url>http://aclweb.org/anthology/W18-6213</url>
    <abstract>Negation is one of the shifters or operators that can change the semantic orientation of a word or a sentence and, consequently, it has to be taken into consideration in sentiment analysis. In this work, we have analyzed the effects of negation on the semantic orientation in Basque. The analysis shows that negation markers can strengthen, weaken or have no effect on sentiment orientation of a word or a group of words. Using the Constraint Grammar formalism, we have designed and evaluated a set of linguistic rules to formalize these three phenomena. The results show that two phenomena, strengthening and no change, have been identified accurately and the third one, weakening, with acceptable results.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>alkorta-gojenola-iruskieta:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6214">
    <title>Leveraging Writing Systems Change for Deep Learning Based Chinese Emotion Analysis</title>
    <author><first>Rong</first><last>Xiang</last></author>
    <author><first>Yunfei</first><last>Long</last></author>
    <author><first>Qin</first><last>Lu</last></author>
    <author><first>Dan</first><last>Xiong</last></author>
    <author><first>I-Hsuan</first><last>Chen</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>91–96</pages>
    <url>http://aclweb.org/anthology/W18-6214</url>
    <abstract>Social media text written in Chinese communities contains mixed scripts including major text written with Chinese characters, an ideograph-based writing system, and some minor text using Latin letters, an alphabet-based writing system. This phenomenon is called writing systems change (WSCs). Past studies have shown that WSCs can be used to express emotions, particularly where the social and political environment is more conservative. However, because WSCs can break the syntax of the major text, it poses more challenges in NLP tasks like emotion classification. In this work, we present a novel deep learning based method to include WSCs as an effective feature for emotion analysis. The method first identifies all WSCs points. Representation of the major text is learned through an LSTM model whereas the presentation of the minority text is learned by a separate CNN.Emotions expressed in the minority text are further highlighted through an attention mechanism before emotion classification. It has proven to be significant that incorporating WSCs features in deep learning models can improve the performance which is valid by both F1-scores and p-value. It indicates that WSCs serve as an effective feature in emotion analysis of the social network.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>xiang-EtAl:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6215">
    <title>Ternary Twitter Sentiment Classification with Distant Supervision and Sentiment-Specific Word Embeddings</title>
    <author><first>Mats</first><last>Byrkjeland</last></author>
    <author><first>Frederik</first><last>Gørvell de Lichtenberg</last></author>
    <author><first>Björn</first><last>Gambäck</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>97–106</pages>
    <url>http://aclweb.org/anthology/W18-6215</url>
    <abstract>The paper proposes the Ternary Sentiment Embedding Model, a new model for creating sentiment embeddings based on the Hybrid Ranking Model of Tang et al. (2016), but trained on ternary-labeled data instead of binary-labeled, utilizing sentiment embeddings from datasets made with different distant supervision methods. </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>byrkjeland-grvelldelichtenberg-gambck:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6216">
    <title>Linking News Sentiment to Microblogs: A Distributional Semantics Approach to Enhance Microblog Sentiment Classification</title>
    <author><first>Tobias</first><last>Daudert</last></author>
    <author><first>Paul</first><last>Buitelaar</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>107–115</pages>
    <url>http://aclweb.org/anthology/W18-6216</url>
    <abstract>Social media’s popularity in society and research is gaining momentum and simultaneously increasing the importance of short textual content such as microblogs. Microblogs are affected by many factors including the news media, therefore, we exploit sentiments conveyed from news to detect and classify sentiment in microblogs. Given that texts can deal with the same entity but might not be vastly related when it comes to sentiment, it becomes necessary to introduce further measures ensuring the relatedness of texts while leveraging the contained sentiments. This paper describes ongoing research introducing distributional semantics to improve the exploitation of news-contained sentiment to enhance microblog sentiment classification.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>daudert-buitelaar:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6217">
    <title>Aspect Based Sentiment Analysis into the Wild</title>
    <author><first>Caroline</first><last>Brun</last></author>
    <author><first>Vassilina</first><last>Nikoulina</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>116–122</pages>
    <url>http://aclweb.org/anthology/W18-6217</url>
    <abstract>In this paper, we test state-of-the-art Aspect Based Sentiment Analysis (ABSA) systems trained on a widely used dataset on actual data. We created a new manually annotated dataset of user generated data from the same domain as the training dataset, but from other sources and analyse the differences between the new and the standard ABSA dataset. We then analyse the results in performance of different versions of the same system on both datasets. We also propose light adaptation methods to increase system robustness.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>brun-nikoulina:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6218">
    <title>The Role of Emotions in Native Language Identification</title>
    <author><first>Ilia</first><last>Markov</last></author>
    <author><first>Vivi</first><last>Nastase</last></author>
    <author><first>Carlo</first><last>Strapparava</last></author>
    <author><first>Grigori</first><last>Sidorov</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>123–129</pages>
    <url>http://aclweb.org/anthology/W18-6218</url>
    <abstract>We explore the hypothesis that emotion is one of the dimensions of language that surfaces from the native language into a second language. To check the role of emotions in native language identification (NLI), we model emotion information through polarity and emotion load features, and use document representations using these features to classify the native language of the author. The results indicate that emotion is relevant for NLI, even for high proficiency levels and across topics.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>markov-EtAl:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6219">
    <title>Self-Attention: A Better Building Block for Sentiment Analysis Neural Network Classifiers</title>
    <author><first>Artaches</first><last>Ambartsoumian</last></author>
    <author><first>Fred</first><last>Popowich</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>130–139</pages>
    <url>http://aclweb.org/anthology/W18-6219</url>
    <abstract>Sentiment Analysis has seen much progress in the past two decades. For the past few years, neural network approaches, primarily RNNs and CNNs, have been the most successful for this task. Recently, a new category of neural networks, self-attention networks (SANs), have been created which utilizes the attention mechanism as the basic building block. Self-attention networks have been shown to be effective for sequence modeling tasks, while having no recurrence or convolutions. In this work we explore the effectiveness of the SANs for sentiment analysis. We demonstrate that SANs are superior in performance to their RNN and CNN counterparts by comparing their classification accuracy on six datasets as well as their model characteristics such as training speed and memory consumption. Finally, we explore the effects of various SAN modifications such as multi-head attention as well as two methods of incorporating sequence position information into SANs.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ambartsoumian-popowich:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6220">
    <title>Dual Memory Network Model for Biased Product Review Classification</title>
    <author><first>Yunfei</first><last>Long</last></author>
    <author><first>Mingyu</first><last>Ma</last></author>
    <author><first>Qin</first><last>Lu</last></author>
    <author><first>Rong</first><last>Xiang</last></author>
    <author><first>Chu-Ren</first><last>Huang</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>140–148</pages>
    <url>http://aclweb.org/anthology/W18-6220</url>
    <abstract>In sentiment analysis (SA) of product reviews,</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>long-EtAl:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6221">
    <title>Measuring Issue Ownership using Word Embeddings</title>
    <author><first>Amaru</first><last>Cuba Gyllensten</last></author>
    <author><first>Magnus</first><last>Sahlgren</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>149–155</pages>
    <url>http://aclweb.org/anthology/W18-6221</url>
    <abstract>Sentiment and topic analysis are common methods used for social media monitoring. Essentially, these methods answers questions such as, “What is being talked about, regarding X”, and “What do people feel, regarding X”. In this paper, we investigate another venue for social media monitoring, namely issue ownership. In political science, issue ownership has been used to explain voter choice and electoral outcomes. The theory states that voters value certain issues, and cast votes according to the party which they feel best address these issues. We argue that issue alignment can be seen as a kind of semantic source similarity of the kind “How similar is source A to issue owner P, when talking about issue X”, and as such can be measured using Word/Document embedding techniques. We present work in progress towards measuring that kind of conditioned similarity, and introduce a new notion of similarity for predictive embeddings. </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>cubagyllensten-sahlgren:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6222">
    <title>Sentiment Expression Boundaries in Sentiment Polarity Classification</title>
    <author><first>Rasoul</first><last>Kaljahi</last></author>
    <author><first>Jennifer</first><last>Foster</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>156–166</pages>
    <url>http://aclweb.org/anthology/W18-6222</url>
    <abstract>We investigate the effect of using sentiment expression boundaries in predicting sentiment polarity in aspect-level sentiment analysis. We manually annotate a freely available English sentiment polarity dataset with these boundaries and carry out a series of experiments which demonstrate that high quality sentiment expressions can boost the performance of polarity classification. Our experiments with various neural architectures also show that CNN networks outperform LSTMs on this task.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kaljahi-foster:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6223">
    <title>Exploring and Learning Suicidal Ideation Connotations on Social Media with Deep Learning</title>
    <author><first>Ramit</first><last>Sawhney</last></author>
    <author><first>Prachi</first><last>Manchanda</last></author>
    <author><first>Puneet</first><last>Mathur</last></author>
    <author><first>Rajiv</first><last>Shah</last></author>
    <author><first>Raj</first><last>Singh</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>167–175</pages>
    <url>http://aclweb.org/anthology/W18-6223</url>
    <abstract>The increasing suicide rates amongst youth and its high correlation with suicidal ideation expression on social media warrants a deeper investigation into models for the detection of suicidal intent in text such as tweets to enable prevention. However, the complexity of the natural language constructs makes this task very challenging. Deep Learning architectures such as LSTMs, CNNs, and RNNs show promise in sentence level classification problems. This work investigates the ability of deep learning architectures to build an accurate and robust model for suicidal ideation detection and compares their performance with standard baselines in text classification problems. The experimental results reveal the merit in C-LSTM based models compared to other deep learning and machine learning based classification models for suicidal ideation detection.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>sawhney-EtAl:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6224">
    <title>UTFPR at IEST 2018: Exploring Character-to-Word Composition for Emotion Analysis</title>
    <author><first>Gustavo</first><last>Paetzold</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>176–181</pages>
    <url>http://aclweb.org/anthology/W18-6224</url>
    <abstract>We introduce the UTFPR system for the Implicit Emotions Shared Task of 2018: A compositional character-to-word recurrent neural network that does not exploit heavy and/or hard-to-obtain resources. We find that our approach can outperform multiple baselines, and offers an elegant and effective solution to the problem of orthographic variance in tweets.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>paetzold:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6225">
    <title>HUMIR at IEST-2018: Lexicon-Sensitive and Left-Right Context-Sensitive BiLSTM for Implicit Emotion Recognition</title>
    <author><first>Behzad</first><last>Naderalvojoud</last></author>
    <author><first>Alaettin</first><last>Ucan</last></author>
    <author><first>Ebru</first><last>Akcapinar Sezer</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>182–188</pages>
    <url>http://aclweb.org/anthology/W18-6225</url>
    <abstract>This paper describes the approaches used in HUMIR system for the WASSA-2018 shared task on the implicit emotion recognition. The objective of this task is to predict the emotion expressed by the target word that has been excluded from the given tweet. We suppose this task as a word sense disambiguation in which the target word is considered as a synthetic word that can express 6 emotions depending on the context. To predict the correct emotion, we propose a deep neural network model that uses two BiLSTM networks to represent the contexts in the left and right sides of the target word. The BiLSTM outputs achieved from the left and right contexts are considered as context-sensitive features. These features are used in a feed-forward neural network to predict the target word emotion. Besides this approach, we also combine the BiLSTM model with lexicon-based and emotion-based features. Finally, we employ all models in the final system using Bagging ensemble method. We achieved macro F-measure value of 68.8 on the official test set and ranked sixth out of 30 participants.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>naderalvojoud-ucan-akcapinarsezer:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6226">
    <title>NLP at IEST 2018: BiLSTM-Attention and LSTM-Attention via Soft Voting in Emotion Classification</title>
    <author><first>Qimin</first><last>Zhou</last></author>
    <author><first>Hao</first><last>Wu</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>189–194</pages>
    <url>http://aclweb.org/anthology/W18-6226</url>
    <abstract>This paper describes our method that competed at WASSA2018 Implicit Emotion Shared Task. The goal of this task is to classify the emotions of excluded words in tweets into six different classes: sad, joy, disgust, surprise, anger and fear. For this, we examine a BiLSTM architecture with attention mechanism (BiLSTM-Attention) and a LSTM architecture with attention mechanism (LSTM-Attention), and try different dropout rates based on these two models. We then exploit an ensemble of these methods to give the final prediction which improves the model performance significantly compared with the baseline model. The proposed method achieves 7th position out of 30 teams and outperforms the baseline method by 12.5% in terms of macro F1.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>zhou-wu:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6227">
    <title>SINAI at IEST 2018: Neural Encoding of Emotional External Knowledge for Emotion Classification</title>
    <author><first>Flor Miriam</first><last>Plaza del Arco</last></author>
    <author><first>Eugenio</first><last>Martínez-Cámara</last></author>
    <author><first>Maite</first><last>Martin</last></author>
    <author><first>L. Alfonso</first><last>Urena Lopez</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>195–200</pages>
    <url>http://aclweb.org/anthology/W18-6227</url>
    <abstract>In this paper, we describe our participation in WASSA 2018 Implicit Emotion Shared Task (IEST 2018). We claim that the use of emotional external knowledge may enhance the performance and the capacity of generalization of an emotion classification system based on neural networks. Accordingly, we submitted four deep learning systems grounded in a sequence encoding layer. They mainly differ in the feature vector space and the recurrent neural network used in the sequence encoding layer. The official results show that the systems that used emotional external knowledge have a higher capacity of generalization, hence our claim holds.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>plazadelarco-EtAl:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6228">
    <title>EmoNLP at IEST 2018: An Ensemble of Deep Learning Models and Gradient Boosting Regression Tree for Implicit Emotion Prediction in Tweets</title>
    <author><first>Man</first><last>Liu</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>201–204</pages>
    <url>http://aclweb.org/anthology/W18-6228</url>
    <abstract>This paper describes our system submitted to IEST 2018, a shared task to predict the emotion types. Six emotion types are involved: anger, joy, fear, surprise, disgust and sad. We perform three different approaches: feed forward neural network (FFNN), convolutional BLSTM (ConBLSTM) and Gradient Boosting Regression Tree Method (GBM). Word embeddings used in convolutional BLSTM are pre-trained on 470 million tweets which are filtered using the emotional words and emojis. In addition, broad sets of features (i.e. syntactic features, lexicon features, cluster features) are adopted to train GBM and FFNN. The three approaches are finally ensembled by the weighted average of predicted probabilities of each emotion type.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>liu:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6229">
    <title>HGSGNLP at IEST 2018: An Ensemble of Machine Learning and Deep Neural Architectures for Implicit Emotion Classification in Tweets</title>
    <author><first>wenting</first><last>wang</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>205–210</pages>
    <url>http://aclweb.org/anthology/W18-6229</url>
    <abstract>This paper describes our system designed for the WASSA-2018 Implicit Emotion Shared Task (IEST). The task is to predict the emotion category expressed in a tweet by removing the terms angry, afraid, happy, sad, surprised, disgusted and their synonyms. Our final submission is an ensemble of one supervised learning model and three deep neural network based models, where each model approaches the problem from essentially different directions. Our system achieves the macro F1 score of 65.8%, which is a 5.9% performance improvement over the baseline and is ranked 12 out of 30 participating teams.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>wang:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6230">
    <title>DataSEARCH at IEST 2018: Multiple Word Embedding based Models for Implicit Emotion Classification of Tweets with Deep Learning</title>
    <author><first>Yasas</first><last>Senarath</last></author>
    <author><first>Uthayasanker</first><last>Thayasivam</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>211–216</pages>
    <url>http://aclweb.org/anthology/W18-6230</url>
    <abstract>This paper describes an approach to solve implicit emotion classification with the use of pre-trained word embedding models to train multiple neural networks. The system described in this paper is composed of a sequential combination of Long Short-Term Memory and Convolutional Neural Network for feature extraction and Feedforward Neural Network for classification. In this paper, we successfully show that features extracted using multiple pre-trained embeddings can be used to improve the overall performance of the system with Emoji being one of the significant features. The evaluations show that our approach outperforms the baseline system by more than 8% without using any external corpus or lexicon. This approach is ranked 8th in Implicit Emotion Shared Task (IEST) at WASSA-2018.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>senarath-thayasivam:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6231">
    <title>NL-FIIT at IEST-2018: Emotion Recognition utilizing Neural Networks and Multi-level Preprocessing</title>
    <author><first>Samuel</first><last>Pecar</last></author>
    <author><first>Michal</first><last>Farkaš</last></author>
    <author><first>Marian</first><last>Simko</last></author>
    <author><first>Peter</first><last>Lacko</last></author>
    <author><first>Maria</first><last>Bielikova</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>217–223</pages>
    <url>http://aclweb.org/anthology/W18-6231</url>
    <abstract>In this paper, we present neural models submitted to Shared Task on Implicit Emotion Recognition, organized as part of WASSA 2018. We propose a Bi-LSTM architecture with regularization through dropout and Gaussian noise. Our models use three different embedding layers: GloVe word embeddings trained on Twitter dataset, ELMo embeddings and also sentence embeddings. We see preprocessing as one of the most important parts of the task. We focused on handling emojis, emoticons, hashtags, and also various shortened word forms. In some cases, we proposed to remove some parts of the text, as they do not affect emotion of the original sentence. We also experimented with other modifications like category weights for learning and stacking multiple layers. Our model achieved a macro average F1 score of 65.55%, significantly outperforming the baseline model produced by a simple logistic regression.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>pecar-EtAl:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6232">
    <title>UWB at IEST 2018: Emotion Prediction in Tweets with Bidirectional Long Short-Term Memory Neural Network</title>
    <author><first>Pavel</first><last>Přibáň</last></author>
    <author><first>Jiří</first><last>Martínek</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>224–230</pages>
    <url>http://aclweb.org/anthology/W18-6232</url>
    <abstract>This paper describes our system created for the WASSA 2018 Implicit Emotion Shared Task. The goal of this task is to predict the emotion of a given tweet, from which a certain emotion word is removed. The removed word can be sad, happy, disgusted, angry, afraid or a synonym of one of them. Our proposed system is based on deep-learning methods. We use Bidirectional Long Short-Term Memory (BiLSTM) with word embeddings as an input. Pre-trained DeepMoji model and pre-trained emoji2vec emoji embeddings are also used as additional inputs.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>pib-martnek:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6233">
    <title>USI-IR at IEST 2018: Sequence Modeling and Pseudo-Relevance Feedback for Implicit Emotion Detection</title>
    <author><first>Esteban</first><last>Rissola</last></author>
    <author><first>Anastasia</first><last>Giachanou</last></author>
    <author><first>Fabio</first><last>Crestani</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>231–234</pages>
    <url>http://aclweb.org/anthology/W18-6233</url>
    <abstract>This paper describes the participation of USI-IR in WASSA 2018 Implicit Emotion Shared Task. We propose a relevance feedback approach employing a sequential model (biLSTM) and word embeddings derived from a large collection of tweets. To this end, we assume that the top-k predictions produce at a first classification step are correct (based on the model accuracy) and use them as new examples to re-train the network.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rissola-giachanou-crestani:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6234">
    <title>EmotiKLUE at IEST 2018: Topic-Informed Classification of Implicit Emotions</title>
    <author><first>Thomas</first><last>Proisl</last></author>
    <author><first>Philipp</first><last>Heinrich</last></author>
    <author><first>Besim</first><last>Kabashi</last></author>
    <author><first>Stefan</first><last>Evert</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>235–242</pages>
    <url>http://aclweb.org/anthology/W18-6234</url>
    <abstract>EmotiKLUE is a submission to the Implicit Emotion Shared Task. It is a deep learning system that combines independent representations of the left and right contexts of the emotion word with the topic distribution of an LDA topic model. EmotiKLUE achieves a macro average F1 score of 67.13%, significantly outperforming the baseline produced by a simple ML classifier. Further enhancements after the evaluation period lead to an improved F1 score of 68.10%.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>proisl-EtAl:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6235">
    <title>BrainT at IEST 2018: Fine-tuning Multiclass Perceptron For Implicit Emotion Classification</title>
    <author><first>Vachagan</first><last>Gratian</last></author>
    <author><first>Marina</first><last>Haid</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>243–247</pages>
    <url>http://aclweb.org/anthology/W18-6235</url>
    <abstract>We present BrainT, a multiclass, averaged perceptron tested on implicit emotion prediction of tweets. We show that the dataset is linearly separable and explore ways in fine-tuning the baseline classifier. Our results indicate that the bag-of-words features benefit the model moderately and prediction can be improved significantly with bigrams, trigrams, skip-one- tetragrams and POS-tags. Furthermore, we find preprocessing of the n-grams, including stemming, lowercasing, stopword filtering, emoji and emoticon conversion, generally not useful. The model is trained on an annotated corpus of 153,383 tweets and predictions on the test data were submitted to the WASSA-2018 Implicit Emotion Shared Task. BrainT attained a Macro F-score of 0.63.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>gratian-haid:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6236">
    <title>Disney at IEST 2018: Predicting Emotions using an Ensemble</title>
    <author><first>Wojciech</first><last>Witon</last></author>
    <author><first>Pierre</first><last>Colombo</last></author>
    <author><first>Ashutosh</first><last>Modi</last></author>
    <author><first>Mubbasir</first><last>Kapadia</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>248–253</pages>
    <url>http://aclweb.org/anthology/W18-6236</url>
    <abstract>This paper describes our participating system in the WASSA 2018 shared task on emotion prediction. The task focusses on implicit emo- tion prediction in a tweet. In this task, key- words corresponding to the six emotion label names (anger, fear, disgust, joy, sad, and sur- prise ) have been removed from the tweet text, making emotion prediction implicit and the task challenging. We propose a model based on ensemble of classifiers for prediction. Each classifier in the ensemble uses sequence of Convolutional Neural Network (CNN) archi- tecture blocks and uses ELMo (Embeddings from Language Model) (Peters et al., 2018) as input. Our system achieves 66.2% F1 score on the test set. The best performing system in the shared task has reported 71.4% F1 score.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>witon-EtAl:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6237">
    <title>Sentylic at IEST 2018: Gated Recurrent Neural Network and Capsule Network Based Approach for Implicit Emotion Detection</title>
    <author><first>Prabod</first><last>Rathnayaka</last></author>
    <author><first>Supun</first><last>Abeysinghe</last></author>
    <author><first>Chamod</first><last>Samarajeewa</last></author>
    <author><first>Isura</first><last>Manchanayake</last></author>
    <author><first>Malaka</first><last>Walpola</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>254–259</pages>
    <url>http://aclweb.org/anthology/W18-6237</url>
    <abstract>In this paper, we present the system we have used for the Implicit WASSA 2018 Implicit Emotion Shared Task. The task is to predict the emotion of a tweet of which the explicit mentions of emotion terms have been removed. The idea is to come up with a model which has the ability to implicitly identify the emotion expressed given the context words. We have used a Gated Recurrent Neural Network (GRU) and a Capsule Network based model for the task. Pre-trained word embeddings have been utilized to incorporate contextual knowledge about words into the model. GRU layer learns latent representations using the input word embeddings. Subsequent Capsule Network layer learns high-level features from that hidden representation. The proposed model managed to achieve a macro-F1 score of 0.692.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rathnayaka-EtAl:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6238">
    <title>Fast Approach to Build an Automatic Sentiment Annotator for Legal Domain using Transfer Learning</title>
    <author><first>Viraj</first><last>Salaka</last></author>
    <author><first>Menuka</first><last>Warushavithana</last></author>
    <author><first>Nisansa</first><last>de Silva</last></author>
    <author><first>Amal Shehan</first><last>Perera</last></author>
    <author><first>Gathika</first><last>Ratnayaka</last></author>
    <author><first>Thejan</first><last>Rupasinghe</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>260–265</pages>
    <url>http://aclweb.org/anthology/W18-6238</url>
    <abstract>This study proposes a novel way of identifying the sentiment of the phrases used in the legal domain. The added complexity of the language used in law, and the inability of the existing systems to accurately predict the sentiments of words in law are the main motivations behind this study. This is a transfer learning approach, which can be used for other domain adaptation tasks as well. The proposed methodology achieves an improvement of over 6% compared to the source model’s accuracy in the legal domain.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>salaka-EtAl:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6239">
    <title>What Makes You Stressed? Finding Reasons From Tweets</title>
    <author><first>Reshmi</first><last>Gopalakrishna Pillai</last></author>
    <author><first>Mike</first><last>Thelwall</last></author>
    <author><first>Constantin</first><last>Orasan</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>266–272</pages>
    <url>http://aclweb.org/anthology/W18-6239</url>
    <abstract>Detecting stress from social media gives a non-intrusive and inexpensive alternative to traditional tools such as questionnaires or physiological sensors for monitoring mental state of individuals. This paper introduces a novel framework for finding reasons for stress from tweets, analyzing multiple categories for the first time. Three word-vector based methods are evaluated on collections of tweets about politics or airlines and are found to be more accurate than standard machine learning algorithms.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>gopalakrishnapillai-thelwall-orasan:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6240">
    <title>EmojiGAN: learning emojis distributions with a generative model</title>
    <author><first>Bogdan</first><last>Mazoure</last></author>
    <author><first>Thang</first><last>DOAN</last></author>
    <author><first>Saibal</first><last>Ray</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>273–279</pages>
    <url>http://aclweb.org/anthology/W18-6240</url>
    <abstract>Generative models have recently experienced a surge in popularity due to the development of more efficient training algorithms and increasing computational power. Models such as adversarial generative networks (GANs) have been successfully used in various areas such as computer vision, medical imaging, style transfer and natural language generation. Adversarial nets were recently shown to yield results in the image-to-text task, where given a set of images, one has to provide their corresponding text description. In this paper, we take a similar approach and propose a image-to-emoji architecture, which is trained on data from social networks and can be used to score a given picture using ideograms. We show empirical results of our algorithm on data obtained from the most influential Instagram accounts.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mazoure-doan-ray:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6241">
    <title>Identifying Opinion-Topics and Polarity of Parliamentary Debate Motions</title>
    <author><first>Gavin</first><last>Abercrombie</last></author>
    <author><first>Riza Theresa</first><last>Batista-Navarro</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>280–285</pages>
    <url>http://aclweb.org/anthology/W18-6241</url>
    <abstract>Analysis of the topics mentioned and opinions expressed in parliamentary debate motions–or proposals–is difficult for human readers, but necessary for understanding and automatic processing of the content of the subsequent speeches. We present a dataset of debate motions with pre-existing ‘policy’ labels, and investigate the utility of these labels for simultaneous topic and opinion polarity analysis. For topic detection, we apply one-versus-the-rest supervised topic classification, finding that good performance is achieved in predicting the policy topics, and that textual features derived from the debate titles associated with the motions are particularly indicative of motion topic. We then examine whether the output could also be used to determine the positions taken by proposers towards the different policies by investigating how well humans agree in interpreting the opinion polarities of the motions. Finding very high levels of agreement, we conclude that the policies used can be reliable labels for use in these tasks, and that successful topic detection can therefore provide opinion analysis of the motions ‘for free’.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>abercrombie-batistanavarro:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6242">
    <title>Homonym Detection For Humor Recognition In Short Text</title>
    <author><first>Sven</first><last>van den Beukel</last></author>
    <author><first>Lora</first><last>Aroyo</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>286–291</pages>
    <url>http://aclweb.org/anthology/W18-6242</url>
    <abstract>In this paper, automatic homophone- and homograph detection are suggested as new useful features for humor recognition systems. The system combines style-features from previous studies on humor recognition in short text with ambiguity-based features. The performance of two potentially useful homograph detection methods is evaluated using crowdsourced annotations as ground truth. Adding homophones and homographs as features to the classifier results in a small but significant improvement over the style-features alone. For the task of humor recognition, recall appears to be a more important quality measure than precision. Although the system was designed for humor recognition in oneliners, it also performs well at the classification of longer humorous texts.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>vandenbeukel-aroyo:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6243">
    <title>Emo2Vec: Learning Generalized Emotion Representation by Multi-task Training</title>
    <author><first>Peng</first><last>Xu</last></author>
    <author><first>Andrea</first><last>Madotto</last></author>
    <author><first>Chien-Sheng</first><last>Wu</last></author>
    <author><first>Ji Ho</first><last>Park</last></author>
    <author><first>Pascale</first><last>Fung</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>292–298</pages>
    <url>http://aclweb.org/anthology/W18-6243</url>
    <abstract>In this paper, we propose Emo2Vec which encodes emotional semantics into vectors. We train Emo2Vec by multi-task learning six different emotion-related tasks, including emotion/sentiment analysis, sarcasm classification, stress detection, abusive language classification, insult detection, and personality recognition. Our evaluation on Emo2Vec shows that it outperforms existing affect-related representations, such as Sentiment-Specific Word Embedding and DeepMoji embeddings with much smaller training corpora. When concatenated with GloVe, Emo2Vec achieves competitive performances to state-of-the-art results on several tasks using simple logistic regression classifier. Finally, we visualize the learned vectors, showing that Emo2Vec can cluster words with similar emotion together.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>xu-EtAl:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6244">
    <title>Learning representations for sentiment classification using Multi-task framework</title>
    <author><first>Hardik</first><last>Meisheri</last></author>
    <author><first>Harshad</first><last>Khadilkar</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>299–308</pages>
    <url>http://aclweb.org/anthology/W18-6244</url>
    <abstract>Most of the existing state of the art sentiment classification techniques involve the use of pre-trained embeddings. This paper postulates a generalized representation that collates training on multiple datasets using a Multi-task learning framework. We incorporate publicly available, pre-trained embeddings with Bidirectional LSTM’s to develop the multi-task model. We validate the representations on an independent test Irony dataset that can contain several sentiments within each sample, with an arbitrary distribution. Our experiments show a significant improvement in results as compared to the available baselines for individual datasets on which independent models are trained. Results also suggest superior performance of the representations generated over Irony dataset.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>meisheri-khadilkar:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6245">
    <title>Super Characters: A Conversion from Sentiment Classification to Image Classification</title>
    <author><first>Baohua</first><last>Sun</last></author>
    <author><first>Lin</first><last>Yang</last></author>
    <author><first>Patrick</first><last>Dong</last></author>
    <author><first>Wenhan</first><last>Zhang</last></author>
    <author><first>Jason</first><last>Dong</last></author>
    <author><first>Charles</first><last>Young</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>309–315</pages>
    <url>http://aclweb.org/anthology/W18-6245</url>
    <abstract>We propose a method named Super Characters for sentiment classification. This method converts the sentiment classification problem into image classification problem by projecting texts into images and then applying CNN models for classification. Text features are extracted automatically from the generated Super Characters images, hence there is no need of any explicit step of embedding the words or characters into numerical vector representations. Experimental results on large social media corpus show that the Super Characters method consistently outperforms other methods for sentiment classification and topic classification tasks on ten large social media datasets of millions of contents in four different languages, including Chinese, Japanese,Korean and English.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>sun-EtAl:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6246">
    <title>Learning Comment Controversy Prediction in Web Discussions Using Incidentally Supervised Multi-Task CNNs</title>
    <author><first>Nils</first><last>Rethmeier</last></author>
    <author><first>Marc</first><last>Hübner</last></author>
    <author><first>Leonhard</first><last>Hennig</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>316–321</pages>
    <url>http://aclweb.org/anthology/W18-6246</url>
    <abstract>Comments on web news contain controversies that manifest as inter-group agreement-conflicts. Tracking such rapidly evolving controversy may be used to ease conflict resolution and author-user interaction.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rethmeier-hbner-hennig:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6247">
    <title>Words Worth: Verbal Content and Hirability Impressions in YouTube Video Resumes</title>
    <author><first>Skanda</first><last>Muralidhar</last></author>
    <author><first>Laurent</first><last>Nguyen</last></author>
    <author><first>Daniel</first><last>Gatica-Perez</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>322–327</pages>
    <url>http://aclweb.org/anthology/W18-6247</url>
    <abstract>Automatic hirability prediction from video resumes is gaining increasing attention in both psychology and computing. Most existing works have investigated hirability from the perspective of nonverbal behavior, with verbal content receiving little interest. In this study, we leverage the advances in deep-learning based text representation techniques (like word embedding) in natural language processing to investigate the relationship between verbal content and perceived hirability ratings. To this end, we use 292 conversational video resumes from YouTube, develop a computational framework to automatically extract various representations of verbal content, and evaluate them in a regression task. We obtain a best performance of R² = 0.23 using GloVe, and R² = 0.22 using Word2Vec representations for manual and automatically transcribed texts respectively. Our inference results indicate the feasibility of using deep learning based verbal content representation in inferring hirability scores from online conversational video resumes.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>muralidhar-nguyen-gaticaperez:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6248">
    <title>Predicting Adolescents’ Educational Track from Chat Messages on Dutch Social Media</title>
    <author><first>Lisa</first><last>Hilte</last></author>
    <author><first>Walter</first><last>Daelemans</last></author>
    <author><first>Reinhild</first><last>Vandekerckhove</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>328–334</pages>
    <url>http://aclweb.org/anthology/W18-6248</url>
    <abstract>We aim to predict Flemish adolescents’ educational track based on their Dutch social media writing. We distinguish between the three main types of Belgian secondary education: General (theory-oriented), Vocational (practice-oriented), and Technical Secondary Education (hybrid). The best results are obtained with a Naive Bayes model, i.e. an F-score of 0.68 (std. dev. 0.05) in 10-fold cross-validation experiments on the train data and an F-score of 0.60 on unseen data. Many of the most informative features are character n-grams containing specific occurrences of chatspeak phenomena such as emoticons. While the detection of the most theory- and practice-oriented educational tracks seems to be a relatively easy task, the hybrid Technical level appears to be much harder to capture based on online writing style, as expected.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>hilte-daelemans-vandekerckhove:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6249">
    <title>Arabizi sentiment analysis based on transliteration and automatic corpus annotation</title>
    <author><first>Imane</first><last>GUELLIL</last></author>
    <author><first>Ahsan</first><last>Adeel</last></author>
    <author><first>Faical</first><last>AZOUAOU</last></author>
    <author><first>fodil</first><last>benali</last></author>
    <author><first>Ala-eddine</first><last>Hachani</last></author>
    <author><first>Amir</first><last>Hussain</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>335–341</pages>
    <url>http://aclweb.org/anthology/W18-6249</url>
    <abstract>Arabizi is a form of writing Arabic text which relies on Latin letters, numerals and punctuation rather than Arabic letters. In the literature, the difficulties associated with Arabizi sentiment analysis have been underestimated, principally due to the complexity of Arabizi. In this paper, we present an approach to automatically classify sentiments of Arabizi messages into positives or negatives. In the proposed approach, Arabizi messages are first transliterated into Arabic. Afterwards, we automatically classify the sentiment of the transliterated corpus using an automatically annotated corpus. For corpus validation, shallow machine learning algorithms such as Support Vectors Machine (SVM) and Naive Bays (NB) are used. Simulations results demonstrate the outperformance of NB algorithm over all others. The highest achieved F1-score is up to 78% and 76% for manually and automatically transliterated dataset respectively. Ongoing work is aimed at improving the transliterator module and annotated sentiment dataset.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>guellil-EtAl:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6250">
    <title>UBC-NLP at IEST 2018: Learning Implicit Emotion With an Ensemble of Language Models</title>
    <author><first>Hassan</first><last>Alhuzali</last></author>
    <author><first>Mohamed</first><last>Elaraby</last></author>
    <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
    <booktitle>Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Brussels, Belgium</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>342–347</pages>
    <url>http://aclweb.org/anthology/W18-6250</url>
    <abstract>We describe UBC-NLP contribution to IEST-2018, focused at learning implicit emotion in Twitter data. Among the 30 participating teams, our system ranked the 4th (with 69.3% F-score). Post competition, we were able to score slightly higher than the 3rd ranking system (reaching 70.7%). Our system is trained on top of a pre-trained language model (LM),fine-tuned on the data provided by the task organizers. Our best results are acquired by an average of an ensemble of language models.We also offer an analysis of system performance and the impact of training data size on the task. For example, we show that training our best model for only one epoch with &lt; 40% of the data enables better performance than the baseline reported by Klinger et al. (2018) for the task.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>alhuzali-elaraby-abdulmageed:2018:WASSA2018</bibkey>
  </paper>

  <paper id="6300">
    <title>Proceedings of the Third Conference on Machine Translation: Research Papers</title>
    <editor><first>Ondřej</first><last>Bojar</last></editor>
    <editor><first>Rajen</first><last>Chatterjee</last></editor>
    <editor><first>Christian</first><last>Federmann</last></editor>
    <editor><first>Mark</first><last>Fishel</last></editor>
    <editor><first>Yvette</first><last>Graham</last></editor>
    <editor><first>Barry</first><last>Haddow</last></editor>
    <editor><first>Matthias</first><last>Huck</last></editor>
    <editor><first>Antonio Jimeno</first><last>Yepes</last></editor>
    <editor><first>Philipp</first><last>Koehn</last></editor>
    <editor><first>Christof</first><last>Monz</last></editor>
    <editor><first>Matteo</first><last>Negri</last></editor>
    <editor><first>Aurélie</first><last>Névéol</last></editor>
    <editor><first>Mariana</first><last>Neves</last></editor>
    <editor><first>Matt</first><last>Post</last></editor>
    <editor><first>Lucia</first><last>Specia</last></editor>
    <editor><first>Marco</first><last>Turchi</last></editor>
    <editor><first>Karin</first><last>Verspoor</last></editor>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-63</url>
    <bibtype>book</bibtype>
    <bibkey>WMT:2018</bibkey>
  </paper>

  <paper id="6301">
    <title>Scaling Neural Machine Translation</title>
    <author><first>Myle</first><last>Ott</last></author>
    <author><first>Sergey</first><last>Edunov</last></author>
    <author><first>David</first><last>Grangier</last></author>
    <author><first>Michael</first><last>Auli</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–9</pages>
    <url>http://www.aclweb.org/anthology/W18-6301</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>ott-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6302">
    <title>Character-level Chinese-English Translation through ASCII Encoding</title>
    <author><first>Nikola</first><last>Nikolov</last></author>
    <author><first>Yuhuang</first><last>Hu</last></author>
    <author><first>Mi Xue</first><last>Tan</last></author>
    <author><first>Richard H.R.</first><last>Hahnloser</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>10–16</pages>
    <url>http://www.aclweb.org/anthology/W18-6302</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>nikolov-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6303">
    <title>Neural Machine Translation of Logographic Language Using Sub-character Level Information</title>
    <author><first>Longtu</first><last>Zhang</last></author>
    <author><first>Mamoru</first><last>Komachi</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>17–25</pages>
    <url>http://www.aclweb.org/anthology/W18-6303</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>zhang-komachi:2018:WMT</bibkey>
  </paper>

  <paper id="6304">
    <title>An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation</title>
    <author><first>Gongbo</first><last>Tang</last></author>
    <author><first>Rico</first><last>Sennrich</last></author>
    <author><first>Joakim</first><last>Nivre</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>26–35</pages>
    <url>http://www.aclweb.org/anthology/W18-6304</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>tang-sennrich-nivre:2018:WMT</bibkey>
  </paper>

  <paper id="6305">
    <title>Discourse-Related Language Contrasts in English-Croatian Human and Machine Translation</title>
    <author><first>Margita</first><last>Šoštarić</last></author>
    <author><first>Christian</first><last>Hardmeier</last></author>
    <author><first>Sara</first><last>Stymne</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>36–48</pages>
    <url>http://www.aclweb.org/anthology/W18-6305</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>otari-hardmeier-stymne:2018:WMT</bibkey>
  </paper>

  <paper id="6306">
    <title>Coreference and Coherence in Neural Machine Translation: A Study Using Oracle Experiments</title>
    <author><first>Dario</first><last>Stojanovski</last></author>
    <author><first>Alexander</first><last>Fraser</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>49–60</pages>
    <url>http://www.aclweb.org/anthology/W18-6306</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>stojanovski-fraser:2018:WMT</bibkey>
  </paper>

  <paper id="6307">
    <title>A Large-Scale Test Set for the Evaluation of Context-Aware Pronoun Translation in Neural Machine Translation</title>
    <author><first>Mathias</first><last>Müller</last></author>
    <author><first>Annette</first><last>Rios</last></author>
    <author><first>Elena</first><last>Voita</last></author>
    <author><first>Rico</first><last>Sennrich</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>61–72</pages>
    <url>http://www.aclweb.org/anthology/W18-6307</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>mller-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6308">
    <title>Beyond Weight Tying: Learning Joint Input-Output Embeddings for Neural Machine Translation</title>
    <author><first>Nikolaos</first><last>Pappas</last></author>
    <author><first>Lesly</first><last>Miculicich</last></author>
    <author><first>James</first><last>Henderson</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>73–83</pages>
    <url>http://www.aclweb.org/anthology/W18-6308</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>pappas-miculicich-henderson:2018:WMT</bibkey>
  </paper>

  <paper id="6309">
    <title>A neural interlingua for multilingual machine translation</title>
    <author><first>Yichao</first><last>Lu</last></author>
    <author><first>Phillip</first><last>Keung</last></author>
    <author><first>Faisal</first><last>Ladhak</last></author>
    <author><first>Vikas</first><last>Bhardwaj</last></author>
    <author><first>Shaonan</first><last>Zhang</last></author>
    <author><first>Jason</first><last>Sun</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>84–92</pages>
    <url>http://www.aclweb.org/anthology/W18-6309</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>lu-EtAl:2018:WMT1</bibkey>
  </paper>

  <paper id="6310">
    <title>Improving Neural Language Models with Weight Norm Initialization and Regularization</title>
    <author><first>Christian</first><last>Herold</last></author>
    <author><first>Yingbo</first><last>Gao</last></author>
    <author><first>Hermann</first><last>Ney</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>93–100</pages>
    <url>http://www.aclweb.org/anthology/W18-6310</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>herold-gao-ney:2018:WMT</bibkey>
  </paper>

  <paper id="6311">
    <title>Contextual Neural Model for Translating Bilingual Multi-Speaker Conversations</title>
    <author><first>Sameen</first><last>Maruf</last></author>
    <author><first>André F. T.</first><last>Martins</last></author>
    <author><first>Gholamreza</first><last>Haffari</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>101–112</pages>
    <url>http://www.aclweb.org/anthology/W18-6311</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>maruf-martins-haffari:2018:WMT</bibkey>
  </paper>

  <paper id="6312">
    <title>Attaining the Unattainable? Reassessing Claims of Human Parity in Neural Machine Translation</title>
    <author><first>Antonio</first><last>Toral</last></author>
    <author><first>Sheila</first><last>Castilho</last></author>
    <author><first>Ke</first><last>Hu</last></author>
    <author><first>Andy</first><last>Way</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>113–123</pages>
    <url>http://www.aclweb.org/anthology/W18-6312</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>toral-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6313">
    <title>Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation</title>
    <author><first>Brian</first><last>Thompson</last></author>
    <author><first>Huda</first><last>Khayrallah</last></author>
    <author><first>Antonios</first><last>Anastasopoulos</last></author>
    <author><first>Arya D.</first><last>McCarthy</last></author>
    <author><first>Kevin</first><last>Duh</last></author>
    <author><first>Rebecca</first><last>Marvin</last></author>
    <author><first>Paul</first><last>McNamee</last></author>
    <author><first>Jeremy</first><last>Gwinnup</last></author>
    <author><first>Tim</first><last>Anderson</last></author>
    <author><first>Philipp</first><last>Koehn</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>124–132</pages>
    <url>http://www.aclweb.org/anthology/W18-6313</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>thompson-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6314">
    <title>Denoising Neural Machine Translation Training with Trusted Data and Online Data Selection</title>
    <author><first>Wei</first><last>Wang</last></author>
    <author><first>Taro</first><last>Watanabe</last></author>
    <author><first>Macduff</first><last>Hughes</last></author>
    <author><first>Tetsuji</first><last>Nakagawa</last></author>
    <author><first>Ciprian</first><last>Chelba</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>133–143</pages>
    <url>http://www.aclweb.org/anthology/W18-6314</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>wang-EtAl:2018:WMT1</bibkey>
  </paper>

  <paper id="6315">
    <title>Using Monolingual Data in Neural Machine Translation: a Systematic Study</title>
    <author><first>Franck</first><last>Burlot</last></author>
    <author><first>François</first><last>Yvon</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>144–155</pages>
    <url>http://www.aclweb.org/anthology/W18-6315</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>burlot-yvon:2018:WMT</bibkey>
  </paper>

  <paper id="6316">
    <title>Neural Machine Translation into Language Varieties</title>
    <author><first>Surafel Melaku</first><last>Lakew</last></author>
    <author><first>Aliia</first><last>Erofeeva</last></author>
    <author><first>Marcello</first><last>Federico</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>156–164</pages>
    <url>http://www.aclweb.org/anthology/W18-6316</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>lakew-erofeeva-federico:2018:WMT</bibkey>
  </paper>

  <paper id="6317">
    <title>Effective Parallel Corpus Mining using Bilingual Sentence Embeddings</title>
    <author><first>Mandy</first><last>Guo</last></author>
    <author><first>Qinlan</first><last>Shen</last></author>
    <author><first>Yinfei</first><last>Yang</last></author>
    <author><first>Heming</first><last>Ge</last></author>
    <author><first>Daniel</first><last>Cer</last></author>
    <author><first>Gustavo</first><last>Hernandez Abrego</last></author>
    <author><first>Keith</first><last>Stevens</last></author>
    <author><first>Noah</first><last>Constant</last></author>
    <author><first>Yun-hsuan</first><last>Sung</last></author>
    <author><first>Brian</first><last>Strope</last></author>
    <author><first>Ray</first><last>Kurzweil</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>165–176</pages>
    <url>http://www.aclweb.org/anthology/W18-6317</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>guo-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6318">
    <title>On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation</title>
    <author><first>Tamer</first><last>Alkhouli</last></author>
    <author><first>Gabriel</first><last>Bretschner</last></author>
    <author><first>Hermann</first><last>Ney</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>177–185</pages>
    <url>http://www.aclweb.org/anthology/W18-6318</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>alkhouli-bretschner-ney:2018:WMT</bibkey>
  </paper>

  <paper id="6319">
    <title>A Call for Clarity in Reporting BLEU Scores</title>
    <author><first>Matt</first><last>Post</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>186–191</pages>
    <url>http://www.aclweb.org/anthology/W18-6319</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>post:2018:WMT</bibkey>
  </paper>

  <paper id="6320">
    <title>Exploring gap filling as a cheaper alternative to reading comprehension questionnaires when evaluating machine translation for gisting</title>
    <author><first>Mikel L.</first><last>Forcada</last></author>
    <author><first>Carolina</first><last>Scarton</last></author>
    <author><first>Lucia</first><last>Specia</last></author>
    <author><first>Barry</first><last>Haddow</last></author>
    <author><first>Alexandra</first><last>Birch</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>192–203</pages>
    <url>http://www.aclweb.org/anthology/W18-6320</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>forcada-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6321">
    <title>Simple Fusion: Return of the Language Model</title>
    <author><first>Felix</first><last>Stahlberg</last></author>
    <author><first>James</first><last>Cross</last></author>
    <author><first>Veselin</first><last>Stoyanov</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>204–211</pages>
    <url>http://www.aclweb.org/anthology/W18-6321</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>stahlberg-cross-stoyanov:2018:WMT</bibkey>
  </paper>

  <paper id="6322">
    <title>Correcting Length Bias in Neural Machine Translation</title>
    <author><first>Kenton</first><last>Murray</last></author>
    <author><first>David</first><last>Chiang</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>212–223</pages>
    <url>http://www.aclweb.org/anthology/W18-6322</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>murray-chiang:2018:WMT</bibkey>
  </paper>

  <paper id="6323">
    <title>Extracting In-domain Training Corpora for Neural Machine Translation Using Data Selection Methods</title>
    <author><first>Catarina Cruz</first><last>Silva</last></author>
    <author><first>Chao-Hong</first><last>Liu</last></author>
    <author><first>Alberto</first><last>Poncelas</last></author>
    <author><first>Andy</first><last>Way</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>224–231</pages>
    <url>http://www.aclweb.org/anthology/W18-6323</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>silva-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6324">
    <title>Massively Parallel Cross-Lingual Learning in Low-Resource Target Language Translation</title>
    <author><first>Zhong</first><last>Zhou</last></author>
    <author><first>Matthias</first><last>Sperber</last></author>
    <author><first>Alexander</first><last>Waibel</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>232–243</pages>
    <url>http://www.aclweb.org/anthology/W18-6324</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>zhou-sperber-waibel:2018:WMT</bibkey>
  </paper>

  <paper id="6325">
    <title>Trivial Transfer Learning for Low-Resource Neural Machine Translation</title>
    <author><first>Tom</first><last>Kocmi</last></author>
    <author><first>Ondřej</first><last>Bojar</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>244–252</pages>
    <url>http://www.aclweb.org/anthology/W18-6325</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>kocmi-bojar:2018:WMT</bibkey>
  </paper>

  <paper id="6326">
    <title>Input Combination Strategies for Multi-Source Transformer Decoder</title>
    <author><first>Jindřich</first><last>Libovický</last></author>
    <author><first>Jindřich</first><last>Helcl</last></author>
    <author><first>David</first><last>Mareček</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>253–260</pages>
    <url>http://www.aclweb.org/anthology/W18-6326</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>libovick-helcl-mareek:2018:WMT</bibkey>
  </paper>

  <paper id="6327">
    <title>Parameter Sharing Methods for Multilingual Self-Attentional Translation Models</title>
    <author><first>Devendra</first><last>Sachan</last></author>
    <author><first>Graham</first><last>Neubig</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Research Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>261–271</pages>
    <url>http://www.aclweb.org/anthology/W18-6327</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>sachan-neubig:2018:WMT</bibkey>
  </paper>

  <paper id="6400">
    <title>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
    <editor><first>Ondřej</first><last>Bojar</last></editor>
    <editor><first>Rajen</first><last>Chatterjee</last></editor>
    <editor><first>Christian</first><last>Federmann</last></editor>
    <editor><first>Mark</first><last>Fishel</last></editor>
    <editor><first>Yvette</first><last>Graham</last></editor>
    <editor><first>Barry</first><last>Haddow</last></editor>
    <editor><first>Matthias</first><last>Huck</last></editor>
    <editor><first>Antonio Jimeno</first><last>Yepes</last></editor>
    <editor><first>Philipp</first><last>Koehn</last></editor>
    <editor><first>Christof</first><last>Monz</last></editor>
    <editor><first>Matteo</first><last>Negri</last></editor>
    <editor><first>Aurélie</first><last>Névéol</last></editor>
    <editor><first>Mariana</first><last>Neves</last></editor>
    <editor><first>Matt</first><last>Post</last></editor>
    <editor><first>Lucia</first><last>Specia</last></editor>
    <editor><first>Marco</first><last>Turchi</last></editor>
    <editor><first>Karin</first><last>Verspoor</last></editor>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-64</url>
    <bibtype>book</bibtype>
    <bibkey>WMT:2018</bibkey>
  </paper>

  <paper id="6401">
    <title>Findings of the 2018 Conference on Machine Translation (WMT18)</title>
    <author><first>Ondřej</first><last>Bojar</last></author>
    <author><first>Christian</first><last>Federmann</last></author>
    <author><first>Mark</first><last>Fishel</last></author>
    <author><first>Yvette</first><last>Graham</last></author>
    <author><first>Barry</first><last>Haddow</last></author>
    <author><first>Philipp</first><last>Koehn</last></author>
    <author><first>Christof</first><last>Monz</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>272–303</pages>
    <url>http://www.aclweb.org/anthology/W18-6401</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>bojar-EtAl:2018:WMT1</bibkey>
  </paper>

  <paper id="6402">
    <title>Findings of the Third Shared Task on Multimodal Machine Translation</title>
    <author><first>Loïc</first><last>Barrault</last></author>
    <author><first>Fethi</first><last>Bougares</last></author>
    <author><first>Lucia</first><last>Specia</last></author>
    <author><first>Chiraag</first><last>Lala</last></author>
    <author><first>Desmond</first><last>Elliott</last></author>
    <author><first>Stella</first><last>Frank</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>304–323</pages>
    <url>http://www.aclweb.org/anthology/W18-6402</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>barrault-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6403">
    <title>Findings of the WMT 2018 Biomedical Translation Shared Task: Evaluation on Medline test sets</title>
    <author><first>Mariana</first><last>Neves</last></author>
    <author><first>Antonio</first><last>Jimeno Yepes</last></author>
    <author><first>Aurélie</first><last>Névéol</last></author>
    <author><first>Cristian</first><last>Grozea</last></author>
    <author><first>Amy</first><last>Siu</last></author>
    <author><first>Madeleine</first><last>Kittner</last></author>
    <author><first>Karin</first><last>Verspoor</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>324–339</pages>
    <url>http://www.aclweb.org/anthology/W18-6403</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>neves-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6404">
    <title>An Empirical Study of Machine Translation for the Shared Task of WMT18</title>
    <author><first>Chao</first><last>Bei</last></author>
    <author><first>Hao</first><last>Zong</last></author>
    <author><first>Yiming</first><last>Wang</last></author>
    <author><first>Baoyong</first><last>Fan</last></author>
    <author><first>Shiqi</first><last>Li</last></author>
    <author><first>Conghu</first><last>Yuan</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>340–344</pages>
    <url>http://www.aclweb.org/anthology/W18-6404</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>bei-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6405">
    <title>Robust parfda Statistical Machine Translation Results</title>
    <author><first>Ergun</first><last>Biçici</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>345–354</pages>
    <url>http://www.aclweb.org/anthology/W18-6405</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>biici:2018:WMT1</bibkey>
  </paper>

  <paper id="6406">
    <title>The TALP-UPC Machine Translation Systems for WMT18 News Shared Translation Task</title>
    <author><first>Noe</first><last>Casas</last></author>
    <author><first>Carlos</first><last>Escolano</last></author>
    <author><first>Marta R.</first><last>Costa-jussà</last></author>
    <author><first>José A. R.</first><last>Fonollosa</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>355–360</pages>
    <url>http://www.aclweb.org/anthology/W18-6406</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>casas-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6407">
    <title>Phrase-based Unsupervised Machine Translation with Compositional Phrase Embeddings</title>
    <author><first>Maksym</first><last>Del</last></author>
    <author><first>Andre</first><last>Tättar</last></author>
    <author><first>Mark</first><last>Fishel</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>361–367</pages>
    <url>http://www.aclweb.org/anthology/W18-6407</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>del-tttar-fishel:2018:WMT</bibkey>
  </paper>

  <paper id="6408">
    <title>Alibaba’s Neural Machine Translation Systems for WMT18</title>
    <author><first>Yongchao</first><last>Deng</last></author>
    <author><first>Shanbo</first><last>Cheng</last></author>
    <author><first>Jun</first><last>Lu</last></author>
    <author><first>Kai</first><last>Song</last></author>
    <author><first>Jingang</first><last>Wang</last></author>
    <author><first>Shenglan</first><last>Wu</last></author>
    <author><first>Liang</first><last>Yao</last></author>
    <author><first>Guchun</first><last>Zhang</last></author>
    <author><first>Haibo</first><last>Zhang</last></author>
    <author><first>Pei</first><last>Zhang</last></author>
    <author><first>Changfeng</first><last>Zhu</last></author>
    <author><first>Boxing</first><last>Chen</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>368–376</pages>
    <url>http://www.aclweb.org/anthology/W18-6408</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>deng-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6409">
    <title>The RWTH Aachen University English-German and German-English Unsupervised Neural Machine Translation Systems for WMT 2018</title>
    <author><first>Miguel</first><last>Graça</last></author>
    <author><first>Yunsu</first><last>Kim</last></author>
    <author><first>Julian</first><last>Schamper</last></author>
    <author><first>Jiahui</first><last>Geng</last></author>
    <author><first>Hermann</first><last>Ney</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>377–385</pages>
    <url>http://www.aclweb.org/anthology/W18-6409</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>graa-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6410">
    <title>Cognate-aware morphological segmentation for multilingual neural translation</title>
    <author><first>Stig-Arne</first><last>Grönroos</last></author>
    <author><first>Sami</first><last>Virpioja</last></author>
    <author><first>Mikko</first><last>Kurimo</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>386–393</pages>
    <url>http://www.aclweb.org/anthology/W18-6410</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>grnroos-virpioja-kurimo:2018:WMT</bibkey>
  </paper>

  <paper id="6411">
    <title>The AFRL WMT18 Systems: Ensembling, Continuation and Combination</title>
    <author><first>Jeremy</first><last>Gwinnup</last></author>
    <author><first>Tim</first><last>Anderson</last></author>
    <author><first>Grant</first><last>Erdmann</last></author>
    <author><first>Katherine</first><last>Young</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>394–398</pages>
    <url>http://www.aclweb.org/anthology/W18-6411</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>gwinnup-EtAl:2018:WMT1</bibkey>
  </paper>

  <paper id="6412">
    <title>The University of Edinburgh’s Submissions to the WMT18 News Translation Task</title>
    <author><first>Barry</first><last>Haddow</last></author>
    <author><first>Nikolay</first><last>Bogoychev</last></author>
    <author><first>Denis</first><last>Emelin</last></author>
    <author><first>Ulrich</first><last>Germann</last></author>
    <author><first>Roman</first><last>Grundkiewicz</last></author>
    <author><first>Kenneth</first><last>Heafield</last></author>
    <author><first>Antonio Valerio</first><last>Miceli Barone</last></author>
    <author><first>Rico</first><last>Sennrich</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>399–409</pages>
    <url>http://www.aclweb.org/anthology/W18-6412</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>haddow-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6413">
    <title>TencentFmRD Neural Machine Translation for WMT18</title>
    <author><first>Bojie</first><last>Hu</last></author>
    <author><first>Ambyer</first><last>Han</last></author>
    <author><first>Shen</first><last>Huang</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>410–417</pages>
    <url>http://www.aclweb.org/anthology/W18-6413</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>hu-han-huang:2018:WMT</bibkey>
  </paper>

  <paper id="6414">
    <title>The MLLP-UPV German-English Machine Translation System for WMT18</title>
    <author><first>Javier</first><last>Iranzo-Sánchez</last></author>
    <author><first>Pau</first><last>Baquero-Arnal</last></author>
    <author><first>Gonçal V.</first><last>Garcés Díaz-Munío</last></author>
    <author><first>Adrià</first><last>Martínez-Villaronga</last></author>
    <author><first>Jorge</first><last>Civera</last></author>
    <author><first>Alfons</first><last>Juan</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>418–424</pages>
    <url>http://www.aclweb.org/anthology/W18-6414</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>iranzosnchez-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6415">
    <title>Microsoft’s Submission to the WMT2018 News Translation Task: How I Learned to Stop Worrying and Love the Data</title>
    <author><first>Marcin</first><last>Junczys-Dowmunt</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>425–430</pages>
    <url>http://www.aclweb.org/anthology/W18-6415</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>junczysdowmunt:2018:WMT1</bibkey>
  </paper>

  <paper id="6416">
    <title>CUNI Submissions in WMT18</title>
    <author><first>Tom</first><last>Kocmi</last></author>
    <author><first>Roman</first><last>Sudarikov</last></author>
    <author><first>Ondřej</first><last>Bojar</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>431–437</pages>
    <url>http://www.aclweb.org/anthology/W18-6416</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>kocmi-sudarikov-bojar:2018:WMT</bibkey>
  </paper>

  <paper id="6417">
    <title>The JHU Machine Translation Systems for WMT 2018</title>
    <author><first>Philipp</first><last>Koehn</last></author>
    <author><first>Kevin</first><last>Duh</last></author>
    <author><first>Brian</first><last>Thompson</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>438–444</pages>
    <url>http://www.aclweb.org/anthology/W18-6417</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>koehn-duh-thompson:2018:WMT</bibkey>
  </paper>

  <paper id="6418">
    <title>JUCBNMT at WMT2018 News Translation Task: Character Based Neural Machine Translation of Finnish to English</title>
    <author><first>Sainik Kumar</first><last>Mahata</last></author>
    <author><first>Dipankar</first><last>Das</last></author>
    <author><first>Sivaji</first><last>Bandyopadhyay</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>445–448</pages>
    <url>http://www.aclweb.org/anthology/W18-6418</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>mahata-das-bandyopadhyay:2018:WMT</bibkey>
  </paper>

  <paper id="6419">
    <title>NICT’s Neural and Statistical Machine Translation Systems for the WMT18 News Translation Task</title>
    <author><first>Benjamin</first><last>Marie</last></author>
    <author><first>Rui</first><last>Wang</last></author>
    <author><first>Atsushi</first><last>Fujita</last></author>
    <author><first>Masao</first><last>Utiyama</last></author>
    <author><first>Eiichiro</first><last>Sumita</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>449–455</pages>
    <url>http://www.aclweb.org/anthology/W18-6419</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>marie-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6420">
    <title>PROMT Systems for WMT 2018 Shared Translation Task</title>
    <author><first>Alexander</first><last>Molchanov</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>456–460</pages>
    <url>http://www.aclweb.org/anthology/W18-6420</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>molchanov:2018:WMT</bibkey>
  </paper>

  <paper id="6421">
    <title>NTT’s Neural Machine Translation Systems for WMT 2018</title>
    <author><first>Makoto</first><last>Morishita</last></author>
    <author><first>Jun</first><last>Suzuki</last></author>
    <author><first>Masaaki</first><last>Nagata</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>461–466</pages>
    <url>http://www.aclweb.org/anthology/W18-6421</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>morishita-suzuki-nagata:2018:WMT</bibkey>
  </paper>

  <paper id="6422">
    <title>The Karlsruhe Institute of Technology Systems for the News Translation Task in WMT 2018</title>
    <author><first>Ngoc-Quan</first><last>Pham</last></author>
    <author><first>Jan</first><last>Niehues</last></author>
    <author><first>Alexander</first><last>Waibel</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>467–472</pages>
    <url>http://www.aclweb.org/anthology/W18-6422</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>pham-niehues-waibel:2018:WMT</bibkey>
  </paper>

  <paper id="6423">
    <title>Tilde’s Machine Translation Systems for WMT 2018</title>
    <author><first>Marcis</first><last>Pinnis</last></author>
    <author><first>Matiss</first><last>Rikters</last></author>
    <author><first>Rihards</first><last>Krišlauks</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>473–481</pages>
    <url>http://www.aclweb.org/anthology/W18-6423</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>pinnis-rikters-krilauks:2018:WMT</bibkey>
  </paper>

  <paper id="6424">
    <title>CUNI Transformer Neural MT System for WMT18</title>
    <author><first>Martin</first><last>Popel</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>482–487</pages>
    <url>http://www.aclweb.org/anthology/W18-6424</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>popel:2018:WMT</bibkey>
  </paper>

  <paper id="6425">
    <title>The University of Helsinki submissions to the WMT18 news task</title>
    <author><first>Alessandro</first><last>Raganato</last></author>
    <author><first>Yves</first><last>Scherrer</last></author>
    <author><first>Tommi</first><last>Nieminen</last></author>
    <author><first>Arvi</first><last>Hurskainen</last></author>
    <author><first>Jörg</first><last>Tiedemann</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>488–495</pages>
    <url>http://www.aclweb.org/anthology/W18-6425</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>raganato-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6426">
    <title>The RWTH Aachen University Supervised Machine Translation Systems for WMT 2018</title>
    <author><first>Julian</first><last>Schamper</last></author>
    <author><first>Jan</first><last>Rosendahl</last></author>
    <author><first>Parnia</first><last>Bahar</last></author>
    <author><first>Yunsu</first><last>Kim</last></author>
    <author><first>Arne</first><last>Nix</last></author>
    <author><first>Hermann</first><last>Ney</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>496–503</pages>
    <url>http://www.aclweb.org/anthology/W18-6426</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>schamper-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6427">
    <title>The University of Cambridge’s Machine Translation Systems for WMT18</title>
    <author><first>Felix</first><last>Stahlberg</last></author>
    <author><first>Adrià</first><last>de Gispert</last></author>
    <author><first>Bill</first><last>Byrne</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>504–512</pages>
    <url>http://www.aclweb.org/anthology/W18-6427</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>stahlberg-degispert-byrne:2018:WMT</bibkey>
  </paper>

  <paper id="6428">
    <title>The LMU Munich Unsupervised Machine Translation Systems</title>
    <author><first>Dario</first><last>Stojanovski</last></author>
    <author><first>Viktor</first><last>Hangya</last></author>
    <author><first>Matthias</first><last>Huck</last></author>
    <author><first>Alexander</first><last>Fraser</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>513–521</pages>
    <url>http://www.aclweb.org/anthology/W18-6428</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>stojanovski-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6429">
    <title>Tencent Neural Machine Translation Systems for WMT18</title>
    <author><first>Mingxuan</first><last>Wang</last></author>
    <author><first>Li</first><last>Gong</last></author>
    <author><first>Wenhuan</first><last>Zhu</last></author>
    <author><first>Jun</first><last>Xie</last></author>
    <author><first>Chao</first><last>Bian</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>522–527</pages>
    <url>http://www.aclweb.org/anthology/W18-6429</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>wang-EtAl:2018:WMT2</bibkey>
  </paper>

  <paper id="6430">
    <title>The NiuTrans Machine Translation System for WMT18</title>
    <author><first>Qiang</first><last>Wang</last></author>
    <author><first>Bei</first><last>Li</last></author>
    <author><first>Jiqiang</first><last>Liu</last></author>
    <author><first>Bojian</first><last>Jiang</last></author>
    <author><first>Zheyang</first><last>Zhang</last></author>
    <author><first>Yinqiao</first><last>Li</last></author>
    <author><first>Ye</first><last>Lin</last></author>
    <author><first>Tong</first><last>Xiao</last></author>
    <author><first>Jingbo</first><last>Zhu</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>528–534</pages>
    <url>http://www.aclweb.org/anthology/W18-6430</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>wang-EtAl:2018:WMT3</bibkey>
  </paper>

  <paper id="6431">
    <title>The University of Maryland’s Chinese-English Neural Machine Translation Systems at WMT18</title>
    <author><first>Weijia</first><last>Xu</last></author>
    <author><first>Marine</first><last>Carpuat</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>535–540</pages>
    <url>http://www.aclweb.org/anthology/W18-6431</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>xu-carpuat:2018:WMT</bibkey>
  </paper>

  <paper id="6432">
    <title>EvalD Reference-Less Discourse Evaluation for WMT18</title>
    <author><first>Ondřej</first><last>Bojar</last></author>
    <author><first>Jiří</first><last>Mírovský</last></author>
    <author><first>Kateřina</first><last>Rysová</last></author>
    <author><first>Magdaléna</first><last>Rysová</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>541–545</pages>
    <url>http://www.aclweb.org/anthology/W18-6432</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>bojar-EtAl:2018:WMT2</bibkey>
  </paper>

  <paper id="6433">
    <title>The WMT’18 Morpheval test suites for English-Czech, English-German, English-Finnish and Turkish-English</title>
    <author><first>Franck</first><last>Burlot</last></author>
    <author><first>Yves</first><last>Scherrer</last></author>
    <author><first>Vinit</first><last>Ravishankar</last></author>
    <author><first>Ondřej</first><last>Bojar</last></author>
    <author><first>Stig-Arne</first><last>Grönroos</last></author>
    <author><first>Maarit</first><last>Koponen</last></author>
    <author><first>Tommi</first><last>Nieminen</last></author>
    <author><first>François</first><last>Yvon</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>546–560</pages>
    <url>http://www.aclweb.org/anthology/W18-6433</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>burlot-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6434">
    <title>Testsuite on Czech–English Grammatical Contrasts</title>
    <author><first>Silvie</first><last>Cinkova</last></author>
    <author><first>Ondřej</first><last>Bojar</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>561–569</pages>
    <url>http://www.aclweb.org/anthology/W18-6434</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>cinkova-bojar:2018:WMT</bibkey>
  </paper>

  <paper id="6435">
    <title>A Pronoun Test Suite Evaluation of the English–German MT Systems at WMT 2018</title>
    <author><first>Liane</first><last>Guillou</last></author>
    <author><first>Christian</first><last>Hardmeier</last></author>
    <author><first>Ekaterina</first><last>Lapshinova-Koltunski</last></author>
    <author><first>Sharid</first><last>Loáiciga</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>570–577</pages>
    <url>http://www.aclweb.org/anthology/W18-6435</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>guillou-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6436">
    <title>Fine-grained evaluation of German-English Machine Translation based on a Test Suite</title>
    <author><first>Vivien</first><last>Macketanz</last></author>
    <author><first>Eleftherios</first><last>Avramidis</last></author>
    <author><first>Aljoscha</first><last>Burchardt</last></author>
    <author><first>Hans</first><last>Uszkoreit</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>578–587</pages>
    <url>http://www.aclweb.org/anthology/W18-6436</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>macketanz-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6437">
    <title>The Word Sense Disambiguation Test Suite at WMT18</title>
    <author><first>Annette</first><last>Rios</last></author>
    <author><first>Mathias</first><last>Müller</last></author>
    <author><first>Rico</first><last>Sennrich</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>588–596</pages>
    <url>http://www.aclweb.org/anthology/W18-6437</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>rios-mller-sennrich:2018:WMT</bibkey>
  </paper>

  <paper id="6438">
    <title>LIUM-CVC Submissions for WMT18 Multimodal Translation Task</title>
    <author><first>Ozan</first><last>Caglayan</last></author>
    <author><first>Adrien</first><last>Bardet</last></author>
    <author><first>Fethi</first><last>Bougares</last></author>
    <author><first>Loïc</first><last>Barrault</last></author>
    <author><first>Kai</first><last>Wang</last></author>
    <author><first>Marc</first><last>Masana</last></author>
    <author><first>Luis</first><last>Herranz</last></author>
    <author><first>Joost</first><last>van de Weijer</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>597–602</pages>
    <url>http://www.aclweb.org/anthology/W18-6438</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>caglayan-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6439">
    <title>The MeMAD Submission to the WMT18 Multimodal Translation Task</title>
    <author><first>Stig-Arne</first><last>Grönroos</last></author>
    <author><first>Benoit</first><last>Huet</last></author>
    <author><first>Mikko</first><last>Kurimo</last></author>
    <author><first>Jorma</first><last>Laaksonen</last></author>
    <author><first>Bernard</first><last>Merialdo</last></author>
    <author><first>Phu</first><last>Pham</last></author>
    <author><first>Mats</first><last>Sjöberg</last></author>
    <author><first>Umut</first><last>Sulubacak</last></author>
    <author><first>Jörg</first><last>Tiedemann</last></author>
    <author><first>Raphael</first><last>Troncy</last></author>
    <author><first>Raúl</first><last>Vázquez</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>603–611</pages>
    <url>http://www.aclweb.org/anthology/W18-6439</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>grnroos-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6440">
    <title>The AFRL-Ohio State WMT18 Multimodal System: Combining Visual with Traditional</title>
    <author><first>Jeremy</first><last>Gwinnup</last></author>
    <author><first>Joshua</first><last>Sandvick</last></author>
    <author><first>Michael</first><last>Hutt</last></author>
    <author><first>Grant</first><last>Erdmann</last></author>
    <author><first>John</first><last>Duselis</last></author>
    <author><first>James</first><last>Davis</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>612–615</pages>
    <url>http://www.aclweb.org/anthology/W18-6440</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>gwinnup-EtAl:2018:WMT2</bibkey>
  </paper>

  <paper id="6441">
    <title>CUNI System for the WMT18 Multimodal Translation Task</title>
    <author><first>Jindřich</first><last>Helcl</last></author>
    <author><first>Jindřich</first><last>Libovický</last></author>
    <author><first>Dusan</first><last>Varis</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>616–623</pages>
    <url>http://www.aclweb.org/anthology/W18-6441</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>helcl-libovick-varis:2018:WMT</bibkey>
  </paper>

  <paper id="6442">
    <title>Sheffield Submissions for WMT18 Multimodal Translation Shared Task</title>
    <author><first>Chiraag</first><last>Lala</last></author>
    <author><first>Pranava Swaroop</first><last>Madhyastha</last></author>
    <author><first>Carolina</first><last>Scarton</last></author>
    <author><first>Lucia</first><last>Specia</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>624–631</pages>
    <url>http://www.aclweb.org/anthology/W18-6442</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>lala-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6443">
    <title>Ensemble Sequence Level Training for Multimodal MT: OSU-Baidu WMT18 Multimodal Machine Translation System Report</title>
    <author><first>Renjie</first><last>Zheng</last></author>
    <author><first>Yilin</first><last>Yang</last></author>
    <author><first>Mingbo</first><last>Ma</last></author>
    <author><first>Liang</first><last>Huang</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>632–636</pages>
    <url>http://www.aclweb.org/anthology/W18-6443</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>zheng-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6444">
    <title>Translation of Biomedical Documents with Focus on Spanish-English</title>
    <author><first>Mirela-Stefania</first><last>Duma</last></author>
    <author><first>Wolfgang</first><last>Menzel</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>637–643</pages>
    <url>http://www.aclweb.org/anthology/W18-6444</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>duma-menzel:2018:WMT1</bibkey>
  </paper>

  <paper id="6445">
    <title>Ensemble of Translators with Automatic Selection of the Best Translation – the submission of FOKUS to the WMT 18 biomedical translation task –</title>
    <author><first>Cristian</first><last>Grozea</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>644–647</pages>
    <url>http://www.aclweb.org/anthology/W18-6445</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>grozea:2018:WMT</bibkey>
  </paper>

  <paper id="6446">
    <title>LMU Munich’s Neural Machine Translation Systems at WMT 2018</title>
    <author><first>Matthias</first><last>Huck</last></author>
    <author><first>Dario</first><last>Stojanovski</last></author>
    <author><first>Viktor</first><last>Hangya</last></author>
    <author><first>Alexander</first><last>Fraser</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>648–654</pages>
    <url>http://www.aclweb.org/anthology/W18-6446</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>huck-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6447">
    <title>Hunter NMT System for WMT18 Biomedical Translation Task: Transfer Learning in Neural Machine Translation</title>
    <author><first>Abdul</first><last>Khan</last></author>
    <author><first>Subhadarshi</first><last>Panda</last></author>
    <author><first>Jia</first><last>Xu</last></author>
    <author><first>Lampros</first><last>Flokas</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>655–661</pages>
    <url>http://www.aclweb.org/anthology/W18-6447</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>khan-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6448">
    <title>UFRGS Participation on the WMT Biomedical Translation Shared Task</title>
    <author><first>Felipe</first><last>Soares</last></author>
    <author><first>Karin</first><last>Becker</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>662–666</pages>
    <url>http://www.aclweb.org/anthology/W18-6448</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>soares-becker:2018:WMT</bibkey>
  </paper>

  <paper id="6449">
    <title>Neural Machine Translation with the Transformer and Multi-Source Romance Languages for the Biomedical WMT 2018 task</title>
    <author><first>Brian</first><last>Tubay</last></author>
    <author><first>Marta R.</first><last>Costa-jussà</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>667–670</pages>
    <url>http://www.aclweb.org/anthology/W18-6449</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>tubay-costajuss:2018:WMT</bibkey>
  </paper>

  <paper id="6450">
    <title>Results of the WMT18 Metrics Shared Task: Both characters and embeddings achieve good performance</title>
    <author><first>Qingsong</first><last>Ma</last></author>
    <author><first>Ondřej</first><last>Bojar</last></author>
    <author><first>Yvette</first><last>Graham</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>671–688</pages>
    <url>http://www.aclweb.org/anthology/W18-6450</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>ma-bojar-graham:2018:WMT</bibkey>
  </paper>

  <paper id="6451">
    <title>Findings of the WMT 2018 Shared Task on Quality Estimation</title>
    <author><first>Lucia</first><last>Specia</last></author>
    <author><first>Frédéric</first><last>Blain</last></author>
    <author><first>Varvara</first><last>Logacheva</last></author>
    <author><first>Ramón</first><last>Astudillo</last></author>
    <author><first>André F. T.</first><last>Martins</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>689–709</pages>
    <url>http://www.aclweb.org/anthology/W18-6451</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>specia-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6452">
    <title>Findings of the WMT 2018 Shared Task on Automatic Post-Editing</title>
    <author><first>Rajen</first><last>Chatterjee</last></author>
    <author><first>Matteo</first><last>Negri</last></author>
    <author><first>Raphael</first><last>Rubino</last></author>
    <author><first>Marco</first><last>Turchi</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>710–725</pages>
    <url>http://www.aclweb.org/anthology/W18-6452</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>chatterjee-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6453">
    <title>Findings of the WMT 2018 Shared Task on Parallel Corpus Filtering</title>
    <author><first>Philipp</first><last>Koehn</last></author>
    <author><first>Huda</first><last>Khayrallah</last></author>
    <author><first>Kenneth</first><last>Heafield</last></author>
    <author><first>Mikel L.</first><last>Forcada</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>726–739</pages>
    <url>http://www.aclweb.org/anthology/W18-6453</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>koehn-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6454">
    <title>Meteor++: Incorporating Copy Knowledge into Machine Translation Evaluation</title>
    <author><first>Yinuo</first><last>Guo</last></author>
    <author><first>Chong</first><last>Ruan</last></author>
    <author><first>Junfeng</first><last>Hu</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>740–745</pages>
    <url>http://www.aclweb.org/anthology/W18-6454</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>guo-ruan-hu:2018:WMT</bibkey>
  </paper>

  <paper id="6455">
    <title>ITER: Improving Translation Edit Rate through Optimizable Edit Costs</title>
    <author><first>Joybrata</first><last>Panja</last></author>
    <author><first>Sudip Kumar</first><last>Naskar</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>746–750</pages>
    <url>http://www.aclweb.org/anthology/W18-6455</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>panja-naskar:2018:WMT</bibkey>
  </paper>

  <paper id="6456">
    <title>RUSE: Regressor Using Sentence Embeddings for Automatic Machine Translation Evaluation</title>
    <author><first>Hiroki</first><last>Shimanaka</last></author>
    <author><first>Tomoyuki</first><last>Kajiwara</last></author>
    <author><first>Mamoru</first><last>Komachi</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>751–758</pages>
    <url>http://www.aclweb.org/anthology/W18-6456</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>shimanaka-kajiwara-komachi:2018:WMT</bibkey>
  </paper>

  <paper id="6457">
    <title>Keep It or Not: Word Level Quality Estimation for Post-Editing</title>
    <author><first>Prasenjit</first><last>Basu</last></author>
    <author><first>Santanu</first><last>Pal</last></author>
    <author><first>Sudip Kumar</first><last>Naskar</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>759–764</pages>
    <url>http://www.aclweb.org/anthology/W18-6457</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>basu-pal-naskar:2018:WMT</bibkey>
  </paper>

  <paper id="6458">
    <title>RTM results for Predicting Translation Performance</title>
    <author><first>Ergun</first><last>Biçici</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>765–769</pages>
    <url>http://www.aclweb.org/anthology/W18-6458</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>biici:2018:WMT2</bibkey>
  </paper>

  <paper id="6459">
    <title>Neural Machine Translation for English-Tamil</title>
    <author><first>Himanshu</first><last>Choudhary</last></author>
    <author><first>Aditya Kumar</first><last>Pathak</last></author>
    <author><first>Rajiv Ratan</first><last>Saha</last></author>
    <author><first>Ponnurangam</first><last>Kumaraguru</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>770–775</pages>
    <url>http://www.aclweb.org/anthology/W18-6459</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>choudhary-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6460">
    <title>The Benefit of Pseudo-Reference Translations in Quality Estimation of MT Output</title>
    <author><first>Melania</first><last>Duma</last></author>
    <author><first>Wolfgang</first><last>Menzel</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>776–781</pages>
    <url>http://www.aclweb.org/anthology/W18-6460</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>duma-menzel:2018:WMT2</bibkey>
  </paper>

  <paper id="6461">
    <title>Supervised and Unsupervised Minimalist Quality Estimators: Vicomtech’s Participation in the WMT 2018 Quality Estimation Task</title>
    <author><first>Thierry</first><last>Etchegoyhen</last></author>
    <author><first>Eva</first><last>Martínez Garcia</last></author>
    <author><first>Andoni</first><last>Azpeitia</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>782–787</pages>
    <url>http://www.aclweb.org/anthology/W18-6461</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>etchegoyhen-martnezgarcia-azpeitia:2018:WMT</bibkey>
  </paper>

  <paper id="6462">
    <title>Contextual Encoding for Translation Quality Estimation</title>
    <author><first>Junjie</first><last>Hu</last></author>
    <author><first>Wei-Cheng</first><last>Chang</last></author>
    <author><first>Yuexin</first><last>Wu</last></author>
    <author><first>Graham</first><last>Neubig</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>788–793</pages>
    <url>http://www.aclweb.org/anthology/W18-6462</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>hu-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6463">
    <title>Sheffield Submissions for the WMT18 Quality Estimation Shared Task</title>
    <author><first>Julia</first><last>Ive</last></author>
    <author><first>Carolina</first><last>Scarton</last></author>
    <author><first>Frédéric</first><last>Blain</last></author>
    <author><first>Lucia</first><last>Specia</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>794–800</pages>
    <url>http://www.aclweb.org/anthology/W18-6463</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>ive-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6464">
    <title>UAlacant machine translation quality estimation at WMT 2018: a simple approach using phrase tables and feed-forward neural networks</title>
    <author><first>Felipe</first><last>Sánchez-Martínez</last></author>
    <author><first>Miquel</first><last>Esplà-Gomis</last></author>
    <author><first>Mikel L.</first><last>Forcada</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>801–808</pages>
    <url>http://www.aclweb.org/anthology/W18-6464</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>snchezmartnez-esplgomis-forcada:2018:WMT</bibkey>
  </paper>

  <paper id="6465">
    <title>Alibaba Submission for WMT18 Quality Estimation Task</title>
    <author><first>Jiayi</first><last>Wang</last></author>
    <author><first>Kai</first><last>Fan</last></author>
    <author><first>Bo</first><last>Li</last></author>
    <author><first>Fengming</first><last>Zhou</last></author>
    <author><first>Boxing</first><last>Chen</last></author>
    <author><first>Yangbin</first><last>Shi</last></author>
    <author><first>Luo</first><last>Si</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>809–815</pages>
    <url>http://www.aclweb.org/anthology/W18-6465</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>wang-EtAl:2018:WMT4</bibkey>
  </paper>

  <paper id="6466">
    <title>Quality Estimation with Force-Decoded Attention and Cross-lingual Embeddings</title>
    <author><first>Elizaveta</first><last>Yankovskaya</last></author>
    <author><first>Andre</first><last>Tättar</last></author>
    <author><first>Mark</first><last>Fishel</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>816–821</pages>
    <url>http://www.aclweb.org/anthology/W18-6466</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>yankovskaya-tttar-fishel:2018:WMT</bibkey>
  </paper>

  <paper id="6467">
    <title>MS-UEdin Submission to the WMT2018 APE Shared Task: Dual-Source Transformer for Automatic Post-Editing</title>
    <author><first>Marcin</first><last>Junczys-Dowmunt</last></author>
    <author><first>Roman</first><last>Grundkiewicz</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>822–826</pages>
    <url>http://www.aclweb.org/anthology/W18-6467</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>junczysdowmunt-grundkiewicz:2018:WMT</bibkey>
  </paper>

  <paper id="6468">
    <title>A Transformer-Based Multi-Source Automatic Post-Editing System</title>
    <author><first>Santanu</first><last>Pal</last></author>
    <author><first>Nico</first><last>Herbig</last></author>
    <author><first>Antonio</first><last>Krüger</last></author>
    <author><first>Josef</first><last>van Genabith</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>827–835</pages>
    <url>http://www.aclweb.org/anthology/W18-6468</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>pal-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6469">
    <title>DFKI-MLT System Description for the WMT18 Automatic Post-editing Task</title>
    <author><first>Daria</first><last>Pylypenko</last></author>
    <author><first>Raphael</first><last>Rubino</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>836–839</pages>
    <url>http://www.aclweb.org/anthology/W18-6469</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>pylypenko-rubino:2018:WMT</bibkey>
  </paper>

  <paper id="6470">
    <title>Multi-encoder Transformer Network for Automatic Post-Editing</title>
    <author><first>Jaehun</first><last>Shin</last></author>
    <author><first>Jong-Hyeok</first><last>Lee</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>840–845</pages>
    <url>http://www.aclweb.org/anthology/W18-6470</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>shin-lee:2018:WMT</bibkey>
  </paper>

  <paper id="6471">
    <title>Multi-source transformer with combined losses for automatic post editing</title>
    <author><first>Amirhossein</first><last>Tebbifakhr</last></author>
    <author><first>Ruchit</first><last>Agrawal</last></author>
    <author><first>Matteo</first><last>Negri</last></author>
    <author><first>Marco</first><last>Turchi</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>846–852</pages>
    <url>http://www.aclweb.org/anthology/W18-6471</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>tebbifakhr-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6472">
    <title>The Speechmatics Parallel Corpus Filtering System for WMT18</title>
    <author><first>Tom</first><last>Ash</last></author>
    <author><first>Remi</first><last>Francis</last></author>
    <author><first>Will</first><last>Williams</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>853–859</pages>
    <url>http://www.aclweb.org/anthology/W18-6472</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>ash-francis-williams:2018:WMT</bibkey>
  </paper>

  <paper id="6473">
    <title>STACC, OOV Density and N-gram Saturation: Vicomtech’s Participation in the WMT 2018 Shared Task on Parallel Corpus Filtering</title>
    <author><first>Andoni</first><last>Azpeitia</last></author>
    <author><first>Thierry</first><last>Etchegoyhen</last></author>
    <author><first>Eva</first><last>Martínez garcia</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>860–866</pages>
    <url>http://www.aclweb.org/anthology/W18-6473</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>azpeitia-etchegoyhen-martnezgarcia:2018:WMT</bibkey>
  </paper>

  <paper id="6474">
    <title>A hybrid pipeline of rules and machine learning to filter web-crawled parallel corpora</title>
    <author><first>Eduard</first><last>Barbu</last></author>
    <author><first>Verginica</first><last>Barbu Mititelu</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>867–871</pages>
    <url>http://www.aclweb.org/anthology/W18-6474</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>barbu-barbumititelu:2018:WMT</bibkey>
  </paper>

  <paper id="6475">
    <title>Coverage and Cynicism: The AFRL Submission to the WMT 2018 Parallel Corpus Filtering Task</title>
    <author><first>Grant</first><last>Erdmann</last></author>
    <author><first>Jeremy</first><last>Gwinnup</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>872–876</pages>
    <url>http://www.aclweb.org/anthology/W18-6475</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>erdmann-gwinnup:2018:WMT</bibkey>
  </paper>

  <paper id="6476">
    <title>MAJE Submission to the WMT2018 Shared Task on Parallel Corpus Filtering</title>
    <author><first>Marina</first><last>Fomicheva</last></author>
    <author><first>Jesús</first><last>González-Rubio</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>877–881</pages>
    <url>http://www.aclweb.org/anthology/W18-6476</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>fomicheva-gonzlezrubio:2018:WMT</bibkey>
  </paper>

  <paper id="6477">
    <title>An Unsupervised System for Parallel Corpus Filtering</title>
    <author><first>Viktor</first><last>Hangya</last></author>
    <author><first>Alexander</first><last>Fraser</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>882–887</pages>
    <url>http://www.aclweb.org/anthology/W18-6477</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>hangya-fraser:2018:WMT</bibkey>
  </paper>

  <paper id="6478">
    <title>Dual Conditional Cross-Entropy Filtering of Noisy Parallel Corpora</title>
    <author><first>Marcin</first><last>Junczys-Dowmunt</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>888–895</pages>
    <url>http://www.aclweb.org/anthology/W18-6478</url>
    <revision id="2">W18-6478v2</revision>
    <bibtype>inproceedings</bibtype>
    <bibkey>junczysdowmunt:2018:WMT2</bibkey>
  </paper>

  <paper id="6479">
    <title>The JHU Parallel Corpus Filtering Systems for WMT 2018</title>
    <author><first>Huda</first><last>Khayrallah</last></author>
    <author><first>Hainan</first><last>Xu</last></author>
    <author><first>Philipp</first><last>Koehn</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>896–899</pages>
    <url>http://www.aclweb.org/anthology/W18-6479</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>khayrallah-xu-koehn:2018:WMT</bibkey>
  </paper>

  <paper id="6480">
    <title>Measuring sentence parallelism using Mahalanobis distances: The NRC unsupervised submissions to the WMT18 Parallel Corpus Filtering shared task</title>
    <author><first>Patrick</first><last>Littell</last></author>
    <author><first>Samuel</first><last>Larkin</last></author>
    <author><first>Darlene</first><last>Stewart</last></author>
    <author><first>Michel</first><last>Simard</last></author>
    <author><first>Cyril</first><last>Goutte</last></author>
    <author><first>Chi-kiu</first><last>Lo</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>900–907</pages>
    <url>http://www.aclweb.org/anthology/W18-6480</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>littell-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6481">
    <title>Accurate semantic textual similarity for cleaning noisy parallel corpora using semantic machine translation evaluation metric: The NRC supervised submissions to the Parallel Corpus Filtering task</title>
    <author><first>Chi-kiu</first><last>Lo</last></author>
    <author><first>Michel</first><last>Simard</last></author>
    <author><first>Darlene</first><last>Stewart</last></author>
    <author><first>Samuel</first><last>Larkin</last></author>
    <author><first>Cyril</first><last>Goutte</last></author>
    <author><first>Patrick</first><last>Littell</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>908–916</pages>
    <url>http://www.aclweb.org/anthology/W18-6481</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>lo-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6482">
    <title>Alibaba Submission to the WMT18 Parallel Corpus Filtering Task</title>
    <author><first>Jun</first><last>Lu</last></author>
    <author><first>Xiaoyu</first><last>Lv</last></author>
    <author><first>Yangbin</first><last>Shi</last></author>
    <author><first>Boxing</first><last>Chen</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>917–922</pages>
    <url>http://www.aclweb.org/anthology/W18-6482</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>lu-EtAl:2018:WMT2</bibkey>
  </paper>

  <paper id="6483">
    <title>UTFPR at WMT 2018: Minimalistic Supervised Corpora Filtering for Machine Translation</title>
    <author><first>Gustavo</first><last>Paetzold</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>923–927</pages>
    <url>http://www.aclweb.org/anthology/W18-6483</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>paetzold:2018:WMT</bibkey>
  </paper>

  <paper id="6484">
    <title>The ILSP/ARC submission to the WMT 2018 Parallel Corpus Filtering Shared Task</title>
    <author><first>Vassilis</first><last>Papavassiliou</last></author>
    <author><first>Sokratis</first><last>Sofianopoulos</last></author>
    <author><first>Prokopis</first><last>Prokopidis</last></author>
    <author><first>Stelios</first><last>Piperidis</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>928–933</pages>
    <url>http://www.aclweb.org/anthology/W18-6484</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>papavassiliou-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6485">
    <title>SYSTRAN Participation to the WMT2018 Shared Task on Parallel Corpus Filtering</title>
    <author><first>Minh Quang</first><last>Pham</last></author>
    <author><first>Josep</first><last>Crego</last></author>
    <author><first>Jean</first><last>Senellart</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>934–938</pages>
    <url>http://www.aclweb.org/anthology/W18-6485</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>pham-crego-senellart:2018:WMT</bibkey>
  </paper>

  <paper id="6486">
    <title>Tilde’s Parallel Corpus Filtering Methods for WMT 2018</title>
    <author><first>Marcis</first><last>Pinnis</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>939–945</pages>
    <url>http://www.aclweb.org/anthology/W18-6486</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>pinnis:2018:WMT</bibkey>
  </paper>

  <paper id="6487">
    <title>The RWTH Aachen University Filtering System for the WMT 2018 Parallel Corpus Filtering Task</title>
    <author><first>Nick</first><last>Rossenbach</last></author>
    <author><first>Jan</first><last>Rosendahl</last></author>
    <author><first>Yunsu</first><last>Kim</last></author>
    <author><first>Miguel</first><last>Graça</last></author>
    <author><first>Aman</first><last>Gokrani</last></author>
    <author><first>Hermann</first><last>Ney</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>946–954</pages>
    <url>http://www.aclweb.org/anthology/W18-6487</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>rossenbach-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6488">
    <title>Prompsit’s submission to WMT 2018 Parallel Corpus Filtering shared task</title>
    <author><first>Víctor M.</first><last>Sánchez-Cartagena</last></author>
    <author><first>Marta</first><last>Bañón</last></author>
    <author><first>Sergio</first><last>Ortiz Rojas</last></author>
    <author><first>Gema</first><last>Ramírez</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>955–962</pages>
    <url>http://www.aclweb.org/anthology/W18-6488</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>snchezcartagena-EtAl:2018:WMT</bibkey>
  </paper>

  <paper id="6489">
    <title>NICT’s Corpus Filtering Systems for the WMT18 Parallel Corpus Filtering Task</title>
    <author><first>Rui</first><last>Wang</last></author>
    <author><first>Benjamin</first><last>Marie</last></author>
    <author><first>Masao</first><last>Utiyama</last></author>
    <author><first>Eiichiro</first><last>Sumita</last></author>
    <booktitle>Proceedings of the Third Conference on Machine Translation: Shared Task Papers</booktitle>
    <month>October</month>
    <year>2018</year>
    <address>Belgium, Brussels</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>963–967</pages>
    <url>http://www.aclweb.org/anthology/W18-6489</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>wang-EtAl:2018:WMT5</bibkey>
  </paper>

  <paper id="6500">
    <title>Proceedings of the 11th International Conference on Natural Language Generation</title>
    <editor><first>Emiel</first><last>Krahmer</last></editor>
    <editor><first>Albert</first><last>Gatt</last></editor>
    <editor><first>Martijn</first><last>Goudbeek</last></editor>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W18-65</url>
    <bibtype>book</bibtype>
    <bibkey>W18-65:2018</bibkey>
  </paper>

  <paper id="6501">
    <title>Deep Graph Convolutional Encoders for Structured Data to Text Generation</title>
    <author><first>Diego</first><last>Marcheggiani</last></author>
    <author><first>Laura</first><last>Perez-Beltrachini</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–9</pages>
    <url>http://www.aclweb.org/anthology/W18-6501</url>
    <abstract>Most previous work on neural text generation from graph-structured data relies on standard sequence to sequence methods. These approaches linearise the input graph to be fed to a recurrent neural network. In this paper, we propose an alternative encoder based on graph convolutional networks that directly exploits the input structure. We report results on two graph to sequence datasets that empirically show the benefits of explicitly encoding the input</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>marcheggiani-perezbeltrachini:2018:W18-65</bibkey>
  </paper>

  <paper id="6502">
    <title>Describing a Knowledge Base</title>
    <author><first>Qingyun</first><last>Wang</last></author>
    <author><first>Xiaoman</first><last>Pan</last></author>
    <author><first>Lifu</first><last>Huang</last></author>
    <author><first>Boliang</first><last>Zhang</last></author>
    <author><first>Zhiying</first><last>Jiang</last></author>
    <author><first>Heng</first><last>Ji</last></author>
    <author><first>Kevin</first><last>Knight</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>10–21</pages>
    <url>http://www.aclweb.org/anthology/W18-6502</url>
    <abstract>We aim to automatically generate natural language narratives about an input structured knowledge base (KB). We build our generation framework based on a pointer network which can copy facts from the input KB, and add two attention mechanisms: (i) slot-aware attention to capture the association between a slot type and its corresponding slot value; and (ii) a new table position self-attention to capture the inter-dependencies among related slots. For evaluation, besides standard metrics including BLEU, METEOR, and ROUGE, we also propose a KB reconstruction based metric by extracting a KB from the generation output and comparing it with the input KB. We also create a new data set which includes 106,216 pairs of structured KBs and their corresponding natural language descriptions for two distinct entity types. Experiments show that our approach significantly outperforms state-of-the-art methods. The reconstructed KB achieves 68.8% - 72.6% F-score.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>wang-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6503">
    <title>Syntactic Manipulation for Generating more Diverse and Interesting Texts</title>
    <author><first>Jan Milan</first><last>Deriu</last></author>
    <author><first>Mark</first><last>Cieliebak</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>22–34</pages>
    <url>http://www.aclweb.org/anthology/W18-6503</url>
    <abstract>Natural Language Generation plays an important role in the domain of dialogue systems as it determines how users perceive the system. Recently, deep-learning based systems have been proposed to tackle this task, as they generalize better and require less amounts of manual effort to implement them for new domains. However, deep learning systems usually adapt a very homogeneous sounding writing style which expresses little variation.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>deriu-cieliebak:2018:W18-65</bibkey>
  </paper>

  <paper id="6504">
    <title>Automated learning of templates for data-to-text generation: comparing rule-based, statistical and neural methods</title>
    <author><first>Chris</first><last>van der Lee</last></author>
    <author><first>Emiel</first><last>Krahmer</last></author>
    <author><first>Sander</first><last>Wubben</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>35–45</pages>
    <url>http://www.aclweb.org/anthology/W18-6504</url>
    <abstract>The current study explored novel techniques and methods for trainable approaches to data-to-text generation. Neural Machine Translation was explored for the conversion from data to text as well as the addition of extra templatization steps of the data input and text output in the conversion process. Evaluation using BLEU did not find the Neural Machine Translation technique to perform any better compared to rule-based or Statistical Machine Translation, and the templatization method seemed to perform similar or sometimes worse compared to direct data-to-text conversion. However, the human evaluation metrics indicated that Neural Machine Translation yielded the highest quality output and that the templatization method was able to increase text quality in multiple situations.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>vanderlee-krahmer-wubben:2018:W18-65</bibkey>
  </paper>

  <paper id="6505">
    <title>End-to-End Content and Plan Selection for Data-to-Text Generation</title>
    <author><first>Sebastian</first><last>Gehrmann</last></author>
    <author><first>Falcon</first><last>Dai</last></author>
    <author><first>Henry</first><last>Elder</last></author>
    <author><first>Alexander</first><last>Rush</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>46–56</pages>
    <url>http://www.aclweb.org/anthology/W18-6505</url>
    <abstract>Learning to generate fluent natural language from structured data with neural networks has become an common approach for NLG. This problem can be challenging when the form of the structured data varies between examples. This paper presents a survey of several extensions to sequence-to-sequence models to account for the latent content selection process, particularly variants of copy attention and coverage decoding. We further propose a training method based on diverse ensembling to encourage the model to learn latent generation of plans during training. An empirical evaluation of these techniques shows an increase in quality of generated text across five automated metrics, as well as human evaluation.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>gehrmann-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6506">
    <title>SimpleNLG-ZH: a Linguistic Realisation Engine for Mandarin</title>
    <author><first>Guanyi</first><last>Chen</last></author>
    <author><first>Kees</first><last>van Deemter</last></author>
    <author><first>Chenghua</first><last>Lin</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>57–66</pages>
    <url>http://www.aclweb.org/anthology/W18-6506</url>
    <abstract>We introduce SimpleNLG-ZH, a realisation engine for Mandarin that follows the software design paradigm of SimpleNLG. We explain the core grammar (morphology and syntax) and the lexicon of SimpleNLG-ZH, which is very different from English and other languages for which SimpleNLG engines have been built. The system was evaluated by regenerating expressions from a body of test sentences and a corpus of human-authored expressions. Human evaluation was conducted to estimate the quality of regenerated sentences.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>chen-vandeemter-lin:2018:W18-651</bibkey>
  </paper>

  <paper id="6507">
    <title>Adapting SimpleNLG to Galician language</title>
    <author><first>Andrea</first><last>Cascallar Fuentes</last></author>
    <author><first>Alejandro</first><last>Ramos Soto</last></author>
    <author><first>Alberto</first><last>Bugarín Diz</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>67–72</pages>
    <url>http://www.aclweb.org/anthology/W18-6507</url>
    <abstract>In this paper, we describe SimpleNLG-GL, an adaptation of the linguistic realisation SimpleNLG library for the Galician language. This implementation is derived from SimpleNLG-ES, the English-Spanish version of this library. It has been tested using a battery of examples which covers the most common rules for Galician.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>cascallarfuentes-ramossoto-bugarndiz:2018:W18-65</bibkey>
  </paper>

  <paper id="6508">
    <title>Going Dutch: Creating SimpleNLG-NL</title>
    <author><first>Ruud</first><last>de Jong</last></author>
    <author><first>Mariët</first><last>Theune</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>73–78</pages>
    <url>http://www.aclweb.org/anthology/W18-6508</url>
    <abstract>This paper presents SimpleNLG-NL, an adaptation of the SimpleNLG surface realisation engine for the Dutch language. It describes a novel method for determining and testing the grammatical constructions to be implemented, using target sentences sampled from a treebank.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>dejong-theune:2018:W18-65</bibkey>
  </paper>

  <paper id="6509">
    <title>Learning to Flip the Bias of News Headlines</title>
    <author><first>Wei-Fan</first><last>Chen</last></author>
    <author><first>Henning</first><last>Wachsmuth</last></author>
    <author><first>Khalid</first><last>Al Khatib</last></author>
    <author><first>Benno</first><last>Stein</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>79–88</pages>
    <url>http://www.aclweb.org/anthology/W18-6509</url>
    <abstract>This paper introduces the task of “flipping” the bias of news articles: Given an article with a political bias (left or right), generate an article with the same topic but opposite bias. To study this task, we create a corpus with bias-labeled articles from allsides.com. As a first step, we analyze the corpus and discuss intrinsic characteristics of bias. They point to the main challenges of bias flipping, which in turn lead to a specific setting in the generation process. The paper in hand scales down the general bias flipping task to focus on bias flipping for news article headlines. A manual annotation of headlines from each side reveals that headlines are self-informative in general and often convey bias. We apply an autoencoder incorporating information from an article’s content to learn how to automatically flip the bias. From 200 generated headlines, 73 are classified as understandable by annotators, and 83 maintain the topic while having opposite bias. Insights from our analysis shed light on how to solve the main challenges of bias flipping.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>chen-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6510">
    <title>Stylistically User-Specific Generation</title>
    <author><first>Abdurrisyad</first><last>Fikri</last></author>
    <author><first>Hiroya</first><last>Takamura</last></author>
    <author><first>Manabu</first><last>Okumura</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>89–98</pages>
    <url>http://www.aclweb.org/anthology/W18-6510</url>
    <abstract>Recent neural models for response generation show good results in terms of general responses. In real conversations, however, depending on the speaker/responder, similar utterances should require different responses. </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>fikri-takamura-okumura:2018:W18-65</bibkey>
  </paper>

  <paper id="6511">
    <title>Explainable Autonomy: A Study of Explanation Styles for Building Clear Mental Models</title>
    <author><first>Francisco Javier</first><last>Chiyah Garcia</last></author>
    <author><first>David A</first><last>Robb</last></author>
    <author><first>Xingkun</first><last>Liu</last></author>
    <author><first>Atanas</first><last>Laskov</last></author>
    <author><first>Pedro</first><last>Patron</last></author>
    <author><first>Helen</first><last>Hastie</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>99–108</pages>
    <url>http://www.aclweb.org/anthology/W18-6511</url>
    <abstract>As vehicles become more autonomous, it is important to maintain a level of transparency about their behaviour and how they work. This is particularly important in remote locations where they cannot be observed. Here, we describe a natural language chat interface that enables the reasoning behind the behaviour of underwater vehicles to be queried. We do this by deriving an interpretable model of autonomy through having an expert ‘speak out-loud’ and provide various levels of detail based on this model. We corroborate previous research that has shown that it is important to inform the user of all possible explanations (high completeness) for improving the user’s general mental model of how a system works. For understanding specific behaviours, a high level of completeness is similarly important, however, we show it is better to have the multiple explanations worded in general terms (low soundness). This work has implications for designing interfaces for autonomy as well as for explainable AI and operator training.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>chiyahgarcia-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6512">
    <title>Treat the system like a human student: Automatic naturalness evaluation of generated text without reference texts</title>
    <author><first>Isabel</first><last>Groves</last></author>
    <author><first>Ye</first><last>Tian</last></author>
    <author><first>Ioannis</first><last>Douratsos</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>109–118</pages>
    <url>http://www.aclweb.org/anthology/W18-6512</url>
    <abstract>The current most popular method for automatic Natural Language Generation (NLG) evaluation is comparing generated text with human-written reference sentences using a metrics system, which has drawbacks around reliability and scalability. We draw inspiration from second language (L2) assessment and extract a set of linguistic features to predict human judgments of sentence naturalness. Our experiment using a small dataset showed that the feature-based approach yields promising results, with the added potential of providing interpretability into the source of the problems.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>groves-tian-douratsos:2018:W18-65</bibkey>
  </paper>

  <paper id="6513">
    <title>Content Aware Source Code Change Description Generation</title>
    <author><first>Pablo</first><last>Loyola</last></author>
    <author><first>Edison</first><last>Marrese-Taylor</last></author>
    <author><first>Jorge</first><last>Balazs</last></author>
    <author><first>Yutaka</first><last>Matsuo</last></author>
    <author><first>Fumiko</first><last>Satoh</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>119–128</pages>
    <url>http://www.aclweb.org/anthology/W18-6513</url>
    <abstract>We propose to study the generation of descriptions from source code changes by integrating the messages included on code commits and the intra-code documentation inside the source in the form of docstrings. Our hypothesis is that although both types of descriptions are not directly aligned in semantic terms —one explaining a change and the other the actual functionality of the code being modified— there could be certain common ground that is useful for the generation. To this end, we propose an architecture that uses the source code-docstring relationship to guide the description generation. We discuss the results of the approach comparing against a baseline based on a sequence-to-sequence model, using standard automatic natural language generation metrics as well as with a human study, thus offering a comprehensive view of the feasibility of the approach.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>loyola-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6514">
    <title>Improving Context Modelling in Multimodal Dialogue Generation</title>
    <author><first>Shubham</first><last>Agarwal</last></author>
    <author><first>Ondřej</first><last>Dušek</last></author>
    <author><first>Ioannis</first><last>Konstas</last></author>
    <author><first>Verena</first><last>Rieser</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>129–134</pages>
    <url>http://www.aclweb.org/anthology/W18-6514</url>
    <abstract>In this work, we investigate the task of textual</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>agarwal-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6515">
    <title>Generating Market Comments Referring to External Resources</title>
    <author><first>Tatsuya</first><last>Aoki</last></author>
    <author><first>Akira</first><last>Miyazawa</last></author>
    <author><first>Tatsuya</first><last>Ishigaki</last></author>
    <author><first>Keiichi</first><last>Goshima</last></author>
    <author><first>Kasumi</first><last>Aoki</last></author>
    <author><first>Ichiro</first><last>Kobayashi</last></author>
    <author><first>Hiroya</first><last>Takamura</last></author>
    <author><first>Yusuke</first><last>Miyao</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>135–139</pages>
    <url>http://www.aclweb.org/anthology/W18-6515</url>
    <abstract>Comments on a stock market often include the reason or the cause of the changes in the stock price “Nikkei turns lower as yen’s rise hits exporters”.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>aoki-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6516">
    <title>SpatialVOC2K: A Multilingual Dataset of Images with Annotations and Features for Spatial Relations between Objects</title>
    <author><first>Anja</first><last>Belz</last></author>
    <author><first>Adrian</first><last>Muscat</last></author>
    <author><first>Pierre</first><last>Anguill</last></author>
    <author><first>Mouhamadou</first><last>Sow</last></author>
    <author><first>Gaetan</first><last>Vincent</last></author>
    <author><first>Yassine</first><last>Zinessabah</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>140–145</pages>
    <url>http://www.aclweb.org/anthology/W18-6516</url>
    <abstract>We present the first multilingual image</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>belz-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6517">
    <title>Adding the Third Dimension to Spatial Relation Detection in 2D Images</title>
    <author><first>Brandon</first><last>Birmingham</last></author>
    <author><first>Adrian</first><last>Muscat</last></author>
    <author><first>Anja</first><last>Belz</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>146–151</pages>
    <url>http://www.aclweb.org/anthology/W18-6517</url>
    <abstract>Spatial relation detection in images has become a popular subject in image description research recently. A range of different language and geometric features have been used in this context, but methods have not so far used explicit information about the third dimension (depth), except when manually added to annotations. The lack of such information hampers detection of many different spatial relations that are inherently 3D. In this paper, we use a fully automatic method for creating a depth map of an image and derive several different object-level depth features from it which we add to an existing feature set to test the effect on spatial relation detection. We show that performance increases are obtained by adding depth features in all scenarios tested.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>birmingham-muscat-belz:2018:W18-65</bibkey>
  </paper>

  <paper id="6518">
    <title>Automatic Opinion Question Generation</title>
    <author><first>Yllias</first><last>Chali</last></author>
    <author><first>Tina</first><last>Baghaee</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>152–158</pages>
    <url>http://www.aclweb.org/anthology/W18-6518</url>
    <abstract>We study the problem of opinion question generation from sentences with the help of community-based question answering systems. For this purpose, we use a sequence to sequence attentional model, and we adopt coverage mechanism to prevent sentences from repeating themselves. Experimental results on the Amazon question/answer dataset show an improvement in automatic evaluation metrics as well as human evaluations from the state-of-the-art question generation systems.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>chali-baghaee:2018:W18-65</bibkey>
  </paper>

  <paper id="6519">
    <title>Modelling Pro-drop with the Rational Speech Acts Model</title>
    <author><first>Guanyi</first><last>Chen</last></author>
    <author><first>Kees</first><last>van Deemter</last></author>
    <author><first>Chenghua</first><last>Lin</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>159–164</pages>
    <url>http://www.aclweb.org/anthology/W18-6519</url>
    <abstract>We extend the classic Referring Expressions Generation task by considering zero pronouns in “pro-drop” languages such as Chinese, modelling their use by means of the Bayesian Rational Speech Acts model. By assuming that highly salient referents are most likely to be referred to by zero pronouns (i.e., pro-drop is more likely for salient referents than the less salient ones), the model offers an attractive explanation of a phenomenon not previously addressed probabilistically.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>chen-vandeemter-lin:2018:W18-652</bibkey>
  </paper>

  <paper id="6520">
    <title>Self-Learning Architecture for Natural Language Generation</title>
    <author><first>Hyungtak</first><last>Choi</last></author>
    <author><first>Siddarth</first><last>K.M.</last></author>
    <author><first>Haehun</first><last>Yang</last></author>
    <author><first>Heesik</first><last>Jeon</last></author>
    <author><first>Inchul</first><last>Hwang</last></author>
    <author><first>Jihie</first><last>Kim</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>165–170</pages>
    <url>http://www.aclweb.org/anthology/W18-6520</url>
    <abstract>In this paper, we propose a self-learning architecture for generating natural language templates for conversational assistants. Generating templates to cover all the combinations of slots in an intent is time consuming and labor-intensive. We examine three different models based on our proposed architecture - Rule-based model, Sequence-to-Sequence (Seq2Seq) model and Semantically Conditioned LSTM (SC-LSTM) model for the IoT domain - to reduce the human labor required for template generation. We demonstrate the feasibility of template generation for the IoT domain using our self-learning architecture. In both automatic and human evaluation, the self-learning architecture performs better than previous works trained with a fully human-labeled dataset. This is promising for commercial conversational assistant solutions.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>choi-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6521">
    <title>Enriching the WebNLG corpus</title>
    <author><first>Thiago</first><last>Castro Ferreira</last></author>
    <author><first>Diego</first><last>Moussallem</last></author>
    <author><first>Emiel</first><last>Krahmer</last></author>
    <author><first>Sander</first><last>Wubben</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>171–176</pages>
    <url>http://www.aclweb.org/anthology/W18-6521</url>
    <abstract>This paper describes the enrichment of WebNLG corpus, with the aim to further extend its usefulness as a resource for evaluating common NLG tasks, including Discourse Ordering, Lexicalization and Referring Expression Generation. We also produce a silver-standard German translation of the corpus to enable the exploitation of NLG approaches to other languages than English. The enriched corpus will be publicly available.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>castroferreira-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6522">
    <title>Towards making NLG a voice for interpretable Machine Learning</title>
    <author><first>James</first><last>Forrest</last></author>
    <author><first>Somayajulu</first><last>Sripada</last></author>
    <author><first>Wei</first><last>Pang</last></author>
    <author><first>George</first><last>Coghill</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>177–182</pages>
    <url>http://www.aclweb.org/anthology/W18-6522</url>
    <abstract>This paper presents a study to understand the issues related to using NLG to humanise explanations from a popular interpretable machine learning framework called LIME.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>forrest-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6523">
    <title>Template-based multilingual football reports generation using Wikidata as a knowledge base</title>
    <author><first>Lorenzo</first><last>Gatti</last></author>
    <author><first>Chris</first><last>van der Lee</last></author>
    <author><first>Mariët</first><last>Theune</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>183–188</pages>
    <url>http://www.aclweb.org/anthology/W18-6523</url>
    <abstract>This paper presents a new version of a football reports generation system called PASS. The original version generated Dutch text and relied on a limited hand-crafted knowledge base. We describe how, in a short amount of time, we extended PASS to produce English texts, exploiting machine translation and Wikidata as a large-scale source of multilingual knowledge.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>gatti-vanderlee-theune:2018:W18-65</bibkey>
  </paper>

  <paper id="6524">
    <title>Automatic Evaluation of Neural Personality-based Chatbots</title>
    <author><first>Yujie</first><last>Xing</last></author>
    <author><first>Raquel</first><last>Fernández</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>189–194</pages>
    <url>http://www.aclweb.org/anthology/W18-6524</url>
    <abstract>Stylistic variation is critical to render the utterances generated by conversational agents natural and engaging. In this paper, we focus on sequence-to-sequence models for open-domain dialogue response generation and propose a new method to evaluate the extent to which such models are able to generate responses that reflect different personality traits.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>xing-fernndez:2018:W18-65</bibkey>
  </paper>

  <paper id="6525">
    <title>Poem Machine - a Co-creative NLG Web Application for Poem Writing</title>
    <author><first>Mika</first><last>Hämäläinen</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>195–196</pages>
    <url>http://www.aclweb.org/anthology/W18-6525</url>
    <abstract>We present Poem Machine, an interactive online tool for co-authoring Finnish poetry with a computationally creative agent. Poem Machine can produce poetry of its own and assist the user in authoring poems. The main target group for the system is primary school children, and its use as a part of teaching is currently under study.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>hmlinen:2018:W18-65</bibkey>
  </paper>

  <paper id="6526">
    <title>Japanese Advertising Slogan Generator using Case Frame and Word Vector</title>
    <author><first>Kango</first><last>Iwama</last></author>
    <author><first>Yoshinobu</first><last>Kano</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>197–198</pages>
    <url>http://www.aclweb.org/anthology/W18-6526</url>
    <abstract>There has been many works published for automatic sentence generation of a variety of domains. However, there would be still no single method available at present that can generate sentences for all of domains. Each domain will require a suitable generation method. We focus on automatic generation of Japanese advertisement slogans in this paper. We use our advertisement slogan database, case frame information, and word vector information. We employed our system to apply for a copy competition for human copywriters, where our advertisement slogan was left as a finalist. Our system could be regarded as the world first system that generates slogans in a practical level, as an advertising agency already employs our system in their business.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>iwama-kano:2018:W18-65</bibkey>
  </paper>

  <paper id="6527">
    <title>Underspecified Universal Dependency Structures as Inputs for Multilingual Surface Realisation</title>
    <author><first>Simon</first><last>Mille</last></author>
    <author><first>Anja</first><last>Belz</last></author>
    <author><first>Bernd</first><last>Bohnet</last></author>
    <author><first>Leo</first><last>Wanner</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>199–209</pages>
    <url>http://www.aclweb.org/anthology/W18-6527</url>
    <abstract>This paper presents the datasets used in the First Multilingual Surface Realisation Shared Task (SR’18), describes in detail how they were created, and evaluates their quality. In addition, we examine (a) the NLG subtask of surface realisation itself, (b) the motivation for, and likely usefulness of, deriving NLG inputs from annotations in resources originally developed for Natural Language Understanding (NLU), (c) whether the resulting inputs supply enough information of the right kind for the final stage in the NLG process, and more tentatively, (d) what role surface realisation is likely to play in the future in the NLG context.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mille-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6528">
    <title>LSTM Hypertagging</title>
    <author><first>Reid</first><last>Fu</last></author>
    <author><first>Michael</first><last>White</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>210–220</pages>
    <url>http://www.aclweb.org/anthology/W18-6528</url>
    <abstract>We implemented an LSTM hypertagger using techniques from Lewis et al. 2016, a recent paper on supertagging for parsing. We compared this new hypertagger with the existing hypertagger in OpenCCG, both in tagging accuracy and in effect on realization performance, and saw significant improvement in both. We did human evaluations to confirm that our findings were significant, and the human evaluations confirmed that they were.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>fu-white:2018:W18-65</bibkey>
  </paper>

  <paper id="6529">
    <title>Sequence-to-Sequence Models for Data-to-Text Natural Language Generation: Word- vs. Character-based Processing and Output Diversity</title>
    <author><first>Glorianna</first><last>Jagfeld</last></author>
    <author><first>Sabrina</first><last>Jenne</last></author>
    <author><first>Ngoc Thang</first><last>Vu</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>221–232</pages>
    <url>http://www.aclweb.org/anthology/W18-6529</url>
    <abstract>We present a comparison of word-based and character-based sequence-to-sequence models for data-to-text natural language generation, which generate natural language descriptions for structured inputs. On the datasets of two recent generation challenges, our models achieve comparable or better automatic evaluation results than the best challenge submissions.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>jagfeld-jenne-vu:2018:W18-65</bibkey>
  </paper>

  <paper id="6530">
    <title>Generating E-Commerce Product Titles and Predicting their Quality</title>
    <author><first>José G.</first><last>Camargo de Souza</last></author>
    <author><first>Michael</first><last>Kozielski</last></author>
    <author><first>Prashant</first><last>Mathur</last></author>
    <author><first>Ernie</first><last>Chang</last></author>
    <author><first>Marco</first><last>Guerini</last></author>
    <author><first>Matteo</first><last>Negri</last></author>
    <author><first>Marco</first><last>Turchi</last></author>
    <author><first>Evgeny</first><last>Matusov</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>233–243</pages>
    <url>http://www.aclweb.org/anthology/W18-6530</url>
    <abstract>E-commerce platforms present products using titles that summarize product information. These titles cannot be created by hand, therefore an algorithmic solution is required. The task of automatically generating these titles given noisy user provided listing titles is one way to achieve the goal. The setting requires the generation process to be fast and the generated title to be both human-readable and concise. Furthermore, we need to understand if such generated titles are usable. As such, we propose approaches that (i) automatically generate product titles, (ii) predict their quality. Our approach scales to millions of products and both automatic and human evaluations performed on real-world data indicate our approaches are effective and applicable to existing e-commerce scenarios.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>camargodesouza-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6531">
    <title>Designing and testing the messages produced by a virtual dietitian</title>
    <author><first>Luca</first><last>Anselma</last></author>
    <author><first>Alessandro</first><last>Mazzei</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>244–253</pages>
    <url>http://www.aclweb.org/anthology/W18-6531</url>
    <abstract>This paper presents a project about the automatic generation of persuasive messages in the context of the diet management. In the first part of the paper we introduce the basic mechanisms related to data interpretation and content selection for a numerical data-to-text generation architecture. In the second part of the paper we discuss a number of factors influencing the design of the messages. In particular, we consider the design of the aggregation procedure. Finally, we present the results of a human-based evaluation concerning this design factor.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>anselma-mazzei:2018:W18-65</bibkey>
  </paper>

  <paper id="6532">
    <title>Generation of Company descriptions using concept-to-text and text-to-text deep models: dataset collection and systems evaluation</title>
    <author><first>Raheel</first><last>Qader</last></author>
    <author><first>Khoder</first><last>Jneid</last></author>
    <author><first>François</first><last>Portet</last></author>
    <author><first>Cyril</first><last>Labbé</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>254–263</pages>
    <url>http://www.aclweb.org/anthology/W18-6532</url>
    <abstract>In this paper we study the performance of several state-of-the-art sequence-to-sequence models applied to generation of short company descriptions. The models are evaluated on a newly created and publicly available company dataset that has been collected from Wikipedia. The dataset consists of around 51K company descriptions that can be used for both concept-to-text and text-to-text generation tasks. Automatic metrics and human evaluation scores computed on the generated company descriptions show promising results despite the difficulty of the task as the dataset (like most available datasets) has not been originally designed for machine learning. In addition, we perform correlation analysis between automatic metrics and human evaluations and show that certain automatic metrics are more correlated to human judgments.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>qader-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6533">
    <title>Automatically Generating Questions about Novel Metaphors in Literature</title>
    <author><first>Natalie</first><last>Parde</last></author>
    <author><first>Rodney</first><last>Nielsen</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>264–273</pages>
    <url>http://www.aclweb.org/anthology/W18-6533</url>
    <abstract>The automatic generation of stimulating questions is crucial to the development of intelligent cognitive exercise applications. We developed an approach that generates appropriate Questioning the Author queries based on novel metaphors in diverse syntactic relations in literature. We show that the generated questions are comparable to human-generated questions in terms of naturalness, sensibility, and depth, and score slightly higher than human-generated questions in terms of clarity. We also show that questions generated about novel metaphors are rated as cognitively deeper than questions generated about non- or conventional metaphors, providing evidence that metaphor novelty can be leveraged to promote cognitive exercise.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>parde-nielsen:2018:W18-65</bibkey>
  </paper>

  <paper id="6534">
    <title>A Master-Apprentice Approach to Automatic Creation of Culturally Satirical Movie Titles</title>
    <author><first>Khalid</first><last>Alnajjar</last></author>
    <author><first>Mika</first><last>Hämäläinen</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>274–283</pages>
    <url>http://www.aclweb.org/anthology/W18-6534</url>
    <abstract>Satire has played a role in indirectly expressing critique towards an authority or a person from the times immemorial. We present an autonomously creative master-apprentice approach consisting of a genetic algorithm and an NMT model to produce humorous and culturally apt satire out of movie titles automatically. Furthermore, we evaluate the approach in terms of its creativity and its output. We provide a solid definition for creativity to maximize the objectiveness of the evaluation.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>alnajjar-hmlinen:2018:W18-65</bibkey>
  </paper>

  <paper id="6535">
    <title>Can Neural Generators for Dialogue Learn Sentence Planning and Discourse Structuring?</title>
    <author><first>Lena</first><last>Reed</last></author>
    <author><first>Shereen</first><last>Oraby</last></author>
    <author><first>Marilyn</first><last>Walker</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>284–295</pages>
    <url>http://www.aclweb.org/anthology/W18-6535</url>
    <abstract>Responses in task-oriented dialogue systems often realize multiple propositions whose ultimate form depends on the use of sentence planning and discourse structuring operations. For example a recommendation may consist of an explicitly evaluative utterance e.g. “Chanpen Thai is the best option.”, along with content related by the justification discourse relation, e.g. “It has great food and service.”, that combines multiple propositions into a single phrase. While neural generation methods integrate sentence planning and surface realization in one end-to-end learning framework, previous work has not shown that neural generators can: (1) perform common sentence planning and discourse structuring operations; (2) make decisions as to whether to realize content in a single sentence or over multiple sentences; (3) generalize sentence planning and discourse relation operations beyond what was seen in training. We systematically create large training corpora that exhibit particular sentence planning operations and then test neural models to see what they learn. We compare models without explicit latent variables for sentence planning with ones that provide explicit supervision during training. We show that only the models with additional supervision can reproduce sentence planning and discourse operations and generalize to situations unseen in training.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>reed-oraby-walker:2018:W18-65</bibkey>
  </paper>

  <paper id="6536">
    <title>Neural Generation of Diverse Questions using Answer Focus, Contextual and Linguistic Features</title>
    <author><first>Vrindavan</first><last>Harrison</last></author>
    <author><first>Marilyn</first><last>Walker</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>296–306</pages>
    <url>http://www.aclweb.org/anthology/W18-6536</url>
    <abstract>Question Generation is the task of automatically creating questions</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>harrison-walker:2018:W18-65</bibkey>
  </paper>

  <paper id="6537">
    <title>Evaluation methodologies in Automatic Question Generation 2013-2018</title>
    <author><first>Jacopo</first><last>Amidei</last></author>
    <author><first>Paul</first><last>Piwek</last></author>
    <author><first>Alistair</first><last>Willis</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>307–317</pages>
    <url>http://www.aclweb.org/anthology/W18-6537</url>
    <abstract>In the last few years Automatic Question Generation (AQG) has attracted increasing interest. In this paper we survey the evaluation methodologies used in AQG. Based on a sample of 37 papers, our research shows that the systems’ development is not accompanied by similar developments in the methodologies used for the systems’ evaluation. Indeed, in the papers we examine here, we find a wide variety of both intrinsic and extrinsic evaluation methodologies. Such diverse evaluation practices make it difficult to reliably compare the quality of different generation systems. Our study suggests that, given the rapidly increasing level of research in the area, a common framework is urgently needed to compare the performance of AQG systems and NLG systems more generally.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>amidei-piwek-willis:2018:W18-65</bibkey>
  </paper>

  <paper id="6538">
    <title>Task Proposal: The TL;DR Challenge</title>
    <author><first>Shahbaz</first><last>Syed</last></author>
    <author><first>Michael</first><last>Völske</last></author>
    <author><first>Martin</first><last>Potthast</last></author>
    <author><first>Nedim</first><last>Lipka</last></author>
    <author><first>Benno</first><last>Stein</last></author>
    <author><first>Hinrich</first><last>Schütze</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>318–321</pages>
    <url>http://www.aclweb.org/anthology/W18-6538</url>
    <abstract>The TL;DR challenge fosters research in</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>syed-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6539">
    <title>Findings of the E2E NLG Challenge</title>
    <author><first>Ondřej</first><last>Dušek</last></author>
    <author><first>Jekaterina</first><last>Novikova</last></author>
    <author><first>Verena</first><last>Rieser</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>322–328</pages>
    <url>http://www.aclweb.org/anthology/W18-6539</url>
    <abstract>This paper summarises the experimental setup and results of the first shared task on end-to-end (E2E) natural language generation (NLG) in spoken dialogue systems (SDS). Recent end-to-end generation systems are promising since they reduce the need for data annotation. However, they are currently limited to small, delexicalised datasets.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>duek-novikova-rieser:2018:W18-65</bibkey>
  </paper>

  <paper id="6540">
    <title>Adapting Descriptions of People to the Point of View of a Moving Observer</title>
    <author><first>Gonzalo</first><last>Méndez</last></author>
    <author><first>Raquel</first><last>Hervas</last></author>
    <author><first>Pablo</first><last>Gervás</last></author>
    <author><first>Ricardo</first><last>de la Rosa</last></author>
    <author><first>Daniel</first><last>Ruiz</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>329–338</pages>
    <url>http://www.aclweb.org/anthology/W18-6540</url>
    <abstract>This paper addresses the task of generating descriptions of people for an observer that is moving within a scene. As the observer moves, the descriptions of the people around him also change. A referring expression generation algorithm adapted to this task needs to continuously monitor the changes in the field of view of the observer, his relative position to the people being described, and the relative position of these people to any landmarks around them, and to take these changes into account in the referring expressions generated. This task presents two advantages: many of the mechanisms already available for static contexts may be applied with small adaptations, and it introduces the concept of changing conditions into the task of referring expression generation. In this paper we describe the design of an algorithm that takes these aspects into account in order to create descriptions of people within a 3D virtual environment. The evaluation of this algorithm has shown that, by changing the descriptions in real time according to the observers point of view, they are able to identify the described person quickly and effectively.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mndez-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6541">
    <title>BENGAL: An Automatic Benchmark Generator for Entity Recognition and Linking</title>
    <author><first>Axel-Cyrille</first><last>Ngonga Ngomo</last></author>
    <author><first>Michael</first><last>Röder</last></author>
    <author><first>Diego</first><last>Moussallem</last></author>
    <author><first>Ricardo</first><last>Usbeck</last></author>
    <author><first>René</first><last>Speck</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>339–349</pages>
    <abstract>The manual creation of gold standards for named entity recognition and entity linking is time- and resource-intensive. Moreover, recent works show that such gold standards contain a large proportion of mistakes in addition to being difficult to maintain. We hence present  Bengal, a novel automatic generation of such gold standards as a complement to manually created benchmarks. The main advantage of our benchmarks is that they can be readily generated at any time, and are cost-effective while being guaranteed to be free of annotation errors. We compare the performance of 11 tools on benchmarks in English generated by  Bengal with their performance on 16 benchmarks created manually. We show that our approach can be ported easily across languages by presenting results achieved on Brazilian Portuguese and Spanish. Overall, our results suggest that our automatic benchmark generation approach can create varied benchmarks that have characteristics similar to those of existing benchmarks. Our approach is open-source. Our experimental results are available at <url>http://faturl.com/bengalexpinlg</url> and the code at <url>https://github.com/dice-group/BENGAL</url>.</abstract>
    <url>http://www.aclweb.org/anthology/W18-6541</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>ngongangomo-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6542">
    <title>Sentence Packaging in Text Generation from Semantic Graphs as a Community Detection Problem</title>
    <author><first>Alexander</first><last>Shvets</last></author>
    <author><first>Simon</first><last>Mille</last></author>
    <author><first>Leo</first><last>Wanner</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>350–359</pages>
    <url>http://www.aclweb.org/anthology/W18-6542</url>
    <abstract>An increasing amount of research tackles the challenge of text generation from abstract ontological or semantic structures, which are in their very nature potentially large connected graphs. These graphs must be “packaged” into sentence-wise subgraphs. We interpret the problem of sentence packaging as a community detection problem with post optimization. Experiments on the texts of the VerbNet/FrameNet structure annotated-Penn Treebank, which have been converted into graphs by a coreference merge using Stanford CoreNLP, show a high F1-score of 0.738.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>shvets-mille-wanner:2018:W18-65</bibkey>
  </paper>

  <paper id="6543">
    <title>Handling Rare Items in Data-to-Text Generation</title>
    <author><first>Anastasia</first><last>Shimorina</last></author>
    <author><first>Claire</first><last>Gardent</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>360–370</pages>
    <url>http://www.aclweb.org/anthology/W18-6543</url>
    <abstract>Neural approaches to data-to-text generation generally handle rare iput items using either delexicalisation or the copy mechanism. We investigate the relative impact of these two approaches on two datasets (E2E and WebNLG) and using two evaluation settings. We show (i) that while copy and coverage markedly improved results compared to a setting where delexicalisation is not applied, delexicalisation usually performs better than copy and coverage; (ii) that in the more challenging evaluation setting where the number of rare items is greater, the performances of copying decreases; and (iii) that the impact of these two mechanisms varies greatly depending on how the dataset is constructed and on how it is split into dev, test and train.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>shimorina-gardent:2018:W18-65</bibkey>
  </paper>

  <paper id="6544">
    <title>Comprehension Driven Document Planning in Natural Language Generation Systems</title>
    <author><first>Craig</first><last>Thomson</last></author>
    <author><first>Ehud</first><last>Reiter</last></author>
    <author><first>Somayajulu</first><last>Sripada</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>371–380</pages>
    <url>http://www.aclweb.org/anthology/W18-6544</url>
    <abstract>This paper proposes an approach to NLG system design which focuses on generating output text which can be more easily processed by the reader. Ways in which cognitive theory might be combined with existing NLG techniques are discussed and two simple experiments in content ordering are presented.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>thomson-reiter-sripada:2018:W18-65</bibkey>
  </paper>

  <paper id="6545">
    <title>Adapting Neural Single-Document Summarization Model for Abstractive Multi-Document Summarization: A Pilot Study</title>
    <author><first>Jianmin</first><last>Zhang</last></author>
    <author><first>Jiwei</first><last>Tan</last></author>
    <author><first>Xiaojun</first><last>Wan</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>381–390</pages>
    <url>http://www.aclweb.org/anthology/W18-6545</url>
    <abstract>Till now, neural abstractive summarization methods have achieved great</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>zhang-tan-wan:2018:W18-65</bibkey>
  </paper>

  <paper id="6546">
    <title>Toward Bayesian Synchronous Tree Substitution Grammars for Sentence Planning</title>
    <author><first>David M.</first><last>Howcroft</last></author>
    <author><first>Dietrich</first><last>Klakow</last></author>
    <author><first>Vera</first><last>Demberg</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>391–396</pages>
    <url>http://www.aclweb.org/anthology/W18-6546</url>
    <abstract>Developing conventional natural language generation systems requires extensive attention from human experts in order to craft complex sets of sentence planning rules. We propose a Bayesian nonparametric approach to learn sentence planning rules by inducing synchronous tree substitution grammars for pairs of text plans and morphosyntactically-specified dependency trees. Our system is able to learn rules which can be used to generate novel texts after training on small datasets.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>howcroft-klakow-demberg:2018:W18-65</bibkey>
  </paper>

  <paper id="6547">
    <title>The Task Matters: Comparing Image Captioning and Task-Based Dialogical Image Description</title>
    <author><first>Nikolai</first><last>Ilinykh</last></author>
    <author><first>Sina</first><last>Zarrieß</last></author>
    <author><first>David</first><last>Schlangen</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>397–402</pages>
    <url>http://www.aclweb.org/anthology/W18-6547</url>
    <abstract>Image captioning models are typically trained on data that is collected from people who are asked to describe an image, without being given any further task context. As we argue here, this context independence is likely to cause problems for transferring to task settings in which image description is bound by task demands.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ilinykh-zarrie-schlangen:2018:W18-65</bibkey>
  </paper>

  <paper id="6548">
    <title>Generating Summaries of Sets of Consumer Products: Learning from Experiments</title>
    <author><first>Kittipitch</first><last>Kuptavanich</last></author>
    <author><first>Ehud</first><last>Reiter</last></author>
    <author><first>Kees</first><last>van Deemter</last></author>
    <author><first>Advaith</first><last>Siddharthan</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>403–407</pages>
    <url>http://www.aclweb.org/anthology/W18-6548</url>
    <abstract>We explored the task of creating a textual summary describing a large set of objects characterised by a small number of features using an e-commerce dataset. When a set of consumer products is large and varied, it can be difficult for a consumer to understand how the products in the set differ; consequently, it can be challenging to choose the most suitable product from the set. To assist consumers, we generated high-level summaries of product sets. Two generation algorithms are presented, discussed, and evaluated with human users. Our evaluation results suggest a positive contribution to consumers’ understanding of the domain.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kuptavanich-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6549">
    <title>Neural sentence generation from formal semantics</title>
    <author><first>Kana</first><last>Manome</last></author>
    <author><first>Masashi</first><last>Yoshikawa</last></author>
    <author><first>Hitomi</first><last>Yanaka</last></author>
    <author><first>Pascual</first><last>Martínez-Gómez</last></author>
    <author><first>Koji</first><last>Mineshima</last></author>
    <author><first>Daisuke</first><last>Bekki</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>408–414</pages>
    <url>http://www.aclweb.org/anthology/W18-6549</url>
    <abstract>Sequence-to-sequence models have shown strong performance in a wide range of NLP tasks, yet their applications to sentence generation from logical representations are underdeveloped. In this paper, we present a first sequence-to-sequence model for generating sentences from logical semantic representations based on event semantics. We use a semantic parsing system based on Combinatory Categorial Grammar (CCG) to obtain data annotated with logical formulas. We augment our sequence-to-sequence model with masking for predicates to contain output sentences. We also propose a novel evaluation method for generation using Recognizing Textual Entailment (RTE): combining parsing and generation, we test whether or not the output sentence entails the original text and vice versa. The experiments showed that our model outperformed a baseline with respect to both BLEU scores and accuracies in RTE.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>manome-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6550">
    <title>Talking about other people: an endless range of possibilities</title>
    <author><first>Emiel</first><last>van Miltenburg</last></author>
    <author><first>Desmond</first><last>Elliott</last></author>
    <author><first>Piek</first><last>Vossen</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>415–420</pages>
    <url>http://www.aclweb.org/anthology/W18-6550</url>
    <abstract>Image description datasets, such as Flickr30K and MS COCO, show a high degree of variation in the ways that crowd-workers talk about the world. Although this gives us a rich and diverse collection of data to work with, it also introduces uncertainty about how the world should be described. This paper shows the extent of this uncertainty in the PEOPLE-domain. We present a taxonomy of different ways to talk about other people. This taxonomy serves as a reference point to think about how other people should be described, and can be used to classify and compute statistics about labels applied to people.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>vanmiltenburg-elliott-vossen:2018:W18-65</bibkey>
  </paper>

  <paper id="6551">
    <title>Meteorologists and Students: A resource for language grounding of geographical descriptors</title>
    <author><first>Alejandro</first><last>Ramos Soto</last></author>
    <author><first>Ehud</first><last>Reiter</last></author>
    <author><first>Kees</first><last>van Deemter</last></author>
    <author><first>Jose</first><last>Alonso</last></author>
    <author><first>Albert</first><last>Gatt</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>421–425</pages>
    <url>http://www.aclweb.org/anthology/W18-6551</url>
    <abstract>We present a data resource which can be useful for research purposes on language grounding tasks in the context of geographical referring expression generation. The resource is composed of two data sets that encompass 25 different geographical descriptors and a set of associated graphical representations, drawn as polygons on a map by two groups of human subjects: teenage students and expert meteorologists.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ramossoto-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6552">
    <title>Cyclegen: Cyclic consistency based product review generator from attributes</title>
    <author><first>Vasu</first><last>Sharma</last></author>
    <author><first>Harsh</first><last>Sharma</last></author>
    <author><first>Ankita</first><last>Bishnu</last></author>
    <author><first>Labhesh</first><last>Patel</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>426–430</pages>
    <url>http://www.aclweb.org/anthology/W18-6552</url>
    <abstract>In this paper we present an automatic review generator system which can generate personalized reviews based on the user identity, product identity and designated rating the user wishes to allot to the review. We combine this with a sentiment analysis system which performs the complimentary task of assigning ratings to reviews based purely on the textual content of the review. We introduce an additional loss term to ensure cyclic consistency of the sentiment rating of the generated review with the conditioning rating used to generate the review. The introduction of this new loss term constraints the generation space while forcing it to generate reviews adhering better to the requested rating. The use of ‘soft’ generation and cyclic consistency allows us to train our model in an end to end fashion. We demonstrate the working of our model on product reviews from Amazon dataset.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>sharma-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6553">
    <title>Neural Transition-based Syntactic Linearization</title>
    <author><first>Linfeng</first><last>Song</last></author>
    <author><first>Yue</first><last>Zhang</last></author>
    <author><first>Daniel</first><last>Gildea</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>431–440</pages>
    <url>http://www.aclweb.org/anthology/W18-6553</url>
    <abstract>The task of linearization is to find a grammatical order given a set of words. Traditional models use statistical methods. Syn-</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>song-zhang-gildea:2018:W18-65</bibkey>
  </paper>

  <paper id="6554">
    <title>Characterizing Variation in Crowd-Sourced Data for Training Neural Language Generators to Produce Stylistically Varied Outputs</title>
    <author><first>Juraj</first><last>Juraska</last></author>
    <author><first>Marilyn</first><last>Walker</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>441–450</pages>
    <url>http://www.aclweb.org/anthology/W18-6554</url>
    <abstract>One of the biggest challenges of end-to-end language generation from meaning representations in dialogue systems is making the outputs more natural and varied. Here we take a large corpus of 50K crowd-sourced utterances in the restaurant domain and develop text analysis methods that systematically characterize types of sentences in the training data. We then automatically label the training data to allow us to conduct two kinds of experiments with a neural generator. First, we test the effect of training the system with different stylistic partitions and quantify the effect of smaller, but more stylistically controlled training data. Second, we try a method of labeling the style variants during training, and show that we can modify the style of the output to some extent using our stylistic labels. We contrast and compare these methods that can be used with any existing large corpus, showing how they vary in terms of semantic quality and stylistic control.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>juraska-walker:2018:W18-65</bibkey>
  </paper>

  <paper id="6555">
    <title>Char2char Generation with Reranking for the E2E NLG Challenge</title>
    <author><first>Shubham</first><last>Agarwal</last></author>
    <author><first>Marc</first><last>Dymetman</last></author>
    <author><first>Eric</first><last>Gaussier</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>451–456</pages>
    <url>http://www.aclweb.org/anthology/W18-6555</url>
    <abstract>This paper describes our submission to</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>agarwal-dymetman-gaussier:2018:W18-65</bibkey>
  </paper>

  <paper id="6556">
    <title>E2E NLG Challenge Submission: Towards Controllable Generation of Diverse Natural Language</title>
    <author><first>Henry</first><last>Elder</last></author>
    <author><first>Sebastian</first><last>Gehrmann</last></author>
    <author><first>Alexander</first><last>O’Connor</last></author>
    <author><first>Qun</first><last>Liu</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>457–462</pages>
    <url>http://www.aclweb.org/anthology/W18-6556</url>
    <abstract>In natural language generation (NLG), the task is to generate utterances from a more abstract input, such as structured data. An added challenge is to generate utterances that contain an accurate representation of the input, while reflecting the fluency and variety of human-generated text. In this paper, we report experiments with NLG models that can be used in task oriented dialogue systems. We explore the use of additional input to the model to encourage diversity and control of outputs. While our submission does not rank highly using automated metrics, qualitative investigation of generated utterances suggests the use of additional information in neural network NLG systems to be a promising research direction.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>elder-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6557">
    <title>E2E NLG Challenge: Neural Models vs. Templates</title>
    <author><first>Yevgeniy</first><last>Puzikov</last></author>
    <author><first>Iryna</first><last>Gurevych</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>463–471</pages>
    <url>http://www.aclweb.org/anthology/W18-6557</url>
    <abstract>E2E NLG Challenge is a shared task on generating restaurant descriptions from sets of key-value pairs. This paper describes the results of our participation in the challenge. We develop a simple, yet effective neural encoder-decoder model which produces fluent restaurant descriptions and outperforms a strong baseline. We further analyze the data provided by the organizers and conclude that the task can also be approached with a template-based model developed in just a few hours.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>puzikov-gurevych:2018:W18-65</bibkey>
  </paper>

  <paper id="6558">
    <title>The E2E NLG Challenge: A Tale of Two Systems</title>
    <author><first>Charese</first><last>Smiley</last></author>
    <author><first>Elnaz</first><last>Davoodi</last></author>
    <author><first>Dezhao</first><last>Song</last></author>
    <author><first>Frank</first><last>Schilder</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>472–477</pages>
    <url>http://www.aclweb.org/anthology/W18-6558</url>
    <abstract>This paper presents the two systems we entered into the 2017 E2E NLG Challenge: TemplGen, a templated-based system and SeqGen, a neural network-based system. Through the automatic evaluation, SeqGen achieved competitive results compared to the template-based approach and to other participating systems as well.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>smiley-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6559">
    <title>Interactive health insight miner: an adaptive, semantic-based approach</title>
    <author><first>Isabel</first><last>Funke</last></author>
    <author><first>Rim</first><last>Helaoui</last></author>
    <author><first>Aki</first><last>Harma</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>478–479</pages>
    <url>http://www.aclweb.org/anthology/W18-6559</url>
    <abstract>This paper describes an ontology-based system for interactive and adaptive mining of data-driven behavioral insights.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>funke-helaoui-harma:2018:W18-65</bibkey>
  </paper>

  <paper id="6560">
    <title>Multi-Language Surface Realisation as REST API based NLG Microservice</title>
    <author><first>Andreas</first><last>Madsack</last></author>
    <author><first>Johanna</first><last>Heininger</last></author>
    <author><first>Nyamsuren</first><last>Davaasambuu</last></author>
    <author><first>Vitaliia</first><last>Voronik</last></author>
    <author><first>Michael</first><last>Käufl</last></author>
    <author><first>Robert</first><last>Weißgraeber</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>480–481</pages>
    <url>http://www.aclweb.org/anthology/W18-6560</url>
    <abstract>We present a readily available NLG API that solves the morphology component for surface realizers in 10 languages (e.g., English, German and Finnish) for any topic and is available as REST API.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>madsack-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6561">
    <title>Statistical NLG for Generating the Content and Form of Referring Expressions</title>
    <author><first>Xiao</first><last>Li</last></author>
    <author><first>Kees</first><last>van Deemter</last></author>
    <author><first>Chenghua</first><last>Lin</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>482–491</pages>
    <url>http://www.aclweb.org/anthology/W18-6561</url>
    <abstract>This paper argues that a new generic approach to statistical NLG can be made to perform Referring Expression Generation (REG) successfully. The model does not only select attributes and values for referring to a target referent, but also performs Linguistic Realisation, generating an actual Noun Phrase. Our evaluations suggest that the attribute selection aspect of the algorithm exceeds classic REG algorithms, while the Noun Phrases generated are as similar to those in a previously developed corpus as were Noun Phrases produced by a new set of human speakers.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>li-vandeemter-lin:2018:W18-65</bibkey>
  </paper>

  <paper id="6562">
    <title>Specificity measures and reference</title>
    <author><first>Albert</first><last>Gatt</last></author>
    <author><first>Nicolás</first><last>Marín</last></author>
    <author><first>Gustavo</first><last>Rivas-Gervilla</last></author>
    <author><first>Daniél</first><last>Sánchez</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>492–502</pages>
    <url>http://www.aclweb.org/anthology/W18-6562</url>
    <abstract>In this paper we study empirically the validity of measures of referential success for referring expressions involving gradual properties. More specifically, we study the ability of several measures to predict the success of a user in choosing the right object, given a referring expression. Experimental results indicate that certain fuzzy measures of success are able to predict human accuracy in reference resolution. Such measures are therefore suitable for the estimation of the success or otherwise of a referring expression produced by a generation algorithm, especially in case the properties in a domain cannot be assumed to have crisp denotations.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>gatt-EtAl:2018:W18-65</bibkey>
  </paper>

  <paper id="6563">
    <title>Decoding Strategies for Neural Referring Expression Generation</title>
    <author><first>Sina</first><last>Zarrieß</last></author>
    <author><first>David</first><last>Schlangen</last></author>
    <booktitle>Proceedings of the 11th International Conference on Natural Language Generation</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg University, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>503–512</pages>
    <url>http://www.aclweb.org/anthology/W18-6563</url>
    <abstract>RNN-based sequence generation is now widely used in NLP and NLG (natural language generation). Most work focusses on how to train RNNs, even though also decoding is not necessarily straightforward: previous work on neural MT found seq2seq models to radically prefer short candidates, and has proposed a number of beam search heuristics to deal with this. In this work, we assess decoding strategies for referring expression generation with neural models. Here, expression length is crucial: output should neither contain too much or too little information, in order to be pragmatically adequate. We find that most beam search heuristics developed for MT do not generalize well to referring expression generation (REG), and do not generally outperform greedy decoding. We observe that beam search heuristics for termination seem to override the model’s knowledge of what a good stopping point is. Therefore, we also explore a recent approach called trainable decoding, which uses a small network to modify the RNN’s hidden state for better decoding results. We find this approach to consistently outperform greedy decoding for REG.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>zarrie-schlangen:2018:W18-65</bibkey>
  </paper>

    <paper id="6600">
        <title>Proceedings of the 3rd Workshop on Computational Creativity in Natural Language Generation (CC-NLG 2018)</title>
        <author><first>Hugo</first><last>Gonçalo Oliveira</last></author>
        <author><first>Ben</first><last>Burtenshaw</last></author>
        <author><first>Raquel</first><last>Hervás</last></author>
        <booktitle>Proceedings of the 3rd Workshop on Computational Creativity in Natural Language Generation (CC-NLG 2018)</booktitle>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-6600</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>inproceedings</bibtype>
    </paper>

    <paper id="6601">
        <title>A Brief Introduction to Natural Language Generation within Computational Creativity</title>
        <author><first>Ben</first><last>Burtenshaw</last></author>
        <booktitle>Proceedings of the 3rd Workshop on Computational Creativity in Natural Language Generation (CC-NLG 2018)</booktitle>
        <pages>2-4</pages>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-6601</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>inproceedings</bibtype>
    </paper>

    <paper id="6602">
        <title>Seeking the Ideal Narrative Model for Computer-Generated Narratives</title>
        <author><first>Mariana</first><last>Ferreira</last></author>
        <author><first>Hugo</first><last>Gonçalo Oliveira</last></author>
        <booktitle>Proceedings of the 3rd Workshop on Computational Creativity in Natural Language Generation (CC-NLG 2018)</booktitle>
        <pages>5-10</pages>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-6602</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>inproceedings</bibtype>
    </paper>

    <paper id="6603">
        <title>Discourse Embellishment Using a Deep Encoder-Decoder Network</title>
        <author><first>Leonid</first><last>Berov</last></author>
        <author><first>Kai</first><last>Standvoss</last></author>
        <booktitle>Proceedings of the 3rd Workshop on Computational Creativity in Natural Language Generation (CC-NLG 2018)</booktitle>
        <pages>11-16</pages>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-6603</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>inproceedings</bibtype>
    </paper>

    <paper id="6604">
        <title>Exploring Lexical-Semantic Knowledge in the Generation of Novel Riddles in Portuguese</title>
	<author><first>Hugo</first><last>Gonçalo Oliveira</last></author>
	<author><first>Ricardo</first><last>Rodrigues</last></author>
        <booktitle>Proceedings of the 3rd Workshop on Computational Creativity in Natural Language Generation (CC-NLG 2018)</booktitle>
        <pages>17-25</pages>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-6604</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>inproceedings</bibtype>
    </paper>

    <paper id="6605">
        <title>Content Determination for Chess as a Source for Suspenseful Narratives</title>
        <author><first>Richard</first><last>Doust</last></author>
        <author><first>Pablo</first><last>Gervás</last></author>
        <booktitle>Proceedings of the 3rd Workshop on Computational Creativity in Natural Language Generation (CC-NLG 2018)</booktitle>
        <pages>26-33</pages>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-6605</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>inproceedings</bibtype>
    </paper>
    
    <paper id="6606">
        <title>Generating Stories Using Role-playing Games and Simulated Human-like Conversations</title>
        <author><first>Alan</first><last>Tapscott</last></author>
        <author><first>Carlos</first><last>León</last></author>
        <author><first>Pablo</first><last>Gervás</last></author>
        <booktitle>Proceedings of the 3rd Workshop on Computational Creativity in Natural Language Generation (CC-NLG 2018)</booktitle>
        <pages>34-42</pages>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-6606</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>inproceedings</bibtype>
    </paper>

   <paper id="6700">
	      <editor><first>Jose M.</first><last>Alonso</last></editor>
        <editor><first>Alejandro</first><last>Catala</last></editor>
        <editor><first>Mariët</first><last>Theune</last></editor>
        <title>Proceedings of the Workshop on Intelligent Interactive Systems and Language Generation (2IS&amp;NLG) </title>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-67</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>book</bibtype>
		</paper>

   <paper id="6701">
        <title>Applications of NLG in practical conversational AI settings</title>
        <author><first>Sander</first><last>Wubben</last></author> 
				<booktitle>Proceedings of the Workshop on Intelligent Interactive Systems and Language Generation (2IS&amp;NLG) </booktitle>
        <pages>1-2</pages>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-6701</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>inproceedings</bibtype>
   </paper>

   <paper id="6702">	   
        <title>Generating Description for Sequential Images with Local-Object Attention Conditioned on Global Semantic Context</title>
        <author><first>Jing</first><last>Su</last></author>
        <author><first>Chenghua</first><last>Lin</last></author>
        <author><first>Mian</first><last>Zhou</last></author>
        <author><first>Qingyun</first><last>Dai</last></author>
	<author><first>Haoyu</first><last>Lv</last></author>
					<booktitle>Proceedings of the Workshop on Intelligent Interactive Systems and Language Generation (2IS&amp;NLG) </booktitle>
        <pages>3-8</pages>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-6702</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>inproceedings</bibtype>
   </paper>

   <paper id="6703">	   
        <title>Automation and Optimisation of Humor Trait Generation in a Vocal Dialogue System</title>
        <author><first>Matthieu</first><last>Riou</last></author>
        <author><first>Stéphane</first><last>Huet</last></author>
	    <author><first>Bassam</first><last>Jabaian</last></author>
	    <author><first>Fabrice</first><last>Lefèvre</last></author>
			<booktitle>Proceedings of the Workshop on Intelligent Interactive Systems and Language Generation (2IS&amp;NLG) </booktitle>
        <pages>9-14</pages>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-6703</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>inproceedings</bibtype>
   </paper>

   <paper id="6704">
        <title>Textual Entailment based Question Generation</title>
        <author><first>Takaaki</first><last>Matsumoto</last></author>
        <author><first>Kimihiro</first><last>Hasegawa</last></author>
        <author><first>Yukari</first><last>Yamakawa</last></author>
	    <author><first>Teruko</first><last>Mitamura</last></author>
				<booktitle>Proceedings of the Workshop on Intelligent Interactive Systems and Language Generation (2IS&amp;NLG) </booktitle>
        <pages>15-19</pages>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-6704</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>inproceedings</bibtype>
   </paper>

   <paper id="6705">
        <title>Trouble on the Road: Finding Reasons for Commuter Stress from Tweets</title>
        <author><first>Reshmi</first><last>Gopalakrishna Pillai</last></author>
        <author><first>Mike</first><last>Thelwall</last></author>
        <author><first>Constantin</first><last>Orasan</last></author>
				<booktitle>Proceedings of the Workshop on Intelligent Interactive Systems and Language Generation (2IS&amp;NLG) </booktitle>
        <pages>20-25</pages>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-6705</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>inproceedings</bibtype>
   </paper>

   <paper id="6706">
        <title>Assisted Nominalization for Academic English Writing</title>
        <author><first>John</first><last>Lee</last></author>
        <author><first>Dariush</first><last>Saberi</last></author>
        <author><first>Marvin</first><last>Lam</last></author>
	    <author><first>Jonathan</first><last>Webster</last></author>
				<booktitle>Proceedings of the Workshop on Intelligent Interactive Systems and Language Generation (2IS&amp;NLG) </booktitle>
        <pages>26-30</pages>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-6706</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>inproceedings</bibtype>
   </paper>

   <paper id="6707">
        <title>Two-Step Training and Mixed Encoding-Decoding for Implementing a Generative Chatbot with a Small Dialogue Corpus</title>
        <author><first>Jintae</first><last>Kim</last></author>
        <author><first>Hyeon-Gu</first><last>Lee</last></author>
        <author><first>Harksoo</first><last>Kim</last></author>
	    <author><first>Yeonsoo</first><last>Lee</last></author>
	    <author><first>Young-Gil</first><last>Kim</last></author>	
			<booktitle>Proceedings of the Workshop on Intelligent Interactive Systems and Language Generation (2IS&amp;NLG) </booktitle>
        <pages>31-35</pages>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-6707</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>inproceedings</bibtype>
   </paper>

   <paper id="6708">
        <title>Supporting Content Design with an Eye Tracker: The Case of Weather-based Recommendations</title>
        <author><first>Alejandro</first><last>Catala</last></author>
        <author><first>Jose M.</first><last>Alonso</last></author>
        <author><first>Alberto</first><last>Bugarin</last></author>
				<booktitle>Proceedings of the Workshop on Intelligent Interactive Systems and Language Generation (2IS&amp;NLG) </booktitle>
        <pages>36-41</pages>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-6708</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>inproceedings</bibtype>
   </paper>

   <paper id="6709">
        <title>ChatEval: A Tool for the Systematic Evaluation of Chatbots</title>
        <author><first>João</first><last>Sedoc</last></author>
        <author><first>Daphne</first><last>Ippolito</last></author>
        <author><first>Arun</first><last>Kirubarajan</last></author>
	   <author><first>Jai</first><last>Thirani</last></author>
	   <author><first>Lyle</first><last>Ungar</last></author>
	   <author><first>Chris</first><last>Callison-Burch</last></author>
		 <booktitle>Proceedings of the Workshop on Intelligent Interactive Systems and Language Generation (2IS&amp;NLG) </booktitle>
        <pages>42-44</pages>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-6709</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>inproceedings</bibtype>
   </paper>
   
   <paper id="6710">
        <title>CheckYourMeal!: diet management with NLG</title>
        <author><first>Luca</first><last>Anselma</last></author>
	   <author><first>Simone</first><last>Donetti</last></author>
	   <author><first>Alessandro</first><last>Mazzei</last></author>
	   <author><first>Andrea</first><last>Pirone</last></author>  
		<booktitle>Proceedings of the Workshop on Intelligent Interactive Systems and Language Generation (2IS&amp;NLG) </booktitle>
        <pages>45-47</pages>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-6710</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>inproceedings</bibtype>
	</paper>

   <paper id="6900">
    <title>Proceedings of the Workshop on NLG for Human–Robot Interaction</title>
    <editor><first>Mary Ellen</first><last>Foster</last></editor>
    <editor><first>Hendrik</first><last>Buschmeier</last></editor>
    <editor><first>Dimitra</first><last>Gkatzia</last></editor>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://aclweb.org/anthology/W18-69</url>
    <bibtype>book</bibtype>
    <bibkey>NLG-HRI:2018</bibkey>
   </paper>

  <paper id="6901">
    <title>Context-sensitive Natural Language Generation for robot-assisted second language tutoring</title>
    <author>
      <first>Bram</first>
      <last>Willemsen</last>
    </author>
    <author>
      <first>Jan</first>
      <last>de Wit</last>
    </author>
    <author>
      <first>Emiel</first>
      <last>Krahmer</last>
    </author>
    <author>
      <first>Mirjam</first>
      <last>de Haas</last>
    </author>
    <author>
      <first>Paul</first>
      <last>Vogt</last>
    </author>
    <booktitle>Proceedings of the Workshop on NLG for Human–Robot Interaction</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–7</pages>
    <url>http://aclweb.org/anthology/W18-6901</url>
    <abstract>
      This paper describes the L2TOR intelligent tutoring system (ITS), focusing primarily on its output generation module. The L2TOR ITS is developed for the purpose of investigating the efficacy of robot-assisted second language tutoring in early childhood. We explain the process of generating contextually-relevant utterances, such as task-specific feedback messages, and discuss challenges regarding multimodality and multilingualism for situated natural language generation from a robot tutoring perspective.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>willemsen-EtAl:2018:NLG-HRI</bibkey>
  </paper>

  <paper id="6902">
    <title>Learning from limited datasets: Implications for Natural Language Generation and Human-Robot Interaction</title>
    <author>
      <first>Jekaterina</first>
      <last>Belakova</last>
    </author>
    <author>
      <first>Dimitra</first>
      <last>Gkatzia</last>
    </author>
    <booktitle>Proceedings of the Workshop on NLG for Human–Robot Interaction</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>8–11</pages>
    <url>http://aclweb.org/anthology/W18-6902</url>
    <abstract>
      One of the most natural ways for human robot communication is through spoken language. Training human-robot interaction systems require access to large datasets which are expensive to obtain and labour intensive. In this paper, we describe an approach for learning from minimal data, using as a toy example language understanding in spoken dialogue systems. Understanding of spoken language is crucial because it has implications for natural language generation, i.e. correctly understanding a user’s utterance will lead to choosing the right response/action. Finally, we discuss implications for Natural Language Generation in Human-Robot Interaction.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>belakova-gkatzia:2018:NLG-HRI</bibkey>
  </paper>

  <paper id="6903">
    <title>Shaping a social robot’s humor with Natural Language Generation and socially-aware reinforcement learning</title>
    <author>
      <first>Hannes</first>
      <last>Ritschel</last>
    </author> 
    <author>
      <first>Elisabeth</first>
      <last>André</last>
    </author>
    <booktitle>Proceedings of the Workshop on NLG for Human–Robot Interaction</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>12–16</pages>
    <url>http://aclweb.org/anthology/W18-6903</url>
    <abstract>
      Humor is an important aspect in human interaction to regulate conversations, increase interpersonal attraction and trust. For social robots, humor is one aspect to make interactions more natural, enjoyable, and to increase credibility and acceptance. In combination with appropriate non-verbal behavior, natural language generation offers the ability to create content on-the-fly. This work outlines the building-blocks for providing an individual, multimodal interaction experience by shaping the robot’s humor with the help of Natural Language Generation and Reinforcement Learning based on human social signals.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ritschel-andre:2018:NLG-HRI</bibkey>
  </paper>

  <paper id="6904">
    <title>From sensors to sense: Integrated heterogeneous ontologies for Natural Language Generation</title>
    <author>
      <first>Mihai</first>
      <last>Pomarlan</last>
    </author>
    <author>
      <first>Robert</first>
      <last>Porzel</last>
    </author>
    <author>
      <first>John</first>
      <last>Bateman</last>
    </author>
    <author>
      <first>Rainer</first>
      <last>Malaka</last>
    </author>
    <booktitle>Proceedings of the Workshop on NLG for Human–Robot Interaction</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>17–21</pages>
    <url>http://aclweb.org/anthology/W18-6904</url>
    <abstract>
      We propose the combination of a robotics ontology (KnowRob) with a linguistically motivated one (GUM) under the upper ontology DUL. We use the DUL Event, Situation, Description pattern to formalize reasoning techniques to convert between a robot’s beliefstate and its linguistic utterances. We plan to employ these techniques to equip robots with a reason-aloud ability, through which they can explain their actions as they perform them, in natural language, at a level of granularity appropriate to the user, their query and the context at hand.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>pomarlan-EtAl:2018:NLG-HRI</bibkey>
  </paper>

  <paper id="6905">
    <title>A farewell to arms: Non-verbal communication for non-humanoid robots</title>
    <author>
      <first>Aaron G.</first>
      <last>Cass</last>
     </author>
    <author>
      <first>Kristina</first> 
      <last>Striegnitz</last>
    </author>
    <author>
      <first>Nick</first> 
      <last>Webb</last>
    </author>
    <booktitle>Proceedings of the Workshop on NLG for Human–Robot Interaction</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>22–26</pages>
    <url>http://aclweb.org/anthology/W18-6905</url>
    <abstract>
      Human-robot interactions situated in a dynamic environment create a unique mix of challenges for conversational systems. We argue that, on the one hand, NLG can contribute to addressing these challenges and that, on the other hand, they pose interesting research problems for NLG. To illustrate our position we describe our research on non-humanoid robots using non-verbal signals to support communication.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>cass-striegnitz-webb:2018:NLG-HRI</bibkey>
  </paper>

  <paper id="6906">
    <title>Being data-driven is not enough: Revisiting interactive instruction giving as a challenge for NLG</title>
    <author>
      <first>Sina</first>
      <last>Zarrieß</last>
    </author>
    <author>
      <first>David</first>
      <last>Schlangen</last>
    </author>
    <booktitle>Proceedings of the Workshop on NLG for Human–Robot Interaction</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Tilburg, The Netherlands</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>27–31</pages>
    <url>http://aclweb.org/anthology/W18-6906</url>
    <abstract>
      Modeling traditional NLG tasks with data-driven techniques has been a major focus of research in NLG in the past decade. We argue that existing modeling techniques are mostly tailored to textual data and are not sufficient to make NLG technology meet the requirements of agents which target fluid interaction and collaboration in the real world. We revisit interactive instruction giving as a challenge for datadriven NLG and, based on insights from previous GIVE challenges, propose that instruction giving should be addressed in a setting that involves visual grounding and spoken language. These basic design decisions will require NLG frameworks that are capable of monitoring their environment as well as timing and revising their verbal output. We believe that these are core capabilities for making NLG technology transferrable to interactive systems.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>zarriess-schlangen:2018:NLG-HRI</bibkey>
  </paper>

   <paper id="7000">
	      <editor><first>Arne</first><last>Jönsson</last></editor>
        <editor><first>Evelina</first><last>Rennes</last></editor>
        <editor><first>Horacio</first><last>Saggion</last></editor>
        <editor><first>Sanja</first><last>Stajner</last></editor>
				<editor><first>Victoria</first><last>Yaneva</last></editor>
				<title>Proceedings of the 1st Workshop on Automatic Text Adaptation (ATA)</title>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-70</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>book</bibtype>
		</paper>

   <paper id="7001">
        <title>The Interface Between Readability and Automatic Text Simplification</title>
        <author><first>Thomas</first><last>François</last></author> 
				<booktitle>Proceedings of the 1st Workshop on Automatic Text Adaptation (ATA)</booktitle>
        <pages>1-2</pages>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-7001</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>inproceedings</bibtype>
   </paper>

   <paper id="7002">	   
        <title>CLEAR – Simple Corpus for Medical French</title>
        <author><first>Natalia</first><last>Grabar</last></author>
        <author><first>Rémi</first><last>Cardon</last></author>
					<booktitle>Proceedings of the 1st Workshop on Automatic Text Adaptation (ATA)</booktitle>
        <pages>3-9</pages>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-7002</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>inproceedings</bibtype>
   </paper>

   <paper id="7003">	   
        <title>Study of Readability of Health Documents with Eye-tracking Approaches</title>
				<author><first>Natalia</first><last>Grabar</last></author>
        <author><first>Emmanuel</first><last>Farce</last></author>
        <author><first>Laurent</first><last>Sparrow</last></author>
			<booktitle>Proceedings of the 1st Workshop on Automatic Text Adaptation (ATA)</booktitle>
        <pages>10-20</pages>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-7003</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>inproceedings</bibtype>
   </paper>
	
<paper id="7004">
        <title>Assisted Lexical Simplification for French Native Children with Reading Difficulties</title>
        <author><first>Firas</first><last>Hmida</last></author>
        <author><first>Mokhtar B.</first><last>Billami</last></author>
        <author><first>Thomas</first><last>François</last></author> 
	    <author><first>Núria</first><last>Gala</last></author>
				<booktitle>Proceedings of the 1st Workshop on Automatic Text Adaptation (ATA)</booktitle>
        <pages>21-28</pages>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-7004</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>inproceedings</bibtype>
   </paper>

   <paper id="7005">
				<title>Reference-less Quality Estimation of Text Simplification Systems</title>
        <author><first>Louis</first><last>Martin</last></author>
        <author><first>Samuel</first><last>Humeau</last></author>
				<author><first>Pierre-Emmanuel</first><last>Mazaré</last></author>
        <author><first>Éric</first><last>de La Clergerie</last></author>
				<author><first>Antoine</first><last>Bordes</last></author>
				<author><first>Benoît</first><last>Sagot</last></author>
				<booktitle>Proceedings of the 1st Workshop on Automatic Text Adaptation (ATA)</booktitle>
        <pages>29-38</pages>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-7005</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>inproceedings</bibtype>
   </paper>

   <paper id="7006">
        <title>Improving Machine Translation of English Relative Clauses with Automatic Text Simplification</title>
        <author><first>Sanja</first><last>Štajner</last></author>
        <author><first>Maja</first><last>Popović</last></author>
				<booktitle>Proceedings of the 1st Workshop on Automatic Text Adaptation (ATA)</booktitle>
        <pages>39-48</pages>
        <year>2018</year>
        <month>November</month>
        <publisher>Association for Computational Linguistics</publisher>
        <url>http://www.aclweb.org/anthology/W18-7006</url>
        <address>Tilburg, the Netherlands</address>
        <bibtype>inproceedings</bibtype>
   </paper>

  <paper id="7100">
        <title>Proceedings of the 7th workshop on NLP for Computer Assisted Language Learning</title>
		<editor><first>Ildikó</first><last>Pilán</last></editor>
        <editor><first>Elena</first><last>Volodina</last></editor>
		<editor><first>David</first><last>Alfter</last></editor>
        <editor><first>Lars</first><last>Borin</last></editor>
        <month>November</month>
        <year>2018</year>
        <address>Stockholm, Sweden</address>
        <publisher>LiU Electronic Press</publisher>
        <url>http://www.aclweb.org/anthology/W18-7100</url>
        <bibtype>book</bibtype>
        <bibkey>W18-71:2018</bibkey>
  </paper>
  <paper id="7101">
    <title>Using authentic texts for grammar exercises for a minority language</title>
    <author>
      <first>Lene</first>
      <last>Antonsen</last>
    </author>
    <author>
      <first>Chiara</first>
      <last>Argese</last>
    </author>
    <booktitle>Proceedings of the 7th workshop on NLP for Computer Assisted Language Learning</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Stockholm, Sweden</address>
    <publisher>LiU Electronic Press</publisher>
    <pages>1-9</pages>
    <url>http://www.aclweb.org/anthology/W18-7101</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>antonsen:2018:W18-71-01</bibkey>
  </paper>
  <paper id="7102">
    <title>Normalization in Context: Inter-Annotator Agreement for Meaning-Based Target Hypothesis Annotation</title>
    <author>
      <first>Adriane</first>
      <last>Boyd</last>
    </author>
    <booktitle>Proceedings of the 7th workshop on NLP for Computer Assisted Language Learning</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Stockholm, Sweden</address>
    <publisher>LiU Electronic Press</publisher>
    <pages>10-22</pages>
    <url>http://www.aclweb.org/anthology/W18-7102</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>boyd:2018:W18-71-02</bibkey>
  </paper>
  <paper id="7103">
    <title>The Role of Diacritics in Increasing the Difficulty of Arabic Lexical Recognition Tests</title>
    <author>
      <first>Osama</first>
      <last>Hamed</last>
    </author>
    <author>
      <first>Torsten</first>
      <last>Zesch</last>
    </author>
    <booktitle>Proceedings of the 7th workshop on NLP for Computer Assisted Language Learning</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Stockholm, Sweden</address>
    <publisher>LiU Electronic Press</publisher>
    <pages>23-31</pages>
    <url>http://www.aclweb.org/anthology/W18-7103</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>hamed:2018:W18-71-03</bibkey>
  </paper>
  <paper id="7104">
    <title>An Automatic Error Tagger for German</title>
    <author>
      <first>Inga</first>
      <last>Kempfert</last>
    </author>
    <author>
      <first>Christine</first>
      <last>Köhn</last>
    </author>
    <booktitle>Proceedings of the 7th workshop on NLP for Computer Assisted Language Learning</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Stockholm, Sweden</address>
    <publisher>LiU Electronic Press</publisher>
    <pages>32-40</pages>
    <url>http://www.aclweb.org/anthology/W18-7104</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>kempfert:2018:W18-71-04</bibkey>
  </paper>
  <paper id="7105">
    <title>Demonstrating the MUSTE Language Learning Environment</title>
    <author>
      <first>Herbert</first>
      <last>Lange</last>
    </author>
    <author>
      <first>Peter</first>
      <last>Ljunglöf</last>
    </author>
    <booktitle>Proceedings of the 7th workshop on NLP for Computer Assisted Language Learning</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Stockholm, Sweden</address>
    <publisher>LiU Electronic Press</publisher>
    <pages>41-46</pages>
    <url>http://www.aclweb.org/anthology/W18-7105</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>lange:2018:W18-71-05</bibkey>
  </paper>
  <paper id="7106">
    <title>Learner Corpus Anonymization in the Age of GDPR: Insights from the Creation of a Learner Corpus of Swedish</title>
    <author>
      <first>Beáta</first>
      <last>Megyesi</last>
    </author>
    <author>
      <first>Lena</first>
      <last>Granstedt</last>
    </author>
    <author>
      <first>Sofia</first>
      <last>Johansson</last>
    </author>
    <author>
      <first>Julia</first>
      <last>Prentice</last>
    </author>
    <author>
      <first>Dan</first>
      <last>Rosén</last>
    </author>
    <author>
      <first>Carl-Johan</first>
      <last>Schenström</last>
    </author>
    <author>
      <first>Gunlög</first>
      <last>Sundberg</last>
    </author>
    <author>
      <first>Mats</first>
      <last>Wirén</last>
    </author>
    <author>
      <first>Elena</first>
      <last>Volodina</last>
    </author>
    <booktitle>Proceedings of the 7th workshop on NLP for Computer Assisted Language Learning</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Stockholm, Sweden</address>
    <publisher>LiU Electronic Press</publisher>
    <pages>47-56</pages>
    <url>http://www.aclweb.org/anthology/W18-7106</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>megyesi:2018:W18-71-06</bibkey>
  </paper>
  <paper id="7107">
    <title>Work Smart - Reducing Effort in Short-Answer Grading</title>
    <author>
      <first>Margot</first>
      <last>Mieskes</last>
    </author>
    <author>
      <first>Ulrike</first>
      <last>Pado</last>
    </author>
    <booktitle>Proceedings of the 7th workshop on NLP for Computer Assisted Language Learning</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Stockholm, Sweden</address>
    <publisher>LiU Electronic Press</publisher>
    <pages>57-68</pages>
    <url>http://www.aclweb.org/anthology/W18-7107</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>mieskes:2018:W18-71-07</bibkey>
  </paper>
  <paper id="7108">
    <title>NLP Corpus Observatory – Looking for Constellations in Parallel Corpora to Improve Learners’ Collocational Skills</title>
    <author>
      <first>Gerold</first>
      <last>Schneider</last>
    </author>
    <author>
      <first>Johannes</first>
      <last>Graën</last>
    </author>
    <booktitle>Proceedings of the 7th workshop on NLP for Computer Assisted Language Learning</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Stockholm, Sweden</address>
    <publisher>LiU Electronic Press</publisher>
    <pages>69-78</pages>
    <url>http://www.aclweb.org/anthology/W18-7108</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>schneider:2018:W18-71-08</bibkey>
  </paper>
  <paper id="7109">
    <title>A Linguistically-Informed Search Engine to Identifiy Reading Material for Functional Illiteracy Classes</title>
    <author>
      <first>Zarah</first>
      <last>Weiss</last>
    </author>
    <author>
      <first>Sabrina</first>
      <last>Dittrich</last>
    </author>
    <author>
      <first>Detmar</first>
      <last>Meurers</last>
    </author>
    <booktitle>Proceedings of the 7th workshop on NLP for Computer Assisted Language Learning</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Stockholm, Sweden</address>
    <publisher>LiU Electronic Press</publisher>
    <pages>79-90</pages>
    <url>http://www.aclweb.org/anthology/W18-7109</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>weiss:2018:W18-71-09</bibkey>
  </paper>
  <paper id="7110">
    <title>Feedback Strategies for Form and Meaning in a Real-life Language Tutoring System</title>
    <author>
      <first>Ramon</first>
      <last>Ziai</last>
    </author>
    <author>
      <first>Bjoern</first>
      <last>Rudzewitz</last>
    </author>
    <author>
      <first>Kordula</first>
      <last>De Kuthy</last>
    </author>
    <author>
      <first>Florian</first>
      <last>Nuxoll</last>
    </author>
    <author>
      <first>Detmar</first>
      <last>Meurers</last>
    </author>
    <booktitle>Proceedings of the 7th workshop on NLP for Computer Assisted Language Learning</booktitle>
    <month>November</month>
    <year>2018</year>
    <address>Stockholm, Sweden</address>
    <publisher>LiU Electronic Press</publisher>
    <pages>91-98</pages>
    <url>http://www.aclweb.org/anthology/W18-7110</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>ziai:2018:W18-71-10</bibkey>
  </paper>
</volume>